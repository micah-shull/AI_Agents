{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUzDCB9h4luq4YdBPxofi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/295_HITL_AuditLogging_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audit logging utilities for HITL Orchestrator"
      ],
      "metadata": {
        "id": "teqcuOzrFQF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0z1nj6oFNwC"
      },
      "outputs": [],
      "source": [
        "\"\"\"Audit logging utilities for HITL Orchestrator\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def create_audit_log(\n",
        "    task_id: str,\n",
        "    risk_level: str,\n",
        "    confidence_score: float,\n",
        "    routing_decision: str,\n",
        "    human_involved: bool,\n",
        "    final_decision: str,\n",
        "    decision_source: str,\n",
        "    latency_seconds: float,\n",
        "    timestamp: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create an audit log entry.\n",
        "\n",
        "    Args:\n",
        "        task_id: Task identifier\n",
        "        risk_level: Task risk level\n",
        "        confidence_score: Agent confidence score\n",
        "        routing_decision: Routing decision made\n",
        "        human_involved: Whether human was involved\n",
        "        final_decision: Final decision outcome\n",
        "        decision_source: \"agent\" or \"human\"\n",
        "        latency_seconds: Time taken in seconds\n",
        "        timestamp: ISO timestamp (defaults to now)\n",
        "\n",
        "    Returns:\n",
        "        Audit log dictionary\n",
        "    \"\"\"\n",
        "    if timestamp is None:\n",
        "        timestamp = datetime.now().isoformat()\n",
        "\n",
        "    log_id = f\"log_{task_id.split('_')[1]}\"  # e.g., \"log_001\" from \"task_001\"\n",
        "\n",
        "    return {\n",
        "        \"log_id\": log_id,\n",
        "        \"task_id\": task_id,\n",
        "        \"risk_level\": risk_level,\n",
        "        \"confidence_score\": confidence_score,\n",
        "        \"routing_decision\": routing_decision,\n",
        "        \"human_involved\": human_involved,\n",
        "        \"final_decision\": final_decision,\n",
        "        \"decision_source\": decision_source,\n",
        "        \"latency_seconds\": latency_seconds,\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_summary_metrics(\n",
        "    routing_decisions: List[Dict[str, Any]],\n",
        "    final_decisions: List[Dict[str, Any]],\n",
        "    audit_logs: List[Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate summary metrics from routing decisions and audit logs.\n",
        "\n",
        "    Args:\n",
        "        routing_decisions: List of routing decisions\n",
        "        final_decisions: List of final decisions\n",
        "        audit_logs: List of audit logs\n",
        "\n",
        "    Returns:\n",
        "        Summary metrics dictionary\n",
        "    \"\"\"\n",
        "    total_tasks = len(routing_decisions)\n",
        "\n",
        "    # Count routing decisions\n",
        "    auto_approved_count = sum(\n",
        "        1 for d in routing_decisions\n",
        "        if d.get(\"routing_decision\") == \"auto_approve\"\n",
        "    )\n",
        "    human_reviewed_count = sum(\n",
        "        1 for d in routing_decisions\n",
        "        if d.get(\"routing_decision\") == \"human_review\"\n",
        "    )\n",
        "    escalated_count = sum(\n",
        "        1 for d in routing_decisions\n",
        "        if d.get(\"routing_decision\") == \"escalate\"\n",
        "    )\n",
        "\n",
        "    # Calculate average confidence\n",
        "    confidence_scores = [d.get(\"confidence_score\", 0.0) for d in routing_decisions]\n",
        "    average_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0\n",
        "\n",
        "    # Calculate average latency\n",
        "    latencies = [log.get(\"latency_seconds\", 0.0) for log in audit_logs]\n",
        "    average_latency = sum(latencies) / len(latencies) if latencies else 0.0\n",
        "\n",
        "    # Count human overrides\n",
        "    human_override_count = sum(\n",
        "        1 for d in final_decisions\n",
        "        if d.get(\"decision_source\") == \"human\" and d.get(\"final_decision\") in [\"override_approved\", \"modified_and_approved\"]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"total_tasks\": total_tasks,\n",
        "        \"auto_approved_count\": auto_approved_count,\n",
        "        \"human_reviewed_count\": human_reviewed_count,\n",
        "        \"escalated_count\": escalated_count,\n",
        "        \"average_confidence_score\": round(average_confidence, 2),\n",
        "        \"average_latency_seconds\": round(average_latency, 2),\n",
        "        \"human_override_count\": human_override_count\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üß† Big Picture: Why Audit Logging Exists\n",
        "\n",
        "This code answers two questions that **humans care deeply about**:\n",
        "\n",
        "1. **‚ÄúWhat happened?‚Äù**\n",
        "2. **‚ÄúHow is the system performing overall?‚Äù**\n",
        "\n",
        "AI systems don‚Äôt fail because they‚Äôre inaccurate.\n",
        "They fail because **no one can explain them afterward**.\n",
        "\n",
        "This module makes your agent **accountable**.\n",
        "\n",
        "---\n",
        "\n",
        "# Part 1: `create_audit_log`\n",
        "\n",
        "## üßæ ‚ÄúWrite it down so we can‚Äôt lie later‚Äù\n",
        "\n",
        "```python\n",
        "def create_audit_log(...)\n",
        "```\n",
        "\n",
        "### What this function does (in plain English)\n",
        "\n",
        "Every time a task finishes, this function creates a **receipt**.\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "* a transaction record\n",
        "* a bank statement\n",
        "* a flight black box entry\n",
        "\n",
        "It captures:\n",
        "\n",
        "* what the task was\n",
        "* how risky it was\n",
        "* who decided\n",
        "* what the outcome was\n",
        "* how long it took\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è± Timestamp logic\n",
        "\n",
        "```python\n",
        "if timestamp is None:\n",
        "    timestamp = datetime.now().isoformat()\n",
        "```\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "> If no one tells us when this happened, record ‚Äúright now‚Äù.\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* every decision is time-stamped\n",
        "* events can be reconstructed later\n",
        "\n",
        "Time is crucial for trust.\n",
        "\n",
        "---\n",
        "\n",
        "## üÜî Log ID generation\n",
        "\n",
        "```python\n",
        "log_id = f\"log_{task_id.split('_')[1]}\"\n",
        "```\n",
        "\n",
        "Plain English:\n",
        "\n",
        "* Take `task_001`\n",
        "* Turn it into `log_001`\n",
        "\n",
        "This keeps:\n",
        "\n",
        "* logs readable\n",
        "* easy to trace\n",
        "* consistent with your datasets\n",
        "\n",
        "It‚Äôs about **human legibility**, not cleverness.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ What gets returned\n",
        "\n",
        "The returned dictionary is a **complete, frozen record**.\n",
        "\n",
        "Once written:\n",
        "\n",
        "* it should never change\n",
        "* it represents the official truth\n",
        "\n",
        "This is the *single source of truth* for audits.\n",
        "\n",
        "---\n",
        "\n",
        "# Part 2: `calculate_summary_metrics`\n",
        "\n",
        "## üìä ‚ÄúZoom out and see the system‚Äù\n",
        "\n",
        "```python\n",
        "def calculate_summary_metrics(...)\n",
        "```\n",
        "\n",
        "This function stops thinking about individual tasks and asks:\n",
        "\n",
        "> **‚ÄúHow is the system behaving overall?‚Äù**\n",
        "\n",
        "Executives, managers, and regulators don‚Äôt read logs ‚Äî\n",
        "they read **summaries**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Total tasks\n",
        "\n",
        "```python\n",
        "total_tasks = len(routing_decisions)\n",
        "```\n",
        "\n",
        "This answers:\n",
        "\n",
        "> ‚ÄúHow much work did the system process?‚Äù\n",
        "\n",
        "Simple, but essential.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Counting routing outcomes\n",
        "\n",
        "```python\n",
        "auto_approved_count\n",
        "human_reviewed_count\n",
        "escalated_count\n",
        "```\n",
        "\n",
        "These numbers answer **strategic questions**:\n",
        "\n",
        "* Are we automating too aggressively?\n",
        "* Are humans overloaded?\n",
        "* Are we flagging too many things as high risk?\n",
        "\n",
        "This is how **automation maturity** is measured.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Average confidence score\n",
        "\n",
        "```python\n",
        "average_confidence\n",
        "```\n",
        "\n",
        "This tells you:\n",
        "\n",
        "* how confident the AI *thinks* it is\n",
        "* whether confidence is trending up or down\n",
        "\n",
        "Later, you‚Äôll compare this to **human overrides**.\n",
        "\n",
        "That‚Äôs where learning happens.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è≥ Average latency\n",
        "\n",
        "```python\n",
        "average_latency\n",
        "```\n",
        "\n",
        "This measures:\n",
        "\n",
        "* system speed\n",
        "* human bottlenecks\n",
        "* operational friction\n",
        "\n",
        "High latency ‚â† bad AI\n",
        "High latency = process problem\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Human override count\n",
        "\n",
        "```python\n",
        "human_override_count\n",
        "```\n",
        "\n",
        "This is the **most important metric** in the entire system.\n",
        "\n",
        "It answers:\n",
        "\n",
        "> ‚ÄúHow often does a human say ‚Äòno‚Äô?‚Äù\n",
        "\n",
        "High overrides mean:\n",
        "\n",
        "* confidence is miscalibrated\n",
        "* rules are too permissive\n",
        "* risk thresholds are wrong\n",
        "\n",
        "This is your **early warning system**.\n",
        "\n",
        "---\n",
        "\n",
        "# üéØ Big Takeaway (Most Important)\n",
        "\n",
        "This module exists to enforce a core rule:\n",
        "\n",
        "> **If an AI decision cannot be audited, it should not be trusted.**\n",
        "\n",
        "You are designing for:\n",
        "\n",
        "* transparency\n",
        "* accountability\n",
        "* long-term adoption\n",
        "\n",
        "Not just correctness.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ How this fits into the whole agent\n",
        "\n",
        "1. Routing decides *who should act*\n",
        "2. Humans (sometimes) act\n",
        "3. Final decisions are made\n",
        "4. **Audit logs record the truth**\n",
        "5. Metrics summarize behavior\n",
        "\n",
        "This is how AI becomes *organizationally acceptable*.\n",
        "\n"
      ],
      "metadata": {
        "id": "_CaGC7UhxcTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5opwc3Xgxe75"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}