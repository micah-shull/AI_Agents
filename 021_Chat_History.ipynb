{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1+BnVUFUt+g752Ezpg+in",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/021_Chat_History.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ✅ **Key Concepts**\n",
        "\n",
        "### 1️⃣ *Programmatic Prompting*\n",
        "\n",
        "* How to call an LLM programmatically (basic function)\n",
        "* Difference between `system` and `user` messages\n",
        "\n",
        "### 2️⃣ *No Inherent Memory*\n",
        "\n",
        "* Show how the LLM forgets if you don’t include prior context\n",
        "* Show how to maintain memory manually with messages\n",
        "\n",
        "### 3️⃣ *Memory Management for Agents*\n",
        "\n",
        "* Basic example of building a simple “Agent Loop”\n",
        "* Highlight the idea that **memory is your responsibility**\n",
        "\n",
        "### 4️⃣ *GAIL Framework*\n",
        "\n",
        "* Clearly outline **GAIL**:\n",
        "\n",
        "  * **Goals:** What does the agent want to achieve?\n",
        "  * **Actions:** What actions can it take (APIs, functions, tools)?\n",
        "  * **Information:** What does it know or need to know?\n",
        "  * **Language:** How does it communicate its reasoning and results?\n",
        "* Include a mini exercise: design a “GAIL” for a tiny agent (like a to-do list manager).\n"
      ],
      "metadata": {
        "id": "QyH9iSsNSDLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU dotenv openai pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzQhaH49ShKR",
        "outputId": "69f70293-be8d-4e2c-8f5c-071d9eb22406"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/757.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.4/757.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.5/757.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Management"
      ],
      "metadata": {
        "id": "FDEidNueT0q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📚 Notebook 1: Stateless vs. Stateful Prompting\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load your API key securely\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "\n",
        "# ✅ Use the official OpenAI Python SDK\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# ================================================\n",
        "# 1️⃣ Define a reusable function for LLM calls\n",
        "# ================================================\n",
        "\n",
        "def generate_response(messages):\n",
        "    \"\"\"Call the LLM and return its response.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",   # ✅ Cheap and fast!\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "QbPvB34ITyaZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stateless - No Memory"
      ],
      "metadata": {
        "id": "I-sE-KclUGSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 2️⃣ Stateless Example: No memory\n",
        "# ================================================\n",
        "\n",
        "# 🔹 First prompt: ask for a simple function\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"✅ First response:\\n\", response)\n",
        "\n",
        "# 🔹 Now, try to refine it — but WITHOUT giving the previous code!\n",
        "# The model will not remember its own output.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Add type hints and docstrings to the function from before.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"\\n🚫 Second response (stateless):\\n\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qm8wtaYUFHy",
        "outputId": "ab0a5a18-ade1-4aef-c6e5-405fdedec906"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ First response:\n",
            " You can create a function that swaps keys and values in a dictionary using a simple dictionary comprehension. Here’s how you can do it:\n",
            "\n",
            "```python\n",
            "def swap_keys_and_values(original_dict):\n",
            "    # Ensure the values are unique to be valid as keys in the swapped dictionary\n",
            "    if len(original_dict) != len(set(original_dict.values())):\n",
            "        raise ValueError(\"Cannot swap keys and values: duplicate values found.\")\n",
            "\n",
            "    swapped_dict = {value: key for key, value in original_dict.items()}\n",
            "    return swapped_dict\n",
            "\n",
            "# Example usage:\n",
            "original = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped = swap_keys_and_values(original)\n",
            "print(swapped)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "1. **Function Definition**: The function `swap_keys_and_values` takes an `original_dict` as input.\n",
            "2. **Duplicate Check**: Before swapping, it checks if all the values in the original dictionary are unique. If there are duplicate values, it raises a `ValueError`.\n",
            "3. **Swapping**: It uses a dictionary comprehension to create a new dictionary with keys and values swapped.\n",
            "4. **Return Value**: The swapped dictionary is returned.\n",
            "\n",
            "### Note:\n",
            "- This function assumes that all the values in the original dictionary are hashable and unique. If the values are not unique, there will be a loss of information upon swapping, as dictionaries cannot have duplicate keys.\n",
            "\n",
            "🚫 Second response (stateless):\n",
            " Sure! However, I don't have access to the previous context or function you've mentioned, so I'll provide a general approach to adding type hints and docstrings to a Python function. If you provide the specific function, I can tailor my response directly to it.\n",
            "\n",
            "Here's an example function and how to apply type hints and docstrings:\n",
            "\n",
            "```python\n",
            "def add_numbers(a, b):\n",
            "    return a + b\n",
            "```\n",
            "\n",
            "Here's the same function with type hints and a docstring added:\n",
            "\n",
            "```python\n",
            "def add_numbers(a: int, b: int) -> int:\n",
            "    \"\"\"\n",
            "    Add two integers and return the result.\n",
            "\n",
            "    Parameters:\n",
            "    a (int): The first integer to add.\n",
            "    b (int): The second integer to add.\n",
            "\n",
            "    Returns:\n",
            "    int: The sum of a and b.\n",
            "    \"\"\"\n",
            "    return a + b\n",
            "```\n",
            "\n",
            "In this example:\n",
            "- Type hints indicate that both `a` and `b` should be integers, and the function will also return an integer.\n",
            "- The docstring explains what the function does, outlines the parameters and their types, and specifies the return type.\n",
            "\n",
            "Please provide the specific function you need help with, and I can assist you with that!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Statefull - with Memory"
      ],
      "metadata": {
        "id": "1BiNoI6aUMlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 3️⃣ Stateful Example: Manual memory\n",
        "# ================================================\n",
        "\n",
        "# 🔹 Keep the first output in our \"working memory\"\n",
        "# This gives the LLM context to build on its own work.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "# Generate the initial function\n",
        "initial_response = generate_response(messages)\n",
        "print(\"\\n✅ First response (again):\\n\", initial_response)\n",
        "\n",
        "# 🔹 Now, reuse the initial response as \"assistant\" memory\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dictionary.\"},\n",
        "    {\"role\": \"assistant\", \"content\": initial_response},\n",
        "    {\"role\": \"user\", \"content\": \"Add type hints and docstrings to the function from before.\"}\n",
        "]\n",
        "\n",
        "# Generate the refined version\n",
        "response_with_memory = generate_response(messages)\n",
        "print(\"\\n✅ Second response (with memory):\\n\", response_with_memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW4Mu0dvULpw",
        "outputId": "1329062e-48c5-4f42-fbf9-0f0a3fa491bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ First response (again):\n",
            " Certainly! Below is a Python function that swaps the keys and values in a given dictionary. This function creates a new dictionary where the keys become the values and the values become the keys. Note that this will only work correctly if the values in the original dictionary are unique and hashable since they will become the new keys.\n",
            "\n",
            "Here's how you can implement this:\n",
            "\n",
            "```python\n",
            "def swap_dict(d):\n",
            "    \"\"\"Swaps keys and values in a dictionary.\n",
            "\n",
            "    Args:\n",
            "        d (dict): The dictionary to swap.\n",
            "\n",
            "    Returns:\n",
            "        dict: A new dictionary with keys and values swapped.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If the input dictionary has non-unique values.\n",
            "\n",
            "    Example:\n",
            "        >>> swap_dict({'a': 1, 'b': 2, 'c': 3})\n",
            "        {1: 'a', 2: 'b', 3: 'c'}\n",
            "    \"\"\"\n",
            "    if len(d) != len(set(d.values())):\n",
            "        raise ValueError(\"Cannot swap keys and values because the values are not unique.\")\n",
            "\n",
            "    return {value: key for key, value in d.items()}\n",
            "\n",
            "# Example usage:\n",
            "original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped_dict = swap_dict(original_dict)\n",
            "print(swapped_dict)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "- The function `swap_dict` takes a single argument, `d`, which is the dictionary you want to swap.\n",
            "- It checks if the number of unique values is the same as the length of the original dictionary using `len(d) != len(set(d.values()))`. If they are not the same, it raises a `ValueError`.\n",
            "- It then creates a new dictionary using a dictionary comprehension where it iterates over the items in the original dictionary and assigns values to keys and keys to values.\n",
            "- The function returns the new swapped dictionary. \n",
            "\n",
            "You can test the function with different dictionaries to see how it works!\n",
            "\n",
            "✅ Second response (with memory):\n",
            " Certainly! Here’s the `swap_dict` function with added type hints and a more detailed docstring:\n",
            "\n",
            "```python\n",
            "from typing import Dict\n",
            "\n",
            "def swap_dict(d: Dict) -> Dict:\n",
            "    \"\"\"Swaps keys and values in a dictionary.\n",
            "\n",
            "    This function takes a dictionary `d` and returns a new dictionary \n",
            "    where the keys and values are swapped. It requires that the values \n",
            "    in the original dictionary are unique and hashable.\n",
            "\n",
            "    Args:\n",
            "        d (Dict): The dictionary with keys and values to swap. \n",
            "                  Keys can be of any immutable type, \n",
            "                  and values should be unique and hashable.\n",
            "\n",
            "    Returns:\n",
            "        Dict: A new dictionary with keys and values swapped.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If the input dictionary has non-unique values.\n",
            "\n",
            "    Example:\n",
            "        >>> swap_dict({'a': 1, 'b': 2, 'c': 3})\n",
            "        {1: 'a', 2: 'b', 3: 'c'}\n",
            "\n",
            "        >>> swap_dict({'name': 'Alice', 'age': 30})\n",
            "        {'Alice': 'name', 30: 'age'}\n",
            "\n",
            "    Notes:\n",
            "        - If the input dictionary has values that are not unique, \n",
            "          a ValueError will be raised to prevent overwriting.\n",
            "    \"\"\"\n",
            "    if len(d) != len(set(d.values())):\n",
            "        raise ValueError(\"Cannot swap keys and values because the values are not unique.\")\n",
            "\n",
            "    return {value: key for key, value in d.items()}\n",
            "\n",
            "# Example usage:\n",
            "original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped_dict = swap_dict(original_dict)\n",
            "print(swapped_dict)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Changes Made:\n",
            "- Added type hints to the function signature indicating that `d` is a `Dict` and the return type is also a `Dict`.\n",
            "- Enhanced the docstring to provide a clear explanation of parameters, return values, raised exceptions, examples, and additional notes regarding the uniqueness constraint of values in the input dictionary.\n",
            "\n",
            "This should provide clear guidance and documentation for anyone using or reading the function in the future!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely — let’s unpack this piece step by step!\n",
        "Your intuition is spot on: this block shows how **you create “memory”** by explicitly **feeding the model its own prior output**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **How “manual memory” works**\n",
        "\n",
        "### ✅ **Key principle**\n",
        "\n",
        "LLMs are **stateless** — they only see what you give them in the `messages` list, every time.\n",
        "So you create “memory” by:\n",
        "\n",
        "1. **Saving the assistant’s past responses**\n",
        "2. Adding them back into the conversation next time\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 **Your example, line by line**\n",
        "\n",
        "```python\n",
        "# This list simulates an ongoing conversation with full context:\n",
        "messages = [\n",
        "    # System role: sets the model’s behavior\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"},\n",
        "\n",
        "    # User role: first request\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dictionary.\"},\n",
        "\n",
        "    # Assistant role: the LLM’s previous answer\n",
        "    {\"role\": \"assistant\", \"content\": initial_response},\n",
        "\n",
        "    # User role: follow-up request that depends on previous output\n",
        "    {\"role\": \"user\", \"content\": \"Add type hints and docstrings to the function from before.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **Why does this work?**\n",
        "\n",
        "When you send this whole `messages` list to the LLM:\n",
        "\n",
        "* The model **sees the entire conversation history** in one go.\n",
        "* It knows exactly **what it said previously** (the code it generated).\n",
        "* So when you say *“Add type hints and docstrings to the function from before”* — “the function from before” is right there!\n",
        "\n",
        "Without this, the model would guess — but with it, the model has 100% of what it needs.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡️ **Analogy**\n",
        "\n",
        "Think of it like **pasting the whole chat log** each time you talk to the model:\n",
        "\n",
        "* The system sets the rules.\n",
        "* The conversation is the *entire state*.\n",
        "* You choose exactly what the model “knows” at each step.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "**You control the “working memory” by managing `messages`.**\n",
        "\n",
        "* Want the model to “forget” something? Leave it out.\n",
        "* Want it to “remember”? Add it as a role: `\"assistant\"`.\n",
        "\n",
        "This pattern is the core of:\n",
        "\n",
        "> 🌀 **The Agent Loop**: *Perceive → Reason → Act → Remember*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZGOIGe8VoLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ This is an important “aha!” moment about **how LLMs handle context** versus how you might expect a software system to manage state in a variable or object.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 **In normal software**\n",
        "\n",
        "If you’re building a chat app, you’d probably:\n",
        "\n",
        "1. Store each message in a `chat_history` list or database.\n",
        "2. Append each new message to that list.\n",
        "3. When needed, show the full conversation to the user.\n",
        "\n",
        "```python\n",
        "chat_history = []\n",
        "chat_history.append(\"User: Write a function to swap keys and values.\")\n",
        "chat_history.append(\"Assistant: Here’s the code...\")\n",
        "chat_history.append(\"User: Add type hints and docstrings.\")\n",
        "```\n",
        "\n",
        "Makes sense, right? The **conversation lives in `chat_history`**, and it grows line by line.\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 **With LLMs**\n",
        "\n",
        "The LLM itself **does not keep state**.\n",
        "Every time you call `client.chat.completions.create()`, it starts from scratch — it only “knows” what’s inside the `messages` you send *right now*.\n",
        "\n",
        "So *you* must build the `messages` list each time to include exactly what the model should “remember.”\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **So why don’t we use `.append()` in the example?**\n",
        "\n",
        "**We *do* append in concept — but we do it outside the LLM!**\n",
        "In this simple example, you see:\n",
        "\n",
        "```python\n",
        "# First interaction: messages has 2 items\n",
        "messages = [\n",
        "    {\"role\": \"system\", ...},\n",
        "    {\"role\": \"user\", ...}\n",
        "]\n",
        "\n",
        "initial_response = generate_response(messages)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Then for the second interaction, we *manually* build the new list:\n",
        "\n",
        "```python\n",
        "# Second interaction: build a new messages list that includes:\n",
        "# 1) The original system and user messages\n",
        "# 2) The model's previous response, now as role=assistant\n",
        "# 3) The new user request\n",
        "messages = [\n",
        "    {\"role\": \"system\", ...},\n",
        "    {\"role\": \"user\", ...},\n",
        "    {\"role\": \"assistant\", \"content\": initial_response},\n",
        "    {\"role\": \"user\", \"content\": \"Add type hints and docstrings...\"}\n",
        "]\n",
        "```\n",
        "\n",
        "**Same idea as `.append()` — we’re just showing you the fully built list for clarity!**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **How you’d do it in real code**\n",
        "\n",
        "In production, you’d maintain a `conversation_history` list:\n",
        "\n",
        "```python\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert engineer.\"}\n",
        "]\n",
        "\n",
        "# User says something\n",
        "conversation_history.append({\"role\": \"user\", \"content\": \"Write a function...\"})\n",
        "\n",
        "# Call LLM\n",
        "assistant_reply = generate_response(conversation_history)\n",
        "\n",
        "# Save the assistant’s reply\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "# Next user message\n",
        "conversation_history.append({\"role\": \"user\", \"content\": \"Add type hints...\"})\n",
        "\n",
        "# Call again with full context\n",
        "assistant_reply = generate_response(conversation_history)\n",
        "\n",
        "# Save again...\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "```\n",
        "\n",
        "💡 **This is the same as “manual memory”!**\n",
        "You build your `messages` list step by step — just like your mental model of `+=` or `.append()`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 **Key insight**\n",
        "\n",
        "✔️ The `generate_response()` function has *no memory*.\n",
        "✔️ The `client.chat.completions.create()` call has *no memory*.\n",
        "✔️ **Your `messages` list *is* the memory!**\n",
        "\n",
        "So:\n",
        "\n",
        "> The “state” lives in the **list of messages you maintain** in your Python process — not inside the model.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **When to use manual build vs `.append()`**\n",
        "\n",
        "* **Teaching/demo:** We often show the whole `messages` list for clarity.\n",
        "* **Real agent loop:** You maintain one list and `.append()` to it as the conversation grows.\n",
        "\n"
      ],
      "metadata": {
        "id": "FK0Mkt0LXKFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Let’s take your example and rewrite it as it would look in **real production** — using a **persistent `chat_history`** that you *append* to as the conversation grows.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **Production-style memory pattern**\n",
        "\n",
        "Here’s the same **“stateless vs. stateful”** example, but with a **real chat loop** that makes it crystal clear where memory lives and how you append to it.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 **Full working example: Production-style conversation**\n",
        "\n",
        "```python\n",
        "# ==========================================\n",
        "# ✅ Setup: Same generate_response function\n",
        "# ==========================================\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def generate_response(messages):\n",
        "    \"\"\"Call the LLM and return its response.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# ✅ 1️⃣ Start a conversation history\n",
        "# ==========================================\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"}\n",
        "]\n",
        "\n",
        "# ==========================================\n",
        "# ✅ 2️⃣ User asks first question\n",
        "# ==========================================\n",
        "user_message_1 = \"Write a function to swap keys and values in a dictionary.\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\": user_message_1})\n",
        "\n",
        "# Call the LLM with full history so far:\n",
        "assistant_reply_1 = generate_response(conversation_history)\n",
        "print(\"✅ First response:\\n\", assistant_reply_1)\n",
        "\n",
        "# Append the assistant's reply to the history:\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply_1})\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# ✅ 3️⃣ User asks a follow-up question\n",
        "# ==========================================\n",
        "user_message_2 = \"Add type hints and docstrings to the function from before.\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\": user_message_2})\n",
        "\n",
        "# Call the LLM again with *entire* conversation:\n",
        "assistant_reply_2 = generate_response(conversation_history)\n",
        "print(\"\\n✅ Second response (with full memory):\\n\", assistant_reply_2)\n",
        "\n",
        "# Append the assistant's reply again\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply_2})\n",
        "\n",
        "# ==========================================\n",
        "# ✅ 4️⃣ Keep going as needed!\n",
        "# ==========================================\n",
        "# Next user message: just append, call, append, repeat.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **What’s different vs. your static example?**\n",
        "\n",
        "| Static Example                                      | Production Pattern                                                      |\n",
        "| --------------------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| You build a new `messages` list manually each time. | You maintain **one `conversation_history`** that grows over time.       |\n",
        "| Good for demos, not reusable.                       | Reusable in real apps, chatbots, agents.                                |\n",
        "| No `.append()` — you just overwrite.                | Uses `.append()` for each message: user → assistant → user → assistant. |\n",
        "| No clear “source of truth”.                         | `conversation_history` is your single source of truth for memory.       |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "**Your job as the developer is:**\n",
        "\n",
        "* Keep a single `conversation_history` list.\n",
        "* Always pass *all relevant* messages to the LLM on every call.\n",
        "* Append the model’s responses to the list after each turn.\n",
        "\n",
        "The **LLM has no memory** — your `conversation_history` is the memory.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **Bonus tip: Use classes or files later**\n",
        "\n",
        "When your agent gets bigger:\n",
        "\n",
        "* You’ll wrap this in a class: `AgentMemory`\n",
        "* You might store the `conversation_history` to disk or a DB to persist state.\n",
        "* You can prune or summarize old messages to control token costs.\n",
        "\n"
      ],
      "metadata": {
        "id": "fObH--mhYjCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ In a *real-world implementation*, you wouldn’t hardcode `assistant_reply_1`, `assistant_reply_2`, etc. — you’d run it in a **loop** or inside a function that handles each turn dynamically.\n",
        "\n",
        "Let’s break down how it looks in practice:\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ **Key idea**\n",
        "\n",
        "In production, you’d have:\n",
        "\n",
        "* A **single `conversation_history`** list that persists across turns.\n",
        "* A **loop or handler** that:\n",
        "\n",
        "  1. Takes the latest user input.\n",
        "  2. Appends it to the conversation.\n",
        "  3. Calls the LLM with the entire history.\n",
        "  4. Gets the assistant’s reply.\n",
        "  5. Appends the reply back to the conversation.\n",
        "  6. Repeat.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **Production-style pattern**\n",
        "\n",
        "Here’s a **realistic reusable loop** you could run in a Colab notebook, a Flask API, or a chatbot server:\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load API key\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def generate_response(messages):\n",
        "    \"\"\"Call the LLM and return its response.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ================================================\n",
        "# ✅ 1️⃣ Initialize the conversation\n",
        "# ================================================\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert Python engineer.\"}\n",
        "]\n",
        "\n",
        "# ================================================\n",
        "# ✅ 2️⃣ Agent loop: keep chatting!\n",
        "# ================================================\n",
        "\n",
        "while True:\n",
        "    # --- 1. Get new user input ---\n",
        "    user_input = input(\"\\n🧑‍💻 You: \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Ending the conversation.\")\n",
        "        break\n",
        "\n",
        "    # --- 2. Add user input to history ---\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # --- 3. Call LLM with full conversation ---\n",
        "    assistant_reply = generate_response(conversation_history)\n",
        "\n",
        "    # --- 4. Show reply ---\n",
        "    print(\"\\n🤖 Assistant:\\n\", assistant_reply)\n",
        "\n",
        "    # --- 5. Append reply to history ---\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key points**\n",
        "\n",
        "✔️ **No hardcoded replies:**\n",
        "Each new user input triggers a fresh LLM call with the *entire* conversation.\n",
        "\n",
        "✔️ **One `conversation_history` list:**\n",
        "Grows automatically as turns happen.\n",
        "\n",
        "✔️ **Flexible stop:**\n",
        "You can type `exit` or `quit` to end the loop.\n",
        "\n",
        "✔️ **Same pattern everywhere:**\n",
        "This is exactly what your agent loop in a real backend looks like — just the input source changes:\n",
        "\n",
        "* Local script → use `input()`\n",
        "* Chatbot → messages from a frontend\n",
        "* Slack bot → messages from Slack’s API\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 **Why this is the same idea**\n",
        "\n",
        "* In your earlier example, `assistant_reply_1` was hardcoded for illustration.\n",
        "* Here, the `assistant_reply` is just a *variable that changes every turn*.\n",
        "* The **conversation memory** is your growing `conversation_history` list.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **Next-level tip**\n",
        "\n",
        "When you build real agents:\n",
        "\n",
        "* Use a `while` loop, `for` loop, or event handler.\n",
        "* Manage token usage: prune or summarize old history if it gets huge.\n",
        "* Save `conversation_history` to disk or DB if you want persistence across sessions.\n",
        "\n",
        "---\n",
        "\n",
        "**And that’s it!**\n",
        "The same simple pattern: **append → call → append → repeat** = the *core* of your agent’s memory loop. 🔁\n",
        "\n"
      ],
      "metadata": {
        "id": "tMo8rO5naRig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 📚 **Your conversation history = your agent’s memory**\n",
        "\n",
        "But every LLM (like GPT-4o-mini) has a **maximum context window** — the total number of tokens it can read in at once.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡️ **How this works in practice**\n",
        "\n",
        "* Every time you call `client.chat.completions.create()`, the entire `messages` list is tokenized.\n",
        "* That means **system + all user messages + all assistant replies + your new prompt** must fit inside the model’s context window.\n",
        "* For example:\n",
        "\n",
        "  * GPT-4o-mini supports \\~128K tokens (depending on the tier).\n",
        "  * Each “turn” in a conversation can add a few dozen to a few thousand tokens (code, reasoning, explanations).\n",
        "\n",
        "If your `conversation_history` gets too big, you hit the limit — and the API will truncate or throw an error.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 **This is why real agents need memory management**\n",
        "\n",
        "When your agent runs for days or has long multi-step tasks, you’ll need strategies to **stay within the context window**:\n",
        "\n",
        "### 🧩 **Practical memory management strategies**\n",
        "\n",
        "1️⃣ **Summarization**\n",
        "\n",
        "* Have the LLM *summarize older conversation parts* into a shorter chunk.\n",
        "* Replace multiple old messages with a concise “memory” message.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "# Replace:\n",
        "# [Many old user/assistant messages]\n",
        "# With:\n",
        "{\"role\": \"system\", \"content\": \"Summary of prior conversation: User is building a calendar agent, they discussed how to call the Google API and manage errors.\"}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ **Trimming**\n",
        "\n",
        "* Drop very old messages if they’re no longer relevant.\n",
        "* Or keep only the system role + the last few turns.\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ **External long-term memory**\n",
        "\n",
        "* Save structured facts or intermediate states to a database or file.\n",
        "* Feed those facts back in when needed.\n",
        "\n",
        "Example:\n",
        "\n",
        "> Your agent builds a task list → stores tasks in a DB → only pulls the relevant tasks for each context.\n",
        "\n",
        "---\n",
        "\n",
        "4️⃣ **Chunking or windowing**\n",
        "\n",
        "* Use a **sliding window** approach:\n",
        "  Always send only the *most recent X turns* plus any critical context.\n",
        "* Good for short conversations that don’t need deep history.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Key takeaway**\n",
        "\n",
        "> **Your `conversation_history` is your “working memory”** — but you, the developer, decide how much context to keep in the LLM window vs. store externally.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 **Agent Loop pattern**\n",
        "\n",
        "1. **Perceive** (user input + tools + environment)\n",
        "2. **Reason** (call LLM with the needed context)\n",
        "3. **Act** (take action)\n",
        "4. **Remember**\n",
        "\n",
        "   * Append new state to your working memory\n",
        "   * Summarize or store to long-term memory if needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMzXN0lpbBkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Let’s level up your agent loop with a **practical `AgentMemory` helper class** that does:\n",
        "\n",
        "✅ Append messages (user & assistant)\n",
        "✅ Keep the full working memory (`conversation_history`)\n",
        "✅ **Auto-summarize** older messages to manage context window size\n",
        "✅ Simple `.prune()` method for dropping irrelevant chunks\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 **Why use a helper class?**\n",
        "\n",
        "In production you don’t want to keep repeating:\n",
        "\n",
        "```python\n",
        "conversation_history.append({...})\n",
        "```\n",
        "\n",
        "or forgetting which messages you’ve added.\n",
        "A simple `AgentMemory` class makes this consistent and extendable.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 🗂️ **How it works**\n",
        "\n",
        "✅ **`AgentMemory`** holds your entire conversation state.\n",
        "✅ You call `.add_user_message()` and `.add_assistant_message()` each turn.\n",
        "✅ `.summarize_old_messages()` uses the LLM itself to compress the old chat, so you stay under your token budget.\n",
        "✅ `.prune_oldest()` is a fallback to just drop old turns if you want.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 **Why it’s practical**\n",
        "\n",
        "* You get **auto memory management**, so you don’t accidentally blow up the context window.\n",
        "* You can swap in more advanced logic later: vector store, DB, semantic retrieval.\n",
        "* It’s modular and testable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2gKQ--9xbXxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# ✅ AgentMemory helper class\n",
        "# ============================================\n",
        "\n",
        "class AgentMemory:\n",
        "    def __init__(self, system_prompt: str):\n",
        "        self.system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
        "        self.history = [self.system_message]\n",
        "\n",
        "    def add_user_message(self, content: str):\n",
        "        self.history.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "    def add_assistant_message(self, content: str):\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": content})\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "\n",
        "    def summarize_old_messages(self):\n",
        "        \"\"\"\n",
        "        Summarize older messages into a compact chunk\n",
        "        and keep the last few turns verbatim.\n",
        "        \"\"\"\n",
        "        if len(self.history) <= 6:\n",
        "            print(\"ℹ️ Nothing to summarize yet.\")\n",
        "            return\n",
        "\n",
        "        # Keep system + last 4 messages\n",
        "        recent_messages = [self.system_message] + self.history[-4:]\n",
        "\n",
        "        # Summarize older part\n",
        "        old_messages = self.history[1:-4]\n",
        "\n",
        "        summary_prompt = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a summarization assistant. Summarize the following conversation.\"},\n",
        "            {\"role\": \"user\", \"content\": \"\\n\\n\".join([f\"{m['role']}: {m['content']}\" for m in old_messages])}\n",
        "        ]\n",
        "\n",
        "        summary = generate_response(summary_prompt)\n",
        "\n",
        "        # Replace with summary message\n",
        "        summarized_memory = {\"role\": \"system\", \"content\": f\"Conversation summary: {summary}\"}\n",
        "\n",
        "        self.history = [summarized_memory] + recent_messages[1:]  # Avoid duplicate system message\n",
        "        print(\"✅ Summarized old messages!\")\n",
        "\n",
        "    def prune_oldest(self, keep_last_n: int = 6):\n",
        "        \"\"\"\n",
        "        Drop oldest user/assistant messages, keep system + last N turns.\n",
        "        \"\"\"\n",
        "        if len(self.history) <= keep_last_n + 1:\n",
        "            print(\"ℹ️ Nothing to prune.\")\n",
        "            return\n",
        "\n",
        "        self.history = [self.system_message] + self.history[-keep_last_n:]\n",
        "        print(f\"✅ Pruned to keep last {keep_last_n} turns.\")\n",
        "\n",
        "# ============================================\n",
        "# ✅ Example usage loop\n",
        "# ============================================\n",
        "\n",
        "agent_memory = AgentMemory(system_prompt=\"You are an expert Python engineer.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\n🧑‍💻 You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Ending the conversation.\")\n",
        "        break\n",
        "\n",
        "    agent_memory.add_user_message(user_input)\n",
        "\n",
        "    assistant_reply = generate_response(agent_memory.get_history())\n",
        "    print(\"\\n🤖 Assistant:\\n\", assistant_reply)\n",
        "\n",
        "    agent_memory.add_assistant_message(assistant_reply)\n",
        "\n",
        "    # Example: if history grows, auto-summarize or prune\n",
        "    if len(agent_memory.get_history()) > 10:\n",
        "        agent_memory.summarize_old_messages()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8swrP7cWaQ6P",
        "outputId": "6d8747f3-6737-4f1b-e365-1f2b65768e39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧑‍💻 You: What is the difference between class and method in python programming?\n",
            "\n",
            "🤖 Assistant:\n",
            " In Python programming, classes and methods are both fundamental concepts associated with object-oriented programming (OOP), but they serve different purposes. Here’s a breakdown of the differences:\n",
            "\n",
            "### Class\n",
            "\n",
            "1. **Definition**: A class is a blueprint for creating objects. It defines the properties (attributes) and behaviors (methods) that the objects created from the class will have.\n",
            "\n",
            "2. **Purpose**: Classes are used to bundle data and functionality together. They help in organizing code by grouping related properties and methods.\n",
            "\n",
            "3. **Instantiation**: You create an instance (or object) of a class to use its functionality. Each object can have its own state (values of attributes).\n",
            "\n",
            "4. **Example**:\n",
            "    ```python\n",
            "    class Dog:\n",
            "        def __init__(self, name):\n",
            "            self.name = name\n",
            "        \n",
            "        def bark(self):\n",
            "            return \"Woof!\"\n",
            "    \n",
            "    my_dog = Dog(\"Rex\")\n",
            "    print(my_dog.bark())  # Outputs: Woof!\n",
            "    ```\n",
            "\n",
            "### Method\n",
            "\n",
            "1. **Definition**: A method is a function that is defined within a class and is associated with the objects of that class. Methods can operate on data that belongs to the object (instance).\n",
            "\n",
            "2. **Purpose**: Methods define the behaviors of the objects created from a class. They can modify object state, perform computations, or interact with other methods.\n",
            "\n",
            "3. **Types of Methods**:\n",
            "   - **Instance methods**: The most common type, which operate on an instance of the class (must include `self` as the first parameter).\n",
            "   - **Class methods**: Operate on the class itself and can be defined using the `@classmethod` decorator. They receive the class as their first parameter, typically named `cls`.\n",
            "   - **Static methods**: Do not operate on an instance or class. They can be defined using the `@staticmethod` decorator and behave like regular functions defined within a class namespace.\n",
            "\n",
            "4. **Example**:\n",
            "    ```python\n",
            "    class Dog:\n",
            "        def __init__(self, name):\n",
            "            self.name = name\n",
            "        \n",
            "        def bark(self):  # This is an instance method\n",
            "            return f\"{self.name} says Woof!\"\n",
            "        \n",
            "        @classmethod\n",
            "        def species(cls):  # This is a class method\n",
            "            return \"Canis lupus familiaris\"\n",
            "        \n",
            "        @staticmethod\n",
            "        def info():  # This is a static method\n",
            "            return \"Dogs are domest\n",
            "\n",
            "🧑‍💻 You: can you summarize what you just said?\n",
            "\n",
            "🤖 Assistant:\n",
            " Sure! Here’s a summary of the differences between a class and a method in Python:\n",
            "\n",
            "### Class\n",
            "- **Definition**: A blueprint for creating objects that defines attributes and methods.\n",
            "- **Purpose**: Organizes code by grouping related data and functionality.\n",
            "- **Instantiation**: You create objects (instances) from a class.\n",
            "- **Example**: \n",
            "    ```python\n",
            "    class Dog:\n",
            "        def __init__(self, name):\n",
            "            self.name = name\n",
            "    ```\n",
            "\n",
            "### Method\n",
            "- **Definition**: A function defined within a class that operates on instances of that class.\n",
            "- **Purpose**: Defines behaviors for objects, such as modifying state or performing computations.\n",
            "- **Types**: \n",
            "  - Instance methods (work on object instances)\n",
            "  - Class methods (work on the class itself, using `@classmethod`)\n",
            "  - Static methods (independent of class/instance, using `@staticmethod`)\n",
            "- **Example**:\n",
            "    ```python\n",
            "    def bark(self):  # Instance method\n",
            "        return f\"{self.name} says Woof!\"\n",
            "    ```\n",
            "\n",
            "In essence, a class defines the structure and behavior of objects, while methods define the actions those objects can perform.\n",
            "\n",
            "🧑‍💻 You: quit\n",
            "👋 Ending the conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQAHgwaBVpvn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}