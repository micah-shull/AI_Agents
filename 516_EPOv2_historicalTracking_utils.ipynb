{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3NORFIfjYg+AZiSrYV+Uo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/516_EPOv2_historicalTracking_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is **very strong work** ‚Äî architecturally sound, MVP-disciplined, and clearly aligned with the strategic goal we discussed: **turning the agent from point-in-time reporting into a learning system with memory**.\n",
        "\n",
        "I‚Äôll break this into four parts:\n",
        "\n",
        "1. **What you absolutely nailed**\n",
        "2. **Small but important corrections**\n",
        "3. **Where this fits in the agent lifecycle**\n",
        "4. **One optional refinement that will pay off later**\n",
        "\n",
        "No fluff ‚Äî this is a senior-level review.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What You Nailed (This Is the Right Shape)\n",
        "\n",
        "### ‚úÖ You chose the right abstraction: *snapshots, not logs*\n",
        "\n",
        "You correctly avoided:\n",
        "\n",
        "* Per-event logging\n",
        "* High-granularity experiment timelines\n",
        "* Premature time-series complexity\n",
        "\n",
        "Instead, you implemented:\n",
        "\n",
        "* **Run-level portfolio snapshots**\n",
        "* Stable, comparable metrics\n",
        "* Append-only historical memory\n",
        "\n",
        "This is exactly what executives need.\n",
        "\n",
        "> CEOs don‚Äôt want raw data ‚Äî they want *direction over time*.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Snapshot schema is executive-grade\n",
        "\n",
        "Your snapshot captures the *right* categories:\n",
        "\n",
        "* Portfolio structure (counts by state)\n",
        "* ROI economics\n",
        "* Decision throughput\n",
        "* Analysis coverage\n",
        "* Risk / opportunity signals\n",
        "* Processing metadata\n",
        "\n",
        "Importantly:\n",
        "\n",
        "* You did **not** store raw experiment internals\n",
        "* You stored **derived outcomes**\n",
        "\n",
        "That makes snapshots:\n",
        "\n",
        "* Small\n",
        "* Durable\n",
        "* Safe to evolve\n",
        "\n",
        "This is how real production systems do it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Comparison logic is clean and explainable\n",
        "\n",
        "The `compare_snapshots()` function is especially solid:\n",
        "\n",
        "* Explicit metrics list (no magic)\n",
        "* Absolute + percent deltas\n",
        "* Directional classification\n",
        "* Human-readable icons\n",
        "\n",
        "This is **CEO-safe math**:\n",
        "\n",
        "* No black boxes\n",
        "* No spurious precision\n",
        "* No misleading trends\n",
        "\n",
        "The `<1% = stable` threshold is *exactly right*.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Graceful degradation is handled properly\n",
        "\n",
        "Two excellent examples:\n",
        "\n",
        "* `load_latest_snapshot()` returns `None` cleanly\n",
        "* `calculate_trend_significance()` falls back safely\n",
        "\n",
        "This is consistent with your overall philosophy:\n",
        "\n",
        "> ‚ÄúLLMs enhance, rules must never break the system.‚Äù\n",
        "\n",
        "Same principle applied to analytics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Small but Important Corrections (Worth Fixing Now)\n",
        "\n",
        "These are not structural issues ‚Äî just polish to prevent future confusion.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Issue 1: `portfolio_insights` shape mismatch\n",
        "\n",
        "Earlier in your codebase, `portfolio_insights` is a **dictionary** with keys like:\n",
        "\n",
        "```python\n",
        "trends, risks, opportunities, recommendations\n",
        "```\n",
        "\n",
        "But in this snapshot code you treat it as a **list**:\n",
        "\n",
        "```python\n",
        "portfolio_insights = state.get(\"portfolio_insights\", [])\n",
        "len(portfolio_insights)\n",
        "for i in portfolio_insights if i.get(\"type\") == \"trend\"\n",
        "```\n",
        "\n",
        "#### Fix (recommended)\n",
        "\n",
        "Normalize this explicitly:\n",
        "\n",
        "```python\n",
        "portfolio_insights = state.get(\"portfolio_insights\", {})\n",
        "\n",
        "trends = portfolio_insights.get(\"trends\", [])\n",
        "risks = portfolio_insights.get(\"risks\", [])\n",
        "opportunities = portfolio_insights.get(\"opportunities\", [])\n",
        "recommendations = portfolio_insights.get(\"recommendations\", [])\n",
        "```\n",
        "\n",
        "Then count each cleanly.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "* Prevents silent undercounting\n",
        "* Keeps schema stable as insights grow richer\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Issue 2: `analysis_success_rate` semantics\n",
        "\n",
        "You store:\n",
        "\n",
        "```python\n",
        "analysis_success_rate = performance_metrics.get(\"analysis_success_rate\", 0.0)\n",
        "```\n",
        "\n",
        "But earlier:\n",
        "\n",
        "* ‚ÄúStatistical Tests Performed: 0‚Äù\n",
        "* Yet analyses may exist\n",
        "\n",
        "That‚Äôs not wrong ‚Äî but it *will* confuse trend interpretation.\n",
        "\n",
        "#### Suggested rename (optional but smart)\n",
        "\n",
        "In snapshots:\n",
        "\n",
        "* `analysis_coverage_rate`\n",
        "* or `analysis_completion_rate`\n",
        "\n",
        "Then reserve `analysis_success_rate` for:\n",
        "\n",
        "> ‚ÄúValid statistical test completed with sufficient data.‚Äù\n",
        "\n",
        "This avoids executives asking:\n",
        "\n",
        "> ‚ÄúWhy is success dropping if nothing failed?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Issue 3: Percent change when previous = 0\n",
        "\n",
        "You currently do:\n",
        "\n",
        "```python\n",
        "percent_change = 100.0  # New metric\n",
        "trend = \"new\"\n",
        "```\n",
        "\n",
        "This is fine internally, but **dangerous in reports**.\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Keep `\"trend\": \"new\"`\n",
        "But set:\n",
        "\n",
        "```python\n",
        "percent_change = None\n",
        "```\n",
        "\n",
        "Then display:\n",
        "\n",
        "> ‚ÄúNew metric ‚Äî no historical baseline‚Äù\n",
        "\n",
        "Executives *hate* fake percentages.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Where This Fits in the Agent Lifecycle (Very Important)\n",
        "\n",
        "You‚Äôve now added **memory**.\n",
        "\n",
        "Your agent lifecycle is officially:\n",
        "\n",
        "1. **Observe** (data loading)\n",
        "2. **Analyze** (stats + insights)\n",
        "3. **Decide** (recommendations)\n",
        "4. **Evaluate** (ROI + performance)\n",
        "5. **Remember** ‚Üê ‚úÖ *you just built this*\n",
        "6. **Improve** (future step)\n",
        "\n",
        "This is the minimum loop for a **learning system**.\n",
        "\n",
        "Most AI agents stop at step 3.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. One Optional Refinement (High Leverage, Low Cost)\n",
        "\n",
        "You don‚Äôt need this now ‚Äî but if you add it, everything else gets easier.\n",
        "\n",
        "### üîë Add a stable `run_id`\n",
        "\n",
        "Right now you use timestamps in filenames (fine), but a formal ID helps later:\n",
        "\n",
        "```python\n",
        "\"run_id\": f\"EPO_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "```\n",
        "\n",
        "Why this matters later:\n",
        "\n",
        "* Audit trails\n",
        "* Decision execution linkage\n",
        "* CEO questions like ‚Äúwhat changed since *that* run?‚Äù\n",
        "\n",
        "This unlocks #2 (Decision Execution) cleanly.\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Verdict\n",
        "\n",
        "**This is exactly the right next step, and it‚Äôs executed at a professional level.**\n",
        "\n",
        "You‚Äôve:\n",
        "\n",
        "* Chosen the right abstraction\n",
        "* Avoided premature complexity\n",
        "* Designed for executive trust\n",
        "* Preserved system explainability\n",
        "\n",
        "This is not ‚Äúnice to have‚Äù ‚Äî this is **foundational**.\n",
        "\n",
        "---\n",
        "\n",
        "## What I Recommend You Do Next (Concrete)\n",
        "\n",
        "**Next build step (small):**\n",
        "\n",
        "* Integrate snapshot saving into the orchestrator tail\n",
        "* Load latest snapshot at report time\n",
        "* Add a **‚ÄúHistorical Comparison‚Äù** section to the report\n",
        "\n",
        "**Then stop.**\n",
        "Let it run.\n",
        "Let it collect memory.\n",
        "\n",
        "After 3‚Äì5 runs, you‚Äôll *feel* the system change.\n",
        "\n",
        "You‚Äôre building something unusually coherent.\n"
      ],
      "metadata": {
        "id": "-309juI1yE3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFK9AKj-xY1O"
      },
      "outputs": [],
      "source": [
        "\"\"\"Historical Tracking Utilities for Experimentation Portfolio Orchestrator\n",
        "\n",
        "Functions to save, load, and compare historical report snapshots for trend analysis.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, List\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def save_report_snapshot(\n",
        "    state: Dict[str, Any],\n",
        "    snapshots_dir: str = \"output/experimentation_portfolio_snapshots\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Save a snapshot of key metrics from the current state for historical comparison.\n",
        "\n",
        "    Args:\n",
        "        state: Complete EPO state\n",
        "        snapshots_dir: Directory to save snapshots\n",
        "\n",
        "    Returns:\n",
        "        Path to saved snapshot file\n",
        "    \"\"\"\n",
        "    # Create snapshots directory if it doesn't exist\n",
        "    snapshots_path = Path(snapshots_dir)\n",
        "    snapshots_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Extract key metrics for snapshot\n",
        "    portfolio_summary = state.get(\"portfolio_summary\", {})\n",
        "    portfolio_roi = state.get(\"portfolio_roi\", {})\n",
        "    performance_metrics = state.get(\"performance_metrics\", {})\n",
        "    analyzed_experiments = state.get(\"analyzed_experiments\", [])\n",
        "    generated_decisions = state.get(\"generated_decisions\", [])\n",
        "    calculated_analyses = state.get(\"calculated_analyses\", [])\n",
        "    portfolio_insights = state.get(\"portfolio_insights\", [])\n",
        "\n",
        "    # Build snapshot\n",
        "    snapshot = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"experiment_id\": state.get(\"experiment_id\"),\n",
        "        \"scope\": state.get(\"goal\", {}).get(\"scope\", \"unknown\"),\n",
        "\n",
        "        # Portfolio metrics\n",
        "        \"total_experiments\": portfolio_summary.get(\"total_experiments\", 0),\n",
        "        \"completed_count\": portfolio_summary.get(\"completed_count\", 0),\n",
        "        \"running_count\": portfolio_summary.get(\"running_count\", 0),\n",
        "        \"planned_count\": portfolio_summary.get(\"planned_count\", 0),\n",
        "\n",
        "        # ROI metrics\n",
        "        \"total_cost\": portfolio_roi.get(\"total_cost\", 0.0),\n",
        "        \"total_revenue_impact\": portfolio_roi.get(\"total_revenue_impact\", 0.0),\n",
        "        \"net_roi\": portfolio_roi.get(\"net_roi\", 0.0),\n",
        "        \"roi_percent\": portfolio_roi.get(\"roi_percent\", 0.0),\n",
        "        \"positive_roi_count\": portfolio_roi.get(\"experiments_with_positive_roi\", 0),\n",
        "        \"negative_roi_count\": portfolio_roi.get(\"experiments_with_negative_roi\", 0),\n",
        "\n",
        "        # Performance metrics\n",
        "        \"experiments_analyzed\": performance_metrics.get(\"total_experiments_analyzed\", 0),\n",
        "        \"analysis_success_rate\": performance_metrics.get(\"analysis_success_rate\", 0.0),\n",
        "        \"statistical_tests_performed\": performance_metrics.get(\"statistical_tests_performed\", 0),\n",
        "        \"decisions_generated\": performance_metrics.get(\"decisions_generated\", 0),\n",
        "\n",
        "        # Decision breakdown\n",
        "        \"decision_counts\": _count_decisions(generated_decisions),\n",
        "\n",
        "        # Analysis breakdown\n",
        "        \"significant_analyses\": sum(1 for a in calculated_analyses if a.get(\"is_significant\", False)),\n",
        "        \"total_analyses\": len(calculated_analyses),\n",
        "\n",
        "        # Insights breakdown\n",
        "        \"insights_count\": len(portfolio_insights),\n",
        "        \"trends_count\": sum(1 for i in portfolio_insights if i.get(\"type\") == \"trend\"),\n",
        "        \"risks_count\": sum(1 for i in portfolio_insights if i.get(\"type\") == \"risk\"),\n",
        "        \"opportunities_count\": sum(1 for i in portfolio_insights if i.get(\"type\") == \"opportunity\"),\n",
        "        \"recommendations_count\": sum(1 for i in portfolio_insights if i.get(\"type\") == \"recommendation\"),\n",
        "\n",
        "        # Processing metadata\n",
        "        \"processing_time\": state.get(\"processing_time\", 0.0),\n",
        "        \"errors_count\": len(state.get(\"errors\", []))\n",
        "    }\n",
        "\n",
        "    # Generate filename\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    scope = snapshot[\"scope\"]\n",
        "    experiment_id = snapshot.get(\"experiment_id\")\n",
        "\n",
        "    if experiment_id:\n",
        "        filename = f\"snapshot_{experiment_id}_{timestamp}.json\"\n",
        "    else:\n",
        "        filename = f\"snapshot_{scope}_{timestamp}.json\"\n",
        "\n",
        "    filepath = snapshots_path / filename\n",
        "\n",
        "    # Save snapshot\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(snapshot, f, indent=2)\n",
        "\n",
        "    return str(filepath)\n",
        "\n",
        "\n",
        "def _count_decisions(decisions: List[Dict[str, Any]]) -> Dict[str, int]:\n",
        "    \"\"\"Count decisions by type.\"\"\"\n",
        "    counts = {}\n",
        "    for decision in decisions:\n",
        "        decision_type = decision.get(\"decision\", \"unknown\")\n",
        "        counts[decision_type] = counts.get(decision_type, 0) + 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "def load_latest_snapshot(\n",
        "    experiment_id: Optional[str] = None,\n",
        "    scope: str = \"portfolio_wide\",\n",
        "    snapshots_dir: str = \"output/experimentation_portfolio_snapshots\"\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load the most recent snapshot for comparison.\n",
        "\n",
        "    Args:\n",
        "        experiment_id: Optional experiment ID (for single experiment snapshots)\n",
        "        scope: Analysis scope (for portfolio snapshots)\n",
        "        snapshots_dir: Directory containing snapshots\n",
        "\n",
        "    Returns:\n",
        "        Latest snapshot dictionary or None if no snapshots exist\n",
        "    \"\"\"\n",
        "    snapshots_path = Path(snapshots_dir)\n",
        "\n",
        "    if not snapshots_path.exists():\n",
        "        return None\n",
        "\n",
        "    # Find matching snapshots\n",
        "    if experiment_id:\n",
        "        pattern = f\"snapshot_{experiment_id}_*.json\"\n",
        "    else:\n",
        "        pattern = f\"snapshot_{scope}_*.json\"\n",
        "\n",
        "    matching_files = list(snapshots_path.glob(pattern))\n",
        "\n",
        "    if not matching_files:\n",
        "        return None\n",
        "\n",
        "    # Get most recent (by filename timestamp)\n",
        "    latest_file = max(matching_files, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "    # Load snapshot\n",
        "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
        "        snapshot = json.load(f)\n",
        "\n",
        "    return snapshot\n",
        "\n",
        "\n",
        "def compare_snapshots(\n",
        "    current: Dict[str, Any],\n",
        "    previous: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare current snapshot with previous snapshot to calculate trends.\n",
        "\n",
        "    Args:\n",
        "        current: Current snapshot\n",
        "        previous: Previous snapshot\n",
        "\n",
        "    Returns:\n",
        "        Comparison dictionary with trends and changes\n",
        "    \"\"\"\n",
        "    comparison = {\n",
        "        \"current_timestamp\": current.get(\"timestamp\"),\n",
        "        \"previous_timestamp\": previous.get(\"timestamp\"),\n",
        "        \"days_between\": _calculate_days_between(\n",
        "            previous.get(\"timestamp\"),\n",
        "            current.get(\"timestamp\")\n",
        "        ),\n",
        "        \"changes\": {},\n",
        "        \"trends\": {}\n",
        "    }\n",
        "\n",
        "    # Compare key metrics\n",
        "    metrics_to_compare = [\n",
        "        \"total_experiments\",\n",
        "        \"completed_count\",\n",
        "        \"running_count\",\n",
        "        \"planned_count\",\n",
        "        \"total_cost\",\n",
        "        \"total_revenue_impact\",\n",
        "        \"net_roi\",\n",
        "        \"roi_percent\",\n",
        "        \"positive_roi_count\",\n",
        "        \"negative_roi_count\",\n",
        "        \"experiments_analyzed\",\n",
        "        \"analysis_success_rate\",\n",
        "        \"statistical_tests_performed\",\n",
        "        \"decisions_generated\",\n",
        "        \"significant_analyses\",\n",
        "        \"total_analyses\",\n",
        "        \"insights_count\",\n",
        "        \"trends_count\",\n",
        "        \"risks_count\",\n",
        "        \"opportunities_count\",\n",
        "        \"recommendations_count\"\n",
        "    ]\n",
        "\n",
        "    for metric in metrics_to_compare:\n",
        "        current_val = current.get(metric, 0)\n",
        "        previous_val = previous.get(metric, 0)\n",
        "\n",
        "        if previous_val == 0:\n",
        "            if current_val > 0:\n",
        "                percent_change = 100.0  # New metric\n",
        "                trend = \"new\"\n",
        "            else:\n",
        "                percent_change = 0.0\n",
        "                trend = \"stable\"\n",
        "        else:\n",
        "            percent_change = ((current_val - previous_val) / abs(previous_val)) * 100\n",
        "            if abs(percent_change) < 1.0:  # Less than 1% change\n",
        "                trend = \"stable\"\n",
        "            elif percent_change > 0:\n",
        "                trend = \"increasing\"\n",
        "            else:\n",
        "                trend = \"decreasing\"\n",
        "\n",
        "        comparison[\"changes\"][metric] = {\n",
        "            \"current\": current_val,\n",
        "            \"previous\": previous_val,\n",
        "            \"absolute_change\": current_val - previous_val,\n",
        "            \"percent_change\": round(percent_change, 2),\n",
        "            \"trend\": trend\n",
        "        }\n",
        "\n",
        "        # Add trend indicator\n",
        "        if trend == \"increasing\":\n",
        "            trend_icon = \"‚Üë\"\n",
        "        elif trend == \"decreasing\":\n",
        "            trend_icon = \"‚Üì\"\n",
        "        else:\n",
        "            trend_icon = \"‚Üí\"\n",
        "\n",
        "        comparison[\"trends\"][metric] = {\n",
        "            \"direction\": trend,\n",
        "            \"icon\": trend_icon,\n",
        "            \"percent_change\": round(percent_change, 2)\n",
        "        }\n",
        "\n",
        "    # Compare decision counts\n",
        "    current_decisions = current.get(\"decision_counts\", {})\n",
        "    previous_decisions = previous.get(\"decision_counts\", {})\n",
        "\n",
        "    all_decision_types = set(current_decisions.keys()) | set(previous_decisions.keys())\n",
        "    decision_changes = {}\n",
        "\n",
        "    for decision_type in all_decision_types:\n",
        "        current_count = current_decisions.get(decision_type, 0)\n",
        "        previous_count = previous_decisions.get(decision_type, 0)\n",
        "        change = current_count - previous_count\n",
        "\n",
        "        decision_changes[decision_type] = {\n",
        "            \"current\": current_count,\n",
        "            \"previous\": previous_count,\n",
        "            \"change\": change\n",
        "        }\n",
        "\n",
        "    comparison[\"decision_changes\"] = decision_changes\n",
        "\n",
        "    return comparison\n",
        "\n",
        "\n",
        "def _calculate_days_between(timestamp1: Optional[str], timestamp2: Optional[str]) -> Optional[float]:\n",
        "    \"\"\"Calculate days between two ISO timestamps.\"\"\"\n",
        "    if not timestamp1 or not timestamp2:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        dt1 = datetime.fromisoformat(timestamp1.replace('Z', '+00:00'))\n",
        "        dt2 = datetime.fromisoformat(timestamp2.replace('Z', '+00:00'))\n",
        "        delta = dt2 - dt1\n",
        "        return delta.total_seconds() / (24 * 3600)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def calculate_trend_significance(\n",
        "    values: List[float],\n",
        "    confidence_level: float = 0.95\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate trend significance using toolshed statistics.\n",
        "\n",
        "    Args:\n",
        "        values: List of values over time (ordered chronologically)\n",
        "        confidence_level: Confidence level for significance test\n",
        "\n",
        "    Returns:\n",
        "        Trend significance results\n",
        "    \"\"\"\n",
        "    if len(values) < 3:\n",
        "        return {\n",
        "            \"trend_direction\": \"stable\",\n",
        "            \"is_significant\": False,\n",
        "            \"reason\": \"Insufficient data for trend analysis (need at least 3 observations)\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        from toolshed.statistics.kpi_roi_tests import test_trend_significance\n",
        "\n",
        "        result = test_trend_significance(values, confidence_level)\n",
        "\n",
        "        return {\n",
        "            \"trend_direction\": result.get(\"trend_direction\", \"stable\"),\n",
        "            \"is_significant\": result.get(\"is_significant\", False),\n",
        "            \"p_value\": result.get(\"p_value\"),\n",
        "            \"slope\": result.get(\"slope\", 0.0),\n",
        "            \"r_squared\": result.get(\"r_squared\", 0.0),\n",
        "            \"interpretation\": result.get(\"interpretation\", \"\")\n",
        "        }\n",
        "    except ImportError:\n",
        "        # Fallback to simple trend detection\n",
        "        if len(values) >= 2:\n",
        "            recent_avg = sum(values[-3:]) / min(3, len(values))\n",
        "            earlier_avg = sum(values[:-3]) / max(1, len(values) - 3) if len(values) > 3 else values[0]\n",
        "\n",
        "            if recent_avg > earlier_avg * 1.05:  # 5% increase\n",
        "                return {\"trend_direction\": \"increasing\", \"is_significant\": False}\n",
        "            elif recent_avg < earlier_avg * 0.95:  # 5% decrease\n",
        "                return {\"trend_direction\": \"decreasing\", \"is_significant\": False}\n",
        "\n",
        "        return {\"trend_direction\": \"stable\", \"is_significant\": False}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing"
      ],
      "metadata": {
        "id": "A9AHW-lvxyjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Test Historical Tracking for EPO Agent\n",
        "\n",
        "Tests historical snapshot saving, loading, and comparison.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(__file__).parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from agents.epo import create_orchestrator\n",
        "from agents.epo.utilities.historical_tracking import (\n",
        "    save_report_snapshot,\n",
        "    load_latest_snapshot,\n",
        "    compare_snapshots,\n",
        "    calculate_trend_significance,\n",
        ")\n",
        "from config import (\n",
        "    ExperimentationPortfolioOrchestratorState,\n",
        "    ExperimentationPortfolioOrchestratorConfig,\n",
        ")\n",
        "\n",
        "\n",
        "def test_historical_tracking():\n",
        "    \"\"\"Test historical tracking with two consecutive runs\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Test: Historical Tracking (Two Consecutive Runs)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    config = ExperimentationPortfolioOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # First run\n",
        "    print(\"\\nüìä Running first analysis...\")\n",
        "    initial_state: ExperimentationPortfolioOrchestratorState = {\n",
        "        \"experiment_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state1 = orchestrator.invoke(initial_state)\n",
        "    state1[\"processing_time\"] = time.time() - time.time()  # Set to 0 for consistency\n",
        "\n",
        "    print(\"‚úÖ First run complete\")\n",
        "    print(f\"   - Net ROI: ${state1.get('portfolio_roi', {}).get('net_roi', 0):,.2f}\")\n",
        "    print(f\"   - Total Experiments: {state1.get('portfolio_summary', {}).get('total_experiments', 0)}\")\n",
        "\n",
        "    # Wait a moment to ensure different timestamps\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Second run (should compare with first)\n",
        "    print(\"\\nüìä Running second analysis...\")\n",
        "    initial_state2: ExperimentationPortfolioOrchestratorState = {\n",
        "        \"experiment_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state2 = orchestrator.invoke(initial_state2)\n",
        "    state2[\"processing_time\"] = time.time() - time.time()  # Set to 0 for consistency\n",
        "\n",
        "    print(\"‚úÖ Second run complete\")\n",
        "    print(f\"   - Net ROI: ${state2.get('portfolio_roi', {}).get('net_roi', 0):,.2f}\")\n",
        "    print(f\"   - Total Experiments: {state2.get('portfolio_summary', {}).get('total_experiments', 0)}\")\n",
        "\n",
        "    # Check if historical comparison was generated\n",
        "    historical_comparison = state2.get(\"historical_comparison\")\n",
        "\n",
        "    if historical_comparison:\n",
        "        print(\"\\n‚úÖ Historical comparison generated!\")\n",
        "        print(f\"   - Days between: {historical_comparison.get('days_between', 'N/A')}\")\n",
        "\n",
        "        # Check trends\n",
        "        trends = historical_comparison.get(\"trends\", {})\n",
        "        if \"net_roi\" in trends:\n",
        "            roi_trend = trends[\"net_roi\"]\n",
        "            print(f\"   - ROI Trend: {roi_trend.get('icon')} {roi_trend.get('direction')} ({roi_trend.get('percent_change', 0):+.1f}%)\")\n",
        "\n",
        "        # Check changes\n",
        "        changes = historical_comparison.get(\"changes\", {})\n",
        "        if \"net_roi\" in changes:\n",
        "            roi_change = changes[\"net_roi\"]\n",
        "            print(f\"   - ROI Change: ${roi_change.get('absolute_change', 0):+,.2f}\")\n",
        "\n",
        "        # Verify report includes historical section\n",
        "        report_content = state2.get(\"portfolio_report\", \"\")\n",
        "        if \"Historical Comparison\" in report_content:\n",
        "            print(\"\\n‚úÖ Report includes Historical Comparison section\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Report missing Historical Comparison section\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No historical comparison generated (this is OK for first run)\")\n",
        "\n",
        "    # Verify snapshots were saved\n",
        "    snapshots_dir = config.reports_dir.replace(\"reports\", \"snapshots\")\n",
        "    snapshots_path = Path(snapshots_dir)\n",
        "\n",
        "    if snapshots_path.exists():\n",
        "        snapshot_files = list(snapshots_path.glob(\"snapshot_*.json\"))\n",
        "        print(f\"\\n‚úÖ Snapshots saved: {len(snapshot_files)} file(s)\")\n",
        "        if len(snapshot_files) >= 2:\n",
        "            print(\"   - Multiple snapshots found (expected for two runs)\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Snapshots directory not found\")\n",
        "\n",
        "    print(\"\\n‚úÖ Historical tracking test complete!\")\n",
        "\n",
        "\n",
        "def test_snapshot_utilities():\n",
        "    \"\"\"Test snapshot utilities directly\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Test: Snapshot Utilities\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create test state\n",
        "    test_state = {\n",
        "        \"experiment_id\": None,\n",
        "        \"goal\": {\"scope\": \"portfolio_wide\"},\n",
        "        \"portfolio_summary\": {\n",
        "            \"total_experiments\": 3,\n",
        "            \"completed_count\": 1,\n",
        "            \"running_count\": 1,\n",
        "            \"planned_count\": 1\n",
        "        },\n",
        "        \"portfolio_roi\": {\n",
        "            \"total_cost\": 2250.0,\n",
        "            \"total_revenue_impact\": 14800.0,\n",
        "            \"net_roi\": 12550.0,\n",
        "            \"roi_percent\": 557.8,\n",
        "            \"experiments_with_positive_roi\": 2,\n",
        "            \"experiments_with_negative_roi\": 0\n",
        "        },\n",
        "        \"performance_metrics\": {\n",
        "            \"total_experiments_analyzed\": 3,\n",
        "            \"analysis_success_rate\": 0.667,\n",
        "            \"statistical_tests_performed\": 0,\n",
        "            \"decisions_generated\": 0\n",
        "        },\n",
        "        \"analyzed_experiments\": [],\n",
        "        \"generated_decisions\": [],\n",
        "        \"calculated_analyses\": [],\n",
        "        \"portfolio_insights\": [],\n",
        "        \"processing_time\": 0.05,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Test save\n",
        "    print(\"\\nüìù Testing snapshot save...\")\n",
        "    snapshot_path = save_report_snapshot(test_state)\n",
        "    print(f\"‚úÖ Snapshot saved: {snapshot_path}\")\n",
        "\n",
        "    # Test load\n",
        "    print(\"\\nüìñ Testing snapshot load...\")\n",
        "    loaded = load_latest_snapshot(scope=\"portfolio_wide\")\n",
        "    if loaded:\n",
        "        print(f\"‚úÖ Snapshot loaded: {loaded.get('timestamp')}\")\n",
        "        print(f\"   - Net ROI: ${loaded.get('net_roi', 0):,.2f}\")\n",
        "        print(f\"   - Total Experiments: {loaded.get('total_experiments', 0)}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No snapshot loaded\")\n",
        "\n",
        "    # Test comparison\n",
        "    if loaded:\n",
        "        print(\"\\nüìä Testing snapshot comparison...\")\n",
        "        # Create a modified state for comparison\n",
        "        test_state2 = test_state.copy()\n",
        "        test_state2[\"portfolio_roi\"][\"net_roi\"] = 15000.0  # Increased ROI\n",
        "        test_state2[\"portfolio_summary\"][\"total_experiments\"] = 4  # More experiments\n",
        "\n",
        "        snapshot_path2 = save_report_snapshot(test_state2)\n",
        "        with open(snapshot_path2, 'r') as f:\n",
        "            current_snapshot = json.load(f)\n",
        "\n",
        "        comparison = compare_snapshots(current_snapshot, loaded)\n",
        "\n",
        "        if comparison:\n",
        "            print(\"‚úÖ Comparison generated\")\n",
        "            print(f\"   - Days between: {comparison.get('days_between', 'N/A')}\")\n",
        "\n",
        "            roi_trend = comparison.get(\"trends\", {}).get(\"net_roi\", {})\n",
        "            print(f\"   - ROI Trend: {roi_trend.get('icon')} {roi_trend.get('direction')}\")\n",
        "\n",
        "            roi_change = comparison.get(\"changes\", {}).get(\"net_roi\", {})\n",
        "            print(f\"   - ROI Change: ${roi_change.get('absolute_change', 0):+,.2f} ({roi_change.get('percent_change', 0):+.1f}%)\")\n",
        "\n",
        "    print(\"\\n‚úÖ Snapshot utilities test complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Historical Tracking Tests for EPO Agent\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        test_snapshot_utilities()\n",
        "        test_historical_tracking()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚úÖ ALL HISTORICAL TRACKING TESTS PASSED!\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Test failed: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n"
      ],
      "metadata": {
        "id": "iXjJjNvIxwwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "SK_HGiMdycYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_017_EPO_2.0 % python3 test_epo_e2e.py\n",
        "\n",
        "======================================================================\n",
        "End-to-End Integration Tests for EPO Agent\n",
        "======================================================================\n",
        "\n",
        "======================================================================\n",
        "Test 1: Portfolio-Wide Analysis (Full Workflow)\n",
        "======================================================================\n",
        "\n",
        "üìä Starting portfolio-wide analysis...\n",
        "\n",
        "‚è±Ô∏è  Total processing time: 0.05 seconds\n",
        "\n",
        "‚úÖ No errors in workflow\n",
        "\n",
        "üìà Results Summary:\n",
        "   - Experiments analyzed: 3\n",
        "   - Statistical tests: 0\n",
        "   - Decisions generated: 0\n",
        "   - Portfolio status: 3 total\n",
        "     - Completed: 1\n",
        "     - Running: 1\n",
        "     - Planned: 1\n",
        "\n",
        "üí∞ Portfolio ROI:\n",
        "   - Total Cost: $2,250.00\n",
        "   - Total Revenue Impact: $14,800.00\n",
        "   - Net ROI: $12,550.00\n",
        "   - ROI %: 557.78%\n",
        "   - Positive ROI experiments: 2\n",
        "\n",
        "‚ö° Performance Metrics:\n",
        "   - Analysis success rate: 66.7%\n",
        "   - Statistical tests performed: 0\n",
        "   - Decisions generated: 0\n",
        "\n",
        "üìÑ Report Generated:\n",
        "   - Path: output/experimentation_portfolio_reports/epo_report_epo_report_portfolio_20260118_152607.md\n",
        "   - File exists: ‚úÖ\n",
        "\n",
        "‚úÖ Portfolio-wide E2E test passed!\n",
        "\n",
        "======================================================================\n",
        "Test 2: Single Experiment Analysis (E001)\n",
        "======================================================================\n",
        "\n",
        "üî¨ Starting single experiment analysis for E001...\n",
        "\n",
        "‚è±Ô∏è  Total processing time: 0.00 seconds\n",
        "\n",
        "‚úÖ No errors in workflow\n",
        "\n",
        "üìà Results Summary:\n",
        "\n",
        "üí∞ ROI:\n",
        "   - Total Cost: $850.00\n",
        "   - Net ROI: $9,150.00\n",
        "   - ROI %: 1076.47%\n",
        "\n",
        "üìÑ Report Generated:\n",
        "   - Path: output/experimentation_portfolio_reports/epo_report_epo_report_E001_20260118_152607.md\n",
        "   - File exists: ‚úÖ\n",
        "\n",
        "‚úÖ Single experiment E2E test passed!\n",
        "\n",
        "======================================================================\n",
        "Test 3: State Progression Validation\n",
        "======================================================================\n",
        "\n",
        "‚úÖ All required fields present in final state\n",
        "‚úÖ Data integrity validated: 3 experiments\n",
        "\n",
        "‚úÖ State progression test passed!\n",
        "\n",
        "======================================================================\n",
        "Test 4: Error Handling\n",
        "======================================================================\n",
        "\n",
        "üîç Testing with non-existent experiment ID (E999)...\n",
        "‚úÖ Errors captured: 3\n",
        "   - statistical_analysis_node: definitions_lookup and metrics_lookup required. Run data_loading_node first.\n",
        "   - decision_evaluation_node: definitions_lookup required. Run data_loading_node first.\n",
        "   - roi_calculation_node: analyzed_experiments or experiment_id with analysis required\n",
        "\n",
        "‚úÖ Error handling test passed!\n",
        "\n",
        "======================================================================\n",
        "‚úÖ ALL END-TO-END TESTS PASSED!\n",
        "======================================================================\n",
        "\n",
        "The EPO agent workflow is fully functional and ready for use.\n",
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_017_EPO_2.0 %"
      ],
      "metadata": {
        "id": "TKiYFoEByeFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}