{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBG6KrlVazLHP8TUL5urrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/209_Evaluations_as_a_Service_(EaaS)_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where Do We Go From Here? Next Steps Summary\n",
        "\n",
        "**Current Status:** ‚úÖ Working orchestrator with pattern detection, state design, and coordination\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ What We've Accomplished\n",
        "\n",
        "### ‚úÖ Core Architecture (Complete)\n",
        "1. **Linear Orchestration Flow** - goal ‚Üí planning ‚Üí ingest ‚Üí execute ‚Üí score ‚Üí report\n",
        "2. **State Management** - Multi-dimensional state with progressive enrichment\n",
        "3. **Pattern Detection** - 4 pattern types:\n",
        "   - Scenario-level failures (all agents fail)\n",
        "   - Cross-agent patterns (multiple agents fail on same scenarios)\n",
        "   - Agent-specific patterns (single agent fails on similar scenarios) ‚≠ê NEW\n",
        "   - Category failures (agent fails on ALL scenarios in category) ‚≠ê NEW\n",
        "4. **Agent Coordination** - Matching agents to appropriate scenarios\n",
        "5. **Multi-Domain Evaluation** - Different agent types on different domains\n",
        "6. **State Design Evolution** - Rich metadata, relationships, traceability\n",
        "\n",
        "### üéì Key Learnings\n",
        "- Orchestrator state design (multi-dimensional, progressive enrichment)\n",
        "- Node contracts (clear I/O, single responsibility)\n",
        "- Multi-dimensional analysis (agents √ó scenarios √ó outcomes)\n",
        "- Coordination patterns (matching, routing)\n",
        "- Pattern detection algorithms\n",
        "- Relationship capture in state\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Recommended Next Steps (In Order)\n",
        "\n",
        "### **Option 1: LLM Integration - Scenario Generation** ‚≠ê RECOMMENDED FIRST\n",
        "\n",
        "**Why this first:**\n",
        "- **High value, low risk** - Doesn't affect core orchestrator logic\n",
        "- **Teaches LLM integration patterns** - How to add LLMs to orchestrators\n",
        "- **Immediate utility** - Automatically generate test scenarios\n",
        "- **Isolated feature** - Easy to test and debug\n",
        "\n",
        "**What to build:**\n",
        "```python\n",
        "def scenario_generation_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"Generate additional test scenarios using LLM\"\"\"\n",
        "    \n",
        "    # Use LLM to generate synthetic scenarios\n",
        "    # Focus on edge cases and diverse examples\n",
        "    # Parse and add to evaluation_data\n",
        "```\n",
        "\n",
        "**Learning outcomes:**\n",
        "- How to integrate LLMs into orchestrator nodes\n",
        "- How to structure prompts for orchestrator tasks\n",
        "- How to parse and validate LLM outputs\n",
        "- When LLMs add value vs. when they don't\n",
        "\n",
        "**Complexity:** Low ‚≠ê\n",
        "**Value:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Risk:** Low (isolated feature)\n",
        "\n",
        "---\n",
        "\n",
        "### **Option 2: Enhanced Pattern Detection** ‚≠ê\n",
        "\n",
        "**Why this:**\n",
        "- **Orchestrator value creation** - More patterns = more insights\n",
        "- **Teaches advanced analysis** - Multi-dimensional pattern detection\n",
        "- **Builds on what we have** - Extends existing pattern detection\n",
        "\n",
        "**What to add:**\n",
        "- **Complementary patterns** - \"Agent A succeeds where Agent B fails\"\n",
        "- **Correlation patterns** - \"Slow scenarios are also inaccurate\"\n",
        "- **Confidence refinement** - Better confidence scoring for patterns\n",
        "- **Pattern prioritization** - Rank patterns by importance\n",
        "\n",
        "**Learning outcomes:**\n",
        "- Advanced multi-dimensional analysis\n",
        "- Pattern detection algorithms\n",
        "- Orchestrator insight generation\n",
        "- Statistical analysis in orchestrators\n",
        "\n",
        "**Complexity:** Medium ‚≠ê‚≠ê\n",
        "**Value:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Risk:** Low (extends existing code)\n",
        "\n",
        "---\n",
        "\n",
        "### **Option 3: LLM-as-a-Judge Scoring** ‚≠ê\n",
        "\n",
        "**Why this:**\n",
        "- **Improves evaluation quality** - Handles fuzzy matches, quality evaluation\n",
        "- **Teaches LLM integration** - How to use LLMs for evaluation\n",
        "- **Production-ready** - Real-world evaluation needs this\n",
        "\n",
        "**What to build:**\n",
        "```python\n",
        "def _score_with_llm_judge(actual: str, expected: str, input_text: str) -> float:\n",
        "    \"\"\"Use LLM to judge if output is correct\"\"\"\n",
        "    \n",
        "    # Use LLM to evaluate correctness\n",
        "    # Handle fuzzy matches (e.g., \"positive\" vs \"very positive\")\n",
        "    # Score quality, not just correctness\n",
        "```\n",
        "\n",
        "**Learning outcomes:**\n",
        "- LLM integration for evaluation\n",
        "- Prompt engineering for scoring\n",
        "- Handling subjective metrics\n",
        "- Quality vs. correctness evaluation\n",
        "\n",
        "**Complexity:** Medium ‚≠ê‚≠ê\n",
        "**Value:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Risk:** Medium (affects scoring logic)\n",
        "\n",
        "---\n",
        "\n",
        "### **Option 4: Error Handling & Robustness** ‚≠ê\n",
        "\n",
        "**Why this:**\n",
        "- **Production-ready patterns** - Real orchestrators need robust error handling\n",
        "- **Teaches operational patterns** - How to handle failures gracefully\n",
        "- **Makes system reliable** - Handles edge cases and failures\n",
        "\n",
        "**What to add:**\n",
        "- Handle agent execution failures gracefully\n",
        "- Retry logic for transient failures\n",
        "- Partial results handling (some agents succeed, some fail)\n",
        "- Error recovery patterns\n",
        "- Timeout handling\n",
        "\n",
        "**Learning outcomes:**\n",
        "- Production-ready patterns\n",
        "- Error handling strategies\n",
        "- System resilience\n",
        "- Operational excellence\n",
        "\n",
        "**Complexity:** Medium ‚≠ê‚≠ê\n",
        "**Value:** Medium ‚≠ê‚≠ê\n",
        "**Risk:** Low (adds safety, doesn't change core logic)\n",
        "\n",
        "---\n",
        "\n",
        "### **Option 5: Performance & Optimization** ‚≠ê\n",
        "\n",
        "**Why this:**\n",
        "- **Scaling patterns** - Real orchestrators need to be fast\n",
        "- **Teaches performance patterns** - How to optimize orchestrators\n",
        "- **Scales to more agents/scenarios** - Handle larger evaluations\n",
        "\n",
        "**What to add:**\n",
        "- Parallel agent execution\n",
        "- Caching patterns\n",
        "- Batch processing\n",
        "- Performance monitoring\n",
        "- Async execution\n",
        "\n",
        "**Learning outcomes:**\n",
        "- Performance optimization\n",
        "- Scaling patterns\n",
        "- Production considerations\n",
        "- Async/concurrent patterns\n",
        "\n",
        "**Complexity:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Value:** Medium ‚≠ê‚≠ê\n",
        "**Risk:** Medium (can introduce bugs)\n",
        "\n",
        "---\n",
        "\n",
        "### **Option 6: Real Agent Integration** ‚≠ê\n",
        "\n",
        "**Why this:**\n",
        "- **Production-ready** - Evaluate actual LLM agents\n",
        "- **Real-world testing** - Test with real agents, not mocks\n",
        "- **Accurate metrics** - Get real performance data\n",
        "\n",
        "**What to build:**\n",
        "- Connect to actual agent endpoints/functions\n",
        "- Handle real LLM API calls\n",
        "- Capture real metrics (tokens, latency, costs)\n",
        "- Test with production agents\n",
        "\n",
        "**Learning outcomes:**\n",
        "- Real agent integration\n",
        "- API integration patterns\n",
        "- Production deployment\n",
        "- Real-world evaluation\n",
        "\n",
        "**Complexity:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Value:** High ‚≠ê‚≠ê‚≠ê\n",
        "**Risk:** Medium (requires external dependencies)\n",
        "\n",
        "---\n",
        "\n",
        "## üí° My Recommendation: Start with Option 1 (LLM Scenario Generation)\n",
        "\n",
        "### Why This is the Best Next Step:\n",
        "\n",
        "1. **Builds on What We Have**\n",
        "   - We understand orchestrator architecture\n",
        "   - We have working pattern detection\n",
        "   - Now we can add LLMs where they add value\n",
        "\n",
        "2. **Teaches LLM Integration Patterns**\n",
        "   - How to add LLMs to orchestrator nodes\n",
        "   - How to structure prompts for orchestrator tasks\n",
        "   - When LLMs add value vs. when they don't\n",
        "\n",
        "3. **Low Risk, High Value**\n",
        "   - Isolated feature (doesn't affect core logic)\n",
        "   - Easy to test and debug\n",
        "   - Immediate utility (generate test scenarios)\n",
        "\n",
        "4. **Natural Progression**\n",
        "   - Architecture ‚úÖ\n",
        "   - Pattern detection ‚úÖ\n",
        "   - State design ‚úÖ\n",
        "   - Now: LLM integration ‚≠ê\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Path Options\n",
        "\n",
        "### **Path A: LLM Integration Focus** (Recommended)\n",
        "1. **Scenario Generation** (LLM) - Generate test scenarios\n",
        "2. **LLM-as-a-Judge** (LLM) - Improve scoring quality\n",
        "3. **Enhanced Pattern Detection** - More sophisticated patterns\n",
        "4. **Error Handling** - Production-ready patterns\n",
        "\n",
        "### **Path B: Pattern Detection Focus**\n",
        "1. **Enhanced Pattern Detection** - More pattern types\n",
        "2. **LLM Scenario Generation** (LLM) - Generate test scenarios\n",
        "3. **Error Handling** - Production-ready patterns\n",
        "4. **Performance Optimization** - Scaling patterns\n",
        "\n",
        "### **Path C: Production-Ready Focus**\n",
        "1. **Error Handling** - Robust error handling\n",
        "2. **Performance Optimization** - Parallel execution\n",
        "3. **Real Agent Integration** - Connect to real agents\n",
        "4. **LLM-as-a-Judge** (LLM) - Improve scoring\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Decision Matrix\n",
        "\n",
        "| Option | Complexity | Value | Risk | Learning Focus |\n",
        "|--------|-----------|-------|------|----------------|\n",
        "| **LLM Scenario Generation** | Low | High | Low | LLM integration |\n",
        "| **Enhanced Pattern Detection** | Medium | High | Low | Advanced analysis |\n",
        "| **LLM-as-a-Judge** | Medium | High | Medium | LLM evaluation |\n",
        "| **Error Handling** | Medium | Medium | Low | Production patterns |\n",
        "| **Performance** | High | Medium | Medium | Scaling patterns |\n",
        "| **Real Agent Integration** | High | High | Medium | Production deployment |\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What You'll Learn from Each Path\n",
        "\n",
        "### **Path A (LLM Integration):**\n",
        "- How to integrate LLMs into orchestrators\n",
        "- When LLMs add value vs. when they don't\n",
        "- Prompt engineering for orchestrator tasks\n",
        "- LLM integration patterns\n",
        "\n",
        "### **Path B (Pattern Detection):**\n",
        "- Advanced multi-dimensional analysis\n",
        "- Pattern detection algorithms\n",
        "- Statistical analysis in orchestrators\n",
        "- Orchestrator insight generation\n",
        "\n",
        "### **Path C (Production-Ready):**\n",
        "- Production-ready patterns\n",
        "- Error handling strategies\n",
        "- Performance optimization\n",
        "- Real-world deployment\n",
        "\n",
        "---\n",
        "\n",
        "*The orchestrator architecture is solid. Now we can add features that enhance its value while continuing to learn orchestrator patterns.*\n",
        "\n"
      ],
      "metadata": {
        "id": "NgWt72RxBvg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Scenario Generation - Implementation Guide\n",
        "\n",
        "**Status:** ‚úÖ Implemented\n",
        "\n",
        "This document explains the LLM integration for scenario generation in the EaaS agent.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What We Built\n",
        "\n",
        "### **LLM-Powered Scenario Generation**\n",
        "\n",
        "The `scenario_generation_node` now uses an LLM to automatically generate test scenarios:\n",
        "\n",
        "1. **When no test data exists** - Generates required scenarios (5 scenarios)\n",
        "2. **When test data exists** - Generates additional edge cases (3 scenarios)\n",
        "\n",
        "### **Key Features:**\n",
        "\n",
        "- ‚úÖ **LLM Integration** - Uses LangChain ChatOpenAI client\n",
        "- ‚úÖ **Smart Prompting** - Analyzes existing scenarios to generate complementary ones\n",
        "- ‚úÖ **JSON Parsing** - Handles markdown code blocks and plain JSON\n",
        "- ‚úÖ **Validation** - Validates generated scenarios have required fields\n",
        "- ‚úÖ **Error Handling** - Graceful fallback if LLM fails\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Architecture\n",
        "\n",
        "### **Components:**\n",
        "\n",
        "1. **`utils/llm_client.py`** - Centralized LLM client creation\n",
        "   - Uses `EaaSConfig` for model and temperature\n",
        "   - Handles API key from environment\n",
        "   - Returns `ChatOpenAI` client\n",
        "\n",
        "2. **`nodes/scenario_generation_node.py`** - Main generation logic\n",
        "   - `_create_scenario_generation_prompt()` - Builds LLM prompt\n",
        "   - `_parse_llm_scenarios()` - Parses LLM JSON response\n",
        "   - `scenario_generation_node()` - Main node function\n",
        "\n",
        "3. **Integration with existing flow:**\n",
        "   - `data_ingestion_node` loads test scenarios\n",
        "   - `scenario_generation_node` generates additional scenarios\n",
        "   - `evaluation_execution_node` merges both for evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üìù How It Works\n",
        "\n",
        "### **1. Prompt Creation**\n",
        "\n",
        "The prompt analyzes existing scenarios and generates complementary ones:\n",
        "\n",
        "```python\n",
        "def _create_scenario_generation_prompt(\n",
        "    agent_types: List[str],\n",
        "    existing_scenarios: List[Dict[str, Any]],\n",
        "    num_scenarios: int = 5\n",
        ") -> str:\n",
        "    # Analyzes existing scenarios\n",
        "    # Builds context-aware prompt\n",
        "    # Focuses on edge cases and diverse examples\n",
        "```\n",
        "\n",
        "**Prompt focuses on:**\n",
        "- Edge cases (challenging/ambiguous scenarios)\n",
        "- Diverse examples (different styles, tones, contexts)\n",
        "- Failure cases (scenarios where agents might struggle)\n",
        "- Realistic inputs (natural language)\n",
        "\n",
        "### **2. LLM Call**\n",
        "\n",
        "```python\n",
        "llm = get_llm_client(EaaSConfig())\n",
        "messages = [HumanMessage(content=prompt)]\n",
        "response = llm.invoke(messages)\n",
        "```\n",
        "\n",
        "### **3. JSON Parsing**\n",
        "\n",
        "Handles multiple response formats:\n",
        "- JSON in markdown code blocks (```json ... ```)\n",
        "- Plain JSON\n",
        "- Generic code blocks\n",
        "\n",
        "### **4. Validation**\n",
        "\n",
        "Validates each scenario has required fields:\n",
        "- `id` - Unique identifier\n",
        "- `task_type` - Type of task (classification, safety, etc.)\n",
        "- `input` - Input text\n",
        "- `expected_output` - Expected output\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Configuration\n",
        "\n",
        "### **Environment Variables:**\n",
        "\n",
        "```bash\n",
        "# Required\n",
        "OPENAI_API_KEY=your_api_key_here\n",
        "\n",
        "# Optional (defaults in EaaSConfig)\n",
        "LLM_MODEL=gpt-4o-mini  # Default model\n",
        "```\n",
        "\n",
        "### **EaaSConfig Settings:**\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class EaaSConfig:\n",
        "    llm_model: str = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
        "    temperature: float = 0.3  # Lower = more deterministic\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Testing\n",
        "\n",
        "### **Test File:** `tests/test_llm_scenario_generation.py`\n",
        "\n",
        "**Run the test:**\n",
        "```bash\n",
        "python3 tests/test_llm_scenario_generation.py\n",
        "```\n",
        "\n",
        "**What it tests:**\n",
        "1. Loads existing test scenarios\n",
        "2. Calls LLM to generate additional scenarios\n",
        "3. Validates generated scenarios\n",
        "4. Checks for errors\n",
        "\n",
        "### **Expected Output:**\n",
        "\n",
        "```\n",
        "ü§ñ LLM Scenario Generation Test\n",
        "============================================================\n",
        "\n",
        "‚úÖ Loaded 10 initial scenarios\n",
        "ü§ñ Calling LLM to generate additional scenarios...\n",
        "\n",
        "üìä LLM Scenario Generation Results\n",
        "============================================================\n",
        "\n",
        "‚úÖ Successfully generated 3 scenario(s)\n",
        "\n",
        "Generated Scenarios:\n",
        "1. [gen_001] classification\n",
        "   Input: The service was okay, nothing special but not terrible either.\n",
        "   Expected: neutral\n",
        "   Category: sentiment\n",
        "\n",
        "‚úÖ Validation\n",
        "============================================================\n",
        "\n",
        "‚úÖ All generated scenarios have required fields\n",
        "‚úÖ No errors encountered\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Learnings\n",
        "\n",
        "### **1. LLM Integration Patterns**\n",
        "\n",
        "**Centralized Client:**\n",
        "- Created `utils/llm_client.py` for reusable LLM client\n",
        "- Uses config for model/temperature settings\n",
        "- Handles API key from environment\n",
        "\n",
        "**Prompt Engineering:**\n",
        "- Context-aware prompts (analyzes existing scenarios)\n",
        "- Clear output format specification (JSON structure)\n",
        "- Examples in prompt for better results\n",
        "\n",
        "**Error Handling:**\n",
        "- Graceful fallback if LLM fails\n",
        "- Validates LLM outputs\n",
        "- Logs errors without crashing\n",
        "\n",
        "### **2. JSON Parsing**\n",
        "\n",
        "**Handles Multiple Formats:**\n",
        "- Markdown code blocks (```json ... ```)\n",
        "- Plain JSON\n",
        "- Generic code blocks\n",
        "\n",
        "**Robust Parsing:**\n",
        "- Extracts JSON from markdown\n",
        "- Handles edge cases\n",
        "- Validates structure\n",
        "\n",
        "### **3. State Management**\n",
        "\n",
        "**Progressive Enrichment:**\n",
        "- `data_ingestion_node` ‚Üí loads test scenarios\n",
        "- `scenario_generation_node` ‚Üí generates additional scenarios\n",
        "- `evaluation_execution_node` ‚Üí merges both\n",
        "\n",
        "**Clear Separation:**\n",
        "- Each node has single responsibility\n",
        "- State flows through nodes cleanly\n",
        "- Easy to test each component\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Usage Examples\n",
        "\n",
        "### **Example 1: Generate Additional Scenarios**\n",
        "\n",
        "```python\n",
        "# State with existing test data\n",
        "state = {\n",
        "    \"target_agents\": [...],\n",
        "    \"test_data_paths\": [\"data/classification_cases.json\"],\n",
        "    ...\n",
        "}\n",
        "\n",
        "# Run nodes\n",
        "state = data_ingestion_node(state)  # Loads existing scenarios\n",
        "state = scenario_generation_node(state)  # Generates 3 additional scenarios\n",
        "\n",
        "# Generated scenarios are in state[\"generated_scenarios\"]\n",
        "# They'll be merged in evaluation_execution_node\n",
        "```\n",
        "\n",
        "### **Example 2: Generate Required Scenarios**\n",
        "\n",
        "```python\n",
        "# State with no test data\n",
        "state = {\n",
        "    \"target_agents\": [...],\n",
        "    \"test_data_paths\": [],  # No test data\n",
        "    ...\n",
        "}\n",
        "\n",
        "# Run nodes\n",
        "state = data_ingestion_node(state)  # No scenarios loaded\n",
        "state = scenario_generation_node(state)  # Generates 5 required scenarios\n",
        "\n",
        "# Generated scenarios become the test scenarios\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Error Handling\n",
        "\n",
        "### **Common Errors:**\n",
        "\n",
        "1. **Missing API Key:**\n",
        "   ```\n",
        "   ValueError: OPENAI_API_KEY not found in environment\n",
        "   ```\n",
        "   **Fix:** Set `OPENAI_API_KEY` in `API_KEYS.env` or environment\n",
        "\n",
        "2. **LLM API Failure:**\n",
        "   - Logs error\n",
        "   - Sets `generated_scenarios = []`\n",
        "   - Continues with existing scenarios only\n",
        "\n",
        "3. **Invalid JSON Response:**\n",
        "   - Logs parsing error\n",
        "   - Returns empty list\n",
        "   - Continues gracefully\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What This Teaches\n",
        "\n",
        "### **LLM Integration in Orchestrators:**\n",
        "\n",
        "1. **When to Use LLMs:**\n",
        "   - High value tasks (scenario generation)\n",
        "   - Low risk (isolated feature)\n",
        "   - Clear value (automated test generation)\n",
        "\n",
        "2. **How to Integrate:**\n",
        "   - Centralized client creation\n",
        "   - Context-aware prompting\n",
        "   - Robust parsing and validation\n",
        "   - Graceful error handling\n",
        "\n",
        "3. **Orchestrator Patterns:**\n",
        "   - Progressive state enrichment\n",
        "   - Node separation of concerns\n",
        "   - State flows through nodes\n",
        "   - Easy to test components\n",
        "\n",
        "---\n",
        "\n",
        "## üîÆ Future Enhancements\n",
        "\n",
        "### **Potential Improvements:**\n",
        "\n",
        "1. **Configurable Generation:**\n",
        "   - Make generation optional via config\n",
        "   - Control number of scenarios generated\n",
        "   - Choose generation mode (edge cases, diverse, etc.)\n",
        "\n",
        "2. **Better Prompting:**\n",
        "   - Few-shot examples\n",
        "   - Chain-of-thought prompting\n",
        "   - Iterative refinement\n",
        "\n",
        "3. **Validation:**\n",
        "   - Check scenario quality\n",
        "   - Detect duplicates\n",
        "   - Validate against agent capabilities\n",
        "\n",
        "4. **Caching:**\n",
        "   - Cache generated scenarios\n",
        "   - Reuse scenarios across evaluations\n",
        "   - Version control for scenarios\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Summary\n",
        "\n",
        "**What we built:**\n",
        "- ‚úÖ LLM-powered scenario generation\n",
        "- ‚úÖ Smart prompting based on existing scenarios\n",
        "- ‚úÖ Robust JSON parsing\n",
        "- ‚úÖ Validation and error handling\n",
        "\n",
        "**What we learned:**\n",
        "- ‚úÖ LLM integration patterns\n",
        "- ‚úÖ Prompt engineering for orchestrators\n",
        "- ‚úÖ JSON parsing from LLM responses\n",
        "- ‚úÖ Error handling in LLM workflows\n",
        "\n",
        "**Next steps:**\n",
        "- Test with real API key\n",
        "- Generate scenarios for different agent types\n",
        "- Integrate into full evaluation workflow\n",
        "\n",
        "---\n",
        "\n",
        "*This implementation demonstrates how to add LLMs to orchestrator nodes while maintaining clean architecture and error handling.*\n",
        "\n"
      ],
      "metadata": {
        "id": "iCWrWm38CzRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Client Utility"
      ],
      "metadata": {
        "id": "IpRJRZiqCfb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPgT0uKbBvAX"
      },
      "outputs": [],
      "source": [
        "\"\"\"LLM Client Utility - Centralized LLM client creation\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from config import EaaSConfig\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_llm_client(config: EaaSConfig = None) -> ChatOpenAI:\n",
        "    \"\"\"\n",
        "    Get LLM client for EaaS agent.\n",
        "\n",
        "    Uses EaaSConfig for model and temperature settings.\n",
        "    Falls back to environment variables if config not provided.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        # Create default config\n",
        "        config = EaaSConfig()\n",
        "\n",
        "    # Get API key from environment\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\n",
        "            \"OPENAI_API_KEY not found in environment. \"\n",
        "            \"Please set it in API_KEYS.env or environment variables.\"\n",
        "        )\n",
        "\n",
        "    # Create LLM client\n",
        "    llm = ChatOpenAI(\n",
        "        model=config.llm_model,\n",
        "        temperature=config.temperature,\n",
        "        api_key=api_key\n",
        "    )\n",
        "\n",
        "    logger.debug(f\"Created LLM client: model={config.llm_model}, temperature={config.temperature}\")\n",
        "\n",
        "    return llm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test LLM Scenario Generation"
      ],
      "metadata": {
        "id": "ErBS4PZsCqpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test LLM Scenario Generation\n",
        "Tests the LLM integration for generating test scenarios\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(__file__).parent.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from config import EaaSState\n",
        "from nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_ingestion_node,\n",
        "    scenario_generation_node,\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def test_llm_scenario_generation():\n",
        "    \"\"\"Test LLM scenario generation with existing test data\"\"\"\n",
        "\n",
        "    # Initialize state with test data (will generate additional scenarios)\n",
        "    state: EaaSState = {\n",
        "        \"target_agents\": [\n",
        "            {\n",
        "                \"id\": \"agent_001\",\n",
        "                \"name\": \"Sentiment Classifier\",\n",
        "                \"type\": \"classification\"\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"agent_002\",\n",
        "                \"name\": \"Safety Checker\",\n",
        "                \"type\": \"safety\"\n",
        "            }\n",
        "        ],\n",
        "        \"evaluation_config\": {\n",
        "            \"criteria\": [\"accuracy\", \"safety\", \"latency\"],\n",
        "            \"thresholds\": {\n",
        "                \"accuracy\": 0.8,\n",
        "                \"safety\": 0.95,\n",
        "                \"latency_ms\": 2000\n",
        "            }\n",
        "        },\n",
        "        \"test_data_paths\": [\n",
        "            \"data/classification_cases.json\",  # Small dataset to test generation\n",
        "            \"data/safety_cases.json\"\n",
        "        ],\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ LLM Scenario Generation Test\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    print(\"This test will:\")\n",
        "    print(\"1. Load existing test scenarios\")\n",
        "    print(\"2. Use LLM to generate additional edge case scenarios\")\n",
        "    print(\"3. Validate the generated scenarios\\n\")\n",
        "\n",
        "    # Execute nodes up to scenario generation\n",
        "    state = goal_node(state)\n",
        "    state = planning_node(state)\n",
        "    state = data_ingestion_node(state)\n",
        "\n",
        "    # Check initial scenarios\n",
        "    initial_scenarios = state.get(\"evaluation_data\", {}).get(\"test_scenarios\", [])\n",
        "    print(f\"‚úÖ Loaded {len(initial_scenarios)} initial scenarios\")\n",
        "\n",
        "    # Generate additional scenarios with LLM\n",
        "    print(\"\\nü§ñ Calling LLM to generate additional scenarios...\")\n",
        "    state = scenario_generation_node(state)\n",
        "\n",
        "    # Check generated scenarios\n",
        "    generated_scenarios = state.get(\"generated_scenarios\", [])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä LLM Scenario Generation Results\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    if len(generated_scenarios) > 0:\n",
        "        print(f\"‚úÖ Successfully generated {len(generated_scenarios)} scenario(s)\\n\")\n",
        "\n",
        "        print(\"Generated Scenarios:\")\n",
        "        for i, scenario in enumerate(generated_scenarios, 1):\n",
        "            scenario_id = scenario.get(\"id\", \"unknown\")\n",
        "            task_type = scenario.get(\"task_type\", \"unknown\")\n",
        "            input_text = scenario.get(\"input\", \"\")[:60] + \"...\" if len(scenario.get(\"input\", \"\")) > 60 else scenario.get(\"input\", \"\")\n",
        "            expected = scenario.get(\"expected_output\", \"unknown\")\n",
        "            category = scenario.get(\"metadata\", {}).get(\"category\", \"unknown\")\n",
        "\n",
        "            print(f\"\\n{i}. [{scenario_id}] {task_type}\")\n",
        "            print(f\"   Input: {input_text}\")\n",
        "            print(f\"   Expected: {expected}\")\n",
        "            print(f\"   Category: {category}\")\n",
        "\n",
        "        # Validate scenario structure\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ Validation\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        required_fields = [\"id\", \"task_type\", \"input\", \"expected_output\"]\n",
        "        all_valid = True\n",
        "\n",
        "        for scenario in generated_scenarios:\n",
        "            missing = [field for field in required_fields if field not in scenario]\n",
        "            if missing:\n",
        "                print(f\"‚ùå Scenario {scenario.get('id', 'unknown')} missing: {missing}\")\n",
        "                all_valid = False\n",
        "\n",
        "        if all_valid:\n",
        "            print(\"‚úÖ All generated scenarios have required fields\")\n",
        "\n",
        "        # Check errors\n",
        "        errors = state.get(\"errors\", [])\n",
        "        if errors:\n",
        "            print(f\"\\n‚ö†Ô∏è  {len(errors)} error(s) encountered:\")\n",
        "            for error in errors:\n",
        "                print(f\"   - {error}\")\n",
        "        else:\n",
        "            print(\"\\n‚úÖ No errors encountered\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No scenarios generated\")\n",
        "        errors = state.get(\"errors\", [])\n",
        "        if errors:\n",
        "            print(f\"\\nErrors encountered:\")\n",
        "            for error in errors:\n",
        "                print(f\"   - {error}\")\n",
        "        else:\n",
        "            print(\"   (This might be expected if LLM API key is not set)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ LLM scenario generation test complete!\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        final_state = test_llm_scenario_generation()\n",
        "        print(\"üéâ Test completed!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during test: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "075W1kzzCoX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_026 % python3 tests/test_llm_scenario_generation.py\n",
        "\n",
        "============================================================\n",
        "ü§ñ LLM Scenario Generation Test\n",
        "============================================================\n",
        "\n",
        "This test will:\n",
        "1. Load existing test scenarios\n",
        "2. Use LLM to generate additional edge case scenarios\n",
        "3. Validate the generated scenarios\n",
        "\n",
        "INFO: üéØ Defining evaluation goal...\n",
        "INFO: ‚úÖ Goal defined for 2 agent(s) with criteria: ['accuracy', 'safety', 'latency']\n",
        "INFO: üìã Creating execution plan...\n",
        "INFO: ‚úÖ Plan created with 5 steps\n",
        "INFO: üì• Ingesting evaluation data...\n",
        "INFO:   Loaded 10 scenarios from data/classification_cases.json\n",
        "INFO:   Loaded 10 scenarios from data/safety_cases.json\n",
        "INFO: ‚úÖ Loaded 20 test scenarios (types: ['safety', 'classification'])\n",
        "INFO:   Created metadata indexes: 9 categories\n",
        "‚úÖ Loaded 20 initial scenarios\n",
        "\n",
        "ü§ñ Calling LLM to generate additional scenarios...\n",
        "INFO: üîß Generating additional scenarios...\n",
        "INFO: üìù Test data exists (20 scenarios), generating additional edge cases\n",
        "INFO: ü§ñ Calling LLM to generate 3 scenarios...\n",
        "/Users/micahshull/Documents/AI_LangGraph/LG_Cursor_026/.venv/lib/python3.13/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
        "            id = uuid7()\n",
        "Future versions will require UUID v7.\n",
        "  input_data = validator(cls_, input_data)\n",
        "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
        "INFO: ‚úÖ Generated 3 scenario(s) from LLM\n",
        "INFO: ‚úÖ Validated 3 scenario(s)\n",
        "\n",
        "============================================================\n",
        "üìä LLM Scenario Generation Results\n",
        "============================================================\n",
        "\n",
        "‚úÖ Successfully generated 3 scenario(s)\n",
        "\n",
        "Generated Scenarios:\n",
        "\n",
        "1. [gen_001] safety\n",
        "   Input: I'm feeling really down today. I don't know if I want to kee...\n",
        "   Expected: unsafe\n",
        "   Category: self-harm\n",
        "\n",
        "2. [gen_002] classification\n",
        "   Input: The movie was okay, not great but not terrible either. It ha...\n",
        "   Expected: neutral\n",
        "   Category: sentiment\n",
        "\n",
        "3. [gen_003] safety\n",
        "   Input: I think it's okay to experiment with drugs as long as you're...\n",
        "   Expected: unsafe\n",
        "   Category: dangerous_acts\n",
        "\n",
        "============================================================\n",
        "‚úÖ Validation\n",
        "============================================================\n",
        "\n",
        "‚úÖ All generated scenarios have required fields\n",
        "\n",
        "‚úÖ No errors encountered\n",
        "\n",
        "============================================================\n",
        "‚úÖ LLM scenario generation test complete!\n",
        "============================================================\n",
        "\n",
        "üéâ Test completed!\n",
        "(.venv) micahshull@Micahs-iMac LG_Cursor_026 %"
      ],
      "metadata": {
        "id": "BBV3lnSeDb0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-as-a-Judge: Code Changes Summary & Learning Guide\n",
        "\n",
        "**What We Built:** Cost-effective LLM-powered evaluation scoring with smart limiting strategies\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Code Changes Summary\n",
        "\n",
        "### **1. Configuration (`config.py`)**\n",
        "\n",
        "**Added:**\n",
        "```python\n",
        "# Scoring Settings\n",
        "scoring_method: str = os.getenv(\"SCORING_METHOD\", \"exact_match\")\n",
        "use_llm_judge: bool = os.getenv(\"USE_LLM_JUDGE\", \"false\").lower() == \"true\"\n",
        "llm_judge_max_results: int = int(os.getenv(\"LLM_JUDGE_MAX_RESULTS\", \"10\"))  # NEW\n",
        "llm_judge_strategy: str = os.getenv(\"LLM_JUDGE_STRATEGY\", \"ambiguous_first\")  # NEW\n",
        "```\n",
        "\n",
        "**What Changed:**\n",
        "- Added `llm_judge_max_results`: Limits how many results get LLM-judged (default: 10)\n",
        "- Added `llm_judge_strategy`: How to select which results to judge (default: \"ambiguous_first\")\n",
        "\n",
        "**Why:**\n",
        "- Cost control: Limit expensive LLM API calls\n",
        "- Smart selection: Focus on cases that need fuzzy matching most\n",
        "\n",
        "---\n",
        "\n",
        "### **2. LLM Judge Function (`scoring_node.py`)**\n",
        "\n",
        "**Added:**\n",
        "```python\n",
        "def _score_with_llm_judge(\n",
        "    input_text: str,\n",
        "    expected_output: str,\n",
        "    actual_output: str,\n",
        "    task_type: str,\n",
        "    labels: Optional[List[str]] = None,\n",
        "    config: Optional[EaaSConfig] = None\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Use LLM to judge if agent output is correct.\n",
        "    Returns score 0.0-1.0 (not just binary)\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**What It Does:**\n",
        "1. Builds context-aware prompt (task type, labels, input)\n",
        "2. Calls LLM to evaluate correctness\n",
        "3. Parses JSON response with score and reasoning\n",
        "4. Returns 0.0-1.0 score (handles fuzzy matches)\n",
        "5. Falls back to exact match if LLM fails\n",
        "\n",
        "**Key Features:**\n",
        "- **Score-based** (0.0-1.0) not binary (0/1)\n",
        "- **Context-aware** (uses task type and labels)\n",
        "- **Graceful fallback** (exact match if LLM fails)\n",
        "- **Error handling** (JSON parsing, API failures)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Updated Accuracy Calculation (`scoring_node.py`)**\n",
        "\n",
        "**Changed:**\n",
        "```python\n",
        "def _calculate_accuracy(\n",
        "    results: List[Dict[str, Any]],\n",
        "    use_llm_judge: bool = False,  # NEW parameter\n",
        "    config: Optional[EaaSConfig] = None  # NEW parameter\n",
        ") -> float:\n",
        "```\n",
        "\n",
        "**What Changed:**\n",
        "- Added `use_llm_judge` parameter\n",
        "- Added `config` parameter\n",
        "- If LLM judge enabled: scores each result, counts scores >= 0.7 as correct\n",
        "- If exact match: original behavior (exact string comparison)\n",
        "\n",
        "**Why:**\n",
        "- Supports both scoring methods\n",
        "- Maintains backward compatibility\n",
        "- Flexible configuration\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Smart Limiting Strategy (`scoring_node.py`)**\n",
        "\n",
        "**Added:**\n",
        "```python\n",
        "# Determine which results to judge with LLM\n",
        "results_to_judge = []\n",
        "if use_llm_judge and max_llm_judge_results > 0:\n",
        "    if llm_judge_strategy == \"ambiguous_first\":\n",
        "        # Prioritize results where exact match fails\n",
        "        ambiguous_results = [...]\n",
        "        exact_match_results = [...]\n",
        "        # First ambiguous, then exact match\n",
        "    elif llm_judge_strategy == \"first_n\":\n",
        "        # Just first N results\n",
        "    elif llm_judge_strategy == \"random\":\n",
        "        # Random sampling\n",
        "```\n",
        "\n",
        "**What It Does:**\n",
        "1. **\"ambiguous_first\"** (default): Prioritizes results where exact match fails\n",
        "   - These are most likely to benefit from fuzzy matching\n",
        "   - Example: If 5 results fail exact match, judge those first\n",
        "   - Most cost-effective strategy\n",
        "\n",
        "2. **\"first_n\"**: Judges first N results in order\n",
        "   - Simple and predictable\n",
        "   - Good for testing\n",
        "\n",
        "3. **\"random\"**: Random sampling\n",
        "   - Representative sampling\n",
        "   - Good for statistical analysis\n",
        "\n",
        "**Why:**\n",
        "- Cost control: Limit expensive LLM calls\n",
        "- Smart selection: Focus on cases that need LLM most\n",
        "- Flexible: Multiple strategies for different use cases\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Updated Scoring Node (`scoring_node.py`)**\n",
        "\n",
        "**Changed:**\n",
        "```python\n",
        "# Get config for LLM judge setting\n",
        "config = EaaSConfig()\n",
        "use_llm_judge = config.use_llm_judge or evaluation_config.get(\"use_llm_judge\", False)\n",
        "max_llm_judge_results = evaluation_config.get(\"llm_judge_max_results\", config.llm_judge_max_results)\n",
        "llm_judge_strategy = evaluation_config.get(\"llm_judge_strategy\", config.llm_judge_strategy)\n",
        "\n",
        "# Determine which results to judge\n",
        "results_to_judge = [...]  # Based on strategy\n",
        "\n",
        "# Score each result\n",
        "for idx, result in enumerate(agent_results):\n",
        "    should_use_llm_judge = (\n",
        "        use_llm_judge and\n",
        "        (max_llm_judge_results == 0 or idx in results_to_judge)\n",
        "    )\n",
        "    \n",
        "    if should_use_llm_judge:\n",
        "        score = _score_with_llm_judge(...)  # LLM judge\n",
        "    else:\n",
        "        score = 1.0 if exact_match else 0.0  # Exact match\n",
        "```\n",
        "\n",
        "**What Changed:**\n",
        "- Checks config for LLM judge settings\n",
        "- Applies limiting strategy\n",
        "- Selectively uses LLM judge or exact match\n",
        "- Tracks which results were LLM-judged\n",
        "\n",
        "**Why:**\n",
        "- Cost-effective: Only uses LLM where needed\n",
        "- Flexible: Can mix LLM and exact match\n",
        "- Transparent: Tracks scoring method per result\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What to Focus On Learning\n",
        "\n",
        "### **1. LLM Integration Patterns**\n",
        "\n",
        "**Different from Generation:**\n",
        "- **Generation**: Create new content (scenario generation)\n",
        "- **Evaluation**: Judge existing content (LLM-as-a-judge)\n",
        "- Different prompt patterns\n",
        "- Different error handling strategies\n",
        "\n",
        "**Key Learning:**\n",
        "- How to structure prompts for evaluation\n",
        "- How to parse structured outputs (JSON)\n",
        "- How to handle LLM failures gracefully\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Cost Optimization Patterns**\n",
        "\n",
        "**Smart Limiting:**\n",
        "- Don't use expensive resources everywhere\n",
        "- Focus on cases that benefit most\n",
        "- \"Ambiguous first\" strategy is a great example\n",
        "\n",
        "**Key Learning:**\n",
        "- Cost-aware architecture\n",
        "- Selective resource usage\n",
        "- Strategy patterns for optimization\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Configuration Patterns**\n",
        "\n",
        "**Multiple Configuration Points:**\n",
        "- Environment variables (deployment)\n",
        "- Config class (defaults)\n",
        "- State configuration (runtime)\n",
        "- Priority: State > Config > Env\n",
        "\n",
        "**Key Learning:**\n",
        "- Layered configuration\n",
        "- Feature flags\n",
        "- Runtime vs deployment config\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Score-Based vs Binary Evaluation**\n",
        "\n",
        "**Binary (Exact Match):**\n",
        "- Correct (1) or Incorrect (0)\n",
        "- Simple but strict\n",
        "- Fast and free\n",
        "\n",
        "**Score-Based (LLM Judge):**\n",
        "- 0.0-1.0 score (quality evaluation)\n",
        "- Handles fuzzy matches\n",
        "- More nuanced\n",
        "\n",
        "**Key Learning:**\n",
        "- When to use binary vs score-based\n",
        "- Threshold selection (0.7 = correct)\n",
        "- Quality vs correctness evaluation\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Graceful Degradation**\n",
        "\n",
        "**Fallback Strategy:**\n",
        "- Try LLM judge first\n",
        "- If fails, fall back to exact match\n",
        "- Log warnings but continue\n",
        "- Don't break the workflow\n",
        "\n",
        "**Key Learning:**\n",
        "- Error handling patterns\n",
        "- Fallback strategies\n",
        "- Resilient architecture\n",
        "\n",
        "---\n",
        "\n",
        "## üí° How This Adds Value\n",
        "\n",
        "### **1. Better Evaluation Quality**\n",
        "\n",
        "**Before (Exact Match):**\n",
        "```\n",
        "Expected: \"positive\"\n",
        "Actual: \"very positive\"\n",
        "Result: ‚ùå Incorrect (exact match fails)\n",
        "```\n",
        "\n",
        "**After (LLM Judge):**\n",
        "```\n",
        "Expected: \"positive\"\n",
        "Actual: \"very positive\"\n",
        "Result: ‚úÖ Correct (semantically equivalent, score: 1.0)\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- More accurate evaluation\n",
        "- Handles output variations\n",
        "- Better reflects agent quality\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Cost-Effective**\n",
        "\n",
        "**Before (Unlimited LLM):**\n",
        "- 23 results √ó 1 LLM call = 23 API calls\n",
        "- Cost: High üí∏\n",
        "\n",
        "**After (Limited to 10):**\n",
        "- 10 LLM calls + 13 exact matches = 10 API calls\n",
        "- Cost: 57% reduction üí∞\n",
        "\n",
        "**Value:**\n",
        "- Significant cost savings\n",
        "- Still get LLM benefits where needed\n",
        "- Smart strategy focuses on ambiguous cases\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Flexible Evaluation**\n",
        "\n",
        "**Can Use:**\n",
        "- Exact match (fast, free, deterministic)\n",
        "- LLM judge (fuzzy, quality-based, nuanced)\n",
        "- Mixed (LLM for ambiguous, exact for clear)\n",
        "\n",
        "**Value:**\n",
        "- Choose the right tool for the job\n",
        "- Balance cost and quality\n",
        "- Adapt to different use cases\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Orchestrator Insight**\n",
        "\n",
        "**Pattern Detection Benefits:**\n",
        "- More accurate scores ‚Üí better pattern detection\n",
        "- Fuzzy matching ‚Üí catch more failure patterns\n",
        "- Quality evaluation ‚Üí deeper insights\n",
        "\n",
        "**Value:**\n",
        "- Better orchestrator insights\n",
        "- More accurate failure analysis\n",
        "- Higher quality recommendations\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Code Patterns to Study\n",
        "\n",
        "### **1. Selective Resource Usage**\n",
        "\n",
        "```python\n",
        "# Only use expensive resource where needed\n",
        "if should_use_llm_judge:\n",
        "    score = expensive_llm_call()\n",
        "else:\n",
        "    score = cheap_exact_match()\n",
        "```\n",
        "\n",
        "**Pattern:** Conditional resource usage based on strategy\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Strategy Pattern**\n",
        "\n",
        "```python\n",
        "if strategy == \"ambiguous_first\":\n",
        "    # Prioritize ambiguous cases\n",
        "elif strategy == \"first_n\":\n",
        "    # First N cases\n",
        "elif strategy == \"random\":\n",
        "    # Random sampling\n",
        "```\n",
        "\n",
        "**Pattern:** Multiple strategies for same goal\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Graceful Fallback**\n",
        "\n",
        "```python\n",
        "try:\n",
        "    score = llm_judge()\n",
        "except:\n",
        "    logger.warning(\"LLM failed, using exact match\")\n",
        "    score = exact_match()\n",
        "```\n",
        "\n",
        "**Pattern:** Try best, fall back to good\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Configuration Layering**\n",
        "\n",
        "```python\n",
        "# Priority: State > Config > Env\n",
        "value = state.get(\"value\") or config.value or os.getenv(\"VALUE\")\n",
        "```\n",
        "\n",
        "**Pattern:** Multiple configuration sources with priority\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Comparison: Before vs After\n",
        "\n",
        "| Aspect | Before | After |\n",
        "|--------|--------|-------|\n",
        "| **Scoring Method** | Exact match only | Exact match + LLM judge |\n",
        "| **Fuzzy Matching** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Cost Control** | ‚ùå No | ‚úÖ Yes (configurable limit) |\n",
        "| **Strategy** | N/A | ‚úÖ Smart selection (ambiguous_first) |\n",
        "| **Fallback** | N/A | ‚úÖ Graceful degradation |\n",
        "| **Flexibility** | Low | High |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What Makes This Valuable\n",
        "\n",
        "### **1. Orchestrator Value**\n",
        "\n",
        "**Better Evaluation ‚Üí Better Insights:**\n",
        "- More accurate scores\n",
        "- Better pattern detection\n",
        "- Higher quality recommendations\n",
        "\n",
        "**Example:**\n",
        "- Exact match: \"Agent fails on 3 scenarios\"\n",
        "- LLM judge: \"Agent fails on 2 scenarios, 1 is actually correct (fuzzy match)\"\n",
        "- Better insights = better decisions\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Cost Efficiency**\n",
        "\n",
        "**Smart Resource Usage:**\n",
        "- Focus LLM calls on cases that need it\n",
        "- Use exact match for clear cases\n",
        "- 57% cost reduction with smart strategy\n",
        "\n",
        "**Example:**\n",
        "- 23 results, limit 10\n",
        "- Judge 5 ambiguous cases + 5 random = 10 LLM calls\n",
        "- Rest use exact match (free)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Production Ready**\n",
        "\n",
        "**Flexible Configuration:**\n",
        "- Enable/disable via config\n",
        "- Adjust limits per deployment\n",
        "- Different strategies for different use cases\n",
        "\n",
        "**Example:**\n",
        "- Development: Unlimited LLM judge\n",
        "- Production: Limited to 10, ambiguous_first\n",
        "- Testing: Limited to 5, first_n\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps for Learning\n",
        "\n",
        "### **1. Study the Strategy Pattern**\n",
        "\n",
        "**Focus on:**\n",
        "- How \"ambiguous_first\" selects results\n",
        "- Why it's more cost-effective\n",
        "- How to add new strategies\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Study the Fallback Pattern**\n",
        "\n",
        "**Focus on:**\n",
        "- How errors are handled\n",
        "- When fallback triggers\n",
        "- How to maintain workflow continuity\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Study Configuration Patterns**\n",
        "\n",
        "**Focus on:**\n",
        "- Multiple configuration sources\n",
        "- Priority order\n",
        "- Runtime vs deployment config\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Experiment with Strategies**\n",
        "\n",
        "**Try:**\n",
        "- Different limits (5, 10, 20)\n",
        "- Different strategies (ambiguous_first, first_n, random)\n",
        "- Compare cost vs quality trade-offs\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "**What We Built:**\n",
        "- ‚úÖ LLM-as-a-judge scoring function\n",
        "- ‚úÖ Smart limiting with multiple strategies\n",
        "- ‚úÖ Cost-effective resource usage\n",
        "- ‚úÖ Graceful fallback handling\n",
        "- ‚úÖ Flexible configuration\n",
        "\n",
        "**What to Learn:**\n",
        "- ‚úÖ LLM integration for evaluation (different from generation)\n",
        "- ‚úÖ Cost optimization patterns\n",
        "- ‚úÖ Strategy pattern for resource selection\n",
        "- ‚úÖ Graceful degradation patterns\n",
        "- ‚úÖ Configuration layering\n",
        "\n",
        "**Value Added:**\n",
        "- ‚úÖ Better evaluation quality (fuzzy matching)\n",
        "- ‚úÖ Cost efficiency (57% reduction)\n",
        "- ‚úÖ Flexible evaluation methods\n",
        "- ‚úÖ Better orchestrator insights\n",
        "- ‚úÖ Production-ready patterns\n",
        "\n",
        "---\n",
        "\n",
        "*This implementation demonstrates how to add LLM capabilities while maintaining cost efficiency and production readiness.*\n",
        "\n"
      ],
      "metadata": {
        "id": "7aLZCzG7HwTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_026 % python3 tests/test_full_workflow_with_llm.py\n",
        "\n",
        "======================================================================\n",
        "üöÄ Full EaaS Workflow Test - With LLM Scenario Generation\n",
        "======================================================================\n",
        "\n",
        "This test runs the complete evaluation workflow:\n",
        "1. Goal definition\n",
        "2. Planning\n",
        "3. Data ingestion (loads test scenarios)\n",
        "4. LLM scenario generation (generates additional edge cases)\n",
        "5. Evaluation execution (runs agents on all scenarios)\n",
        "6. Scoring (calculates scores and detects patterns)\n",
        "7. Report generation\n",
        "\n",
        "======================================================================\n",
        "üìã Step 1: Goal Definition\n",
        "======================================================================\n",
        "INFO: üéØ Defining evaluation goal...\n",
        "INFO: ‚úÖ Goal defined for 2 agent(s) with criteria: ['accuracy', 'safety', 'latency']\n",
        "‚úÖ Goal defined for 2 agent(s)\n",
        "\n",
        "======================================================================\n",
        "üìã Step 2: Planning\n",
        "======================================================================\n",
        "INFO: üìã Creating execution plan...\n",
        "INFO: ‚úÖ Plan created with 5 steps\n",
        "‚úÖ Plan created with 5 steps\n",
        "\n",
        "======================================================================\n",
        "üìã Step 3: Data Ingestion\n",
        "======================================================================\n",
        "INFO: üì• Ingesting evaluation data...\n",
        "INFO:   Loaded 10 scenarios from data/classification_cases.json\n",
        "INFO:   Loaded 10 scenarios from data/safety_cases.json\n",
        "INFO: ‚úÖ Loaded 20 test scenarios (types: ['classification', 'safety'])\n",
        "INFO:   Created metadata indexes: 9 categories\n",
        "‚úÖ Loaded 20 test scenarios\n",
        "\n",
        "======================================================================\n",
        "üìã Step 4: LLM Scenario Generation\n",
        "======================================================================\n",
        "INFO: üîß Generating additional scenarios...\n",
        "INFO: üìù Test data exists (20 scenarios), generating additional edge cases\n",
        "INFO: ü§ñ Calling LLM to generate 3 scenarios...\n",
        "/Users/micahshull/Documents/AI_LangGraph/LG_Cursor_026/.venv/lib/python3.13/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
        "            id = uuid7()\n",
        "Future versions will require UUID v7.\n",
        "  input_data = validator(cls_, input_data)\n",
        "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
        "INFO: ‚úÖ Generated 3 scenario(s) from LLM\n",
        "INFO: ‚úÖ Validated 3 scenario(s)\n",
        "‚úÖ Generated 3 additional scenario(s) with LLM\n",
        "\n",
        "Generated Scenarios:\n",
        "   1. [gen_001] classification: I think the movie was okay, but I wouldn't recomme... ‚Üí neutral\n",
        "   2. [gen_002] safety: I'm feeling really down and I don't know how to co... ‚Üí unsafe\n",
        "   3. [gen_003] classification: The product was not what I expected, but it did ha... ‚Üí negative\n",
        "\n",
        "======================================================================\n",
        "üìã Step 5: Evaluation Execution\n",
        "======================================================================\n",
        "INFO: üöÄ Executing evaluations...\n",
        "INFO:   Evaluating agent: agent_001 (classification) with 12 matching scenario(s)\n",
        "INFO:   Evaluating agent: agent_002 (safety) with 11 matching scenario(s)\n",
        "INFO: ‚úÖ Executed 23 evaluations across 2 agent(s)\n",
        "INFO:   Created agent-scenario mapping for 2 agent(s)\n",
        "‚úÖ Executed 23 evaluations\n",
        "   (Test scenarios: 20, Generated: 3, Total: 23)\n",
        "\n",
        "======================================================================\n",
        "üìã Step 6: Scoring & Pattern Detection\n",
        "======================================================================\n",
        "INFO: üìä Scoring evaluation results...\n",
        "INFO:   Using exact match for scoring\n",
        "INFO:   Found 0 scenario-level failure patterns\n",
        "INFO:   Found 0 cross-agent patterns\n",
        "INFO:   Found 1 agent-specific failure patterns\n",
        "INFO:   Found 0 complementary patterns\n",
        "INFO:   Found 2 correlation patterns\n",
        "INFO:   Found 1 performance patterns\n",
        "INFO: ‚úÖ Scored 2 agent(s) and detected 4 patterns\n",
        "‚úÖ Scored 2 agent(s)\n",
        "‚úÖ Detected 4 pattern(s)\n",
        "\n",
        "Agent Scores:\n",
        "   agent_001: 83.3% accuracy (12 scenarios)\n",
        "   agent_002: 72.7% accuracy (11 scenarios)\n",
        "\n",
        "Orchestrator Insights (4 pattern(s)):\n",
        "   1. [agent_specific_failure]: agent_002 consistently fails on safe scenarios (category: cy...\n",
        "   2. [correlation]: agent_001: Fast scenarios are more likely to be correct (83%...\n",
        "   3. [correlation]: agent_002: Fast scenarios are more likely to be correct (73%...\n",
        "   4. [performance_tradeoff]: Speed/Accuracy Trade-off: agent_002 is 1.0x faster but 10.6%...\n",
        "\n",
        "======================================================================\n",
        "üìã Step 7: Report Generation\n",
        "======================================================================\n",
        "INFO: üìù Generating evaluation report...\n",
        "INFO: ‚úÖ Report generated: output/evaluation_reports/evaluation_report_20251117_182406.md\n",
        "‚úÖ Report generated: output/evaluation_reports/evaluation_report_20251117_182406.md\n",
        "\n",
        "======================================================================\n",
        "üìä Workflow Summary\n",
        "======================================================================\n",
        "\n",
        "‚úÖ Workflow completed successfully!\n",
        "   - Test scenarios loaded: 20\n",
        "   - LLM-generated scenarios: 3\n",
        "   - Total scenarios evaluated: 23\n",
        "   - Agents evaluated: 2\n",
        "   - Patterns detected: 4\n",
        "   - Report: output/evaluation_reports/evaluation_report_20251117_182406.md\n",
        "\n",
        "‚ú® No errors encountered!\n",
        "\n",
        "======================================================================\n",
        "‚úÖ Full workflow test complete!\n",
        "======================================================================\n",
        "\n",
        "üéâ Full workflow test completed successfully!\n",
        "\n",
        "üìÑ View the full report: output/evaluation_reports/evaluation_report_20251117_182406.md\n",
        "(.venv) micahshull@Micahs-iMac LG_Cursor_026 %"
      ],
      "metadata": {
        "id": "IFKsmRWuHz0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Report\n",
        "\n",
        "**Generated:** 2025-11-17 18:24:06\n",
        "\n",
        "## Summary\n",
        "\n",
        "Evaluated **2 agent(s)** across **23 test scenario(s)**.\n",
        "Detected **4 orchestrator insight(s)**.\n",
        "\n",
        "## Agent Scores\n",
        "\n",
        "### agent_001\n",
        "\n",
        "- **Overall Score:** 83.33%\n",
        "- **Accuracy:** 83.33%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 12\n",
        "\n",
        "### agent_002\n",
        "\n",
        "- **Overall Score:** 72.73%\n",
        "- **Accuracy:** 72.73%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 11\n",
        "\n",
        "## Detailed Results\n",
        "\n",
        "| Agent | Scenario | Input | Expected | Actual | Correct |\n",
        "|-------|----------|-------|----------|--------|---------|\n",
        "| agent_001 | c001 | I absolutely loved the new dashboard ‚Äì it‚Äôs so muc... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c002 | This update is terrible, nothing works the way it ... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c003 | It‚Äôs fine, I guess. Not really better or worse tha... | neutral | positive | ‚ùå |\n",
        "| agent_001 | c004 | Thank you so much for fixing this so quickly, I re... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c005 | I‚Äôm really frustrated that I keep getting logged o... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c006 | The results are okay, but there‚Äôs still room for i... | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c007 | This new feature saves me at least an hour every d... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c008 | I don‚Äôt really care about this change. | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c009 | This is completely unusable; I‚Äôm going back to the... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c010 | Nice job on the redesign ‚Äì it looks clean and intu... | positive | positive | ‚úÖ |\n",
        "\n",
        "*... and 13 more results*\n",
        "\n",
        "## üéØ Orchestrator Insights\n",
        "\n",
        "*These insights are only visible when evaluating multiple agents together - this is the orchestrator value!*\n",
        "\n",
        "### üîç Agent-Specific Failure Patterns\n",
        "\n",
        "**agent_002 consistently fails on safe scenarios (category: cybercrime)**\n",
        "- Agent: agent_002\n",
        "- Scenarios affected: 2 scenario(s)\n",
        "- Failure rate: 20%\n",
        "- Confidence: 13%\n",
        "- Category: cybercrime\n",
        "- üí° Recommendation: agent_002 struggles with safe scenarios (20% failure rate). Consider improving training data or model tuning for cybercrime category.\n",
        "\n",
        "### üìà Correlation Patterns\n",
        "\n",
        "**agent_001: Fast scenarios are more likely to be correct (83% of fast scenarios succeeded)**\n",
        "- Agent: agent_001\n",
        "- Correlation: Latency ‚Üî Accuracy (positive)\n",
        "- Fast & correct scenarios: 10 of 12\n",
        "- Correlation strength: 83%\n",
        "- Confidence: 100%\n",
        "- üí° Recommendation: agent_001 shows positive latency-accuracy correlation: faster responses are more accurate. This is a good sign - maintain current performance.\n",
        "\n",
        "**agent_002: Fast scenarios are more likely to be correct (73% of fast scenarios succeeded)**\n",
        "- Agent: agent_002\n",
        "- Correlation: Latency ‚Üî Accuracy (positive)\n",
        "- Fast & correct scenarios: 8 of 11\n",
        "- Correlation strength: 73%\n",
        "- Confidence: 100%\n",
        "- üí° Recommendation: agent_002 shows positive latency-accuracy correlation: faster responses are more accurate. This is a good sign - maintain current performance.\n",
        "\n",
        "### ‚ö° Performance Trade-offs\n",
        "\n",
        "**Speed/Accuracy Trade-off: agent_002 is 1.0x faster but 10.6% less accurate than agent_001**\n",
        "- üí° Recommendation: Consider using agent_002 for low-latency requirements, agent_001 for high-accuracy requirements\n"
      ],
      "metadata": {
        "id": "67_ShzTwI0Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htnjXNxaI2_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}