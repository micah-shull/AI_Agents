{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9LXtDA4fzBMJVQC9HO3Mv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/525_IRMOv2_riskAnalysis_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Risk Analysis Utilities – Turning Risk Into a Programmable, Explainable Model\n",
        "\n",
        "These utilities define how the orchestrator evaluates risk across the AI ecosystem.\n",
        "\n",
        "Rather than treating risk as a vague label or subjective judgment, this module converts risk into a **transparent, weighted scoring model** that leadership can understand, audit, and adjust.\n",
        "\n",
        "This is not probabilistic risk estimation.\n",
        "It is **policy-driven risk evaluation**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Design Matters\n",
        "\n",
        "Most AI agents treat risk as:\n",
        "\n",
        "* A label (“high”, “medium”, “low”)\n",
        "* A heuristic\n",
        "* Or an opaque model output\n",
        "\n",
        "That makes them hard to trust and impossible to govern.\n",
        "\n",
        "This agent does the opposite:\n",
        "\n",
        "* Every component of risk is explicit\n",
        "* Every score is traceable\n",
        "* Every trade-off is visible\n",
        "\n",
        "Executives can ask *why* a risk is critical — and get a clear answer.\n",
        "\n",
        "---\n",
        "\n",
        "## Normalization: Making Qualitative Signals Comparable\n",
        "\n",
        "### Severity & Criticality Normalization\n",
        "\n",
        "Human-readable labels like “high” or “medium” are useful, but they are not computable.\n",
        "\n",
        "These utilities convert:\n",
        "\n",
        "* **Risk severity**\n",
        "* **Agent criticality**\n",
        "\n",
        "into consistent 0–100 numeric scores.\n",
        "\n",
        "This allows:\n",
        "\n",
        "* Fair comparisons across risks\n",
        "* Stable scoring logic\n",
        "* Clear weighting without ambiguity\n",
        "\n",
        "If leadership disagrees with how “high” maps to impact, they can change the mapping — not the logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Time-Based Urgency: Risk That Ages Gets Louder\n",
        "\n",
        "### Urgency Calculation\n",
        "\n",
        "Risk is not static.\n",
        "\n",
        "A medium issue ignored for 30 days can be more dangerous than a new critical one.\n",
        "\n",
        "The urgency calculation:\n",
        "\n",
        "* Increases risk weight as time passes\n",
        "* Prevents “permanent medium” issues from being ignored\n",
        "* Reflects operational reality\n",
        "\n",
        "This ensures the agent escalates based on **inaction**, not just severity.\n",
        "\n",
        "That’s a key executive concern.\n",
        "\n",
        "---\n",
        "\n",
        "## Impact Modeling: Not All Risks Are Equal\n",
        "\n",
        "### Risk Type → Impact Mapping\n",
        "\n",
        "Different risks hurt in different ways.\n",
        "\n",
        "This model explicitly encodes that reality:\n",
        "\n",
        "* Integration risks have the highest impact\n",
        "* Security and compliance are weighted heavily\n",
        "* Cost and operational risks still matter, but differently\n",
        "\n",
        "Nothing is hidden.\n",
        "\n",
        "These are **business judgments**, encoded directly into the system.\n",
        "\n",
        "---\n",
        "\n",
        "## Weighted Risk Scoring: Transparent Trade-Offs\n",
        "\n",
        "### The Core Risk Equation\n",
        "\n",
        "Each risk score is computed using four visible factors:\n",
        "\n",
        "* Severity\n",
        "* Agent criticality\n",
        "* Business impact\n",
        "* Time-based urgency\n",
        "\n",
        "Each factor has an explicit weight.\n",
        "\n",
        "This means:\n",
        "\n",
        "* Risk prioritization is explainable\n",
        "* Trade-offs are intentional\n",
        "* Scores are consistent across runs\n",
        "\n",
        "There is no hidden intelligence deciding what matters.\n",
        "\n",
        "Leadership already decided — the agent simply applies it.\n",
        "\n",
        "---\n",
        "\n",
        "## Agent-Level Risk Assessment: From Signals to Decisions\n",
        "\n",
        "### Aggregation & Classification\n",
        "\n",
        "For each agent, risks are:\n",
        "\n",
        "* Grouped by type\n",
        "* Scored individually\n",
        "* Aggregated into a total risk score\n",
        "\n",
        "That score is then classified into:\n",
        "\n",
        "* Medium\n",
        "* High\n",
        "* Critical\n",
        "\n",
        "The thresholds are stable and predictable.\n",
        "\n",
        "No surprises. No drifting behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## Priority Actions: Focusing Attention Where It Matters\n",
        "\n",
        "Instead of overwhelming users with every issue, the agent surfaces:\n",
        "\n",
        "* The top 3 highest-scoring risks per agent\n",
        "* With scores and context attached\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* Executive attention is focused\n",
        "* Teams know where to act first\n",
        "* Risk discussions are grounded in evidence\n",
        "\n",
        "This is risk management, not risk reporting.\n",
        "\n",
        "---\n",
        "\n",
        "## Portfolio-Wide Consistency\n",
        "\n",
        "The `analyze_all_agent_risks` function applies the same logic uniformly across all agents.\n",
        "\n",
        "There are:\n",
        "\n",
        "* No special cases\n",
        "* No dynamic heuristics\n",
        "* No agent-specific hacks\n",
        "\n",
        "This makes portfolio-level comparisons fair and defensible.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Executives Care About This Approach\n",
        "\n",
        "This risk model ensures the agent is:\n",
        "\n",
        "* **Reliable** — same inputs, same outputs\n",
        "* **Predictable** — thresholds and weights define behavior\n",
        "* **Explainable** — every score can be decomposed\n",
        "* **Programmable** — leadership intent is encoded\n",
        "* **Safe to scale** — no black-box escalation\n",
        "\n",
        "This is not an agent that *reacts*.\n",
        "It is an agent that **applies policy consistently**.\n",
        "\n",
        "---\n",
        "\n",
        "## Architectural Takeaway\n",
        "\n",
        "This module demonstrates a core principle of your system:\n",
        "\n",
        "> **Risk is not something AI should “detect.”\n",
        "> It is something organizations should define — and AI should enforce.**\n",
        "\n",
        "By making risk explicit, weighted, and auditable, this orchestrator earns the right to be trusted.\n",
        "\n"
      ],
      "metadata": {
        "id": "_J57tml01J9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfl2i3pMpQsV"
      },
      "outputs": [],
      "source": [
        "\"\"\"Risk analysis utilities\"\"\"\n",
        "\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def normalize_severity(severity: str) -> float:\n",
        "    \"\"\"Convert severity string to numeric score (0-100)\"\"\"\n",
        "    severity_map = {\n",
        "        \"critical\": 100.0,\n",
        "        \"high\": 75.0,\n",
        "        \"medium\": 50.0,\n",
        "        \"low\": 25.0\n",
        "    }\n",
        "    return severity_map.get(severity.lower(), 50.0)\n",
        "\n",
        "\n",
        "def normalize_criticality(criticality: str) -> float:\n",
        "    \"\"\"Convert criticality string to numeric score (0-100)\"\"\"\n",
        "    criticality_map = {\n",
        "        \"critical\": 100.0,\n",
        "        \"high\": 75.0,\n",
        "        \"medium\": 50.0,\n",
        "        \"low\": 25.0\n",
        "    }\n",
        "    return criticality_map.get(criticality.lower(), 50.0)\n",
        "\n",
        "\n",
        "def calculate_risk_urgency(risk: Dict[str, Any]) -> float:\n",
        "    \"\"\"Calculate time-based urgency score (0-100)\"\"\"\n",
        "    detected_at = risk.get(\"detected_at\")\n",
        "    if not detected_at:\n",
        "        return 50.0  # Default urgency\n",
        "\n",
        "    try:\n",
        "        detected_time = datetime.fromisoformat(detected_at.replace(\"Z\", \"+00:00\"))\n",
        "        now = datetime.now(detected_time.tzinfo)\n",
        "        days_open = (now - detected_time).days\n",
        "\n",
        "        # Higher urgency for older risks\n",
        "        if days_open >= 30:\n",
        "            return 100.0  # Critical urgency\n",
        "        elif days_open >= 14:\n",
        "            return 75.0  # High urgency\n",
        "        elif days_open >= 7:\n",
        "            return 50.0  # Medium urgency\n",
        "        else:\n",
        "            return 25.0  # Low urgency\n",
        "    except (ValueError, AttributeError):\n",
        "        return 50.0\n",
        "\n",
        "\n",
        "def calculate_risk_score(\n",
        "    risk: Dict[str, Any],\n",
        "    agent: Dict[str, Any],\n",
        "    weights: Dict[str, float]\n",
        ") -> float:\n",
        "    \"\"\"Calculate weighted risk score for a single risk\"\"\"\n",
        "    severity_score = normalize_severity(risk.get(\"severity\", \"medium\"))\n",
        "    criticality_score = normalize_criticality(agent.get(\"criticality\", \"medium\"))\n",
        "    urgency_score = calculate_risk_urgency(risk)\n",
        "\n",
        "    # Impact score based on risk type\n",
        "    risk_type = risk.get(\"risk_type\", \"unknown\")\n",
        "    impact_map = {\n",
        "        \"integration\": 100.0,  # Integration risks are high impact\n",
        "        \"operational\": 75.0,\n",
        "        \"cost\": 50.0,\n",
        "        \"compliance\": 90.0,\n",
        "        \"security\": 95.0\n",
        "    }\n",
        "    impact_score = impact_map.get(risk_type.lower(), 50.0)\n",
        "\n",
        "    # Weighted score\n",
        "    total_score = (\n",
        "        severity_score * weights.get(\"severity\", 0.40) +\n",
        "        criticality_score * weights.get(\"criticality\", 0.30) +\n",
        "        impact_score * weights.get(\"impact\", 0.20) +\n",
        "        urgency_score * weights.get(\"urgency\", 0.10)\n",
        "    )\n",
        "\n",
        "    return round(total_score, 1)\n",
        "\n",
        "\n",
        "def assess_agent_risks(\n",
        "    agent_id: str,\n",
        "    risks: List[Dict[str, Any]],\n",
        "    agent: Dict[str, Any],\n",
        "    weights: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Assess all risks for a single agent\"\"\"\n",
        "    # Group risks by type\n",
        "    integration_risks = [r for r in risks if r.get(\"risk_type\") == \"integration\"]\n",
        "    operational_risks = [r for r in risks if r.get(\"risk_type\") == \"operational\"]\n",
        "    cost_risks = [r for r in risks if r.get(\"risk_type\") == \"cost\"]\n",
        "    other_risks = [r for r in risks if r.get(\"risk_type\") not in [\"integration\", \"operational\", \"cost\"]]\n",
        "\n",
        "    # Calculate scores for each risk\n",
        "    all_risk_scores = []\n",
        "    for risk in risks:\n",
        "        score = calculate_risk_score(risk, agent, weights)\n",
        "        all_risk_scores.append(score)\n",
        "\n",
        "    # Total risk score (average of all risk scores, weighted by severity)\n",
        "    if all_risk_scores:\n",
        "        total_risk_score = sum(all_risk_scores) / len(all_risk_scores)\n",
        "    else:\n",
        "        total_risk_score = 0.0\n",
        "\n",
        "    # Determine risk level\n",
        "    if total_risk_score >= 75.0:\n",
        "        risk_level = \"critical\"\n",
        "    elif total_risk_score >= 50.0:\n",
        "        risk_level = \"high\"\n",
        "    else:\n",
        "        risk_level = \"medium\"\n",
        "\n",
        "    # Priority actions (risks that need attention)\n",
        "    priority_actions = []\n",
        "    for risk in sorted(risks, key=lambda r: calculate_risk_score(r, agent, weights), reverse=True)[:3]:\n",
        "        priority_actions.append({\n",
        "            \"risk_id\": risk.get(\"risk_id\"),\n",
        "            \"risk_type\": risk.get(\"risk_type\"),\n",
        "            \"severity\": risk.get(\"severity\"),\n",
        "            \"signal\": risk.get(\"signal\"),\n",
        "            \"score\": calculate_risk_score(risk, agent, weights)\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"agent_id\": agent_id,\n",
        "        \"integration_risks\": integration_risks,\n",
        "        \"operational_risks\": operational_risks,\n",
        "        \"cost_risks\": cost_risks,\n",
        "        \"other_risks\": other_risks,\n",
        "        \"total_risk_score\": round(total_risk_score, 1),\n",
        "        \"risk_level\": risk_level,\n",
        "        \"priority_actions\": priority_actions\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_all_agent_risks(\n",
        "    agents: List[Dict[str, Any]],\n",
        "    risks_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    weights: Dict[str, float]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Analyze risks for all agents\"\"\"\n",
        "    assessments = []\n",
        "    for agent in agents:\n",
        "        agent_id = agent[\"agent_id\"]\n",
        "        risks = risks_lookup.get(agent_id, [])\n",
        "        assessment = assess_agent_risks(agent_id, risks, agent, weights)\n",
        "        assessments.append(assessment)\n",
        "    return assessments\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def risk_analysis_node(\n",
        "    state: IntegrationRiskManagementOrchestratorState,\n",
        "    config: IntegrationRiskManagementOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Risk Analysis Node: Assess risks for all agents\"\"\"\n",
        "    errors = state.get(\"errors\", [])\n",
        "    agents = state.get(\"agents\", [])\n",
        "    risks_lookup = state.get(\"risks_lookup\", {})\n",
        "\n",
        "    if not agents:\n",
        "        return {\n",
        "            \"errors\": errors + [\"risk_analysis_node: agents required\"]\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        risk_assessments = analyze_all_agent_risks(\n",
        "            agents,\n",
        "            risks_lookup,\n",
        "            config.risk_scoring_weights\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"risk_assessments\": risk_assessments,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"errors\": errors + [f\"risk_analysis_node: {str(e)}\"]\n",
        "        }"
      ],
      "metadata": {
        "id": "UslYywib0K9s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
