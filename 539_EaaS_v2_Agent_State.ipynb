{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFvt3N8R0Mvsyb/yygrHy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/539_EaaS_v2_Agent_State.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation-as-a-Service (EaaS) Orchestrator Agent\n",
        "\n",
        "This agent evaluates the performance of other AI agents by:\n",
        "1. Loading test scenarios with expected outcomes\n",
        "2. Executing scenarios through target agents\n",
        "3. Comparing actual vs expected outputs\n",
        "4. Scoring performance (correctness, response time, output quality)\n",
        "5. Generating comprehensive evaluation reports"
      ],
      "metadata": {
        "id": "SaeyL0zz0fZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why This Is Different From Most AI Agents in Production\n",
        "\n",
        "Most AI agents today:\n",
        "\n",
        "* hide logic in prompts\n",
        "* lack explicit performance standards\n",
        "* cannot explain why something passed or failed\n",
        "* blur the line between reasoning and governance\n",
        "\n",
        "This system:\n",
        "\n",
        "* exposes its standards\n",
        "* encodes business expectations\n",
        "* tracks performance over time\n",
        "* supports policy-driven decision making\n",
        "\n",
        "In other words, it behaves less like a demo — and more like infrastructure.\n",
        "\n",
        "---\n",
        "\n",
        "## The Executive Takeaway\n",
        "\n",
        "What leaders see in this configuration is not just flexibility — it’s **control**.\n",
        "\n",
        "They can:\n",
        "\n",
        "* define success\n",
        "* monitor risk\n",
        "* detect regressions\n",
        "* approve deployment with confidence\n",
        "\n",
        "And they can do it without needing to understand model internals.\n",
        "\n",
        "That’s why this design isn’t just technically sound — it’s *organizationally adoptable*.\n"
      ],
      "metadata": {
        "id": "nEt5WpQN4KcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Evaluation-as-a-Service Orchestrator State — Architecture Review\n",
        "\n",
        "## What This State Represents in Practical Terms\n",
        "\n",
        "This state defines the **entire operating context** for the EaaS Orchestrator.\n",
        "\n",
        "In real-world terms, this is the **single source of truth** that allows the agent to:\n",
        "\n",
        "* Load realistic business scenarios\n",
        "* Simulate how agents behave\n",
        "* Evaluate decisions objectively\n",
        "* Track performance over time\n",
        "* Detect regressions and improvements\n",
        "* Produce executive-ready reports\n",
        "\n",
        "Nothing here is abstract — every field corresponds to a concrete step in how a real evaluation service would run in production.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Design Builds Trust and Control\n",
        "\n",
        "A critical strength of this state is that it **separates facts from interpretation**.\n",
        "\n",
        "You clearly distinguish:\n",
        "\n",
        "* **Inputs** (scenarios, agents, data)\n",
        "* **Execution results** (what actually happened)\n",
        "* **Scoring** (objective measurements)\n",
        "* **Analysis** (aggregations and trends)\n",
        "* **Narrative output** (reports and summaries)\n",
        "\n",
        "That separation is what makes the system:\n",
        "\n",
        "* auditable\n",
        "* testable\n",
        "* defensible to leadership\n",
        "\n",
        "An executive can point to *exactly* where a decision came from.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Architectural Highlights\n",
        "\n",
        "### 1. Scenario-First Evaluation Model\n",
        "\n",
        "Your `journey_scenarios` field anchors the entire evaluation process in **realistic customer journeys**, not synthetic benchmarks.\n",
        "\n",
        "This means the agent isn’t asking:\n",
        "\n",
        "> “Did the model do well?”\n",
        "\n",
        "It’s asking:\n",
        "\n",
        "> “Did the system behave correctly in situations that matter to the business?”\n",
        "\n",
        "That framing is a huge credibility boost.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Explicit Agent Simulation (Not Black-Box Evaluation)\n",
        "\n",
        "By loading `specialist_agents` as structured definitions instead of opaque functions, you make agent behavior:\n",
        "\n",
        "* inspectable\n",
        "* configurable\n",
        "* reproducible\n",
        "\n",
        "This reinforces a key principle:\n",
        "\n",
        "> The evaluator measures *system behavior*, not model magic.\n",
        "\n",
        "That’s exactly what enterprises want.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Historical Data as a First-Class Citizen\n",
        "\n",
        "Your inclusion of:\n",
        "\n",
        "* `historical_evaluation_runs`\n",
        "* `historical_run_metrics`\n",
        "* `historical_scenario_evaluations`\n",
        "\n",
        "is not cosmetic — it fundamentally changes the nature of the agent.\n",
        "\n",
        "This turns EaaS into:\n",
        "\n",
        "* a monitoring system\n",
        "* a regression detector\n",
        "* a release gatekeeper\n",
        "\n",
        "Executives don’t just see **today’s performance**, they see **directionality**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Clean Separation Between Execution and Scoring\n",
        "\n",
        "The split between:\n",
        "\n",
        "* `executed_evaluations`\n",
        "* `evaluation_scores`\n",
        "* `agent_performance_summary`\n",
        "\n",
        "is exactly right.\n",
        "\n",
        "This allows you to:\n",
        "\n",
        "* change scoring rules without rerunning executions\n",
        "* audit failures independently\n",
        "* explain *why* an agent failed, not just that it did\n",
        "\n",
        "That’s accountability by design.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Health States That Map to Business Language\n",
        "\n",
        "Your `health_thresholds` configuration is a great example of translating technical signals into **business-readable status**:\n",
        "\n",
        "* healthy\n",
        "* degraded\n",
        "* critical\n",
        "\n",
        "This is the language decision-makers actually use — and you’ve encoded it directly into the system.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This State Supports ROI and Executive Confidence\n",
        "\n",
        "From a leadership perspective, this state enables answers to questions like:\n",
        "\n",
        "* Which agents are safe to deploy?\n",
        "* Which ones need attention?\n",
        "* Did last week’s change improve or degrade performance?\n",
        "* Where is human review still required?\n",
        "* Are we trending toward stability or risk?\n",
        "\n",
        "Crucially, those answers are backed by:\n",
        "\n",
        "* explicit data\n",
        "* historical comparisons\n",
        "* repeatable logic\n",
        "\n",
        "Not opinion. Not intuition.\n",
        "\n",
        "---\n",
        "\n",
        "## Config Design: Pragmatic and Scalable\n",
        "\n",
        "Your config class reinforces the same philosophy:\n",
        "\n",
        "* Rules and thresholds are **visible and adjustable**\n",
        "* LLM usage is clearly marked as **optional enhancement**\n",
        "* Tooling integrations are feature-flagged, not assumed\n",
        "* Scoring logic is parameterized, not hard-coded\n",
        "\n",
        "That makes this agent:\n",
        "\n",
        "* safe to evolve\n",
        "* easy to explain\n",
        "* resistant to accidental complexity\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "\n",
        "This state definition is:\n",
        "\n",
        "* Architecturally sound\n",
        "* Enterprise-credible\n",
        "* MVP-disciplined\n",
        "* Portfolio-ready\n",
        "\n",
        "Most importantly, it clearly communicates this idea:\n",
        "\n",
        "> **The system does not “judge” agents — it measures them, compares them, and reports the results transparently.**\n",
        "\n",
        "That’s exactly what an Evaluation-as-a-Service agent should do.\n",
        "\n"
      ],
      "metadata": {
        "id": "BfP_9M-p3Czo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Evaluation-as-a-Service (EaaS) Orchestrator Agent\n",
        "# ============================================================================\n",
        "\n",
        "class EvalAsServiceOrchestratorState(TypedDict, total=False):\n",
        "    \"\"\"State for Evaluation-as-a-Service Orchestrator Agent\"\"\"\n",
        "\n",
        "    # Input fields\n",
        "    scenario_id: Optional[str]              # Specific scenario to evaluate (None = evaluate all)\n",
        "    target_agent_id: Optional[str]          # Specific agent to evaluate (None = evaluate all)\n",
        "\n",
        "    # Goal & Planning fields (MVP: Fixed goal, template-based plan)\n",
        "    goal: Dict[str, Any]                    # Goal definition (from goal_node)\n",
        "    plan: List[Dict[str, Any]]              # Execution plan (from planning_node)\n",
        "\n",
        "    # Data Ingestion\n",
        "    journey_scenarios: List[Dict[str, Any]]  # Loaded test scenarios\n",
        "    # Structure per scenario:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"customer_id\": \"C001\",\n",
        "    #   \"order_id\": \"O1001\",\n",
        "    #   \"customer_message\": \"Hi, my order hasn't arrived yet...\",\n",
        "    #   \"expected_issue_type\": \"where_is_my_order\",\n",
        "    #   \"expected_resolution_path\": [\"shipping_update_agent\"],\n",
        "    #   \"expected_outcome\": \"provide_delivery_update\"\n",
        "    # }\n",
        "\n",
        "    specialist_agents: List[Dict[str, Any]]  # Loaded specialist agents to evaluate\n",
        "    # Structure per agent:\n",
        "    # {\n",
        "    #   \"agent_id\": \"refund_agent\",\n",
        "    #   \"description\": \"Issues refunds for lost or incorrect orders.\",\n",
        "    #   \"actions\": {...}\n",
        "    # }\n",
        "\n",
        "    supporting_data: Dict[str, Any]         # Supporting data (customers, orders, logistics, marketing)\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"customers\": [...],\n",
        "    #   \"orders\": [...],\n",
        "    #   \"logistics\": {...},\n",
        "    #   \"marketing_signals\": [...]\n",
        "    # }\n",
        "\n",
        "    decision_rules: Dict[str, Any]          # Orchestrator decision rules for validation\n",
        "\n",
        "    # Historical Data (for baseline comparison and regression detection)\n",
        "    historical_evaluation_runs: List[Dict[str, Any]]  # Historical evaluation runs metadata\n",
        "    # Structure per run:\n",
        "    # {\n",
        "    #   \"run_id\": \"RUN_2025_12_20\",\n",
        "    #   \"run_timestamp\": \"2025-12-20T09:00:00Z\",\n",
        "    #   \"target_orchestrator\": \"customer_support_orchestrator\",\n",
        "    #   \"target_version\": \"v0.9\",\n",
        "    #   \"scenario_count\": 10,\n",
        "    #   \"evaluation_type\": \"pre_release\",\n",
        "    #   \"notes\": \"...\"\n",
        "    # }\n",
        "\n",
        "    historical_run_metrics: List[Dict[str, Any]]  # Summary metrics per historical run\n",
        "    # Structure per run:\n",
        "    # {\n",
        "    #   \"run_id\": \"RUN_2025_12_20\",\n",
        "    #   \"overall_pass_rate\": 0.75,\n",
        "    #   \"issue_classification_accuracy\": 0.78,\n",
        "    #   \"resolution_path_accuracy\": 0.72,\n",
        "    #   \"outcome_accuracy\": 0.75,\n",
        "    #   \"high_risk_failures\": 3,\n",
        "    #   \"human_review_rate\": 0.40,\n",
        "    #   \"avg_latency_ms\": 910,\n",
        "    #   \"p95_latency_ms\": 1300,\n",
        "    #   \"regression_flags\": [],\n",
        "    #   \"improvement_flags\": []\n",
        "    # }\n",
        "\n",
        "    historical_scenario_evaluations: List[Dict[str, Any]]  # Detailed scenario evaluations from past runs\n",
        "    # Structure per evaluation:\n",
        "    # {\n",
        "    #   \"run_id\": \"RUN_2025_12_20\",\n",
        "    #   \"scenario_id\": \"S006\",\n",
        "    #   \"actual_issue_type\": \"delivery_delay\",\n",
        "    #   \"expected_issue_type\": \"delivery_delay_with_churn_risk\",\n",
        "    #   \"issue_type_match\": false,\n",
        "    #   \"actual_resolution_path\": [...],\n",
        "    #   \"expected_resolution_path\": [...],\n",
        "    #   \"resolution_path_match\": false,\n",
        "    #   \"actual_outcome\": \"...\",\n",
        "    #   \"expected_outcome\": \"...\",\n",
        "    #   \"outcome_match\": false,\n",
        "    #   \"latency_ms\": 910,\n",
        "    #   \"confidence_score\": 0.61,\n",
        "    #   \"requires_human_review\": true,\n",
        "    #   \"failure_reason\": \"...\"\n",
        "    # }\n",
        "\n",
        "    # Evaluation Execution\n",
        "    executed_evaluations: List[Dict[str, Any]]  # Completed evaluations\n",
        "    # Structure per evaluation:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"target_agent_id\": \"shipping_update_agent\",\n",
        "    #   \"input\": {...},\n",
        "    #   \"actual_output\": {...},\n",
        "    #   \"expected_output\": {...},\n",
        "    #   \"execution_time_seconds\": 0.5,\n",
        "    #   \"status\": \"completed\" | \"failed\" | \"timeout\",\n",
        "    #   \"error\": Optional[str]\n",
        "    # }\n",
        "\n",
        "    # Scoring & Analysis\n",
        "    evaluation_scores: List[Dict[str, Any]]  # Scores per evaluation\n",
        "    # Structure per score:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"target_agent_id\": \"shipping_update_agent\",\n",
        "    #   \"correctness_score\": 0.95,  # 0-1, matches expected outcome\n",
        "    #   \"response_time_score\": 0.90,  # 0-1, based on thresholds\n",
        "    #   \"output_quality_score\": 0.85,  # 0-1, based on structure/format\n",
        "    #   \"overall_score\": 0.90,  # Weighted average\n",
        "    #   \"passed\": True,  # Overall score >= threshold\n",
        "    #   \"issues\": [\"slight_format_deviation\"]\n",
        "    # }\n",
        "\n",
        "    agent_performance_summary: Dict[str, Any]  # Performance summary per agent\n",
        "    # Structure per agent:\n",
        "    # {\n",
        "    #   \"agent_id\": \"shipping_update_agent\",\n",
        "    #   \"total_evaluations\": 10,\n",
        "    #   \"passed_count\": 9,\n",
        "    #   \"failed_count\": 1,\n",
        "    #   \"average_score\": 0.88,\n",
        "    #   \"average_response_time\": 0.45,\n",
        "    #   \"health_status\": \"healthy\" | \"degraded\" | \"critical\"\n",
        "    # }\n",
        "\n",
        "    # Quality Control Metrics (using toolshed)\n",
        "    performance_metrics: Dict[str, Any]      # Performance tracking metrics\n",
        "    workflow_analysis: List[Dict[str, Any]]  # Workflow health analysis\n",
        "    validation_results: List[Dict[str, Any]]  # Data validation results\n",
        "\n",
        "    # Progress Tracking (using toolshed)\n",
        "    progress_percentage: float              # 0-100\n",
        "    evaluations_completed: int              # Count of completed evaluations\n",
        "    evaluations_total: int                  # Total evaluations to run\n",
        "    elapsed_time_seconds: float             # Time since evaluation start\n",
        "    estimated_remaining_seconds: float      # Estimated time to completion\n",
        "    evaluation_start_time: Optional[str]     # ISO timestamp when evaluation started\n",
        "\n",
        "    # Summary Metrics\n",
        "    evaluation_summary: Dict[str, Any]\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"total_scenarios\": 10,\n",
        "    #   \"total_evaluations\": 30,\n",
        "    #   \"total_passed\": 27,\n",
        "    #   \"total_failed\": 3,\n",
        "    #   \"overall_pass_rate\": 0.90,\n",
        "    #   \"average_score\": 0.87,\n",
        "    #   \"agents_evaluated\": 4,\n",
        "    #   \"healthy_agents\": 3,\n",
        "    #   \"degraded_agents\": 1,\n",
        "    #   \"critical_agents\": 0\n",
        "    # }\n",
        "\n",
        "    # Historical Comparison & Regression Detection\n",
        "    baseline_comparison: Optional[Dict[str, Any]]  # Comparison to baseline run\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"baseline_run_id\": \"RUN_2026_01_10\",\n",
        "    #   \"current_pass_rate\": 0.92,\n",
        "    #   \"baseline_pass_rate\": 0.88,\n",
        "    #   \"improvement\": 0.04,\n",
        "    #   \"improvement_percentage\": 4.55,\n",
        "    #   \"regression_detected\": false,\n",
        "    #   \"regression_details\": []\n",
        "    # }\n",
        "\n",
        "    regression_analysis: Optional[Dict[str, Any]]  # Regression analysis vs previous runs\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"regressions_detected\": [],\n",
        "    #   \"improvements_detected\": [],\n",
        "    #   \"trend_analysis\": {...}\n",
        "    # }\n",
        "\n",
        "    trend_analysis: Optional[Dict[str, Any]]  # Trend analysis across historical runs\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"pass_rate_trend\": \"improving\" | \"declining\" | \"stable\",\n",
        "    #   \"latency_trend\": \"improving\" | \"declining\" | \"stable\",\n",
        "    #   \"accuracy_trend\": \"improving\" | \"declining\" | \"stable\"\n",
        "    # }\n",
        "\n",
        "    # Output\n",
        "    evaluation_report: str                  # Final markdown report\n",
        "    report_file_path: Optional[str]        # Path to saved report file\n",
        "\n",
        "    # Metadata\n",
        "    errors: Annotated[List[str], operator.add]  # Any errors encountered\n",
        "    processing_time: Optional[float]      # Time taken to process\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvalAsServiceOrchestratorConfig:\n",
        "    \"\"\"Configuration for Evaluation-as-a-Service Orchestrator Agent\"\"\"\n",
        "    llm_model: str = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
        "    temperature: float = 0.3\n",
        "    reports_dir: str = \"output/eval_as_service_reports\"  # Where to save reports\n",
        "\n",
        "    # Data file paths\n",
        "    data_dir: str = \"agents/data\"\n",
        "    journey_scenarios_file: str = \"journey_scenarios.json\"\n",
        "    specialist_agents_file: str = \"specialist_agents.json\"\n",
        "    customers_file: str = \"customers.json\"\n",
        "    orders_file: str = \"orders.json\"\n",
        "    logistics_file: str = \"logistics_api.json\"\n",
        "    marketing_signals_file: str = \"marketing_signals.json\"\n",
        "    decision_rules_file: str = \"orchestrator_decision_rules.json\"\n",
        "    evaluation_runs_file: str = \"evaluation_runs.json\"\n",
        "    run_summary_metrics_file: str = \"run_summary_metrics.json\"\n",
        "    scenario_evaluations_file: str = \"scenario_evaluations.json\"\n",
        "\n",
        "    # Evaluation Settings\n",
        "    pass_threshold: float = 0.80  # Minimum score to pass (0-1)\n",
        "    response_time_threshold_seconds: float = 2.0  # Max acceptable response time\n",
        "    enable_parallel_evaluation: bool = False  # MVP: Sequential only\n",
        "\n",
        "    # Scoring Weights\n",
        "    scoring_weights: Dict[str, float] = field(default_factory=lambda: {\n",
        "        \"correctness\": 0.50,      # Matches expected outcome\n",
        "        \"response_time\": 0.20,     # Response time performance\n",
        "        \"output_quality\": 0.30     # Output structure/format quality\n",
        "    })\n",
        "\n",
        "    # Health Status Thresholds\n",
        "    health_thresholds: Dict[str, float] = field(default_factory=lambda: {\n",
        "        \"healthy\": 0.85,      # >= 85% average score\n",
        "        \"degraded\": 0.70,     # 70-85% average score\n",
        "        \"critical\": 0.0       # < 70% average score\n",
        "    })\n",
        "\n",
        "    # Toolshed Integration\n",
        "    enable_progress_tracking: bool = True   # Use toolshed.progress\n",
        "    enable_performance_tracking: bool = True  # Use toolshed.performance\n",
        "    enable_workflow_analysis: bool = True    # Use toolshed.workflows\n",
        "    enable_validation: bool = True          # Use toolshed.validation\n",
        "    enable_kpi_tracking: bool = True        # Use toolshed.kpi\n",
        "    enable_reporting: bool = True          # Use toolshed.reporting\n",
        "\n",
        "    # LLM Enhancement (Optional - Phase 8)\n",
        "    enable_llm_summaries: bool = True      # Enable LLM-enhanced executive summaries\n",
        "    llm_summary_max_tokens: int = 500      # Max tokens for LLM summaries\n"
      ],
      "metadata": {
        "id": "WYBOQErj0sOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Why These Configurable Controls Matter (And Why Leaders Care)\n",
        "\n",
        "This configuration block is where the Evaluation-as-a-Service agent fundamentally separates itself from most AI systems in production today.\n",
        "\n",
        "Instead of embedding judgment inside opaque logic or prompts, this system **exposes its standards explicitly** — allowing leaders to see, adjust, and govern how AI performance is measured.\n",
        "\n",
        "That difference is subtle technically, but enormous organizationally.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Settings: Turning Expectations Into Policy\n",
        "\n",
        "### `pass_threshold`\n",
        "\n",
        "This setting defines the **minimum acceptable performance level** for an agent to be considered production-ready.\n",
        "\n",
        "In practical terms, this answers a question executives constantly ask but rarely get a clear answer to:\n",
        "\n",
        "> *“How good is good enough?”*\n",
        "\n",
        "By making this threshold explicit and configurable:\n",
        "\n",
        "* Performance standards are no longer subjective\n",
        "* Teams stop arguing over anecdotes\n",
        "* Deployment decisions become policy-driven, not opinion-driven\n",
        "\n",
        "Most agents in production today **do not have a formal pass/fail definition**. They rely on informal testing or gut feel. This system replaces that ambiguity with clarity.\n",
        "\n",
        "---\n",
        "\n",
        "### `response_time_threshold_seconds`\n",
        "\n",
        "This setting encodes **user experience expectations** directly into evaluation.\n",
        "\n",
        "It ensures that an agent is not only correct, but *fast enough to be usable*.\n",
        "\n",
        "From a business perspective, this is critical because:\n",
        "\n",
        "* Slow responses silently kill adoption\n",
        "* Latency failures often go unnoticed until customers complain\n",
        "* Engineering teams optimize accuracy while ignoring experience\n",
        "\n",
        "Here, response time is treated as a **first-class performance constraint**, not an afterthought.\n",
        "\n",
        "That’s how real products are judged — and evaluated.\n",
        "\n",
        "---\n",
        "\n",
        "### `enable_parallel_evaluation`\n",
        "\n",
        "This flag is a quiet but important signal of maturity.\n",
        "\n",
        "By explicitly choosing **sequential execution for MVP**, the system prioritizes:\n",
        "\n",
        "* determinism\n",
        "* debuggability\n",
        "* auditability\n",
        "\n",
        "Leaders are often nervous about AI systems because they move too fast to understand. This design choice shows restraint and intentionality — speed is added *after* trust is established.\n",
        "\n",
        "Most AI systems start with scale and struggle to retrofit control. You’ve done the opposite.\n",
        "\n",
        "---\n",
        "\n",
        "## Scoring Weights: Aligning AI Behavior With Business Priorities\n",
        "\n",
        "### `scoring_weights`\n",
        "\n",
        "This configuration answers a deceptively simple question:\n",
        "\n",
        "> *“What do we value most?”*\n",
        "\n",
        "By breaking performance into:\n",
        "\n",
        "* correctness\n",
        "* response time\n",
        "* output quality\n",
        "\n",
        "and weighting them explicitly, the system allows leadership to **encode business priorities into evaluation logic**.\n",
        "\n",
        "This is powerful because:\n",
        "\n",
        "* Different teams can prioritize differently\n",
        "* The same agent can be evaluated under different standards\n",
        "* Tradeoffs become visible and intentional\n",
        "\n",
        "In contrast, most AI agents are optimized implicitly through prompts or training — priorities are hidden, not declared.\n",
        "\n",
        "This system makes value judgments transparent.\n",
        "\n",
        "---\n",
        "\n",
        "## Health Status Thresholds: Translating Metrics Into Decisions\n",
        "\n",
        "### `health_thresholds`\n",
        "\n",
        "These thresholds convert raw scores into **clear operational states**:\n",
        "\n",
        "* healthy\n",
        "* degraded\n",
        "* critical\n",
        "\n",
        "This is not cosmetic — it’s decision-enabling.\n",
        "\n",
        "Executives don’t want charts. They want answers like:\n",
        "\n",
        "* “Is this safe to run?”\n",
        "* “Do we need intervention?”\n",
        "* “Can we scale this?”\n",
        "\n",
        "By mapping performance into health states, the system:\n",
        "\n",
        "* shortens decision cycles\n",
        "* enables automated gating\n",
        "* supports escalation workflows\n",
        "\n",
        "Most AI systems stop at metrics. This system goes one step further and **interprets them operationally**.\n",
        "\n",
        "---\n",
        "\n",
        "## Toolshed Integrations: Instrumentation by Default\n",
        "\n",
        "The toolshed flags signal another major difference from typical agents:\n",
        "\n",
        "This system assumes that **observability is mandatory**, not optional.\n",
        "\n",
        "Each flag controls whether the agent:\n",
        "\n",
        "* tracks progress\n",
        "* logs performance\n",
        "* analyzes workflows\n",
        "* validates inputs\n",
        "* monitors KPIs\n",
        "* generates structured reports\n",
        "\n",
        "This reflects how mature software systems operate — yet most AI agents in production today have little to no instrumentation.\n",
        "\n",
        "Here, visibility is built in from day one.\n",
        "\n",
        "For leaders, this is deeply reassuring:\n",
        "\n",
        "* Problems don’t stay hidden\n",
        "* Performance can be reviewed after the fact\n",
        "* Accountability doesn’t rely on heroics\n",
        "\n",
        "---\n",
        "\n",
        "## LLM Enhancements: Intelligence Without Loss of Control\n",
        "\n",
        "### `enable_llm_summaries`\n",
        "\n",
        "This final section is subtle — and extremely important.\n",
        "\n",
        "The LLM is positioned as a **communication layer**, not a decision-maker.\n",
        "\n",
        "It summarizes results that are:\n",
        "\n",
        "* already computed\n",
        "* already scored\n",
        "* already validated\n",
        "\n",
        "This preserves:\n",
        "\n",
        "* control\n",
        "* explainability\n",
        "* auditability\n",
        "\n",
        "Most agents invert this relationship, letting LLMs *decide* and then attempting to explain later. This system does the opposite.\n",
        "\n",
        "The LLM explains what the system already knows to be true.\n",
        "\n",
        "That distinction is what allows executives to trust it.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gX7phh7Q4EWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EaaS Orchestrator - Phase 0 Planning Document\n",
        "\n",
        "## Data Structure Analysis\n",
        "\n",
        "### Journey Scenarios (`journey_scenarios.json`)\n",
        "- **Structure**: List of 10 test scenarios\n",
        "- **Key Fields**:\n",
        "  - `scenario_id`: Unique identifier (S001-S010)\n",
        "  - `customer_id`: Links to customer data (C001-C005)\n",
        "  - `order_id`: Links to order data (O1001-O1005)\n",
        "  - `customer_message`: Input message for the orchestrator\n",
        "  - `expected_issue_type`: Expected classification (e.g., \"where_is_my_order\", \"delivery_delay\")\n",
        "  - `expected_resolution_path`: List of agent IDs in order (e.g., [\"shipping_update_agent\"])\n",
        "  - `expected_outcome`: Expected final outcome string\n",
        "\n",
        "### Specialist Agents (`specialist_agents.json`)\n",
        "- **Structure**: Dictionary with 4 agents\n",
        "- **Agents**: refund_agent, shipping_update_agent, apology_message_agent, escalation_agent\n",
        "- **Key Fields**:\n",
        "  - `agent_id`: Unique identifier\n",
        "  - `description`: What the agent does\n",
        "  - `actions`: Dictionary of actions with response templates\n",
        "  - Action-specific configs (e.g., `default_refund_amounts`, `priority_rules`)\n",
        "\n",
        "### Supporting Data\n",
        "- **customers.json**: List of 5 customers with loyalty_tier, churn_risk\n",
        "- **orders.json**: List of 5 orders with status, carrier, warehouse_issue_flag\n",
        "- **logistics_api.json**: Nested dict by carrier → order_id → status/details\n",
        "- **marketing_signals.json**: Customer engagement metrics\n",
        "\n",
        "### Decision Rules (`orchestrator_decision_rules.json`)\n",
        "- **Structure**: Contains both JSON rules and Python functions\n",
        "- **Functions Available**:\n",
        "  - `classify_issue(order, ticket, customer, logistics)` → issue_type\n",
        "  - `determine_resolution_path(issue_type)` → list of agent IDs\n",
        "  - `determine_expected_outcome(issue_type)` → outcome string\n",
        "- **Note**: This file contains Python code, not pure JSON. We'll need to extract the functions or implement them.\n",
        "\n",
        "## Architecture Plan\n",
        "\n",
        "### Node Structure (Linear Flow)\n",
        "1. **goal_node** - Define evaluation goal\n",
        "2. **planning_node** - Create execution plan\n",
        "3. **data_loading_node** - Load all data files\n",
        "4. **evaluation_execution_node** - Run scenarios through agents\n",
        "5. **scoring_analysis_node** - Compare and score results\n",
        "6. **report_generation_node** - Generate final report\n",
        "\n",
        "### Utility Structure\n",
        "```\n",
        "utilities/\n",
        "├── data_loading.py      # Load JSON files, build lookups\n",
        "├── evaluation_execution.py  # Execute scenarios, simulate agent calls\n",
        "├── scoring.py          # Score correctness, response time, quality\n",
        "├── analysis.py         # Aggregate scores, calculate metrics\n",
        "└── report_generation.py  # Generate markdown reports\n",
        "```\n",
        "\n",
        "## Key Assumptions\n",
        "\n",
        "1. **Agent Simulation**: Since we're evaluating agents, we'll simulate their behavior based on:\n",
        "   - Specialist agent definitions (response templates)\n",
        "   - Decision rules (to determine which agents get called)\n",
        "   - Supporting data (to populate responses)\n",
        "\n",
        "2. **Evaluation Logic**: For each scenario:\n",
        "   - Extract customer_message, customer_id, order_id\n",
        "   - Load supporting data (customer, order, logistics, marketing)\n",
        "   - Use decision rules to determine expected issue_type and resolution_path\n",
        "   - Simulate orchestrator calling agents in resolution_path\n",
        "   - Compare actual vs expected outputs\n",
        "\n",
        "3. **Scoring Dimensions**:\n",
        "   - **Correctness** (50% weight): Does actual match expected?\n",
        "   - **Response Time** (20% weight): Is it within threshold?\n",
        "   - **Output Quality** (30% weight): Is structure/format correct?\n",
        "\n",
        "4. **Data Loading**: All data files are in `agents/data/` directory\n",
        "\n",
        "5. **Decision Rules**: The orchestrator_decision_rules.json file contains Python code. We'll need to either:\n",
        "   - Extract and import the functions\n",
        "   - Re-implement the logic based on the JSON structure\n",
        "\n",
        "## Testing Strategy\n",
        "\n",
        "- **Phase 1 Test**: Goal and planning nodes work independently\n",
        "- **Phase 2 Test**: Data loading works, all files load correctly\n",
        "- **Phase 3 Test**: Evaluation execution runs scenarios, produces outputs\n",
        "- **Phase 4 Test**: Scoring produces correct scores, analysis aggregates correctly\n",
        "- **Phase 5 Test**: Report generation creates valid markdown\n",
        "\n",
        "## Toolshed Integration\n",
        "\n",
        "- **Progress Tracking**: Track evaluation progress (X of Y scenarios)\n",
        "- **KPI Tracking**: Track pass rates, average scores\n",
        "- **Reporting**: Use toolshed.reporting for report structure\n",
        "- **Validation**: Validate data structures on load\n"
      ],
      "metadata": {
        "id": "CV5Z6M200qjy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBWyxPyx4JmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}