{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ+nSvuuLQqd54Mq8MiQWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/082_FromPlanning_ToReflection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key elements you should focus on in this notebook are:\n",
        "\n",
        "1. **Shift from Planning to Reflection**\n",
        "\n",
        "   * The earlier `PlanFirstCapability` was about forward planning before actions.\n",
        "   * `ProgressTrackingCapability` focuses on looking back at *what just happened* after each loop iteration.\n",
        "   * This mirrors the human process: **plan â†’ act â†’ evaluate â†’ adjust**.\n",
        "\n",
        "2. **Track Progress Tool**\n",
        "\n",
        "   * Itâ€™s similar to the `create_plan` tool, but instead of asking *â€œWhat should we do?â€*, it asks *â€œWhat have we done, whatâ€™s left, and whatâ€™s blocking us?â€*\n",
        "   * This helps the agent avoid going blindly forward without reassessment.\n",
        "\n",
        "3. **Integration into the Agent Loop**\n",
        "\n",
        "   * The hook `end_agent_loop()` is used so progress tracking happens after each iteration.\n",
        "   * This hook ensures that the **most recent results are included in the reflection**, rather than relying on older assumptions.\n",
        "\n",
        "4. **Frequency Control (`track_frequency`)**\n",
        "\n",
        "   * You can set how often progress tracking runs (e.g., every iteration, every 2nd iteration, etc.).\n",
        "   * This lets you balance between reflection quality and efficiency.\n",
        "\n",
        "5. **Benefits for Long-Running Tasks**\n",
        "\n",
        "   * Maintains focus on goals across multiple steps.\n",
        "   * Creates an audit trail of reasoning and actions.\n",
        "   * Helps identify blockers early.\n",
        "   * Avoids repeating work or drifting off-task.\n",
        "\n",
        "Focus on **the difference between pre-action planning and post-action reflection**â€”because together, they form a full feedback loop that makes agents more autonomous and adaptable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2PrVyQ2aRlsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ðŸŽ¯ Tracking Progress: Turning Agents into Thoughtful Problem-Solvers\n",
        "\n",
        "When tackling **complex, multi-step tasks**, agents canâ€™t just charge forward blindlyâ€”they need to pause, reflect, and adapt.\n",
        "\n",
        "Enter **progress tracking**: a capability that adds **reflection** to the end of every agent loop iteration. By stopping to assess **what just happened**, the agent gains clarity on its current position and what needs to happen next.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Why It Matters\n",
        "\n",
        "Without reflection, an agent risks:\n",
        "\n",
        "* **Repeating work** itâ€™s already done\n",
        "* **Drifting** away from its original goal\n",
        "* **Missing blockers** that could derail progress later\n",
        "\n",
        "With progress tracking, we give the agent the superpower of **self-awareness** during execution.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ›  How It Works\n",
        "\n",
        "We introduce a function called `track_progress()` that **evaluates the agentâ€™s current state** after each action.\n",
        "Itâ€™s like our earlier planning functionâ€”but instead of *â€œWhat should I do?â€*, it asks:\n",
        "\n",
        "> ðŸ’­ *â€œWhat did I just do, whatâ€™s left, and whatâ€™s in my way?â€*\n",
        "\n",
        "Hereâ€™s what it considers:\n",
        "\n",
        "* **Available tools** for the next step\n",
        "* **Memory context** from earlier steps\n",
        "* **Completed actions** so far\n",
        "* **Any blockers or issues** that have appeared\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”„ The Continuous Feedback Loop\n",
        "\n",
        "By running `track_progress()` at the **end** of each loop iteration:\n",
        "\n",
        "1. âœ… The agent **confirms progress** on completed steps\n",
        "2. ðŸš§ Identifies **blockers early** before they cause failure\n",
        "3. ðŸ§­ Dynamically **adapts the plan** based on real-time feedback\n",
        "4. ðŸ“ˆ Improves **overall efficiency** for complex workflows\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  The Human Parallel\n",
        "\n",
        "This mirrors how **humans** tackle problems:\n",
        "\n",
        "1. **Plan** â†’ Decide what needs to be done\n",
        "2. **Act** â†’ Execute a step\n",
        "3. **Evaluate** â†’ See what happened\n",
        "4. **Adjust** â†’ Modify the approach if needed\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ’¡ **Bottom line:** Adding reflection transforms your agent from a **mindless executor** into a **thoughtful problem-solver**, capable of navigating complex tasks with strategy and precision.\n",
        "\n"
      ],
      "metadata": {
        "id": "lU_mNbX_SSub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Progress Tracking Tool\n",
        "\n",
        "A few notes to focus on:\n",
        "\n",
        "* **Dependency injection:** `_memory` (notice the underscore) and `action_registry` are injected by your Environment. The agent only â€œseesâ€ the tool name and explicit args (none here besides injected ones).\n",
        "* **Context curation:** Youâ€™re filtering memory to `['user','system']`, which keeps the prompt lean and safer. Nice.\n",
        "* **Tool cataloging:** It pulls `action_registry.get_actions()` to list available toolsâ€”great for suggesting next steps.\n",
        "\n",
        "If you want to tighten it up even more (optional polish):\n",
        "\n",
        "* Return **structured JSON** (e.g., `{ progress, blockers, next_steps, suggested_tools }`) using your `prompt_llm_for_json` pattern. Thatâ€™ll make downstream use easier.\n",
        "* Add a **cap** on how much memory you include (e.g., last N items) to control token usage.\n",
        "* Consider a **`track_progress` tag** (you already used `\"prompts\"`) if youâ€™re grouping analytics/ops tools separately.\n",
        "\n"
      ],
      "metadata": {
        "id": "aW_cT13TTO6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zXDzZPAOC4h"
      },
      "outputs": [],
      "source": [
        "@register_tool(tags=[\"prompts\"])\n",
        "def track_progress(action_context: ActionContext,\n",
        "                   _memory: Memory,\n",
        "                   action_registry: ActionRegistry) -> str:\n",
        "    \"\"\"Generate a progress report based on the current task, available tools, and memory context.\"\"\"\n",
        "\n",
        "    # Get tool descriptions for the prompt\n",
        "    tool_descriptions = \"\\n\".join(\n",
        "        f\"- {action.name}: {action.description}\"\n",
        "        for action in action_registry.get_actions()\n",
        "    )\n",
        "\n",
        "    # Get relevant memory content\n",
        "    memory_content = \"\\n\".join(\n",
        "        f\"{m['type']}: {m['content']}\"\n",
        "        for m in _memory.items\n",
        "        if m['type'] in ['user', 'system']\n",
        "    )\n",
        "\n",
        "    # Construct the prompt as a string\n",
        "    prompt = f\"\"\"Given the current task and available tools, generate a progress report.\n",
        "\n",
        "Think through this step by step:\n",
        "\n",
        "1. Identify the key components of the task and the intended outcome.\n",
        "2. Assess the progress made so far based on available information.\n",
        "3. Identify any blockers or issues preventing completion.\n",
        "4. Suggest the next steps to move forward efficiently.\n",
        "5. Recommend any tool usage that might help complete the task.\n",
        "\n",
        "Write your progress report in clear, structured points.\n",
        "\n",
        "Available tools:\n",
        "{tool_descriptions}\n",
        "\n",
        "Task context from memory:\n",
        "{memory_content}\n",
        "\n",
        "Provide a well-organized report on the current progress and next steps.\"\"\"\n",
        "\n",
        "    return prompt_llm(action_context=action_context, prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Progress Tracking Capability â€” What it does & why it matters\n",
        "\n",
        "**Goal:** add a lightweight â€œreflect â†’ adjustâ€ step after each tool action so the agent doesnâ€™t just *do*, it also *thinks about what it did* and what to do next.\n",
        "\n",
        "## What this capability adds\n",
        "\n",
        "* **End-of-loop reflection:** It runs **after each iteration** of the agent loop (or every N iterations) and writes a concise progress report into memory.\n",
        "* **Context-aware summaries:** The report is generated using:\n",
        "\n",
        "  * the **current memory** (task context, prior steps), and\n",
        "  * the **action registry** (which tools are available next),\n",
        "    so suggestions are realistic and grounded in what the agent can actually do.\n",
        "* **Persistent breadcrumbs:** Each report is stored in memory, creating an **audit trail** of what happened, blockers, and next steps.\n",
        "\n",
        "## The key knobs to notice\n",
        "\n",
        "* `track_frequency`: controls **how often** to run reflection.\n",
        "\n",
        "  * `1` = every loop (max insight, more tokens/latency)\n",
        "  * `2+` = periodic check-ins (balanced cost)\n",
        "* `memory_type`: where the report lands (e.g., `\"system\"` to increase its weight in the next prompt, or `\"assistant\"` if you want it treated as normal assistant output).\n",
        "* Internal counter (`iteration_count`): ensures **deterministic cadence** for reflection (e.g., â€œevery 3rd loopâ€).\n",
        "\n",
        "## Why it runs at the *end* of the loop\n",
        "\n",
        "* You reflect with **fresh results** from the just-executed action.\n",
        "* You can **spot blockers early** (API failure, missing data) and immediately course-correct.\n",
        "* It avoids re-planning on stale assumptions from before the action ran.\n",
        "\n",
        "## Trade-offs (be intentional)\n",
        "\n",
        "* **Cost/latency:** Each reflection is an extra LLM call. Use `track_frequency` to tune.\n",
        "* **Context size:** Reports add tokens to memory. Consider **summarizing or pruning** old reports over time.\n",
        "* **Signal quality:** If everything is already simple and linear, this may be overkill; for complex, branching tasks itâ€™s gold.\n",
        "\n",
        "## How it plays with other capabilities\n",
        "\n",
        "* **Plan-First + Progress-Tracking** = plan â†’ act â†’ reflect â†’ adjust.\n",
        "  The plan sets direction; progress tracking keeps it **adaptive**.\n",
        "* Works well with **Selective Memory Sharing**: share only the latest progress report when delegating to another agent.\n",
        "* Pairs nicely with **Safety/Staging**: reflect on risky steps and propose safer alternatives.\n",
        "\n",
        "## Practical tips\n",
        "\n",
        "* **Structure the output**: have `track_progress` return JSON (`progress`, `blockers`, `next_steps`, `suggested_tools`). Easier to parse in the next loop.\n",
        "* **Throttle on success**: if `blockers` is empty for a few iterations, temporarily **increase `track_frequency`** to save cost.\n",
        "* **Tag memories**: prefix content (e.g., `\"[Progress]\"`) so you can filter them quickly for prompts or dashboards.\n",
        "* **Metrics hooks**: optionally emit counters (iterations, blockers found) for observability.\n",
        "\n",
        "**Bottom line:** This capability gives your agent a simple, composable â€œmeta-cognitionâ€ loopâ€”small overhead, big gains in reliability, debuggability, and outcome quality, especially on multi-step or long-running tasks.\n"
      ],
      "metadata": {
        "id": "yd2500BlTp7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgressTrackingCapability(Capability):\n",
        "    def __init__(self, memory_type=\"system\", track_frequency=1):\n",
        "        super().__init__(\n",
        "            name=\"Progress Tracking\",\n",
        "            description=\"Tracks progress and enables reflection after actions\"\n",
        "        )\n",
        "        self.memory_type = memory_type\n",
        "        self.track_frequency = track_frequency\n",
        "        self.iteration_count = 0\n",
        "\n",
        "    def end_agent_loop(self, agent, action_context: ActionContext):\n",
        "        \"\"\"Generate and store progress report at the end of each iteration.\"\"\"\n",
        "        self.iteration_count += 1\n",
        "\n",
        "        # Only track progress on specified iterations\n",
        "        if self.iteration_count % self.track_frequency != 0:\n",
        "            return\n",
        "\n",
        "        # Get the memory and action registry from context\n",
        "        memory = action_context.get_memory()\n",
        "        action_registry = action_context.get_action_registry()\n",
        "\n",
        "        # Generate progress report\n",
        "        progress_report = track_progress(\n",
        "            action_context=action_context,\n",
        "            _memory=memory,\n",
        "            action_registry=action_registry\n",
        "        )\n",
        "\n",
        "        # Add the progress report to memory\n",
        "        memory.add_memory({\n",
        "            \"type\": self.memory_type,\n",
        "            \"content\": f\"Progress Report (Iteration {self.iteration_count}):\\n{progress_report}\"\n",
        "        })"
      ],
      "metadata": {
        "id": "tI3-lAx1Tqbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This capability uses the track_progress tool to generate detailed progress reports.\n",
        "# Letâ€™s see how it transforms agent behavior in practice:\n",
        "\n",
        "# Create an agent with progress tracking\n",
        "agent = Agent(\n",
        "    goals=[\n",
        "        Goal(\n",
        "            name=\"data_processing\",\n",
        "            description=\"Process and analyze customer feedback data\"\n",
        "        )\n",
        "    ],\n",
        "    capabilities=[\n",
        "        ProgressTrackingCapability(track_frequency=2)  # Track every 2nd iteration\n",
        "    ],\n",
        "    # ... other agent configuration\n",
        ")\n",
        "\n",
        "# Example execution flow\n",
        "memory = agent.run(\"Analyze customer feedback from Q4 and identify top issues\")\n",
        "# After each iteration (or every N iterations), the agent will pause to reflect."
      ],
      "metadata": {
        "id": "7tsLzi1OVveS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflect -> Adapt\n",
        "\n",
        "Because it works *after* a tool executes, the agent is looking at **real-world results**, not just its predictions:\n",
        "\n",
        "* If the tool failed (API timeout, bad credentials), the reflection will flag a **blocker** right away.\n",
        "* If a step finished but uncovered new information, the reflection can recommend **adjusting the sequence** or adding new steps.\n",
        "* If everythingâ€™s going smoothly, it simply confirms progress and moves to the next planned action.\n",
        "\n",
        "Itâ€™s a lot like how a human would work on a multi-step task:\n",
        "\n",
        "1. Try something.\n",
        "2. Look at the outcome.\n",
        "3. Ask â€œIs this working? Do I need to change course?â€\n",
        "4. If yes, adjust the plan before moving forward.\n",
        "\n",
        "Because the progress report goes into **memory**, the **next loopâ€™s prompt** already contains the updated situation, so the LLM naturally incorporates that into its next decision â€” without you hardcoding any new branching logic.\n",
        "\n",
        "If you combine this with a **Plan-First** capability, you basically have:\n",
        "\n",
        "> Plan â†’ Act â†’ Reflect â†’ Adapt â†’ Repeat\n",
        "\n",
        "â€¦which is about as close as you can get to giving an agent *situational awareness* without manually micro-managing every step.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VQopmFhW55-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## âœ… Benefits of End-of-Loop Progress Tracking\n",
        "\n",
        "Tracking progress **at the end** of each loop iteration â€” rather than at the beginning â€” offers key advantages:\n",
        "\n",
        "* **Assess the real impact** of the most recent action.\n",
        "* **Base decisions on fresh data** in memory, not outdated assumptions from the original plan.\n",
        "* **Adapt strategy dynamically** based on actual results.\n",
        "* **Create a clear audit trail** of agent decision-making for transparency and debugging.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Using Progress Reports in Decision Making\n",
        "\n",
        "Stored progress reports become part of the **agentâ€™s memory**, shaping its future decisions.\n",
        "When the agent chooses its next action, it can reference these reports to:\n",
        "\n",
        "* âœ… Avoid **repeating completed steps**.\n",
        "* ðŸš§ Address **blockers** that were identified.\n",
        "* ðŸ“… Follow through on **recommended next steps**.\n",
        "* ðŸ›  Use **suggested tools** effectively.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Why This Matters for Long-Running Tasks\n",
        "\n",
        "The combination of **immediate reflection** + **persistent memory** helps the agent:\n",
        "\n",
        "* Stay focused on its **goals**.\n",
        "* Adapt to **new information** and **changing circumstances**.\n",
        "* Spot when things are **going wrong** â€” and course-correct early.\n",
        "\n",
        "This is especially valuable for **complex or long-running workflows**, where **maintaining context** is crucial for success.\n",
        "\n",
        "---\n",
        "\n",
        "This section basically formalizes the *\"plan â†’ act â†’ reflect â†’ adapt\"* cycle we talked about earlier â€” except here the reflection is **baked into the loop** so it happens automatically every time.\n"
      ],
      "metadata": {
        "id": "NCthnr_8XmVh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8gOaIbRW9a5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}