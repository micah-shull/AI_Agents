{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0AOdRkWM16vHHFmJGlCDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/002_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines\n",
        "\n",
        "## 🔢 Think of Pipelines Like Job Titles\n",
        "\n",
        "Every model has a specialty — a job it's good at — and **pipelines are how you assign the right task to the right worker**.\n",
        "\n",
        "So when you choose a pipeline like `\"text-generation\"`, you're saying:\n",
        "\n",
        "> \"Hey model, your job is to finish the sentence or generate text from a prompt.\"\n",
        "\n",
        "Let’s go one by one:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 1. **GPT-style LLMs** → `\"text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Completing text\n",
        "- Continuing a sentence or thought\n",
        "- Open-ended generation\n",
        "\n",
        "**Examples:**\n",
        "- GPT-2\n",
        "- Falcon\n",
        "- DialoGPT\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text-generation\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Einstein was born in\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "\"1879 in the Kingdom of Württemberg.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. **T5 or FLAN-style Models** → `\"text2text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Following instructions\n",
        "- Question-answering\n",
        "- Summarization\n",
        "- Translation\n",
        "\n",
        "**Examples:**\n",
        "- `flan-t5-base`\n",
        "- `t5-small`\n",
        "- `flan-ul2`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text2text-generation\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Translate to French: I love learning AI agents\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "\"J'aime apprendre les agents d'IA\"\n",
        "```\n",
        "\n",
        "> These models take one string and return another string — they're like \"input → output\" machines.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3. **BERT-style Classifiers** → `\"text-classification\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Classifying text into categories\n",
        "- Sentiment analysis\n",
        "- Spam detection\n",
        "\n",
        "**Examples:**\n",
        "- `bert-base-uncased`\n",
        "- `distilbert-base-uncased-finetuned-sst-2-english`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text-classification\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"This product is amazing!\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.999}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 4. **Chatbot Models** → `\"conversational\"` or `\"text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Holding a conversation\n",
        "- Responding turn-by-turn\n",
        "- Remembering short-term history\n",
        "\n",
        "**Examples:**\n",
        "- `DialoGPT`\n",
        "- `Blenderbot`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"conversational\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Hi, who are you?\"\n",
        "```\n",
        "\n",
        "**It responds like a chatbot:**\n",
        "```python\n",
        "\"I'm a friendly bot. How can I help you today?\"\n",
        "```\n",
        "\n",
        "> Some of these work best with a conversation history object instead of plain text.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ In Plainest English:\n",
        "\n",
        "| Model Is Like...         | Best Used For                       | Use This Pipeline      |\n",
        "|--------------------------|-------------------------------------|------------------------|\n",
        "| A novelist               | Finishing stories                   | `\"text-generation\"`    |\n",
        "| A question-answer robot  | Following commands exactly          | `\"text2text-generation\"` |\n",
        "| A judge                  | Labeling or scoring things          | `\"text-classification\"` |\n",
        "| A chatbot friend         | Talking back and forth              | `\"conversational\"`      |\n"
      ],
      "metadata": {
        "id": "U6GKGSJDefeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import libraries"
      ],
      "metadata": {
        "id": "FOO9MrQIekWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up_KS031VUkX"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers huggingface_hub\n",
        "# !pip install python-dotenv\n",
        "# # !pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using Hugging Face models, the **pipeline you choose must match the task the model is designed for** — otherwise, the outputs might not make sense, or the model might not work at all.\n",
        "\n",
        "---\n",
        "\n",
        "# 🧠 Why You Use Different Pipelines for Different Models\n",
        "\n",
        "## 🔧 What is a Hugging Face `pipeline`?\n",
        "\n",
        "A **pipeline** is a wrapper that:\n",
        "- Automatically loads the right tokenizer/model\n",
        "- Handles input formatting\n",
        "- Returns friendly outputs (like just the generated text or label)\n",
        "\n",
        "Different models are trained for different **tasks**, and each task has a matching pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Common Hugging Face Pipelines\n",
        "\n",
        "| Pipeline                    | Description                                           | Example Models                          |\n",
        "|----------------------------|-------------------------------------------------------|-----------------------------------------|\n",
        "| `text-generation`          | Predict the **next words**                           | `gpt2`, `falcon-rw-1b`, `DialoGPT`      |\n",
        "| `text2text-generation`     | Convert input to another string (task-following)     | `flan-t5-base`, `t5-small`, `bart-base` |\n",
        "| `text-classification`      | Predict category/label from text                     | `bert-base`, `distilbert`               |\n",
        "| `question-answering`       | Answer a question using a given context              | `bert-large-uncased-whole-word-masking-finetuned-squad` |\n",
        "| `summarization`            | Summarize longer input                               | `bart-large-cnn`, `t5-base`             |\n",
        "| `translation`              | Translate between languages                          | `t5`, `mbart`, `opus-mt-en-fr`          |\n",
        "| `conversational`           | Chat-style models with memory                        | `DialoGPT`, `Blenderbot`                |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 So which to use?\n",
        "\n",
        "| Model Type             | Pipeline you should use           |\n",
        "|------------------------|-----------------------------------|\n",
        "| GPT-style LLMs         | `\"text-generation\"`               |\n",
        "| T5/FLAN-style models   | `\"text2text-generation\"`          |\n",
        "| BERT-style classifiers | `\"text-classification\"`           |\n",
        "| Chatbots               | `\"conversational\"` (or `text-generation` in a loop) |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Let’s Put It All Together\n",
        "\n",
        "Here’s a complete working example using **`google/flan-t5-base`** with the correct pipeline:\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Full Working Agent Step (with `flan-t5-base`)\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load instruction-tuned model\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "# Use text2text pipeline\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Prompt to generate tool call\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Generate & Print Output\n",
        "\n",
        "```python\n",
        "response = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "\n",
        "print(\"🤖 Agent Response:\\n\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ib6txFDVmbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Parse the Model Output & Run the Tool\n",
        "\n",
        "This is where we:\n",
        "1. Use an **LLM to decide which function to call**\n",
        "2. **Parse** the output into Python\n",
        "3. Run the tool (`search_wikipedia`)\n",
        "4. Print the final result\n"
      ],
      "metadata": {
        "id": "buk2JLEaL56-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 1: Final Working Prompt + Response"
      ],
      "metadata": {
        "id": "JVo-AbLjesG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supress warning\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# load model and libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "HF-ULtP3Vv1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📥 Prompt + Generation"
      ],
      "metadata": {
        "id": "6-EejVTbLseU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\n",
        "Return ONLY the function name and parameters in the exact format above. Do NOT answer the question directly.\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97vgP5kYOXEq",
        "outputId": "86ee62e4-c48a-4d50-d82a-425ea24f87f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " numeric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Respond in this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-QlaCRvWGoF",
        "outputId": "aa10bd4c-8393-4979-dd87-7a1242c74f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"years\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DllWAR0xOOYo",
        "outputId": "33c0117e-bced-4d7f-afba-7040928b0e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"Marie Curie\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# supress warning\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# model_id = \"google/flan-t5-base\"\n",
        "model_id = \"google/flan-t5-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdyABvWcORtr",
        "outputId": "33aa44be-5045-4870-c926-69dffc9bb745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"when did marie curie die\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\n",
        "Return ONLY the function name and parameters in the exact format above.\n",
        "Both lines must be present, and the second line must contain a valid JSON object.\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC_XuydKPhIl",
        "outputId": "1ce94f32-f3c7-45f4-86b8-cfbc0ed22a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Why the Model Is Struggling\n",
        "\n",
        "What you're experiencing is **exactly** what happens when professionals work with LLMs in production systems. You're doing all the right things, yet the model returns:\n",
        "\n",
        "```text\n",
        "function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\"\n",
        "```\n",
        "\n",
        "It:\n",
        "- ✅ Gets the **intent** right\n",
        "- ✅ Gets the **label names** (`function_name`, `function_parms`)\n",
        "- ✅ Gives a meaningful **query**\n",
        "- ❌ But fails to wrap `\"query\": ...` in `{}` — breaking your `json.loads()` logic\n",
        "\n",
        "So what's the deal?\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 The Underlying Challenge (LLMs ≠ Parsers)\n",
        "\n",
        "### 1. **LLMs Are Language Models, Not Code Generators**\n",
        "They're trained to generate the *most likely next token* — not to strictly obey formatting rules.\n",
        "\n",
        "Even though you're showing the model the format and telling it clearly, it still thinks:\n",
        "> “I know what you *really* want. You want the **query**, right? Let me just give you that.”\n",
        "\n",
        "It's being helpful in a **human** way — not in a **structured machine-readable** way.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Models Like Flan-T5 Are Instruction-Tuned — Not Format-Rigid**\n",
        "\n",
        "Even `flan-t5-large`, while better, is not trained specifically to return JSON or structured APIs. It’s trained on:\n",
        "- Following instructions\n",
        "- Answering questions\n",
        "- Doing general NLP tasks\n",
        "\n",
        "But structure-following is **not its core strength**. It will:\n",
        "- Drift from the format\n",
        "- Omit required punctuation (like `{}` or `:`)\n",
        "- Rephrase answers\n",
        "- Drop line breaks\n",
        "Even when instructed otherwise.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **This Is Completely Normal**\n",
        "\n",
        "This process you’re going through — designing prompts, checking format compliance, post-processing output — is **the norm** when working with LLMs.\n",
        "\n",
        "✅ **Even OpenAI does this internally** in ChatGPT and API tools. They:\n",
        "- Use strong prompt scaffolding\n",
        "- Run output through **regex matchers**\n",
        "- Use **fallback correction logic**\n",
        "- Sometimes even **rerun the model with a correction prompt** if format isn’t followed\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ How Pros Handle This\n",
        "\n",
        "| Technique | Description |\n",
        "|----------|-------------|\n",
        "| **Few-shot prompting** | You're already doing this — and it's helping! |\n",
        "| **Post-processing output** | Add format validation, fix missing braces, retry on fail |\n",
        "| **Use larger or more specialized models** | Like `flan-ul2`, `zephyr`, or `gpt-3.5-turbo` |\n",
        "| **Guardrails or templates** | Frameworks like LangChain or GuardrailsAI enforce structure |\n",
        "| **Expect “fuzzy output” and plan for it** | Your job as the agent designer is to catch & correct |\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Takeaway: You’re Building the Right Mental Model\n",
        "\n",
        "You’ve now experienced firsthand:\n",
        "\n",
        "> 💬 “Even if a model understands what you want, that doesn’t mean it will **say it in the exact form** you expect — and you have to help it get there.”\n",
        "\n",
        "That’s agent work in a nutshell.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ What You Can Do Now\n",
        "\n",
        "Here are a few realistic next moves:\n",
        "\n",
        "### 🔁 **Add a Format-Correction Layer**\n",
        "You've already seen we can patch missing `{}` easily:\n",
        "```python\n",
        "# If output lacks valid JSON object, wrap it manually\n",
        "if not params_match:\n",
        "    query_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', output)\n",
        "    if query_match:\n",
        "        query_val = query_match.group(1)\n",
        "        params_raw = '{ \"query\": \"' + query_val + '\" }'\n",
        "```\n",
        "\n",
        "### 🚀 **Try a More Structured Model**\n",
        "If you're curious, try:\n",
        "```python\n",
        "model_id = \"google/flan-ul2\"  # Larger, more reliable\n",
        "```\n",
        "\n",
        "Or try using **GuardrailsAI** or **LangChain Output Parsers**, which give you programmatic enforcement.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bvv0eTsBS-No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 2: Parse the Response (Extract Function + Parameters)"
      ],
      "metadata": {
        "id": "a3X4Hqc3L07t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\n",
        "Return ONLY the function name and parameters in the exact format above.\n",
        "Both lines must be present, and the second line must contain a valid JSON object.\n",
        "\"\"\".strip()\n",
        "\n",
        "# 🔮 Generate model output\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n",
        "\n",
        "# 🧠 Try to extract using regex\n",
        "fn_match = re.search(r\"function_name:\\s*(\\w+)\", output)\n",
        "params_match = re.search(r\"function_parms:\\s*(\\{.*\\})\", output)\n",
        "\n",
        "# 🛠 Fallback: manually wrap \"query\": \"...\" if braces are missing\n",
        "if not params_match:\n",
        "    query_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', output)\n",
        "    if query_match:\n",
        "        query_val = query_match.group(1)\n",
        "        params_raw = '{ \"query\": \"' + query_val + '\" }'\n",
        "    else:\n",
        "        print(\"⚠️ Could not extract function parameters.\")\n",
        "        raise ValueError(\"Invalid model output structure.\")\n",
        "else:\n",
        "    params_raw = params_match.group(1)\n",
        "\n",
        "# ✅ Extract function name\n",
        "if fn_match:\n",
        "    function_name = fn_match.group(1)\n",
        "else:\n",
        "    print(\"⚠️ Could not extract function name.\")\n",
        "    raise ValueError(\"Invalid model output structure.\")\n",
        "\n",
        "# 🔄 Convert JSON string to dictionary\n",
        "function_parms = json.loads(params_raw)\n",
        "\n",
        "# ✅ Final result\n",
        "print(\"🔧 Parsed Action:\")\n",
        "print(\"Function:\", function_name)\n",
        "print(\"Parameters:\", function_parms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1JiqYQmL0HW",
        "outputId": "73e68605-1a4f-4e50-d747-b13a528fcc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\" \n",
            "🔧 Parsed Action:\n",
            "Function: search_wikipedia\n",
            "Parameters: {'query': 'when was Marie Curie born'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🤖 Model Output Query Functions!\n",
        "```\n",
        "function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\"\n",
        "```\n",
        "\n",
        "Despite the model not wrapping the `\"query\": \"...\"` inside `{}`, your fallback logic **caught that**, repaired the format, and parsed it successfully.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Final Output:\n",
        "```python\n",
        "Function: search_wikipedia\n",
        "Parameters: {'query': 'when was Marie Curie born'}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What This Confirms\n",
        "\n",
        "| ✅ Insight | 📌 Why It’s Important |\n",
        "|-----------|------------------------|\n",
        "| Your prompt works | The model picked the correct function and a relevant query |\n",
        "| Your code is robust | It recovered from a common LLM formatting issue |\n",
        "| Your parsing logic is solid | It handles both ideal and imperfect outputs gracefully |\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Optional Next Steps\n",
        "\n",
        "Now that you’ve got this running like a pro:\n",
        "\n",
        "### 🔁 Add a Retry or Re-prompt If the Model Really Fails\n",
        "You can wrap your logic in a function and retry once if both regexes fail.\n",
        "\n",
        "### 🔗 Call a real function next!\n",
        "Example:\n",
        "```python\n",
        "def search_wikipedia(query):\n",
        "    return f\"📚 Searching Wikipedia for: {query}\"\n",
        "\n",
        "result = search_wikipedia(function_parms[\"query\"])\n",
        "print(result)\n",
        "```\n",
        "\n",
        "### 🧪 Test with other questions\n",
        "Try:\n",
        "- \"What are the symptoms of scurvy?\"\n",
        "- \"Where was the Mona Lisa painted?\"\n",
        "- \"When did the Cold War end?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ic1rj6RyYx-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 3: Run the Tool (search_wikipedia)\n",
        "\n",
        "**The next step is to “close the loop” by letting your agent actually *run* the selected function** (`search_wikipedia`) using the model’s parsed output. 🔁\n",
        "\n",
        "You’ve already:\n",
        "1. Got the model generating structured actions ✅\n",
        "2. Parsed the output into `function_name` and `function_parms` ✅\n",
        "3. Defined a real `search_wikipedia` function ✅\n",
        "\n",
        "Now you just need to **let the model “call” the tool** it selected — like an actual agent would.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Tool Registry (You're Already Doing This)\n",
        "\n",
        "```python\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "```\n",
        "\n",
        "Perfect — this acts like the agent’s toolbox. Each string maps to a function.\n",
        "\n",
        "## 🔁 You Now Have a Working AI Agent\n",
        "\n",
        "It:\n",
        "- Interprets questions\n",
        "- Selects a tool\n",
        "- Parses structured output\n",
        "- Executes real code based on the model’s decision\n",
        "\n",
        "This is exactly what LangChain, OpenAI Agents, and production assistants do under the hood!\n",
        "\n"
      ],
      "metadata": {
        "id": "QXX98GKOMMAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# --- Tool definition ---\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "# --- Tool registry ---\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "# --- Prompt ---\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\n",
        "Return ONLY the function name and parameters in the exact format above.\n",
        "Both lines must be present, and the second line must contain a valid JSON object.\n",
        "\"\"\".strip()\n",
        "\n",
        "# --- Run model ---\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n",
        "\n",
        "# --- Parse output ---\n",
        "fn_match = re.search(r\"function_name:\\s*(\\w+)\", output)\n",
        "params_match = re.search(r\"function_parms:\\s*(\\{.*\\})\", output)\n",
        "\n",
        "if not params_match:\n",
        "    query_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', output)\n",
        "    if query_match:\n",
        "        query_val = query_match.group(1)\n",
        "        params_raw = '{ \"query\": \"' + query_val + '\" }'\n",
        "    else:\n",
        "        raise ValueError(\"Could not extract query.\")\n",
        "else:\n",
        "    params_raw = params_match.group(1)\n",
        "\n",
        "if fn_match:\n",
        "    function_name = fn_match.group(1)\n",
        "else:\n",
        "    raise ValueError(\"Could not extract function name.\")\n",
        "\n",
        "function_parms = json.loads(params_raw)\n",
        "\n",
        "# --- Execute tool ---\n",
        "if function_name in available_tools:\n",
        "    result = available_tools[function_name](**function_parms)\n",
        "else:\n",
        "    result = f\"❌ Unknown function: {function_name}\"\n",
        "\n",
        "print(\"📥 Tool Output:\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9pSHuW-MOhO",
        "outputId": "25f8db0d-b6a6-4e5d-c168-700844ee13b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\" \n",
            "📥 Tool Output:\n",
            "Skłodowska-<span class=\"searchmatch\">Curie</span> (Polish: [ˈmarja salɔˈmɛa skwɔˈdɔfska kʲiˈri] ; née Skłodowska; 7 November 1867 – 4 July 1934), known simply as <span class=\"searchmatch\">Marie</span> <span class=\"searchmatch\">Curie</span> (/ˈkjʊəri/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🤖 Model Output:\n",
        "The model correctly selected `search_wikipedia` and gave a properly structured response with the `query: \"when was Marie Curie born\"`, which is perfect.\n",
        "\n",
        "### 📥 Tool Output:\n",
        "You successfully called the **Wikipedia API** and got a **relevant snippet** in return. The output includes useful information, but notice the HTML tags like `<span class=\"searchmatch\">`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Why This Happened (HTML Formatting)\n",
        "\n",
        "The output from Wikipedia’s API often includes **HTML formatting** to highlight search matches (like `Marie Curie`), which is **perfectly normal**. The model is generating a valid query, and the Wikipedia API is returning the matching results in HTML.\n",
        "\n",
        "### What You Can Do:\n",
        "You have a couple of options to clean up that HTML.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solution 1: Strip HTML Tags (Simple Approach)\n",
        "\n",
        "You can use **BeautifulSoup** (from `bs4` library) to clean up the HTML:\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Strip HTML tags from the Wikipedia result\n",
        "cleaned_result = BeautifulSoup(result, \"html.parser\").get_text()\n",
        "\n",
        "print(\"📥 Cleaned Tool Output:\")\n",
        "print(cleaned_result)\n",
        "```\n",
        "\n",
        "This will remove any HTML tags and give you just the text content.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solution 2: Return HTML if You Want to Keep It\n",
        "\n",
        "If you’d like to return the **raw HTML** (e.g., for rich text or web rendering), you can leave the output as-is. This is useful if you plan to format or present the data later in a web interface.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Next Steps\n",
        "\n",
        "- **Post-process the tool output** depending on your use case (clean HTML or keep raw)\n",
        "- **Add more tools** (e.g., `get_weather`, `save_note`) and link them together\n",
        "- **Create a flexible agent** that adapts to different user inputs dynamically\n"
      ],
      "metadata": {
        "id": "cVgzVmxibRdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Strip HTML Tags (Simple Approach)"
      ],
      "metadata": {
        "id": "0DpMI4rfMgnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Tool definition ---\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "# --- Tool registry ---\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "# --- Prompt ---\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\n",
        "Return ONLY the function name and parameters in the exact format above.\n",
        "Both lines must be present, and the second line must contain a valid JSON object.\n",
        "\"\"\".strip()\n",
        "\n",
        "# --- Run model ---\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n",
        "\n",
        "# --- Parse output ---\n",
        "fn_match = re.search(r\"function_name:\\s*(\\w+)\", output)\n",
        "params_match = re.search(r\"function_parms:\\s*(\\{.*\\})\", output)\n",
        "\n",
        "if not params_match:\n",
        "    query_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', output)\n",
        "    if query_match:\n",
        "        query_val = query_match.group(1)\n",
        "        params_raw = '{ \"query\": \"' + query_val + '\" }'\n",
        "    else:\n",
        "        raise ValueError(\"Could not extract query.\")\n",
        "else:\n",
        "    params_raw = params_match.group(1)\n",
        "\n",
        "if fn_match:\n",
        "    function_name = fn_match.group(1)\n",
        "else:\n",
        "    raise ValueError(\"Could not extract function name.\")\n",
        "\n",
        "function_parms = json.loads(params_raw)\n",
        "\n",
        "# --- Execute tool ---\n",
        "if function_name in available_tools:\n",
        "    result = available_tools[function_name](**function_parms)\n",
        "else:\n",
        "    result = f\"❌ Unknown function: {function_name}\"\n",
        "\n",
        "print(\"📥 Tool Output:\")\n",
        "print(result)\n",
        "\n",
        "# Strip HTML tags from the Wikipedia result\n",
        "cleaned_result = BeautifulSoup(result, \"html.parser\").get_text()\n",
        "\n",
        "print(\"📥 Cleaned Tool Output:\")\n",
        "print(cleaned_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTY4ObjLMUAj",
        "outputId": "350898e6-68cc-476a-fa11-9f90b67e1eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " function_name: search_wikipedia function_parms:  \"query\": \"when was Marie Curie born\" \n",
            "📥 Tool Output:\n",
            "Skłodowska-<span class=\"searchmatch\">Curie</span> (Polish: [ˈmarja salɔˈmɛa skwɔˈdɔfska kʲiˈri] ; née Skłodowska; 7 November 1867 – 4 July 1934), known simply as <span class=\"searchmatch\">Marie</span> <span class=\"searchmatch\">Curie</span> (/ˈkjʊəri/\n",
            "📥 Cleaned Tool Output:\n",
            "Skłodowska-Curie (Polish: [ˈmarja salɔˈmɛa skwɔˈdɔfska kʲiˈri] ; née Skłodowska; 7 November 1867 – 4 July 1934), known simply as Marie Curie (/ˈkjʊəri/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Tool definition ---\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Step 1: Search\n",
        "    search_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=search_params)\n",
        "    search_results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if not search_results:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    page_id = search_results[0][\"pageid\"]\n",
        "\n",
        "    # Step 2: Get page extract (clean summary)\n",
        "    extract_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "        \"explaintext\": True,\n",
        "        \"pageids\": page_id,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    extract_response = requests.get(url, params=extract_params)\n",
        "    pages = extract_response.json()[\"query\"][\"pages\"]\n",
        "    extract = next(iter(pages.values()))[\"extract\"]\n",
        "\n",
        "    return extract\n",
        "\n",
        "# --- Tool registry ---\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "# --- Prompt ---\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Examples:\n",
        "Question: What is the capital of France?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"capital of France\" }\n",
        "\n",
        "Question: Who painted the Mona Lisa?\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Mona Lisa painter\" }\n",
        "\n",
        "### Now answer:\n",
        "Question: How old was Marie Curie when she died?\n",
        "function_name:\n",
        "\n",
        "Important rules:\n",
        "- Use ONLY the functions shown above. Do not invent new ones.\n",
        "- Do NOT use =. Use colon format only.\n",
        "- Both lines must be included. The second must be valid JSON with \"query\".\n",
        "\"\"\".strip()\n",
        "\n",
        "# --- Run model ---\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n",
        "\n",
        "# --- Parse output ---\n",
        "fn_match = re.search(r\"function_name:\\s*(\\w+)\", output)\n",
        "params_match = re.search(r\"function_parms:\\s*(\\{.*\\})\", output)\n",
        "\n",
        "if not params_match:\n",
        "    query_match = re.search(r'\"query\":\\s*\"([^\"]+)\"', output)\n",
        "    if not query_match:\n",
        "        print(\"⚠️ Unexpected model output:\\n\", output)\n",
        "        print(\"🤖 I couldn't understand the request. Please try asking differently.\")\n",
        "        result = \"🤖 I couldn't understand the request. Please try asking differently.\"\n",
        "    else:\n",
        "        query_val = query_match.group(1)\n",
        "        params_raw = json.dumps({ \"query\": query_val })\n",
        "        params_match = True  # Manually mark that we can proceed\n",
        "\n",
        "result = None\n",
        "\n",
        "# --- Parse function name ---\n",
        "if not fn_match:\n",
        "    function_name = None\n",
        "    result = \"❌ Could not extract function name.\"\n",
        "\n",
        "elif function_name not in available_tools:\n",
        "    result = f\"❌ Unknown function: {function_name}\"\n",
        "\n",
        "elif not params_match:\n",
        "    # This will be skipped if we manually set params_match = True above\n",
        "    result = \"❌ Missing or invalid parameters.\"\n",
        "\n",
        "else:\n",
        "    # Clean parse\n",
        "    function_parms = json.loads(params_raw)\n",
        "    result = available_tools[function_name](**function_parms)\n",
        "\n",
        "print(\"📥 Tool Output:\")\n",
        "print(result)\n",
        "\n",
        "# Only clean if it's text from the tool, not an error\n",
        "if \"❌\" not in result and \"🤖\" not in result:\n",
        "    cleaned_result = BeautifulSoup(result, \"html.parser\").get_text()\n",
        "    print(\"📥 Cleaned Tool Output:\")\n",
        "    print(cleaned_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAAjuKCccY0",
        "outputId": "f0d34d55-e362-40ce-d2f9-2ebf00b594ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Model Output:\n",
            " Do NOT use =. Use colon format only.\n",
            "⚠️ Unexpected model output:\n",
            " Do NOT use =. Use colon format only.\n",
            "🤖 I couldn't understand the request. Please try asking differently.\n",
            "📥 Tool Output:\n",
            "❌ Could not extract function name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 💡 Challenges of Building AI Agents\n",
        "\n",
        "Creating AI agents may seem easy at first — “just call a model!” — but under the hood, it’s a complex system of moving parts. Here are the key challenges:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 1. **LLMs Are Fuzzy, Not Deterministic**\n",
        "\n",
        "> **Problem**: LLMs don’t always return output in the exact format you expect, even with clear prompts and examples.\n",
        "\n",
        "You told the model:\n",
        "```\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "```\n",
        "\n",
        "But the model said:\n",
        "```\n",
        "Do NOT use =. Use colon format only.\n",
        "```\n",
        "\n",
        "**Why?** Because LLMs are trained to predict *the most likely next token*, not to obey rules. They:\n",
        "- Repeat instructions\n",
        "- Invent new formats\n",
        "- Skip line breaks\n",
        "- Hallucinate function names\n",
        "\n",
        "🧠 **Takeaway**: Agents must **validate and interpret** model output — they can’t trust it blindly.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧭 2. **Prompt Design is an Art**\n",
        "\n",
        "> **Problem**: Even small phrasing changes can break or fix the model’s response.\n",
        "\n",
        "You learned that:\n",
        "- Too many rules in a row = model echoes them\n",
        "- Missing separators = model doesn’t know when to respond\n",
        "- Examples too far away = model forgets them\n",
        "\n",
        "🧠 **Takeaway**: Prompts aren’t just instructions — they’re **UI for the model’s brain**. And you have to design them like interfaces.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 3. **Parsing Model Output is Fragile**\n",
        "\n",
        "> **Problem**: The model might give:\n",
        "```text\n",
        "function_name: search_wikipedia function_parms: \"query\": \"marie curie\"\n",
        "```\n",
        "...which breaks `json.loads()` and regex patterns.\n",
        "\n",
        "Even with:\n",
        "- Few-shot examples ✅\n",
        "- JSON-like output ✅\n",
        "\n",
        "…you **still need fallback logic**, regex, and format correction.\n",
        "\n",
        "🧠 **Takeaway**: Expect **messy, ambiguous output** — and build resilient parsing into every agent.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 4. **Tool Use Requires Precision**\n",
        "\n",
        "> **Problem**: Once the model chooses a tool, the agent needs:\n",
        "- The correct function name\n",
        "- Valid parameter names\n",
        "- Safely deserialized arguments\n",
        "\n",
        "But if even one step fails (e.g., function name doesn’t match the registry), the agent breaks.\n",
        "\n",
        "🧠 **Takeaway**: AI agents must be **part model, part rules-based router**, and both parts must work together smoothly.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 5. **You Need Fallbacks, Retries, and Graceful Degradation**\n",
        "\n",
        "> **Problem**: A real agent can’t just crash or say “I don’t know.”\n",
        "\n",
        "It must:\n",
        "- Retry with a simpler prompt\n",
        "- Ask clarifying questions\n",
        "- Route to a default response\n",
        "- Log and explain what went wrong\n",
        "\n",
        "🧠 **Takeaway**: Building an agent means designing a **system of recovery**, not just hoping the model gets it right.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ What You’re Doing Right (and Why It Matters)\n",
        "\n",
        "You've already implemented:\n",
        "- ✔️ Prompt scaffolding\n",
        "- ✔️ Tool dispatch via function routing\n",
        "- ✔️ Fallback parsing logic\n",
        "- ✔️ Error handling\n",
        "- ✔️ HTML cleaning\n",
        "- ✔️ Output validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pwxb41th8tzx"
      }
    }
  ]
}