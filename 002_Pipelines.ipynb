{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPSoPSzu+JuPMJWXtb7jxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/002_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines\n",
        "\n",
        "## üî¢ Think of Pipelines Like Job Titles\n",
        "\n",
        "Every model has a specialty ‚Äî a job it's good at ‚Äî and **pipelines are how you assign the right task to the right worker**.\n",
        "\n",
        "So when you choose a pipeline like `\"text-generation\"`, you're saying:\n",
        "\n",
        "> \"Hey model, your job is to finish the sentence or generate text from a prompt.\"\n",
        "\n",
        "Let‚Äôs go one by one:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **GPT-style LLMs** ‚Üí `\"text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Completing text\n",
        "- Continuing a sentence or thought\n",
        "- Open-ended generation\n",
        "\n",
        "**Examples:**\n",
        "- GPT-2\n",
        "- Falcon\n",
        "- DialoGPT\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text-generation\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Einstein was born in\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "\"1879 in the Kingdom of W√ºrttemberg.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **T5 or FLAN-style Models** ‚Üí `\"text2text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Following instructions\n",
        "- Question-answering\n",
        "- Summarization\n",
        "- Translation\n",
        "\n",
        "**Examples:**\n",
        "- `flan-t5-base`\n",
        "- `t5-small`\n",
        "- `flan-ul2`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text2text-generation\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Translate to French: I love learning AI agents\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "\"J'aime apprendre les agents d'IA\"\n",
        "```\n",
        "\n",
        "> These models take one string and return another string ‚Äî they're like \"input ‚Üí output\" machines.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **BERT-style Classifiers** ‚Üí `\"text-classification\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Classifying text into categories\n",
        "- Sentiment analysis\n",
        "- Spam detection\n",
        "\n",
        "**Examples:**\n",
        "- `bert-base-uncased`\n",
        "- `distilbert-base-uncased-finetuned-sst-2-english`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"text-classification\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"This product is amazing!\"\n",
        "```\n",
        "\n",
        "**It returns:**\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.999}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 4. **Chatbot Models** ‚Üí `\"conversational\"` or `\"text-generation\"`\n",
        "\n",
        "**What these models are good at:**\n",
        "- Holding a conversation\n",
        "- Responding turn-by-turn\n",
        "- Remembering short-term history\n",
        "\n",
        "**Examples:**\n",
        "- `DialoGPT`\n",
        "- `Blenderbot`\n",
        "\n",
        "**Pipeline:**\n",
        "```python\n",
        "pipeline(\"conversational\")\n",
        "```\n",
        "\n",
        "**You give it:**\n",
        "```python\n",
        "\"Hi, who are you?\"\n",
        "```\n",
        "\n",
        "**It responds like a chatbot:**\n",
        "```python\n",
        "\"I'm a friendly bot. How can I help you today?\"\n",
        "```\n",
        "\n",
        "> Some of these work best with a conversation history object instead of plain text.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ In Plainest English:\n",
        "\n",
        "| Model Is Like...         | Best Used For                       | Use This Pipeline      |\n",
        "|--------------------------|-------------------------------------|------------------------|\n",
        "| A novelist               | Finishing stories                   | `\"text-generation\"`    |\n",
        "| A question-answer robot  | Following commands exactly          | `\"text2text-generation\"` |\n",
        "| A judge                  | Labeling or scoring things          | `\"text-classification\"` |\n",
        "| A chatbot friend         | Talking back and forth              | `\"conversational\"`      |\n"
      ],
      "metadata": {
        "id": "U6GKGSJDefeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import libraries"
      ],
      "metadata": {
        "id": "FOO9MrQIekWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up_KS031VUkX"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers huggingface_hub\n",
        "# !pip install python-dotenv\n",
        "# # !pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load api key"
      ],
      "metadata": {
        "id": "lFTrH4DLeoDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n",
        "# Explicitly load your .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "\n",
        "# Now it can find the variable\n",
        "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "\n",
        "if not token:\n",
        "    raise ValueError(\"üö® Hugging Face token not found. Is your .env file set correctly?\")"
      ],
      "metadata": {
        "id": "5tTqt3LSVcw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using Hugging Face models, the **pipeline you choose must match the task the model is designed for** ‚Äî otherwise, the outputs might not make sense, or the model might not work at all.\n",
        "\n",
        "---\n",
        "\n",
        "# üß† Why You Use Different Pipelines for Different Models\n",
        "\n",
        "## üîß What is a Hugging Face `pipeline`?\n",
        "\n",
        "A **pipeline** is a wrapper that:\n",
        "- Automatically loads the right tokenizer/model\n",
        "- Handles input formatting\n",
        "- Returns friendly outputs (like just the generated text or label)\n",
        "\n",
        "Different models are trained for different **tasks**, and each task has a matching pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Common Hugging Face Pipelines\n",
        "\n",
        "| Pipeline                    | Description                                           | Example Models                          |\n",
        "|----------------------------|-------------------------------------------------------|-----------------------------------------|\n",
        "| `text-generation`          | Predict the **next words**                           | `gpt2`, `falcon-rw-1b`, `DialoGPT`      |\n",
        "| `text2text-generation`     | Convert input to another string (task-following)     | `flan-t5-base`, `t5-small`, `bart-base` |\n",
        "| `text-classification`      | Predict category/label from text                     | `bert-base`, `distilbert`               |\n",
        "| `question-answering`       | Answer a question using a given context              | `bert-large-uncased-whole-word-masking-finetuned-squad` |\n",
        "| `summarization`            | Summarize longer input                               | `bart-large-cnn`, `t5-base`             |\n",
        "| `translation`              | Translate between languages                          | `t5`, `mbart`, `opus-mt-en-fr`          |\n",
        "| `conversational`           | Chat-style models with memory                        | `DialoGPT`, `Blenderbot`                |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† So which to use?\n",
        "\n",
        "| Model Type             | Pipeline you should use           |\n",
        "|------------------------|-----------------------------------|\n",
        "| GPT-style LLMs         | `\"text-generation\"`               |\n",
        "| T5/FLAN-style models   | `\"text2text-generation\"`          |\n",
        "| BERT-style classifiers | `\"text-classification\"`           |\n",
        "| Chatbots               | `\"conversational\"` (or `text-generation` in a loop) |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Let‚Äôs Put It All Together\n",
        "\n",
        "Here‚Äôs a complete working example using **`google/flan-t5-base`** with the correct pipeline:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Full Working Agent Step (with `flan-t5-base`)\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load instruction-tuned model\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "# Use text2text pipeline\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Prompt to generate tool call\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Generate & Print Output\n",
        "\n",
        "```python\n",
        "response = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "\n",
        "print(\"ü§ñ Agent Response:\\n\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Recap: What You Just Learned\n",
        "\n",
        "- You **match the pipeline to the model‚Äôs purpose**\n",
        "- `flan-t5-base` is a **text2text model**, so you use `\"text2text-generation\"`\n",
        "- `DialoGPT` is chat-based ‚Üí use `\"text-generation\"` or `\"conversational\"`\n",
        "- Each model expects input in a different **style**, based on how it was trained\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you‚Äôd like help **parsing the model's response into Python code** or **running the tool based on its decision** ‚Äî you're very close to building your complete agent loop!"
      ],
      "metadata": {
        "id": "1ib6txFDVmbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model"
      ],
      "metadata": {
        "id": "JVo-AbLjesG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load instruction-tuned model\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "# Use text2text pipeline\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "b075232dcdf44a36b90eff2584b70227",
            "20000762df1a41448cb26b04afa589ee",
            "d15d00d3a9f3429e981a411060f0276a",
            "8e598e8e4db5448f83599b27e4a44cf0",
            "91512416e2454f2599323d5f8eabe043",
            "2badfd82d2da4538a6b30029d5cc74fb",
            "619472835e1842299882da441a554cfa",
            "56782040ab574b84823999b2f82cf6ec",
            "0713c613f5f140c090c231a598cbf0fe",
            "a340cdadea4147a38dc8a36b12da9422",
            "bc83b17e0bad48a68227568fb4dd85f8",
            "11fab57787ef4ad5adabc3d0702d5d3e",
            "00f4ccc8a43b4c4eab35b32c0a779468",
            "cd8bb672aefa4968b379570be6cdcae2",
            "f2e92db7999d4bfd83b9f58e6a49e684",
            "679795457fe343a1aa8e53e594b560ce",
            "f6a6cdde6ddc4b8ea55c7d46e9cc4548",
            "23c49ec3df2949058e19d4a431753c9b",
            "ec6dc05d2c9c4384a552360c7cbe676d",
            "a660ff9c7be94f4d80d1324f02b3d05f",
            "fb678c7d7d28452b9ee468cc54a93796",
            "54a9f1b565ae481f93952bf3ffa1a612",
            "07cc627a820b4f3d98ff94fb4f71e9df",
            "a9e2b7a4b9d047c1abb97faf5ea35801",
            "2b1be61588264f419d521150e89d0122",
            "cc2d815594ff4210b0eafafba672be9f",
            "de5ca20523ac4432ba06a48a3ad5ec42",
            "2814f99b8c694739b104413170608449",
            "266fd623411848238f071b3eca6bd6aa",
            "6f50e230b66445cfb1a4d05d5e08a77a",
            "7413bc2ecc664d2382487d6d74fe484c",
            "bad82a6702f7407ba29dd65f69f16a0d",
            "9c4693bbbe1a4ac895f689402d9b607e",
            "295ffa1c46f24a78961e7c74e39b494d",
            "d909ac0abd4a4fecae479adc7c11d569",
            "c840e8cdb345461fb3cb5e73a69a8c83",
            "c27df3ff0aa2450883a3146cfb44c987",
            "836f6208e75e458784b515e4b7c10577",
            "e6009c0f1d3c45a484cb68282f9a5026",
            "5f621103945f440baa220c2c19ba41e7",
            "dde414528e224b728b0b2cad46cb7e9f",
            "7b879a263f4a47fba8146e8ba526d306",
            "8bb5788721ae46c2b7d9dc36d38e663a",
            "b5a0031c87bc45bd8f7fcb3407cc4775",
            "c87a82c7fd2f4c00b2cf93545667f6c9",
            "5058c4da87764140957863ea118d0647",
            "fca4516589074af299885b9b7f4cd3de",
            "4a17ecc3f6ad48ea9a2635a17466be25",
            "123ddfe3cf434f288b1bbe9de5ccc490",
            "5525f269abca45adae014a4207f620ca",
            "c50e848ab9b346d09c438fc5e86f34e1",
            "448385e9db0e47148eb834cc5f9369f1",
            "6fc0904e8a064e539b2e16713b8682ed",
            "c3ea3b67415646d785cd8476be8dbed3",
            "86d629d9aa12410c8e6417fb843d6007",
            "a957b9c1a43a446aad863aab87b7f517",
            "78f2a6f309964bd1a51da1e29d855040",
            "4d5ef1d1943943979d25bdfb9f8dd403",
            "30906314481344518a38cd9aede52396",
            "56307069005048f89613505bc76e7081",
            "6af0b8b6f1344e319ef83bd745b9f111",
            "b5e9463144ef4696b12e663870f2bad8",
            "8fbc25d29b1740f3bc12b9e478c2faa0",
            "d802f7deaefc44b5ac320056c9dd4fd5",
            "2d37b1f2d0864f1eadafd42107455116",
            "10c2d4d5c1474bddb0a025ccb4e72d4e",
            "855a9b550f3a438b9be579d8004b3762",
            "6420762c76604940ae7dbcb4ca50a82e",
            "6668581e4d5149ad8d85ba2c75899045",
            "dd05e9053fa944d68bb3191a862fee27",
            "6784a48d0e8b4510bc77a0396a6debf0",
            "02d3e4472e734ef6aaa655b3a3fed6c9",
            "ebf3ec9d3d6347ebb74b3f9bf9601100",
            "2a5b4c01ccf8483ba19130bea0a56bff",
            "41a4bf96cf60483193b9753a31511dff",
            "62518cc3b73a44c590b08844562d2afa",
            "b3da1843ff874730a2ebf5c36844bbd8"
          ]
        },
        "id": "HF-ULtP3Vv1n",
        "outputId": "7b667958-3c32-4b40-a953-08a360a87a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b075232dcdf44a36b90eff2584b70227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11fab57787ef4ad5adabc3d0702d5d3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07cc627a820b4f3d98ff94fb4f71e9df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "295ffa1c46f24a78961e7c74e39b494d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c87a82c7fd2f4c00b2cf93545667f6c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a957b9c1a43a446aad863aab87b7f517"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "855a9b550f3a438b9be579d8004b3762"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "response = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "\n",
        "print(\"ü§ñ Agent Response:\\n\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-QlaCRvWGoF",
        "outputId": "722aeee0-228e-4b46-921a-ca9a1f6f8708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Agent Response:\n",
            "\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're about to complete your **first fully functional AI agent loop** ‚Äî using a real model, real tool, and structured logic. Let‚Äôs go!\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ Parse the Model Output & Run the Tool\n",
        "\n",
        "This is where we:\n",
        "1. Use an **LLM to decide which function to call**\n",
        "2. **Parse** the output into Python\n",
        "3. Run the tool (`search_wikipedia`)\n",
        "4. Print the final result\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 1: Final Working Prompt + Response (from Exercise 4)\n",
        "\n",
        "You're using `flan-t5-base`, so we‚Äôll keep the same generation code:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üì• Prompt + Generation\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"ü§ñ Model Output:\\n\", output)\n",
        "```\n",
        "\n",
        "Let‚Äôs assume the model gives something like:\n",
        "\n",
        "```plaintext\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"Marie Curie\" }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 2: Parse the Response (Extract Function + Parameters)\n",
        "\n",
        "```python\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Extract function name using regex\n",
        "fn_match = re.search(r\"function_name:\\s*(\\w+)\", output)\n",
        "params_match = re.search(r\"function_parms:\\s*({.*})\", output)\n",
        "\n",
        "if not fn_match or not params_match:\n",
        "    raise ValueError(\"Could not extract function call from model output.\")\n",
        "\n",
        "function_name = fn_match.group(1)\n",
        "params_raw = params_match.group(1)\n",
        "\n",
        "# Convert params from string to dictionary\n",
        "function_parms = json.loads(params_raw)\n",
        "\n",
        "print(\"üîß Parsed Action:\")\n",
        "print(\"Function:\", function_name)\n",
        "print(\"Parameters:\", function_parms)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 3: Run the Tool (Same `search_wikipedia` from earlier)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "    \n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "# Available tools\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 4: Call the Function Dynamically\n",
        "\n",
        "```python\n",
        "if function_name not in available_tools:\n",
        "    raise ValueError(f\"Unknown tool: {function_name}\")\n",
        "\n",
        "# Run the tool\n",
        "tool_fn = available_tools[function_name]\n",
        "observation = tool_fn(**function_parms)\n",
        "\n",
        "print(\"üëÅÔ∏è Observation (Tool Output):\", observation)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 5 (Optional): Use the LLM Again to Form the Final Answer\n",
        "\n",
        "If you want to be extra agent-y, you can feed the observation back to the model to form a final response. But even now, **you‚Äôve built a fully working, goal-driven agent loop**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Final Recap: What You Just Built\n",
        "\n",
        "‚úÖ LLM makes decisions  \n",
        "‚úÖ Output is parsed and validated  \n",
        "‚úÖ External tool is called based on that decision  \n",
        "‚úÖ You get a real-world result  \n",
        "\n",
        "This is **the heart of AI agents** ‚Äî you're now building with the same architecture as LangChain, Auto-GPT, and ReAct-style agents!\n",
        "\n",
        "---\n",
        "\n",
        "Would you like to:\n",
        "- Turn this into a loop (with memory)?\n",
        "- Add more tools?\n",
        "- Build a simple UI (e.g., with Gradio)?\n",
        "\n",
        "You're crushing it ‚Äî let‚Äôs keep going!"
      ],
      "metadata": {
        "id": "A4YfS_BhfWuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: Install and import dependencies ===\n",
        "!pip install transformers --quiet\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "QKP02QQSgrIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 2: Load the model (FLAN-T5-Base) ===\n",
        "# model_id = \"google/flan-t5-base\" # trained to answer directly, not follow structured output or tool-based reasoning like we're asking for.\n",
        "model_id = \"declare-lab/flan-alpaca-base\" # models are pretrained to answer, not to act like an agent unless explicitly trained to do so.\n",
        "model_id = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "e84fdcb83aae4ca6bf72e4e66b714dde",
            "5c73439bcee642d79f967eda8dd07fce",
            "a7777796b24b42bb84f42281305fedf7",
            "ebca4802a2014929ac2d624ee6495593",
            "e68bbda17a114e5ba9188a12ea46e15b",
            "3cfe8f1e93594f4b8c57abaa1fbde69e",
            "b1f9bc8742e445c4a84fbcf882f0926e",
            "0e93678a669248e3be82db43d5f13797",
            "a018f4e0aaca4ff295dd359c3dfd6c9a",
            "53206171d3c0472aa224a71c16f77cd7",
            "8745f55660814eee9a0ae07c92ec71b6",
            "512987d3b902462ab6d30baa0177f8db",
            "b4c7bc2060a341f696bdac46f788ddba",
            "86ed12309091493d950a35e5becb0016",
            "f6ef31ac65554ba98f9b98b22529d4df",
            "3bc8497b7c2845bbba5f2813285d45e4",
            "a16acc8528d446bfbe330e65132602af",
            "0e6e20cacf594cd08b568d770e6e374e",
            "82f599f6954e47058539b94013388495",
            "e17658ad225e4f45a89e3dcb7e8ed0a8",
            "06951df13b7f48379c388bd6902b3edb",
            "e8f77dc9b24a4bf9a2893323bc16bdba",
            "9c85e22034854547bb40fef03c6c8759",
            "e4e8de193fa8403bb4666129bcf807bc",
            "f830ee5643fe47f2b8cd6398f2b409f4",
            "129bb4157c484aca923c4ad8e97073f3",
            "8d2395ef54634b1b967bca9a68c8fbf2",
            "dc96550cb30d41848d1cf3a2f6f5d490",
            "a48d91b460c9440e849ef9870e31511f",
            "460602cae8ec4b02be5e4d4c0a38e1a3",
            "442e0df4ca334d50a4ee100fad02d15e",
            "d9c03573f9794024909a79ae48253c16",
            "5290a366866748b5a640429138617f4c",
            "12790993993f4f9888aa4e362406bb97",
            "913b4339e6b942b89572f419fc7827ea",
            "0dd7b4be316e4440a121bed75058ba60",
            "49c2cffe2d0e488e82a80e7450a8f246",
            "ce74df7129a84c3eb372ed38eeeb5afe",
            "b8181e6fa50247e3b594199acb489414",
            "1ccb9172836746bca39d9d1e7a51df49",
            "17743f54a8d34e79977e0a89abd5e071",
            "21eb3c0a28cb451a9e756368aa54c201",
            "11a7b07f38aa490f93e010b561e82e0b",
            "e0004d8342304f148106d463d56cd1a0",
            "2913c72351c74a0a8c3f56a890802b7b",
            "be6825bc0b4841a2829df96c2acff729",
            "f8337a48e6904e0bbe9084673b0dc105",
            "cd517ac2930849baa75e1140e0735413",
            "ce9ea87dd9834fb8968fa05575f55eb8",
            "27c78d5266a44b2882fe3e8805d5ba49",
            "b7b4fb3489bc4c4a81fa3a6c762b6ecb",
            "84594d31795445b2bb93c7767861262a",
            "857279b8dbab45dfb5cfaaba7f3bd878",
            "fb252d0fb1774db797dd28c9f8baf524",
            "21bdb13a240746048b9c490ec2a8da7e",
            "893853c4b1e940b4878cf4eec27787bc",
            "7722e2daa5484a41808c5056e07c9619",
            "c4765d60853b407cbed36d9015818b61",
            "59d875ede590454ba407820a60686d15",
            "caa401be076e4460b223bf5a7f596380",
            "df83aa23811e4dc29526c8ca5dcde296",
            "10db6a9b79fc49178d7336a3be2103f6",
            "7845672dcf3a42f98470b6c962e0fc65",
            "d3d1be98b98c47728f350ae173b22e8a",
            "02e1bf02db844e5383046b4a92d2eb39",
            "44e9cd6f81f9480da407454fe7a44336",
            "04e8f95f7d9d44bab97a1b82eef2b2a0",
            "e8c7c0654c064da28831842064bd883e",
            "cd9bca755c714ca69cace86af0405a92",
            "f1a095cebd6c45d0ab22dc1d72b43f97",
            "89c05549629140c490185d261437a13f",
            "5fa216e1b8d54ed2bd934deed3da8b56",
            "3dc633ed34b44811ad59c067d7147a76",
            "ff8104c2f0704bd5a62c4044ee988bf8",
            "d4e1abfeba7649e095a625823db61f38",
            "0e64c02f8a5d49968b35cb24764dfd25",
            "9965334ea717449cad73bd501ad72446"
          ]
        },
        "id": "8NmM8rVsgrGK",
        "outputId": "5cbbaef1-7396-40bf-be25-cda1b538640d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e84fdcb83aae4ca6bf72e4e66b714dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "512987d3b902462ab6d30baa0177f8db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c85e22034854547bb40fef03c6c8759"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12790993993f4f9888aa4e362406bb97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2913c72351c74a0a8c3f56a890802b7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "893853c4b1e940b4878cf4eec27787bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04e8f95f7d9d44bab97a1b82eef2b2a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 3: Define the tool ===\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n"
      ],
      "metadata": {
        "id": "zSVmj-MngrDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: Create the prompt and generate model output ===\n",
        "\n",
        "user_question = \"How old was Marie Curie when she died?\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Do not answer directly. You MUST return the function name and parameters in this format:\n",
        "\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Available tools:\n",
        "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
        "\n",
        "User question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "print(\"ü§ñ Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxObkvK_grBM",
        "outputId": "cc93018e-4b56-4cb3-e2fd-a1e56e63fa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Output:\n",
            " 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install SimplerLLM"
      ],
      "metadata": {
        "id": "FRkv2f0qlYLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 5: Extract function name and parameters ===\n",
        "\n",
        "from SimplerLLM.tools.json_helpers import extract_json_from_text\n",
        "\n",
        "print(\"\\nüì¶ Raw output (debug):\\n\", repr(output))\n",
        "\n",
        "from SimplerLLM.tools.json_helpers import extract_json_from_text\n",
        "\n",
        "# Use this instead of regex\n",
        "action_json = extract_json_from_text(output)\n",
        "\n",
        "if action_json:\n",
        "    function_name = action_json[0]['function_name']\n",
        "    function_parms = action_json[0]['function_parms']\n",
        "else:\n",
        "    raise ValueError(\"‚ùå Could not extract function call from model output.\")\n",
        "\n",
        "\n",
        "if not fn_match or not params_match:\n",
        "    raise ValueError(\"‚ùå Could not extract function call from model output.\")\n",
        "\n",
        "function_name = fn_match.group(1)\n",
        "params_raw = params_match.group(1)\n",
        "\n",
        "try:\n",
        "    function_parms = json.loads(params_raw)\n",
        "except json.JSONDecodeError as e:\n",
        "    raise ValueError(f\"‚ùå Failed to parse JSON parameters: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Parsed Function Call:\")\n",
        "print(\"Function Name:\", function_name)\n",
        "print(\"Function Parameters:\", function_parms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "Li4paWI3gq-V",
        "outputId": "9cde665e-0537-4b1e-c064-725b53d49200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Raw output (debug):\n",
            " '18'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2f6eb4a8b6bb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfunction_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'function_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mfunction_parms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'function_parms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6: Call the function dynamically ===\n",
        "\n",
        "if function_name not in available_tools:\n",
        "    raise ValueError(f\"‚ùå Unknown function: {function_name}\")\n",
        "\n",
        "tool_fn = available_tools[function_name]\n",
        "observation = tool_fn(**function_parms)\n",
        "\n",
        "print(\"\\nüëÅÔ∏è Observation (Tool Output):\\n\", observation)\n"
      ],
      "metadata": {
        "id": "BQGerLU4gq7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#‚úÖ Full Agent Pipeline Using extract_json_from_text"
      ],
      "metadata": {
        "id": "T94xueQTl3x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n"
      ],
      "metadata": {
        "id": "7kCf49qbgq2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "b37e5e78534c499993ad82194e2c81f7",
            "e549c2cf950f4b44b2aaa387e0732fba",
            "5d5477d4c96348b18153e14beafd3277",
            "b2a726287d944c97ae2ce3a2eafaa6c3",
            "05794d35185646d0897daf39dbc94c13",
            "47449440958842819c65c3d1fe44bcee",
            "1a1a8131eb4146358163ab750c31da7d",
            "ab9523c5cdf047b4b08ca3093edec11f",
            "f72b938adafe467b9449f3f427ea98ef",
            "37b01b1ac5974f88a9791944d6aa8896",
            "8e8a12e038ac4c5190a10365e412fc61",
            "ccc3728c91a948ffb3c72e29544d1905",
            "1c4ed3ccee124087829e4b4e28827be4",
            "483113784804479ab4c135c02612e1bc",
            "6cd60376e9c74a178aeabbfc09182719",
            "889b2e3b7f284040a7fc597104e12ed3",
            "d61f2a20b0db4ac89930e89effaff720",
            "3df372516795420790e671ba8cf25f4a",
            "8f9f084679194c3baed4f632d601fe29",
            "3604407936ba47faa1c3ceabb1337939",
            "c45ab066b65141289b793f249e5f9444",
            "e92a6a95ee7c474a98f3e80a27eebada",
            "158ea1cc456f4847ac1c011b67f2e08d",
            "a679f4052655447194f06d6e650f0c57",
            "51a957d002cd423ab0a8e0c7cec4bd18",
            "74e25d2f4a5a496f8e607c0295c1d073",
            "b5a5f8f2da5443ccbc04e1a0dcfc5c34",
            "9e38d5757d324ff6bad2a5ab85fd180f",
            "dba3b79336f548999f097bc2fc10aab1",
            "86816decd9ef460693dc80a06824e931",
            "d5dbe34cde0c4e16a69e66b75ad277e6",
            "5315c942c829466b8c547a6dc49d5e0a",
            "1910eab0f0454bc0823da92d39a859b4",
            "64ca8eb19c694a6190ce6999d14bdcfa",
            "e6d9ceb6c575459e8fa1d942322b1eaa",
            "191b25f1201f44df8ffae449bb8407fa",
            "c443eb45bc80484e9e505e3f8a053c97",
            "bb06119fda044aa992963bd1fe281a3e",
            "844b5b7621fb4ac9aaa57deb23af4860",
            "c777b47b9b9741bfb3cc0ebe50b71725",
            "ab0bf242ce344d57b07f6f86a5bd019f",
            "e44ff33d6f5f443983bb1d3d999aeb86",
            "8981d4ba01e847588b9fd39580c7f8fb",
            "ffce13d8aaf74f489a7335368c9a1b38",
            "b4f0dc0b81594674a6e5e034e84ed38c",
            "77f68bec26c1420fa8a260c8b103f607",
            "11a618e6430445868ae186ea9cdb9975",
            "46d4ed67914e4e159af3f51d7f556fa1",
            "42c05afd2f144fd2830e11c44c10a542",
            "1611422c5b8744e7ac0cec13a9bda39e",
            "ccf88434356e45d08664aab36f6cf33a",
            "02282fde412c4799a7eac3a16cce5d21",
            "3d8c00cdea3c4e6eaa0ba0fa40379d84",
            "c17af1c3bb5845cc8730493b7d4d2d3f",
            "cdfd0f07d8cc462bbf78f6995eb287c3",
            "fccdd0472268464db34cf4bdf20d28e1",
            "0ad7906bc51e41a7ba00ed3003917701",
            "8b27b216e47345f1a124c0fa185de7c5",
            "f92dbeaafc694cc9bc2ea108eb0f3b1a",
            "b6a1f0d5ac9a4cfea61a5cb81cc43b8c",
            "f384cae9276c40eb8e1fb4a30c82b6e7",
            "96e01069143440e9862cf798b30cc1bf",
            "670e7fbf0858413e93e7ff57be6ea15a",
            "4517f46aa3fa4612ac1a266f7686021e",
            "a47f72479bdb41439d3e78a688d86b1d",
            "7644a174d9614655b6c6017518003616",
            "c63fbf7f94b142cfaa77f9f32dfcb5a9",
            "28d4efd65d044e9a9bb300f04196007e",
            "65ed14b1efaf4ec79a659cdcaf910332",
            "56f7924ee29b4394b1dd71f24889fb5f",
            "ae2a8d76afed43e385daf73860a26fb7",
            "843e4ac52498410aa21f439611a8f5a2",
            "b43f9fb760054ca0a97328c8e011c398",
            "e30123c536ee4227bce6be1b5a2eadfc",
            "1f361305406e48cc9a56a7c5ab3cf4e6",
            "813b9625369948a7a43cc7008e2aa7bb",
            "3420dfdd91cf4ba4a6e52819533744ab"
          ]
        },
        "id": "wW1dXZvzl52G",
        "outputId": "93b8c6f9-1d1a-4d2f-e285-8ca697f9c723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b37e5e78534c499993ad82194e2c81f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccc3728c91a948ffb3c72e29544d1905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "158ea1cc456f4847ac1c011b67f2e08d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64ca8eb19c694a6190ce6999d14bdcfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4f0dc0b81594674a6e5e034e84ed38c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fccdd0472268464db34cf4bdf20d28e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c63fbf7f94b142cfaa77f9f32dfcb5a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n"
      ],
      "metadata": {
        "id": "NlhmfG1Ul-MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Who discovered penicillin?\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are an AI agent. Do not answer the question directly.\n",
        "\n",
        "Instead, return ONLY a JSON object using this exact format:\n",
        "{{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {{\n",
        "    \"query\": \"...\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Your only task is to pick the right tool and fill in the parameters.\n",
        "\n",
        "User: {user_question}\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "gyEudk06mCeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = generator(prompt, max_new_tokens=100, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "# print(\"ü§ñ Raw Model Output:\\n\")\n",
        "# print(response)\n",
        "\n",
        "response = '''\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"john s. wilson\"\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "JXPML_c_mC4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse the mocked JSON response\n",
        "parsed_json = json.loads(response)\n",
        "\n",
        "function_name = parsed_json[\"function_name\"]\n",
        "function_parms = parsed_json[\"function_parms\"]\n",
        "\n",
        "# Step 2: Check if the tool exists\n",
        "if function_name not in available_tools:\n",
        "    raise ValueError(f\"‚ùå Unknown tool: {function_name}\")\n",
        "\n",
        "# Step 3: Call the tool with parameters\n",
        "tool_fn = available_tools[function_name]\n",
        "observation = tool_fn(**function_parms)\n",
        "\n",
        "# Step 4: Print the final result\n",
        "print(\"üëÅÔ∏è Observation (Tool Output):\\n\", observation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYYdPSrwmE8W",
        "outputId": "4326078b-5639-4a88-e3d1-2fed3d44fed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üëÅÔ∏è Observation (Tool Output):\n",
            " <span class=\"searchmatch\">John</span> <span class=\"searchmatch\">S</span>. <span class=\"searchmatch\">Wilson</span> may refer to: <span class=\"searchmatch\">John</span> <span class=\"searchmatch\">Wilson</span> (1920s pitcher) (1903‚Äì1980), <span class=\"searchmatch\">John</span> Samuel <span class=\"searchmatch\">Wilson</span>, Major League Baseball pitcher <span class=\"searchmatch\">John</span> <span class=\"searchmatch\">S</span>. <span class=\"searchmatch\">Wilson</span> (music critic) (1913‚Äì2002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Widgets"
      ],
      "metadata": {
        "id": "pwVEwm3JOogX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/AI AGENTS/002_Pipelines.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"‚úÖ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"‚úÖ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "id": "fbqM1rk9mI0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d60182-4037-4cc1-b8aa-9e302295025a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    }
  ]
}