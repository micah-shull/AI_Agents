{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1GHn/8tU3rLDo77tfTGdZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/028_Tool_Design%26Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ðŸ› ï¸ **Tool Design & Structure**\n",
        "\n",
        "In this lecture, we learned that **tool descriptions** are essential for agents to understand and apply tools effectively. Instead of relying on vague commands, agents need clear, structured information about the tools they will use. Here are the core concepts we need to grasp:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Tool Definitions Are Essential for Action**\n",
        "\n",
        "* **Agents cannot perform tasks** unless they understand the **tools** at their disposal.\n",
        "* Each tool must be **described with clear metadata** that specifies:\n",
        "\n",
        "  * **Tool name**: What does the tool do?\n",
        "  * **Arguments (inputs)**: What does the tool require in order to function?\n",
        "  * **Outputs**: What results or changes can we expect after the tool runs?\n",
        "\n",
        "### **2. Tool Descriptions Enable Clear Interfacing**\n",
        "\n",
        "* **JSON Schema** is a structured way of defining tool properties (inputs, outputs, constraints).\n",
        "\n",
        "  * **Inputs** can be basic data types (e.g., strings, numbers) or more complex (e.g., arrays, enums).\n",
        "  * **Outputs** can be validated for consistency and expected results.\n",
        "\n",
        "### **3. Tools Need to Be Interpretable by the LLM**\n",
        "\n",
        "* The **LLM** must understand when and how to use each tool.\n",
        "* We need to provide **examples** and clear definitions so that the LLM can **recognize tool requirements** and **apply the correct logic**.\n",
        "\n",
        "### **4. JSON Schema as a Tool â€œContractâ€**\n",
        "\n",
        "* By using JSON Schema, we define the **contract** between the agent and the tool:\n",
        "\n",
        "  * What the tool is called\n",
        "  * What it needs to function (parameters)\n",
        "  * What it outputs (results)\n",
        "\n",
        "  This contract allows the **LLM to execute the tool without confusion**, knowing what the inputs are, and how to apply them.\n",
        "\n",
        "### **5. Extensibility and Flexibility of Tool Design**\n",
        "\n",
        "* Tools can be **simple** (like listing files) or **complex** (e.g., analyzing sentiment in documents).\n",
        "* As tools grow in complexity, you can introduce more advanced features:\n",
        "\n",
        "  * **Enums** (for predefined options)\n",
        "  * **Booleans** (e.g., active/inactive states)\n",
        "  * **Arrays** (lists of files, multiple parameters)\n",
        "\n",
        "  Flexibility in tool design ensures that your agent can evolve over time and handle a variety of tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Objectives for This Notebook:**\n",
        "\n",
        "1. **Understand the structure of tool descriptions** using JSON.\n",
        "2. **Build several examples** of tools with varied complexity:\n",
        "\n",
        "   * Basic tools with simple inputs/outputs\n",
        "   * Tools requiring lists, boolean flags, and enums\n",
        "3. **Understand how to format and define each tool clearly** to make it LLM-friendly and executable.\n",
        "\n",
        "By the end of this notebook, weâ€™ll have a solid foundation for building tools, and weâ€™ll be ready to integrate them into an agent in the next notebook.\n"
      ],
      "metadata": {
        "id": "_dXGmNl-gk2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… Example 1: list_files\n",
        "A simple tool that requires no input. It just lists files in a folder."
      ],
      "metadata": {
        "id": "NtDknYbOg6Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"name\": \"list_files\",\n",
        "  \"description\": \"Lists all files in the current working directory.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {},\n",
        "    \"required\": []\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "4FDCCOsoeBia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ” Python Code Version"
      ],
      "metadata": {
        "id": "YnY47Vm7iNAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# ðŸ”¹ Step 1: Imports and Setup\n",
        "source_dir = \"/content/docs_folder\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    raise FileNotFoundError(f\"ðŸ“ Directory not found: {source_dir}\")\n",
        "\n",
        "# List and build full file paths\n",
        "file_list = [\n",
        "    os.path.join(source_dir, f)\n",
        "    for f in os.listdir(source_dir)\n",
        "    if os.path.isfile(os.path.join(source_dir, f))\n",
        "]"
      ],
      "metadata": {
        "id": "3Y5NqeLwhC7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  These Arenâ€™t One-to-One\n",
        "\n",
        "Python code **does** the task.\n",
        "Tool JSON **describes** the task in a way the LLM can recognize and invoke it.\n",
        "\n",
        "Think of the tool JSON as **an API spec**, not an implementation.\n",
        "\n",
        "You're uncovering some of the most important (and often misunderstood) differences between *traditional code* and *tool design for agents*. Letâ€™s walk through your points and connect the dots:\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§© 1. Why `\"type\"`, `\"properties\"`, `\"required\"`?\n",
        "\n",
        "These fields come from **JSON Schema**, which is a standard format used to describe data structures.\n",
        "OpenAI uses JSON Schema so the LLM can:\n",
        "\n",
        "* understand what arguments a tool accepts,\n",
        "* validate them,\n",
        "* and generate calls in the correct structure.\n",
        "\n",
        "Letâ€™s explain each:\n",
        "\n",
        "| JSON Schema Field  | What It Means                                                | Python Equivalent                                                 |\n",
        "| ------------------ | ------------------------------------------------------------ | ----------------------------------------------------------------- |\n",
        "| `\"type\": \"object\"` | This tool expects an object (i.e., a dictionary) as input    | A function that takes named arguments                             |\n",
        "| `\"properties\"`     | Lists the possible fields/parameters inside the input object | Function parameters like `file_name: str`                         |\n",
        "| `\"required\"`       | Specifies which parameters must be included                  | Equivalent to **non-default function args** (i.e. not `**kwargs`) |\n",
        "\n",
        "### Why not use `\"args\"`?\n",
        "\n",
        "Because `\"args\"` and `\"kwargs\"` are Python-specific.\n",
        "JSON Schema is **language-agnostic**, and OpenAI tools aim to be platform-neutral.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ 2. Whereâ€™s `source_dir` in the tool?\n",
        "\n",
        "Great observation. Itâ€™s **abstracted away**.\n",
        "\n",
        "The tool description:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"list_files\",\n",
        "  \"description\": \"Lists all files in the current working directory.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {},\n",
        "    \"required\": []\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "...assumes that:\n",
        "\n",
        "* The **agent runtime environment** has already defined a `source_dir`.\n",
        "* Or, the tool itself (on the backend) is **hardcoded** to know where to look.\n",
        "* Or the agent is *managing context* so `cwd = /content/docs_folder`.\n",
        "\n",
        "So yes, youâ€™re right: **the LLM is trusting that the tool does what it says**.\n",
        "It doesnâ€™t see `os.listdir()` or `source_dir` â€” it just learns:\n",
        "\n",
        "> â€œWhen I want to get a list of files, call `list_files` with no parameters.â€\n",
        "\n",
        "The *developer* is responsible for wiring up the actual behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  3. Isn't This Asking Too Much of the LLM?\n",
        "\n",
        "Thatâ€™s an insightful concern, and hereâ€™s the truth:\n",
        "\n",
        "### âœ… Yes, we're asking a lot â€” but intentionally.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "| Traditional Code                  | Agent-Oriented Approach                            |\n",
        "| --------------------------------- | -------------------------------------------------- |\n",
        "| You define everything             | You *describe* capabilities                        |\n",
        "| You write imperative instructions | You delegate tool selection to the LLM             |\n",
        "| Focus is on implementation        | Focus is on *orchestration* and intent recognition |\n",
        "| LLM is a text generator           | LLM is a **planner** and **dispatcher**            |\n",
        "\n",
        "The LLM is **not building the tool on the fly** â€” itâ€™s choosing **which tool** to call and **what arguments** to provide. Thatâ€™s a very different job than execution.\n",
        "\n",
        "The actual execution still happens in your backend Python code â€” the LLM just produces a JSON like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"list_files\",\n",
        "  \"args\": {}\n",
        "}\n",
        "```\n",
        "\n",
        "Then your app does:\n",
        "\n",
        "```python\n",
        "if tool_name == \"list_files\":\n",
        "    return list_files()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Why All This?\n",
        "\n",
        "Because this pattern **scales**.\n",
        "\n",
        "Once you define tools:\n",
        "\n",
        "* The LLM can choose, combine, and sequence them\n",
        "* You can add, update, or swap tools without retraining anything\n",
        "* You separate **intelligence** (LLM) from **execution** (tools)\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… TL;DR â€“ Whatâ€™s Going On?\n",
        "\n",
        "You're designing a **modular architecture** where:\n",
        "\n",
        "* The **LLM plans** (\"What should I do?\")\n",
        "* The **tools act** (\"Do this\")\n",
        "* Your **code glues them together**\n",
        "\n",
        "So yes â€” itâ€™s different from writing raw Python.\n",
        "But it enables automation thatâ€™s general, adaptive, and reusable.\n",
        "\n"
      ],
      "metadata": {
        "id": "q6QJNOJTkvAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## âœ… Whatâ€™s Happening at Each Layer\n",
        "\n",
        "### 1. **JSON Tool Schema** (For the LLM)\n",
        "\n",
        "This is:\n",
        "\n",
        "* **Not Python**\n",
        "* A **structured description** of what a tool *does*\n",
        "* What the **LLM reads** to understand:\n",
        "\n",
        "  * \"What tools are available?\"\n",
        "  * \"What arguments can I provide?\"\n",
        "  * \"What is this tool used for?\"\n",
        "\n",
        "ðŸ’¡ Think of this as an *API contract* between the LLM and your Python code.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Python Tool Function** (For Execution)\n",
        "\n",
        "This is:\n",
        "\n",
        "* Real Python code (your actual tool logic)\n",
        "* Executes the task â€” e.g., list files, read a file, summarize content\n",
        "* Matches the interface defined in the JSON\n",
        "\n",
        "ðŸ§© Your job is to **make sure the Python function accepts the inputs** the LLM defines via the JSON schema.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **LLM Role**\n",
        "\n",
        "The LLM:\n",
        "\n",
        "* Reads the available tool schemas (via prompt)\n",
        "* Decides: â€œAh, based on the user request, I should call `read_file` with `filename: 'lecture_01.txt'`â€\n",
        "* Outputs a **JSON invocation** like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"read_file\",\n",
        "  \"args\": { \"filename\": \"lecture_01.txt\" }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Your Agent Runtime**\n",
        "\n",
        "Your agent:\n",
        "\n",
        "* Parses the LLMâ€™s output\n",
        "* Extracts `tool_name` and `args`\n",
        "* Calls the corresponding Python function like:\n",
        "\n",
        "```python\n",
        "read_file(filename=\"lecture_01.txt\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  In Summary:\n",
        "\n",
        "| Layer       | Role          | Format                    | Purpose                     |\n",
        "| ----------- | ------------- | ------------------------- | --------------------------- |\n",
        "| Tool Schema | For the LLM   | JSON (JSON Schema format) | Describe tool structure     |\n",
        "| Tool Code   | For your app  | Python                    | Actually does the work      |\n",
        "| Tool Call   | From the LLM  | JSON (tool\\_name + args)  | Tells your code what to run |\n",
        "| Execution   | By your agent | Python                    | Calls the real function     |\n",
        "\n"
      ],
      "metadata": {
        "id": "FAlKM1wElsv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Who Builds the Tool?\n",
        "\n",
        "**You build the actual tool** (in Python), and the **LLM simply chooses when and how to use it.**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ Your Role:\n",
        "\n",
        "You are the **tool builder**. This means:\n",
        "\n",
        "* Writing the actual Python functions that do the work (`read_file()`, `summarize_text()`, etc.)\n",
        "* Defining JSON schema metadata to describe the tool to the LLM\n",
        "* Making sure the inputs the LLM is allowed to use (via JSON) align with the parameters your function accepts\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ¤– The LLMâ€™s Role:\n",
        "\n",
        "The LLM is the **tool orchestrator**. It:\n",
        "\n",
        "* Reads your schema definitions\n",
        "* Interprets the user request\n",
        "* Decides: â€œAh, I should call `read_file` with `filename='foo.txt'`â€\n",
        "* Outputs a structured call\n",
        "* Doesnâ€™t know or care *how* the tool works â€” just what itâ€™s allowed to do\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ” Example Flow\n",
        "\n",
        "You write this:\n",
        "\n",
        "```python\n",
        "def read_file(filename: str):\n",
        "    with open(filename, \"r\") as f:\n",
        "        return f.read()\n",
        "```\n",
        "\n",
        "You define this JSON:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"read_file\",\n",
        "  \"description\": \"Reads the content of a file from disk.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"filename\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The name of the file to read\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"filename\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "The LLM sees this and says:\n",
        "\n",
        "> \"To answer the user, I should use `read_file` with `filename='doc_001.txt'`.\"\n",
        "\n",
        "Then your agent calls:\n",
        "\n",
        "```python\n",
        "read_file(filename=\"doc_001.txt\")\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CwK_RVhEl-rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through building a complete tool, step-by-step â€” **both the Python code and the JSON schema**, side-by-side.\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Build a `count_words` tool\n",
        "\n",
        "It will take a text file and return the number of words in it.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Step 1: Define the Python Tool (the actual function)\n",
        "\n",
        "```python\n",
        "def count_words(filename: str) -> int:\n",
        "    with open(filename, 'r') as f:\n",
        "        content = f.read()\n",
        "    return len(content.split())\n",
        "```\n",
        "\n",
        "âœ… **What this does**:\n",
        "\n",
        "* Takes in a file path\n",
        "* Reads the file content\n",
        "* Splits it into words\n",
        "* Returns the word count\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”§ Step 2: Define the Tool Schema (what the LLM sees)\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"count_words\",\n",
        "  \"description\": \"Counts the number of words in a specified text file.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"filename\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The name of the file to count words in\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"filename\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "âœ… **What this tells the LLM**:\n",
        "\n",
        "* The tool is called `count_words`\n",
        "* It expects a single parameter called `filename` (a string)\n",
        "* It doesnâ€™t know how it works, just what it does and what it needs\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ” Step 3: What the LLM Might Output (based on a user request)\n",
        "\n",
        "If the user says:\n",
        "\n",
        "> â€œHow many words are in `lecture_notes.txt`?â€\n",
        "\n",
        "The LLM might output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"count_words\",\n",
        "  \"args\": {\n",
        "    \"filename\": \"lecture_notes.txt\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Your orchestrator would then call:\n",
        "\n",
        "```python\n",
        "count_words(\"lecture_notes.txt\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ¯ Recap\n",
        "\n",
        "| Component          | Your Responsibility        | LLMâ€™s Responsibility           |\n",
        "| ------------------ | -------------------------- | ------------------------------ |\n",
        "| Python function    | Write real logic in Python | â€”                              |\n",
        "| JSON schema        | Describe inputs + purpose  | Understand how to use the tool |\n",
        "| Agent orchestrator | Handle tool execution      | â€”                              |\n",
        "| LLM                | Decide when to use tool    | Choose tool and fill arguments |\n",
        "\n"
      ],
      "metadata": {
        "id": "QopYJHZNmYKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸ¤– LLM-Orchestrated Agents = **Chat UX + Code API**\n",
        "\n",
        "### ðŸ”· What *you* do (as the developer):\n",
        "\n",
        "1. **Write the tools** â†’ Python functions that do specific things well\n",
        "2. **Describe the tools** â†’ JSON specs that help the LLM understand what they are and how to use them\n",
        "3. **Build the agent loop** â†’ A system that routes LLM requests into function calls\n",
        "\n",
        "### ðŸ”· What the *LLM* does (as the orchestrator):\n",
        "\n",
        "1. **Understands the user request**\n",
        "2. **Chooses the right tool(s)** based on natural language and schema\n",
        "3. **Fills in the tool arguments** using reasoning\n",
        "4. **Delegates execution to code** (it doesnâ€™t run the tool â€” it just picks and preps)\n",
        "5. **Processes the output** and continues the conversation\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ Why it works so well\n",
        "\n",
        "* **LLMs are good at reasoning and language**, but not running code\n",
        "* **Python is good at execution**, but not interpreting vague instructions\n",
        "* This pattern **bridges both worlds**, letting each do what it does best\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Your takeaway\n",
        "\n",
        "> You're not making the LLM do everything.\n",
        ">\n",
        "> You're building a system where **LLM = brain**, **Python = hands**.\n",
        "\n",
        "This separation of concerns is what makes agents:\n",
        "\n",
        "* Cheaper ðŸ’¸ (LLM doesnâ€™t have to learn to do everything)\n",
        "* Faster âš¡ (code runs tools directly)\n",
        "* Safer âœ… (tools are verified, tested, limited in scope)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b2Ah_oNhoEEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s build a slightly more advanced tool next â€” one that:\n",
        "\n",
        "âœ… Has multiple parameters\n",
        "âœ… Expects inputs from the LLM\n",
        "âœ… Could be conditionally used depending on the user request\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› ï¸ Tool Concept: `search_documents`\n",
        "\n",
        "This tool searches for a keyword or phrase in a set of documents and returns matching filenames.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§ª Step 1: Write the **actual Python function**\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "def search_documents(query: str, folder: str = \"/content/docs_folder\"):\n",
        "    results = []\n",
        "    for filename in os.listdir(folder):\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                if query.lower() in content.lower():\n",
        "                    results.append(filename)\n",
        "    return results\n",
        "```\n",
        "\n",
        "* âœ… This code works.\n",
        "* âœ… It's testable independently of the LLM.\n",
        "* âœ… It only does one thing well â€” thatâ€™s what you want from tools.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§¾ Step 2: Define the **tool schema** in JSON format\n",
        "\n",
        "This tells the LLM **what** the tool does, and what arguments it needs:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"search_documents\",\n",
        "  \"description\": \"Searches all documents in the folder for a given keyword or phrase.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"query\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The keyword or phrase to search for in the documents.\"\n",
        "      },\n",
        "      \"folder\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The folder path to search in (default is '/content/docs_folder').\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"query\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### ðŸ” Notice:\n",
        "\n",
        "* `query` is **required**\n",
        "* `folder` is **optional** (weâ€™ll default to a value in the Python code)\n",
        "* The LLM now understands how to **call this tool** like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"search_documents\",\n",
        "  \"args\": {\n",
        "    \"query\": \"vector database\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Why this matters\n",
        "\n",
        "You now have a tool that the LLM can choose to use when a user asks things like:\n",
        "\n",
        "* â€œWhich files talk about vector databases?â€\n",
        "* â€œSearch all my notes for mentions of LangChain.â€\n",
        "* â€œDo any documents explain agent loops?â€\n",
        "\n",
        "Youâ€™re not manually mapping user intent â†’ tool calls anymore.\n",
        "**The LLM handles that orchestration.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YtzgSxq1sFwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ðŸ§° OpenAI â€œToolsâ€ (Function Calling): What the API expects\n",
        "\n",
        "**OpenAIâ€™s API defines the shape** when you use *tool/function calling*. You send a tool schema, and the model replies with an **assistant message** that contains a `tool_calls` array. Each item has a function `name` and an `arguments` JSON string. Example of what you actually read off the SDK:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"role\": \"assistant\",\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "        \"name\": \"search_documents\",\n",
        "        \"arguments\": \"{\\\"query\\\":\\\"vector database\\\"}\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "That structure (including `tool_calls â†’ function â†’ name/arguments`) is OpenAI-specific and documented in their tool/function calling guide.\n",
        "\n",
        "* Your example\n",
        "\n",
        "  ```json\n",
        "  {\"tool_name\":\"search_documents\",\"args\":{\"query\":\"vector database\"}}\n",
        "  ```\n",
        "\n",
        "  is a **teaching abstraction** (or a prompt-only fallback). Itâ€™s fine if youâ€™re not using tool calling, but when you *do* use tool calling, you donâ€™t need code fences or custom wrappersâ€”the SDK gives you the call in `message.tool_calls`, and you `json.loads` the `arguments`.\n",
        "\n",
        "* **Other platforms differ.** Anthropic, Gemini, etc., have their own shapes (same concept: name + JSON args; different field names). Azure OpenAI mirrors OpenAIâ€™s shape closely, since itâ€™s the same API surface hosted in Azure.\n",
        "\n",
        "* If you want to **force strict JSON** for non-tool outputs (like summaries), OpenAI also has â€œStructured Outputsâ€/JSON Schema support to guarantee valid JSONâ€”again, an API-defined contract.\n",
        "\n",
        "**Bottom line:**\n",
        "Yesâ€”the exact return format depends on the provider. With OpenAI tool calling, the â€œtool call objectâ€ shape is determined by OpenAI; your schema influences the **`arguments`** fields, not the outer envelope. You can still normalize it into your own `{tool_name, args}` shape internally if you prefer, but you donâ€™t have to.\n",
        "\n",
        "* 1: https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com \"Function calling - OpenAI API\"\n",
        "* 2: https://platform.openai.com/docs/guides/structured-outputs?utm_source=chatgpt.com \"Structured model outputs - OpenAI API\"\n",
        "\n",
        "\n",
        "## 1) What you SEND (request)\n",
        "\n",
        "* Add a `tools` array. Each tool is type `function` with a **name**, **description**, and **parameters** (a JSON Schema).\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"search_documents\",\n",
        "      \"description\": \"Searches all documents in the folder for a given keyword or phrase.\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"query\": {\"type\": \"string\", \"description\": \"Keyword or phrase.\"},\n",
        "          \"folder\": {\"type\": \"string\", \"description\": \"Folder path\"}\n",
        "        },\n",
        "        \"required\": [\"query\"]\n",
        "      }\n",
        "    }\n",
        "  }]\n",
        "  ```\n",
        "\n",
        "  You pass this via `tools=tools` in your API call. The model uses the schema to decide if and how to call the function. ([OpenAI Platform][1])\n",
        "\n",
        "* (Optional) **Control tool use** with `tool_choice`:\n",
        "\n",
        "  * `\"auto\"` â†’ model may call a tool,\n",
        "  * `\"none\"` â†’ model wonâ€™t,\n",
        "  * or a specific function to force exactly that tool.\n",
        "    Thereâ€™s also **`allowed_tools`** to restrict to a subset. ([OpenAI Platform][1])\n",
        "\n",
        "* (Optional) **Structured Outputs (strict)**: you can make the model **strictly** follow your function schema by enabling structured outputs for tools; this increases reliability of the `arguments`. ([OpenAI][2], [OpenAI Platform][3])\n",
        "\n",
        "## 2) What you GET BACK (response)\n",
        "\n",
        "* The assistant message may include **`tool_calls`** (an array). Each item has:\n",
        "\n",
        "  * `function.name` â†’ the tool to run\n",
        "  * `function.arguments` â†’ **a JSON string** you must `json.loads`\n",
        "* You then **execute** your Python function with those args. ([OpenAI Platform][1])\n",
        "\n",
        "## 3) How to RETURN tool results to the model\n",
        "\n",
        "* With **Chat Completions**, you send another API call with an appended **`role: \"tool\"`** message that includes:\n",
        "\n",
        "  * `tool_call_id` (echo the id from the tool call),\n",
        "  * the tool **output** as message `content`.\n",
        "* The model can then read those results and continue (e.g., summarize, decide next step). ([OpenAI Platform][1])\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Practical rules & gotchas\n",
        "\n",
        "* **Schema is a contract.** Use JSON Schema features (`type`, `required`, `enum`, `minItems`, etc.). This helps the model propose correct args and helps *you* validate before running code. ([OpenAI Platform][1])\n",
        "* **Arguments are JSON.** The `arguments` you parse are a JSON string. Always `json.loads` and then **validate** (Pydantic/JSON Schema) before executing. ([OpenAI Platform][1])\n",
        "* **Keep tools focused.** One clear purpose per tool and concise descriptions improve accuracy (and reduce tokens). ([OpenAI Platform][4])\n",
        "* **Prefer constraints over prose.** If your API/model supports **Structured Outputs** (strict), enable it to enforce your schema for function arguments. It materially improves reliability over prompt-only formatting. ([OpenAI][2], [OpenAI Platform][3])\n",
        "* **Latency & cost.** Tool definitions live in the prompt context; keep names/descriptions compact. Use `tool_choice`/`allowed_tools` to reduce unnecessary calls. ([OpenAI Platform][5])\n",
        "* **Multiple tool calls.** Models can propose **more than one** call in a turn (parallel or sequential). Your code should loop through `tool_calls` and respond with one `role:\"tool\"` message per call (each with its own `tool_call_id`). ([OpenAI Platform][1])\n",
        "* **Large inputs.** Donâ€™t stuff big blobs into `arguments`. Pass references (file IDs/paths) and let your Python do the I/O, or use built-ins like **File Search** / **Actions** when appropriate. ([OpenAI Platform][6])\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”§ Minimal Python pattern (Chat Completions)\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Find mentions of 'vector database' in my docs\"}]\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    tools=[{\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_documents\",\n",
        "            \"description\": \"Search docs for a keyword/phrase.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\"type\": \"string\"},\n",
        "                    \"folder\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }],\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "\n",
        "msg = resp.choices[0].message\n",
        "for call in msg.tool_calls or []:\n",
        "    name = call.function.name\n",
        "    args = json.loads(call.function.arguments)\n",
        "\n",
        "    # run your Python tool safely (validate args first!)\n",
        "    result = search_documents(**args)\n",
        "\n",
        "    # send tool result back\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": None,\n",
        "        \"tool_calls\": [call]  # keep the call in history if you want\n",
        "    })\n",
        "    messages.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"tool_call_id\": call.id,\n",
        "        \"name\": name,\n",
        "        \"content\": json.dumps({\"results\": result})\n",
        "    })\n",
        "\n",
        "# then call the model again to let it use the tool result(s)\n",
        "final = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "print(final.choices[0].message.content)\n",
        "```\n",
        "\n",
        "*(Shape and field names follow OpenAIâ€™s tool/function calling guides and API reference.)* ([OpenAI Platform][1])\n",
        "\n",
        "---\n",
        "\n",
        "## TL;DR\n",
        "\n",
        "* **Define** tools via `tools=[{\"type\":\"function\",\"function\":{name, description, parameters}}]`.\n",
        "* **Receive** tool calls in `message.tool_calls[*].function.arguments` (JSON string).\n",
        "* **Execute** your code, then **reply** with a `role:\"tool\"` message (including the `tool_call_id`).\n",
        "* Use **Structured Outputs** (strict) + **validation** to maximize reliability; keep schemas deliberate and compact. ([OpenAI Platform][1], [OpenAI][2])\n",
        "\n",
        "If you want, I can tailor a version of this snippet to your exact `search_documents` function and folder layout.\n",
        "\n",
        "[1]: https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com \"Function calling - OpenAI API\"\n",
        "[2]: https://openai.com/index/introducing-structured-outputs-in-the-api/?utm_source=chatgpt.com \"Introducing Structured Outputs in the API\"\n",
        "[3]: https://platform.openai.com/docs/guides/structured-outputs?utm_source=chatgpt.com \"Structured model outputs - OpenAI API\"\n",
        "[4]: https://platform.openai.com/docs/guides/tools?utm_source=chatgpt.com \"Using tools - OpenAI API\"\n",
        "[5]: https://platform.openai.com/docs/api-reference/introduction?utm_source=chatgpt.com \"API Reference\"\n",
        "[6]: https://platform.openai.com/docs/guides/tools-file-search?utm_source=chatgpt.com \"File search - OpenAI API\"\n"
      ],
      "metadata": {
        "id": "nygUz91TEO53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s now build a tool that accepts **multiple argument types**, including a **boolean** and a **list**. This adds more flexibility and reflects real-world use cases agents often need.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› ï¸ Tool Concept: `filter_documents`\n",
        "\n",
        "This tool filters documents based on their filename and/or content, optionally returning only those that include **all** the keywords provided.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§ª Step 1: Python Function\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "def filter_documents(keywords: list, require_all: bool = False, folder: str = \"/content/docs_folder\"):\n",
        "    matched_files = []\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        path = os.path.join(folder, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read().lower()\n",
        "\n",
        "        checks = [kw.lower() in content for kw in keywords]\n",
        "\n",
        "        if (require_all and all(checks)) or (not require_all and any(checks)):\n",
        "            matched_files.append(filename)\n",
        "\n",
        "    return matched_files\n",
        "```\n",
        "\n",
        "* âœ… `keywords`: list of strings to match\n",
        "* âœ… `require_all`: a boolean for AND vs OR matching\n",
        "* âœ… Flexible and simple\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§¾ Step 2: Tool Schema for the LLM\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"filter_documents\",\n",
        "  \"description\": \"Filters documents based on one or more keywords in their content. Can match all or any.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"keywords\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "          \"type\": \"string\"\n",
        "        },\n",
        "        \"description\": \"List of keywords to match in the document content.\"\n",
        "      },\n",
        "      \"require_all\": {\n",
        "        \"type\": \"boolean\",\n",
        "        \"description\": \"If true, all keywords must match; if false, any match is accepted.\"\n",
        "      },\n",
        "      \"folder\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The folder path to search in (default is '/content/docs_folder').\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"keywords\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### ðŸ” Now the LLM can do things like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"filter_documents\",\n",
        "  \"args\": {\n",
        "    \"keywords\": [\"agent\", \"action\", \"memory\"],\n",
        "    \"require_all\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Why this is powerful\n",
        "\n",
        "* The LLM gets **structured control**: it knows how to call this tool correctly.\n",
        "* You get to **enforce safety and input types** through schema.\n",
        "* Booleans and lists open up a whole new level of query logic.\n",
        "\n"
      ],
      "metadata": {
        "id": "HBHl7bZrsl96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You should design tools with provider-specific API requirements in mind**. Different LLM providers expose *very similar concepts* (name + JSON args + schema), but the **envelope and field names differ**. The safest approach is:\n",
        "\n",
        "* **Provider-agnostic core**: your Python function(s) with clear signatures and validation.\n",
        "* **Thin adapters** per provider (OpenAI, Anthropic, Gemini) that translate between that providerâ€™s tool-calling shape and your core.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ§° Tool-Calling API requirements (what to watch for)\n",
        "\n",
        "## 1) Whatâ€™s common across providers\n",
        "\n",
        "* You **declare tools** with a **name**, short **description**, and **JSON Schema** for inputs.\n",
        "* The model may return a **tool call**: *(tool name + JSON arguments)*.\n",
        "* You **execute** your Python code and **send the result back** to the model so it can continue.\n",
        "\n",
        "## 2) Provider differences (envelopes & fields)\n",
        "\n",
        "| Provider               | How you declare tools (request)                                                       | What the model returns                                    | Where args live                                                                          |\n",
        "| ---------------------- | ------------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
        "| **OpenAI**             | `tools=[{\"type\":\"function\",\"function\":{name, description, parameters(JSON Schema)}}]` | Assistant message with `tool_calls[]` (possibly multiple) | `tool_calls[i].function.arguments` (JSON string you `json.loads`) ([OpenAI Platform][1]) |\n",
        "| **Anthropic (Claude)** | `tools=[{name, description, input_schema(JSON Schema)}]`                              | A `tool_use` block with `name` and `input`                | `content[].tool_use.input` (already structured JSON) ([Anthropic][2])                    |\n",
        "| **Google Gemini**      | Function declarations in the request/session                                          | A **function call** with parameters                       | Arguments appear as structured params in the call object ([Google AI for Developers][3]) |\n",
        "\n",
        "> Bottom line: same idea, **different field names/locations**. Build an adapter that normalizes each providerâ€™s call into `{tool_name, args}` before dispatching.\n",
        "\n",
        "## 3) Reliability features you can (and should) use\n",
        "\n",
        "* **Structured/Strict outputs** (JSON Schema enforcement) for non-tool responses and/or tool args where supported. This boosts correctness of JSON and types. Still validate in your code. ([OpenAI Platform][4], [OpenAI][5])\n",
        "* **Tool selection controls**: e.g., OpenAIâ€™s `tool_choice` to force or forbid a tool call in a turn. ([OpenAI Platform][1])\n",
        "* **Multiple tool calls in one turn**: handle arrays; loop over calls and return one tool result per call (OpenAI uses `tool_call_id`). ([OpenAI Platform][1])\n",
        "\n",
        "## 4) Practical requirements & guardrails (provider-agnostic)\n",
        "\n",
        "* **Schema as contract**: keep it tight; mark required fields; use enums for tool names/choices. Validate with Pydantic/JSON Schema before execution. ([OpenAI Platform][4])\n",
        "* **Argument size**: pass **references** (file paths/IDs), not big blobs, to avoid token bloat. ([OpenAI Platform][1])\n",
        "* **Error handling**: fail fast on validation; **retry â‰¤ 1â€“2** with stricter instructions; otherwise return a safe error.\n",
        "* **Security**: treat model output as untrusted input; whitelist tools; check paths, URLs, and ranges; make side-effecting tools **idempotent**.\n",
        "* **Observability**: log raw tool calls, validation errors, retry counts; include a `schema_version` to evolve safely.\n",
        "\n",
        "## 5) Architecture tip (ports & adapters)\n",
        "\n",
        "* **Core (provider-agnostic)**:\n",
        "\n",
        "  * `search_documents(query: str, folder: str) -> list[str]`\n",
        "  * Pydantic models for args/results.\n",
        "* **Adapters**:\n",
        "\n",
        "  * **OpenAI adapter**: read `tool_calls[*].function.name/arguments`, `json.loads`, validate â†’ call core â†’ send `role:\"tool\"` message with `tool_call_id`. ([OpenAI Platform][1])\n",
        "  * **Anthropic adapter**: read `tool_use` block `input` â†’ validate â†’ call core â†’ reply with `tool_result`. ([Anthropic][6])\n",
        "  * **Gemini adapter**: read function call params â†’ validate â†’ call core â†’ send `FunctionResponse`. ([Google AI for Developers][7])\n",
        "\n",
        "---\n",
        "\n",
        "**Answering your direct question:**\n",
        "You should **always** keep the **API requirements of the platform youâ€™re using** in mind (OpenAI in your course), because they determine the request fields, the response envelope, and how you return tool results. If you plan to be multi-provider, isolate those differences in small adapters and keep your tool code and validation provider-agnostic.\n",
        "\n",
        "[1]: https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com \"Function calling - OpenAI API\"\n",
        "[2]: https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview?utm_source=chatgpt.com \"Tool use with Claude\"\n",
        "[3]: https://ai.google.dev/gemini-api/docs/function-calling?utm_source=chatgpt.com \"Function calling with the Gemini API | Google AI for Developers\"\n",
        "[4]: https://platform.openai.com/docs/guides/structured-outputs?utm_source=chatgpt.com \"Structured model outputs - OpenAI API\"\n",
        "[5]: https://openai.com/index/introducing-structured-outputs-in-the-api/?utm_source=chatgpt.com \"Introducing Structured Outputs in the API\"\n",
        "[6]: https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/implement-tool-use?utm_source=chatgpt.com \"How to implement tool use\"\n",
        "[7]: https://ai.google.dev/gemini-api/docs/live-tools?utm_source=chatgpt.com \"Tool use with Live API | Gemini API | Google AI for Developers\"\n"
      ],
      "metadata": {
        "id": "0s46OMtQMS5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Izi_241pmX7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}