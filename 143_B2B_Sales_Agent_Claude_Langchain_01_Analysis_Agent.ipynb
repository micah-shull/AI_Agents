{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsUq6958jGoXSqzBElmrCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/143_B2B_Sales_Agent_Claude_Langchain_01_Analysis_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LangChain Analysis Agent - Identifies pain points and opportunities using LLM chains\n",
        "\n",
        "This agent demonstrates:\n",
        "- LangChain LLM integration\n",
        "- Prompt templates and chains\n",
        "- Structured output with Pydantic models\n",
        "- Error handling and validation\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel as PydanticV1BaseModel\n",
        "from langchain_models import (\n",
        "    CompanyInfo, AnalysisResult, PainPoint, Opportunity,\n",
        "    PainPointSeverity, OpportunityPriority\n",
        ")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AnalysisInput(PydanticV1BaseModel):\n",
        "    \"\"\"Input schema for analysis\"\"\"\n",
        "    company_info: str\n",
        "\n",
        "class LangChainAnalysisAgent:\n",
        "    \"\"\"\n",
        "    LangChain Analysis Agent that uses LLM chains to analyze company information\n",
        "\n",
        "    This demonstrates:\n",
        "    - LangChain LLM integration\n",
        "    - Prompt templates for structured analysis\n",
        "    - Chain composition for complex workflows\n",
        "    - Structured output with Pydantic models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str = \"langchain_analysis_agent\", use_mock: bool = True):\n",
        "        self.agent_id = agent_id\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{agent_id}\")\n",
        "        self.use_mock = use_mock\n",
        "\n",
        "        if use_mock:\n",
        "            # Use mock LLM for demonstration (no API key needed)\n",
        "            self.llm = None\n",
        "            self.logger.info(\"Using mock LLM for demonstration\")\n",
        "        else:\n",
        "            # Initialize OpenAI LLM (requires API key)\n",
        "            self.llm = ChatOpenAI(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                temperature=0.1,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "            self.logger.info(\"Using OpenAI LLM\")\n",
        "\n",
        "        # Initialize prompt templates\n",
        "        self._setup_prompt_templates()\n",
        "\n",
        "        # Initialize chains\n",
        "        self._setup_chains()\n",
        "\n",
        "        self.logger.info(f\"LangChain Analysis Agent initialized: {agent_id}\")\n",
        "\n",
        "    def _setup_prompt_templates(self):\n",
        "        \"\"\"Set up prompt templates for analysis\"\"\"\n",
        "\n",
        "        # Pain point analysis template\n",
        "        self.pain_point_template = PromptTemplate(\n",
        "            input_variables=[\"company_info\"],\n",
        "            template=\"\"\"\n",
        "Analyze the following company information and identify potential pain points:\n",
        "\n",
        "Company Information:\n",
        "{company_info}\n",
        "\n",
        "Please identify pain points in the following format:\n",
        "- Category: [category name]\n",
        "- Description: [description]\n",
        "- Severity: [low/medium/high/critical]\n",
        "- Evidence: [supporting evidence]\n",
        "- Solution: [potential solution]\n",
        "\n",
        "Focus on business challenges, operational issues, and growth constraints.\n",
        "Return up to 3 most relevant pain points.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        # Opportunity analysis template\n",
        "        self.opportunity_template = PromptTemplate(\n",
        "            input_variables=[\"company_info\"],\n",
        "            template=\"\"\"\n",
        "Analyze the following company information and identify potential opportunities:\n",
        "\n",
        "Company Information:\n",
        "{company_info}\n",
        "\n",
        "Please identify opportunities in the following format:\n",
        "- Category: [category name]\n",
        "- Description: [description]\n",
        "- Priority: [low/medium/high/urgent]\n",
        "- Evidence: [supporting evidence]\n",
        "- Value: [potential value]\n",
        "\n",
        "Focus on growth opportunities, efficiency improvements, and market expansion.\n",
        "Return up to 3 most relevant opportunities.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "        # Strategy recommendation template\n",
        "        self.strategy_template = PromptTemplate(\n",
        "            input_variables=[\"pain_points\", \"opportunities\"],\n",
        "            template=\"\"\"\n",
        "Based on the following pain points and opportunities, recommend a sales approach:\n",
        "\n",
        "Pain Points:\n",
        "{pain_points}\n",
        "\n",
        "Opportunities:\n",
        "{opportunities}\n",
        "\n",
        "Recommend one of these approaches:\n",
        "1. pain_point_focused - Address critical challenges first\n",
        "2. opportunity_focused - Leverage growth potential\n",
        "3. relationship_building - General relationship building\n",
        "\n",
        "Provide your recommendation and reasoning.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    def _setup_chains(self):\n",
        "        \"\"\"Set up LangChain chains\"\"\"\n",
        "        if not self.use_mock:\n",
        "            self.pain_point_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=self.pain_point_template,\n",
        "                output_key=\"pain_points\"\n",
        "            )\n",
        "\n",
        "            self.opportunity_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=self.opportunity_template,\n",
        "                output_key=\"opportunities\"\n",
        "            )\n",
        "\n",
        "            self.strategy_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=self.strategy_template,\n",
        "                output_key=\"strategy\"\n",
        "            )\n",
        "\n",
        "    def analyze_company(self, company_info: CompanyInfo) -> AnalysisResult:\n",
        "        \"\"\"\n",
        "        Analyze company information using LangChain LLM chains\n",
        "\n",
        "        Args:\n",
        "            company_info: CompanyInfo object from Research Agent\n",
        "\n",
        "        Returns:\n",
        "            AnalysisResult with identified pain points and opportunities\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Starting LangChain analysis for {company_info.name}\")\n",
        "\n",
        "            if self.use_mock:\n",
        "                # Use mock analysis for demonstration\n",
        "                return self._mock_analysis(company_info)\n",
        "            else:\n",
        "                # Use real LLM chains\n",
        "                return self._llm_analysis(company_info)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"LangChain analysis failed for {company_info.name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _mock_analysis(self, company_info: CompanyInfo) -> AnalysisResult:\n",
        "        \"\"\"Mock analysis for demonstration purposes\"\"\"\n",
        "\n",
        "        # Generate mock pain points based on company characteristics\n",
        "        pain_points = []\n",
        "        opportunities = []\n",
        "\n",
        "        if company_info.size.value == \"startup\":\n",
        "            pain_points.append(PainPoint(\n",
        "                category=\"growth\",\n",
        "                description=\"Scaling challenges and resource constraints\",\n",
        "                severity=PainPointSeverity.HIGH,\n",
        "                evidence=[\"Startup stage company\", \"Limited resources\"],\n",
        "                potential_solution=\"Scalable solutions and growth support\"\n",
        "            ))\n",
        "\n",
        "            opportunities.append(Opportunity(\n",
        "                category=\"expansion\",\n",
        "                description=\"Market expansion opportunities\",\n",
        "                priority=OpportunityPriority.HIGH,\n",
        "                evidence=[\"Startup growth phase\", \"Market potential\"],\n",
        "                potential_value=\"High growth potential\"\n",
        "            ))\n",
        "\n",
        "        elif company_info.size.value == \"mid-market\":\n",
        "            pain_points.append(PainPoint(\n",
        "                category=\"operations\",\n",
        "                description=\"Operational efficiency challenges\",\n",
        "                severity=PainPointSeverity.MEDIUM,\n",
        "                evidence=[\"Mid-market company\", \"Growth phase\"],\n",
        "                potential_solution=\"Process optimization solutions\"\n",
        "            ))\n",
        "\n",
        "            opportunities.append(Opportunity(\n",
        "                category=\"efficiency\",\n",
        "                description=\"Operational optimization opportunities\",\n",
        "                priority=OpportunityPriority.MEDIUM,\n",
        "                evidence=[\"Mid-market size\", \"Growth potential\"],\n",
        "                potential_value=\"Moderate efficiency gains\"\n",
        "            ))\n",
        "\n",
        "        # Industry-specific analysis\n",
        "        if company_info.industry.lower() == \"manufacturing\":\n",
        "            pain_points.append(PainPoint(\n",
        "                category=\"sustainability\",\n",
        "                description=\"Sustainability compliance challenges\",\n",
        "                severity=PainPointSeverity.MEDIUM,\n",
        "                evidence=[\"Manufacturing industry\", \"Regulatory requirements\"],\n",
        "                potential_solution=\"Sustainability and compliance solutions\"\n",
        "            ))\n",
        "\n",
        "            opportunities.append(Opportunity(\n",
        "                category=\"automation\",\n",
        "                description=\"Automation and digitization opportunities\",\n",
        "                priority=OpportunityPriority.HIGH,\n",
        "                evidence=[\"Manufacturing industry\", \"Technology adoption\"],\n",
        "                potential_value=\"Significant efficiency improvements\"\n",
        "            ))\n",
        "\n",
        "        elif company_info.industry.lower() == \"saas\":\n",
        "            pain_points.append(PainPoint(\n",
        "                category=\"customer_acquisition\",\n",
        "                description=\"Customer acquisition cost challenges\",\n",
        "                severity=PainPointSeverity.HIGH,\n",
        "                evidence=[\"SaaS industry\", \"Competitive market\"],\n",
        "                potential_solution=\"Customer acquisition optimization\"\n",
        "            ))\n",
        "\n",
        "            opportunities.append(Opportunity(\n",
        "                category=\"ai_integration\",\n",
        "                description=\"AI/ML integration opportunities\",\n",
        "                priority=OpportunityPriority.HIGH,\n",
        "                evidence=[\"SaaS industry\", \"Technology focus\"],\n",
        "                potential_value=\"Competitive advantage\"\n",
        "            ))\n",
        "\n",
        "        # Generate industry insights\n",
        "        industry_insights = [\n",
        "            f\"{company_info.industry} companies are increasingly focused on digital transformation\",\n",
        "            \"Market consolidation is creating partnership opportunities\",\n",
        "            \"Customer expectations are driving innovation in the industry\"\n",
        "        ]\n",
        "\n",
        "        # Determine recommended approach\n",
        "        high_severity_pains = [p for p in pain_points if p.severity in [PainPointSeverity.HIGH, PainPointSeverity.CRITICAL]]\n",
        "        high_priority_opps = [o for o in opportunities if o.priority in [OpportunityPriority.HIGH, OpportunityPriority.URGENT]]\n",
        "\n",
        "        if high_severity_pains:\n",
        "            recommended_approach = \"pain_point_focused\"\n",
        "        elif high_priority_opps:\n",
        "            recommended_approach = \"opportunity_focused\"\n",
        "        else:\n",
        "            recommended_approach = \"relationship_building\"\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence_score = 0.7  # Mock confidence score\n",
        "\n",
        "        return AnalysisResult(\n",
        "            company_name=company_info.name,\n",
        "            pain_points=pain_points,\n",
        "            opportunities=opportunities,\n",
        "            industry_insights=industry_insights,\n",
        "            recommended_approach=recommended_approach,\n",
        "            confidence_score=confidence_score\n",
        "        )\n",
        "\n",
        "    def _llm_analysis(self, company_info: CompanyInfo) -> AnalysisResult:\n",
        "        \"\"\"Real LLM analysis using LangChain chains\"\"\"\n",
        "\n",
        "        # Convert company info to string for LLM\n",
        "        company_info_str = company_info.model_dump_json()\n",
        "\n",
        "        # Run pain point analysis\n",
        "        pain_point_result = self.pain_point_chain.run(company_info=company_info_str)\n",
        "\n",
        "        # Run opportunity analysis\n",
        "        opportunity_result = self.opportunity_chain.run(company_info=company_info_str)\n",
        "\n",
        "        # Run strategy analysis\n",
        "        strategy_result = self.strategy_chain.run(\n",
        "            pain_points=pain_point_result,\n",
        "            opportunities=opportunity_result\n",
        "        )\n",
        "\n",
        "        # Parse results and create AnalysisResult\n",
        "        # Note: In a real implementation, you'd parse the LLM output\n",
        "        # and convert it to structured data\n",
        "\n",
        "        return AnalysisResult(\n",
        "            company_name=company_info.name,\n",
        "            pain_points=[],  # Would parse from pain_point_result\n",
        "            opportunities=[],  # Would parse from opportunity_result\n",
        "            industry_insights=[\"LLM-generated insights\"],\n",
        "            recommended_approach=\"pain_point_focused\",  # Would parse from strategy_result\n",
        "            confidence_score=0.8\n",
        "        )\n",
        "\n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return agent status for monitoring\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"status\": \"ready\",\n",
        "            \"framework\": \"langchain\",\n",
        "            \"use_mock\": self.use_mock,\n",
        "            \"llm_available\": self.llm is not None,\n",
        "            \"chains_configured\": len(self._get_chain_names()) if not self.use_mock else 0\n",
        "        }\n",
        "\n",
        "    def _get_chain_names(self) -> List[str]:\n",
        "        \"\"\"Get list of configured chain names\"\"\"\n",
        "        if self.use_mock:\n",
        "            return []\n",
        "        return [\"pain_point_chain\", \"opportunity_chain\", \"strategy_chain\"]\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== LangChain Analysis Agent Demo ===\\n\")\n",
        "\n",
        "    # Create agent\n",
        "    analysis_agent = LangChainAnalysisAgent(use_mock=True)\n",
        "\n",
        "    # Test with mock company data\n",
        "    from langchain_models import CompanyInfo, CompanySize\n",
        "\n",
        "    test_company = CompanyInfo(\n",
        "        name=\"Acme Corporation\",\n",
        "        industry=\"Manufacturing\",\n",
        "        size=CompanySize.MID_MARKET,\n",
        "        location=\"Chicago, IL\",\n",
        "        website=\"https://acmecorp.com\",\n",
        "        description=\"Leading manufacturer of industrial equipment\",\n",
        "        recent_news=[\"Expansion into European markets\", \"New sustainability initiative\"],\n",
        "        key_contacts=[{\"name\": \"Sarah Johnson\", \"title\": \"CEO\", \"email\": \"sarah@acmecorp.com\"}]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        analysis_result = analysis_agent.analyze_company(test_company)\n",
        "\n",
        "        print(f\"Analysis for: {analysis_result.company_name}\")\n",
        "        print(f\"Confidence Score: {analysis_result.confidence_score:.2f}\")\n",
        "        print(f\"Recommended Approach: {analysis_result.recommended_approach}\")\n",
        "\n",
        "        print(f\"\\nPain Points ({len(analysis_result.pain_points)}):\")\n",
        "        for i, pain_point in enumerate(analysis_result.pain_points, 1):\n",
        "            print(f\"  {i}. {pain_point.description}\")\n",
        "            print(f\"     Category: {pain_point.category}\")\n",
        "            print(f\"     Severity: {pain_point.severity}\")\n",
        "            print(f\"     Evidence: {', '.join(pain_point.evidence)}\")\n",
        "\n",
        "        print(f\"\\nOpportunities ({len(analysis_result.opportunities)}):\")\n",
        "        for i, opportunity in enumerate(analysis_result.opportunities, 1):\n",
        "            print(f\"  {i}. {opportunity.description}\")\n",
        "            print(f\"     Category: {opportunity.category}\")\n",
        "            print(f\"     Priority: {opportunity.priority}\")\n",
        "            print(f\"     Evidence: {', '.join(opportunity.evidence)}\")\n",
        "\n",
        "        print(f\"\\nIndustry Insights:\")\n",
        "        for insight in analysis_result.industry_insights:\n",
        "            print(f\"  â€¢ {insight}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Analysis failed: {str(e)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Show agent status\n",
        "    status = analysis_agent.get_status()\n",
        "    print(\"Agent Status:\")\n",
        "    for key, value in status.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "OTNmq6aqblE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s walk through this **LangChain Analysis Agent** step by step so you fully understand what itâ€™s doing and what you should learn from it.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ What This Agent Does\n",
        "\n",
        "The **LangChainAnalysisAgent** takes structured company info (from your ResearchAgent) and produces an **AnalysisResult** with:\n",
        "\n",
        "* Pain points\n",
        "* Opportunities\n",
        "* Industry insights\n",
        "* A recommended sales approach (pain-point, opportunity, or relationship)\n",
        "* Confidence score\n",
        "\n",
        "Itâ€™s the same role as your earlier `AnalysisAgent`, but rebuilt on **LangChainâ€™s primitives**.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ Key Components\n",
        "\n",
        "### 1. **LLM Setup**\n",
        "\n",
        "```python\n",
        "if use_mock:\n",
        "    self.llm = None\n",
        "else:\n",
        "    self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, max_tokens=1000)\n",
        "```\n",
        "\n",
        "* `use_mock=True` means it wonâ€™t call the real OpenAI API â€” just use hand-coded mock logic.\n",
        "* If `use_mock=False`, it spins up a LangChain `ChatOpenAI` instance.\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Good practice to have a mock mode â†’ lets you develop/test without burning tokens or needing API keys.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Prompt Templates**\n",
        "\n",
        "```python\n",
        "self.pain_point_template = PromptTemplate(...)\n",
        "self.opportunity_template = PromptTemplate(...)\n",
        "self.strategy_template = PromptTemplate(...)\n",
        "```\n",
        "\n",
        "Each template is structured and instructs the LLM to produce outputs in a predictable format.\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Instead of writing raw prompts inline, encapsulate them as **PromptTemplates**. This makes them reusable and easier to iterate on.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Chains**\n",
        "\n",
        "```python\n",
        "self.pain_point_chain = LLMChain(llm=self.llm, prompt=self.pain_point_template)\n",
        "```\n",
        "\n",
        "* A `Chain` in LangChain = LLM + Prompt + (optionally) OutputParser.\n",
        "* Here you have three chains: **pain point**, **opportunity**, **strategy**.\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Break complex workflows into **composable chains** instead of one mega-prompt.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Mock Analysis Logic**\n",
        "\n",
        "```python\n",
        "if company_info.size.value == \"startup\":\n",
        "    pain_points.append(PainPoint(...))\n",
        "    opportunities.append(Opportunity(...))\n",
        "```\n",
        "\n",
        "This hard-codes analysis logic for different company sizes/industries.\n",
        "\n",
        "* Startups get â€œscaling challengesâ€ + â€œexpansion opportunities.â€\n",
        "* Manufacturing companies get â€œsustainability complianceâ€ + â€œautomation opportunities.â€\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Mocks let you simulate LLM behavior so your pipeline can run end-to-end *before* you wire in real AI.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Real LLM Analysis**\n",
        "\n",
        "```python\n",
        "pain_point_result = self.pain_point_chain.run(company_info=company_info_str)\n",
        "opportunity_result = self.opportunity_chain.run(company_info=company_info_str)\n",
        "strategy_result = self.strategy_chain.run(...)\n",
        "```\n",
        "\n",
        "* Each chain runs independently.\n",
        "* The strategy chain combines pain points + opportunities.\n",
        "\n",
        "ğŸ‘‰ **Lesson:** This is how you **compose multiple chains** for multi-step reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Structured Outputs**\n",
        "\n",
        "The agent returns an `AnalysisResult` object:\n",
        "\n",
        "```python\n",
        "return AnalysisResult(\n",
        "    company_name=company_info.name,\n",
        "    pain_points=pain_points,\n",
        "    opportunities=opportunities,\n",
        "    industry_insights=industry_insights,\n",
        "    recommended_approach=recommended_approach,\n",
        "    confidence_score=confidence_score\n",
        ")\n",
        "```\n",
        "\n",
        "Instead of messy free text, everything is wrapped in a structured model (`Pydantic`).\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Always enforce schemas on LLM output to make downstream automation reliable.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Agent Status**\n",
        "\n",
        "```python\n",
        "def get_status(self):\n",
        "    return { \"agent_id\": ..., \"status\": \"ready\", \"framework\": \"langchain\", ... }\n",
        "```\n",
        "\n",
        "This is for monitoring/observability.\n",
        "\n",
        "ğŸ‘‰ **Lesson:** Treat agents as microservices â†’ each should have a clear â€œhealth check.â€\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ Key Takeaways for You\n",
        "\n",
        "1. **Prompt templates** make workflows reusable & maintainable.\n",
        "2. **Chains** allow modular composition (pain points â†’ opportunities â†’ strategy).\n",
        "3. **Mocks** let you test pipelines before integrating a real LLM.\n",
        "4. **Structured outputs (Pydantic)** prevent chaos â†’ enforce predictable JSON-like returns.\n",
        "5. **Agent status** = observability hook â†’ good for orchestrators & dashboards.\n",
        "\n",
        "---\n",
        "\n",
        "âš¡ The big picture: This code shows how LangChain replaces your manual `if/else` orchestrator logic with **chains and templates**, while still keeping reliability via **structured outputs**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rex88Qy7cxUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸš€ â€” letâ€™s break down the **major LangChain features** youâ€™re using in this `LangChainAnalysisAgent` and why they matter.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 1. **PromptTemplate**\n",
        "\n",
        "* **What it does:** Lets you define reusable, parameterized prompts with placeholders.\n",
        "* **Why it matters:**\n",
        "\n",
        "  * No need to hardcode giant strings everywhere.\n",
        "  * Makes prompts maintainable and tweakable.\n",
        "  * Forces you to think of prompts as modular building blocks.\n",
        "\n",
        "ğŸ‘‰ Without LangChain: Youâ€™d write raw strings and manually `.format()` them.\n",
        "ğŸ‘‰ With LangChain: Cleaner, reusable objects tied directly to LLM execution.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 2. **LLMChain**\n",
        "\n",
        "* **What it does:** Bundles together an LLM + a PromptTemplate (and optionally an output parser).\n",
        "* **Why it matters:**\n",
        "\n",
        "  * Turns prompts into **callable objects** you can run like functions.\n",
        "  * Makes multi-step workflows modular.\n",
        "  * Handles the formatting and LLM call boilerplate for you.\n",
        "\n",
        "ğŸ‘‰ Without LangChain: Youâ€™d manually build strings, call the API, and parse responses.\n",
        "ğŸ‘‰ With LangChain: `result = chain.run(company_info=\"Acme Corp\")` â†’ done.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 3. **Structured Output (via Pydantic models)**\n",
        "\n",
        "* **What it does:** Encapsulates results in predictable schemas (`AnalysisResult`, `PainPoint`, `Opportunity`).\n",
        "* **Why it matters:**\n",
        "\n",
        "  * Downstream agents donâ€™t choke on free-form text.\n",
        "  * Gives you machine-readable, validated JSON-like objects.\n",
        "  * Prevents hallucination chaos by enforcing shape & fields.\n",
        "\n",
        "ğŸ‘‰ Without LangChain: Youâ€™d regex/split strings and hope the format is consistent.\n",
        "ğŸ‘‰ With LangChain: Typed dataclasses or Pydantic models â†’ robust contracts.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 4. **Mock Mode**\n",
        "\n",
        "* **What it does:** Lets you simulate outputs without calling an LLM.\n",
        "* **Why it matters:**\n",
        "\n",
        "  * Test pipelines locally without burning tokens.\n",
        "  * Debug logic separately from model behavior.\n",
        "  * Build confidence before scaling.\n",
        "\n",
        "ğŸ‘‰ Without LangChain: Youâ€™d need custom `if/else` hacks.\n",
        "ğŸ‘‰ With LangChain: Integrated toggles + simple fallbacks.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 5. **Agent-Like Structure (Chaining Multiple Steps)**\n",
        "\n",
        "* **What it does:** Pain point analysis, opportunity analysis, and strategy generation are separate chains that run sequentially.\n",
        "* **Why it matters:**\n",
        "\n",
        "  * Decomposes one big fuzzy prompt into **small, controllable steps**.\n",
        "  * Easier to debug: if strategy fails, you know pain point and opportunity worked fine.\n",
        "  * Scales to more steps easily.\n",
        "\n",
        "ğŸ‘‰ Without LangChain: Youâ€™d have one huge, fragile prompt.\n",
        "ğŸ‘‰ With LangChain: Composable micro-prompts = cleaner reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ 6. **Observability (get\\_status)**\n",
        "\n",
        "* **What it does:** Returns agent ID, framework, and health status.\n",
        "* **Why it matters:**\n",
        "\n",
        "  * Helps orchestration and dashboards monitor agent readiness.\n",
        "  * Moves you toward â€œagents as servicesâ€ with health checks.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸŸ¦ Big Picture Improvements\n",
        "\n",
        "âœ… **Less boilerplate:** No manual string building, API calling, or parsing.\n",
        "âœ… **Modularity:** PainPoint, Opportunity, and Strategy chains are plug-and-play.\n",
        "âœ… **Maintainability:** You can swap templates, models, or outputs without rewriting logic.\n",
        "âœ… **Reliability:** Structured results ensure downstream agents donâ€™t break.\n",
        "âœ… **Scalability:** Easy to add more analysis chains later (e.g., SWOTAgent).\n",
        "\n",
        "---\n",
        "\n",
        "âš¡ **Key takeaway:** LangChain isnâ€™t magic â€” but it abstracts away the repetitive plumbing so you can focus on **workflow design, prompt strategy, and structured outputs**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N6dqF6z6d7Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## âš¡ What Iâ€™d Add Before Moving On\n",
        "\n",
        "1. **Parsing Real LLM Output**\n",
        "\n",
        "   * Right now, `_llm_analysis()` returns empty lists for pain points and opportunities and just inserts placeholders .\n",
        "   * In production, youâ€™d want to parse the text returned by `pain_point_chain.run()` and map it back into your `PainPoint` and `Opportunity` objects.\n",
        "   * This could be done with **LangChainâ€™s structured output parsers** or even JSON-mode from newer OpenAI models.\n",
        "\n",
        "2. **Confidence Score Calculation**\n",
        "\n",
        "   * Currently hardcoded (0.7 in mock, 0.8 in LLM mode) .\n",
        "   * You might want a heuristic like:\n",
        "\n",
        "     * # of strong evidence items\n",
        "     * # of high-severity pain points\n",
        "     * Consistency across chains\n",
        "   * Or, you could even ask the LLM to self-report a confidence measure.\n",
        "\n",
        "3. **Extensibility Hooks**\n",
        "\n",
        "   * The design is great for *core analysis*.\n",
        "   * But you may later want to add specialized chains (e.g., *competitive threats*, *regulatory risks*, *financial health*).\n",
        "   * With the current modular setup, you can drop in new chains easily.\n",
        "\n",
        "4. **Error Handling for LLM Mode**\n",
        "\n",
        "   * In `_llm_analysis`, if the model produces malformed text, the code might crash when parsing.\n",
        "   * Adding try/except with fallbacks to a safe â€œrelationship\\_buildingâ€ strategy would make the agent more resilient.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸŸ¦ Bottom Line\n",
        "\n",
        "The **Analysis Agent** is already a strong demo: modular, testable, and orchestration-ready.\n",
        "The **next step** is making `_llm_analysis` fully production-ready by:\n",
        "\n",
        "* parsing outputs â†’ structured models,\n",
        "* computing real confidence, and\n",
        "* handling LLM quirks gracefully.\n"
      ],
      "metadata": {
        "id": "ITFZWbaloeol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdhWweexaXxS"
      },
      "outputs": [],
      "source": []
    }
  ]
}