{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAzog0hfdzHQNnR6fOGZr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/319_EaaS_Nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nodes for EaaS Orchestrator Agent"
      ],
      "metadata": {
        "id": "ypDROhtDpSfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Nodes for EaaS Orchestrator Agent\n",
        "\n",
        "Orchestration logic for the evaluation workflow.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "from toolshed.progress import calculate_progress, calculate_elapsed_time, estimate_remaining_time\n",
        "from toolshed.performance import create_metrics_config, track_execution_time\n",
        "from toolshed.workflows import analyze_workflow_health\n",
        "from toolshed.validation import validate_data_structure\n",
        "from agents.eval_as_service.utilities import (\n",
        "    load_journey_scenarios,\n",
        "    load_specialist_agents,\n",
        "    load_supporting_data,\n",
        "    load_decision_rules,\n",
        "    build_agent_lookup,\n",
        "    build_scenario_lookup,\n",
        "    simulate_agent_execution,\n",
        "    score_evaluation,\n",
        "    calculate_agent_performance_summary\n",
        ")\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "\n",
        "\n",
        "def goal_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"Define the goal for evaluation.\"\"\"\n",
        "    scenario_id = state.get('scenario_id')\n",
        "    target_agent_id = state.get('target_agent_id')\n",
        "\n",
        "    if scenario_id:\n",
        "        goal_description = f\"Evaluate agent performance for scenario {scenario_id}\"\n",
        "    elif target_agent_id:\n",
        "        goal_description = f\"Evaluate agent {target_agent_id} across all scenarios\"\n",
        "    else:\n",
        "        goal_description = \"Evaluate all agents across all scenarios\"\n",
        "\n",
        "    return {\n",
        "        \"goal\": {\n",
        "            \"description\": goal_description,\n",
        "            \"type\": \"evaluation\",\n",
        "            \"scope\": {\n",
        "                \"scenario_id\": scenario_id,\n",
        "                \"target_agent_id\": target_agent_id\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def planning_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"Create execution plan for evaluation.\"\"\"\n",
        "    plan = [\n",
        "        {\n",
        "            \"step\": 1,\n",
        "            \"task\": \"Load evaluation data\",\n",
        "            \"description\": \"Load scenarios, agents, and supporting data\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 2,\n",
        "            \"task\": \"Execute evaluations\",\n",
        "            \"description\": \"Run scenarios through target agents\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 3,\n",
        "            \"task\": \"Score evaluations\",\n",
        "            \"description\": \"Compare actual outputs to expected outcomes\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 4,\n",
        "            \"task\": \"Analyze performance\",\n",
        "            \"description\": \"Calculate agent performance summaries\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 5,\n",
        "            \"task\": \"Generate report\",\n",
        "            \"description\": \"Create comprehensive evaluation report\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "\n",
        "def data_loading_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Load all required data for evaluation.\"\"\"\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    try:\n",
        "        # Load scenarios\n",
        "        scenarios = load_journey_scenarios(config.data_dir, config.journey_scenarios_file)\n",
        "\n",
        "        # Filter by scenario_id if specified\n",
        "        scenario_id = state.get('scenario_id')\n",
        "        if scenario_id:\n",
        "            scenarios = [s for s in scenarios if s.get('scenario_id') == scenario_id]\n",
        "\n",
        "        # Load agents\n",
        "        agents_dict = load_specialist_agents(config.data_dir, config.specialist_agents_file)\n",
        "\n",
        "        # Filter by target_agent_id if specified\n",
        "        target_agent_id = state.get('target_agent_id')\n",
        "        if target_agent_id:\n",
        "            agents_dict = {k: v for k, v in agents_dict.items() if k == target_agent_id}\n",
        "\n",
        "        # Load supporting data\n",
        "        supporting_data = load_supporting_data(\n",
        "            config.data_dir,\n",
        "            config.customers_file,\n",
        "            config.orders_file,\n",
        "            config.logistics_file,\n",
        "            config.marketing_signals_file\n",
        "        )\n",
        "\n",
        "        # Load decision rules\n",
        "        decision_rules = load_decision_rules(config.data_dir, config.decision_rules_file)\n",
        "\n",
        "        # Build lookups\n",
        "        agent_lookup = build_agent_lookup(agents_dict)\n",
        "        scenario_lookup = build_scenario_lookup(scenarios)\n",
        "\n",
        "        # Validate data structure if enabled\n",
        "        if config.enable_validation:\n",
        "            try:\n",
        "                validate_data_structure(scenarios, required_fields=['scenario_id', 'customer_id', 'order_id'])\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Validation warning: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            \"journey_scenarios\": scenarios,\n",
        "            \"specialist_agents\": agents_dict,\n",
        "            \"supporting_data\": supporting_data,\n",
        "            \"decision_rules\": decision_rules,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Data loading error: {str(e)}\")\n",
        "        return {\"errors\": errors}\n"
      ],
      "metadata": {
        "id": "GgeNtfQuoybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Orchestrator Nodes: Turning Evaluation into a Workflow\n",
        "\n",
        "This section introduces the first **orchestration nodes** of the Evaluation-as-a-Service (EaaS) agent. These nodes define how an evaluation run is structured, sequenced, and executed from start to finish.\n",
        "\n",
        "Rather than hard-coding a single evaluation flow, the orchestrator models evaluation as a **series of explicit steps**, each with a clear responsibility.\n",
        "\n",
        "This makes the system easier to understand, extend, and govern.\n",
        "\n",
        "---\n",
        "\n",
        "## Goal Definition: Making Intent Explicit\n",
        "\n",
        "```python\n",
        "goal_node(...)\n",
        "```\n",
        "\n",
        "The goal node defines *why* the evaluation is being run.\n",
        "\n",
        "Depending on the inputs, the goal may be:\n",
        "\n",
        "* evaluating a single scenario\n",
        "* evaluating a specific agent across all scenarios\n",
        "* evaluating all agents across all scenarios\n",
        "\n",
        "By explicitly recording the goal in state, the system ensures that:\n",
        "\n",
        "* evaluation intent is visible\n",
        "* scope is clearly defined\n",
        "* reports can explain *what was evaluated and why*\n",
        "\n",
        "This prevents evaluation runs from becoming ambiguous or open-ended.\n",
        "\n",
        "---\n",
        "\n",
        "## Planning: Making the Workflow Visible\n",
        "\n",
        "```python\n",
        "planning_node(...)\n",
        "```\n",
        "\n",
        "The planning node creates a simple, transparent execution plan that outlines the evaluation workflow step by step.\n",
        "\n",
        "Even though the plan is static in this MVP, its presence is important:\n",
        "\n",
        "* it documents the evaluation lifecycle\n",
        "* it provides a mental model for how the system operates\n",
        "* it creates a foundation for future dynamic planning\n",
        "\n",
        "From a governance perspective, this plan acts as a **contract** for how evaluations are performed.\n",
        "\n",
        "---\n",
        "\n",
        "## Data Loading as a Dedicated Phase\n",
        "\n",
        "```python\n",
        "data_loading_node(...)\n",
        "```\n",
        "\n",
        "The data loading node is where all required inputs for evaluation are gathered and prepared.\n",
        "\n",
        "This includes:\n",
        "\n",
        "* journey scenarios\n",
        "* specialist agent definitions\n",
        "* supporting business data\n",
        "* decision rules\n",
        "* lookup structures\n",
        "\n",
        "By isolating data loading into its own node, the system reinforces a key design principle:\n",
        "\n",
        "> **Evaluation should not begin until inputs are complete and validated.**\n",
        "\n",
        "---\n",
        "\n",
        "## Scope Control Without Rewrites\n",
        "\n",
        "This node supports targeted evaluation through simple filters:\n",
        "\n",
        "* scenario-level filtering\n",
        "* agent-level filtering\n",
        "\n",
        "This allows teams to:\n",
        "\n",
        "* re-evaluate a single failure\n",
        "* isolate a problematic agent\n",
        "* run focused audits\n",
        "\n",
        "All without changing orchestration logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Validation as a Guardrail\n",
        "\n",
        "When validation is enabled, the node checks that critical data fields are present before evaluation proceeds.\n",
        "\n",
        "Validation errors are captured as warnings rather than hard failures, which:\n",
        "\n",
        "* preserves system resilience\n",
        "* surfaces data issues early\n",
        "* prevents silent corruption of metrics\n",
        "\n",
        "This reinforces the idea that **data quality is enforced at the boundary**, not assumed downstream.\n",
        "\n",
        "---\n",
        "\n",
        "## Errors Are Collected, Not Hidden\n",
        "\n",
        "Errors encountered during data loading are accumulated into state rather than causing uncontrolled failures.\n",
        "\n",
        "This ensures that:\n",
        "\n",
        "* failures are visible\n",
        "* partial results can still be reported\n",
        "* evaluation runs remain explainable\n",
        "\n",
        "Transparent error handling is essential for trust, especially when evaluations are automated.\n",
        "\n",
        "---\n",
        "\n",
        "## Why These Nodes Matter Together\n",
        "\n",
        "Taken as a group, these first nodes establish a clear pattern:\n",
        "\n",
        "* **Goal** defines intent\n",
        "* **Plan** defines structure\n",
        "* **Data loading** defines inputs\n",
        "\n",
        "Only after these steps does the system move on to execution and scoring.\n",
        "\n",
        "This mirrors how mature operational systems are designed and makes evaluation runs:\n",
        "\n",
        "* predictable\n",
        "* repeatable\n",
        "* defensible\n",
        "\n",
        "---\n",
        "\n",
        "## From Ad-Hoc Scripts to Orchestrated Evaluation\n",
        "\n",
        "Many agent systems mix data loading, execution, and scoring into a single block of logic. This orchestrator deliberately avoids that pattern.\n",
        "\n",
        "By breaking evaluation into nodes:\n",
        "\n",
        "* each step is easier to reason about\n",
        "* failures are easier to localize\n",
        "* future enhancements are easier to add\n",
        "\n",
        "This structure is what allows the system to grow from a simple evaluator into a full **AI governance workflow**.\n",
        "\n"
      ],
      "metadata": {
        "id": "cmMS6Vq8v8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_execution_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Execute evaluations by running scenarios through agents.\"\"\"\n",
        "    scenarios = state.get('journey_scenarios', [])\n",
        "    agents = state.get('specialist_agents', {})\n",
        "    supporting_data = state.get('supporting_data', {})\n",
        "\n",
        "    executed_evaluations = []\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    # Track start time for progress\n",
        "    start_time = state.get('evaluation_start_time')\n",
        "    if not start_time:\n",
        "        start_time = datetime.now().isoformat()\n",
        "\n",
        "    # Determine which agents to evaluate for each scenario\n",
        "    for scenario in scenarios:\n",
        "        expected_resolution_path = scenario.get('expected_resolution_path', [])\n",
        "\n",
        "        # Evaluate each agent in the expected resolution path\n",
        "        for agent_id in expected_resolution_path:\n",
        "            if agent_id not in agents:\n",
        "                errors.append(f\"Agent {agent_id} not found for scenario {scenario.get('scenario_id')}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Track execution time\n",
        "                execution_start = datetime.now()\n",
        "\n",
        "                # Simulate agent execution\n",
        "                result = simulate_agent_execution(\n",
        "                    agent_id,\n",
        "                    scenario,\n",
        "                    supporting_data,\n",
        "                    agents\n",
        "                )\n",
        "\n",
        "                execution_end = datetime.now()\n",
        "                execution_time = (execution_end - execution_start).total_seconds()\n",
        "\n",
        "                evaluation = {\n",
        "                    \"scenario_id\": scenario.get('scenario_id'),\n",
        "                    \"target_agent_id\": agent_id,\n",
        "                    \"input\": {\n",
        "                        \"customer_message\": scenario.get('customer_message'),\n",
        "                        \"customer_id\": scenario.get('customer_id'),\n",
        "                        \"order_id\": scenario.get('order_id')\n",
        "                    },\n",
        "                    \"actual_output\": result.get('output'),\n",
        "                    \"expected_output\": {\n",
        "                        \"expected_issue_type\": scenario.get('expected_issue_type'),\n",
        "                        \"expected_resolution_path\": expected_resolution_path,\n",
        "                        \"expected_outcome\": scenario.get('expected_outcome')\n",
        "                    },\n",
        "                    \"execution_time_seconds\": execution_time,\n",
        "                    \"status\": result.get('status', 'failed'),\n",
        "                    \"error\": result.get('error')\n",
        "                }\n",
        "\n",
        "                executed_evaluations.append(evaluation)\n",
        "\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Evaluation error for scenario {scenario.get('scenario_id')}, agent {agent_id}: {str(e)}\")\n",
        "                executed_evaluations.append({\n",
        "                    \"scenario_id\": scenario.get('scenario_id'),\n",
        "                    \"target_agent_id\": agent_id,\n",
        "                    \"status\": \"failed\",\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "    # Update progress\n",
        "    total = len(executed_evaluations)\n",
        "    completed = len([e for e in executed_evaluations if e.get('status') == 'completed'])\n",
        "\n",
        "    progress = calculate_progress(completed=completed, total=total) if total > 0 else 0.0\n",
        "\n",
        "    elapsed = calculate_elapsed_time(start_time)\n",
        "    remaining = estimate_remaining_time(\n",
        "        completed=completed,\n",
        "        total=total,\n",
        "        elapsed_minutes=elapsed / 60.0\n",
        "    ) if total > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"executed_evaluations\": executed_evaluations,\n",
        "        \"evaluations_completed\": completed,\n",
        "        \"evaluations_total\": total,\n",
        "        \"progress_percentage\": progress,\n",
        "        \"elapsed_time_seconds\": elapsed,\n",
        "        \"estimated_remaining_seconds\": remaining * 60.0,\n",
        "        \"evaluation_start_time\": start_time,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "mWYuqg3Uo8ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Evaluation Execution: Running Controlled Experiments\n",
        "\n",
        "The `evaluation_execution_node` is responsible for executing evaluations in a **structured, repeatable way**. This is where scenarios are run through agents and concrete evidence of agent behavior is produced.\n",
        "\n",
        "Rather than treating execution as an opaque black box, this node treats it as a **controlled experiment** with clearly defined inputs, outputs, and measurements.\n",
        "\n",
        "---\n",
        "\n",
        "## Execution Follows Explicit Expectations\n",
        "\n",
        "Each scenario defines an `expected_resolution_path`, which specifies which agents should be involved.\n",
        "\n",
        "The orchestrator uses this path to determine:\n",
        "\n",
        "* which agents are evaluated\n",
        "* in what context\n",
        "* for which scenario\n",
        "\n",
        "This ensures agents are evaluated **only when they are expected to act**, preventing irrelevant or misleading measurements.\n",
        "\n",
        "---\n",
        "\n",
        "## One Scenario, One Agent, One Record\n",
        "\n",
        "For every scenario–agent pairing, the node creates a complete evaluation record that includes:\n",
        "\n",
        "* the scenario identifier\n",
        "* the target agent\n",
        "* the exact inputs provided\n",
        "* the agent’s actual output\n",
        "* the expected outcome\n",
        "* execution time\n",
        "* execution status and errors\n",
        "\n",
        "Each evaluation becomes a **self-contained unit of evidence** that can be scored, audited, or revisited later.\n",
        "\n",
        "---\n",
        "\n",
        "## Execution Is Measured, Not Assumed\n",
        "\n",
        "Execution time is measured explicitly rather than assumed or estimated.\n",
        "\n",
        "This allows:\n",
        "\n",
        "* latency to be scored objectively\n",
        "* performance regressions to be detected\n",
        "* operational behavior to be tracked over time\n",
        "\n",
        "Speed becomes a first-class metric, not an afterthought.\n",
        "\n",
        "---\n",
        "\n",
        "## Errors Are Captured Without Breaking the System\n",
        "\n",
        "If an agent fails or an exception occurs:\n",
        "\n",
        "* the failure is recorded\n",
        "* the error message is preserved\n",
        "* evaluation continues for other scenarios and agents\n",
        "\n",
        "This design avoids cascading failures and ensures that partial results are still meaningful and explainable.\n",
        "\n",
        "---\n",
        "\n",
        "## Progress Tracking Is Built In\n",
        "\n",
        "The node calculates:\n",
        "\n",
        "* how many evaluations were completed\n",
        "* how many remain\n",
        "* overall progress percentage\n",
        "* elapsed time\n",
        "* estimated time remaining\n",
        "\n",
        "This transforms evaluation from a “fire and forget” process into a **predictable operation** that can be monitored and scheduled.\n",
        "\n",
        "From a business perspective, this predictability matters just as much as correctness.\n",
        "\n",
        "---\n",
        "\n",
        "## Determinism at the Execution Layer\n",
        "\n",
        "Execution follows a fixed pattern:\n",
        "\n",
        "* same scenarios\n",
        "* same agents\n",
        "* same inputs\n",
        "* same structure\n",
        "\n",
        "When execution is deterministic, downstream scoring and trend analysis become trustworthy. If performance changes, it reflects real behavioral changes — not variability in how evaluations were run.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Node Is So Important\n",
        "\n",
        "This node creates the **raw material** for everything that follows:\n",
        "\n",
        "* scoring\n",
        "* health classification\n",
        "* trend analysis\n",
        "* reporting\n",
        "\n",
        "If execution were inconsistent or opaque, none of those layers would be reliable.\n",
        "\n",
        "By designing execution as a controlled, measurable step, the orchestrator preserves integrity across the entire evaluation pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Pattern at Work\n",
        "\n",
        "This node reinforces a core architectural pattern used throughout the system:\n",
        "\n",
        "> **Prepare inputs carefully → execute predictably → measure explicitly → analyze confidently**\n",
        "\n",
        "That pattern is what allows evaluation results to scale from individual tests into organizational insight.\n",
        "\n"
      ],
      "metadata": {
        "id": "Il_In3FJxVke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## State as the Thread That Connects Everything\n",
        "\n",
        "The orchestrator is built around a shared **state object** that flows through every node in the workflow.\n",
        "\n",
        "Each node:\n",
        "\n",
        "* reads what it needs from state\n",
        "* performs a focused piece of work\n",
        "* adds new information to state\n",
        "* passes the updated state forward\n",
        "\n",
        "Nothing is hidden, and nothing is overwritten without intent.\n",
        "\n",
        "---\n",
        "\n",
        "## Nodes Read from State, They Don’t Recreate Context\n",
        "\n",
        "When the execution node begins, it retrieves what it needs directly from state:\n",
        "\n",
        "```python\n",
        "scenarios = state.get('journey_scenarios', [])\n",
        "agents = state.get('specialist_agents', {})\n",
        "supporting_data = state.get('supporting_data', {})\n",
        "```\n",
        "\n",
        "This means:\n",
        "\n",
        "* scenarios were already loaded and filtered\n",
        "* agents were already selected and validated\n",
        "* supporting data was already prepared\n",
        "\n",
        "The execution node does not re-fetch or reinterpret data. It trusts the state.\n",
        "\n",
        "---\n",
        "\n",
        "## Utilities Do the Work, Nodes Coordinate It\n",
        "\n",
        "The nodes themselves are intentionally lightweight.\n",
        "\n",
        "They do not:\n",
        "\n",
        "* load files\n",
        "* manipulate raw data structures\n",
        "* implement scoring logic\n",
        "* perform domain-specific calculations\n",
        "\n",
        "Instead, they **delegate work to utilities** that encapsulate reusable business logic.\n",
        "\n",
        "This creates a clean division of responsibility:\n",
        "\n",
        "* utilities implement *how* things are done\n",
        "* nodes decide *when* and *in what order* they are done\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Pattern Matters\n",
        "\n",
        "This separation delivers several important benefits:\n",
        "\n",
        "### 1. Clarity\n",
        "\n",
        "Each node has a single purpose. Readers can understand the workflow without digging into implementation details.\n",
        "\n",
        "### 2. Reusability\n",
        "\n",
        "Utilities can be reused across nodes, workflows, or even other agents without duplication.\n",
        "\n",
        "### 3. Testability\n",
        "\n",
        "Utilities can be tested independently from orchestration logic, improving reliability.\n",
        "\n",
        "### 4. Traceability\n",
        "\n",
        "Because all outputs are written to state, the full evaluation story can be reconstructed at any point.\n",
        "\n",
        "---\n",
        "\n",
        "## State Accumulates Knowledge Over Time\n",
        "\n",
        "As the workflow progresses, state gradually becomes richer:\n",
        "\n",
        "* early nodes add goals and plans\n",
        "* data loading nodes add scenarios and context\n",
        "* execution nodes add evaluation records\n",
        "* scoring nodes add metrics and classifications\n",
        "* reporting nodes add summaries and artifacts\n",
        "\n",
        "State is not just storage — it is the **memory of the evaluation run**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Design Scales\n",
        "\n",
        "This pattern scales cleanly because:\n",
        "\n",
        "* new nodes can be added without breaking existing ones\n",
        "* new utilities can be introduced without changing orchestration\n",
        "* additional metrics can be attached to state without refactoring\n",
        "\n",
        "As the system grows, complexity is distributed rather than tangled.\n",
        "\n",
        "---\n",
        "\n",
        "## A Simple Mental Model\n",
        "\n",
        "A useful way to think about the orchestrator is:\n",
        "\n",
        "> **Utilities do the work.\n",
        "> Nodes coordinate the work.\n",
        "> State remembers the work.**\n",
        "\n",
        "That mental model makes the entire system easier to reason about and easier to explain to others.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Leaders Care About This\n",
        "\n",
        "From a governance perspective, this architecture ensures:\n",
        "\n",
        "* transparency into how results were produced\n",
        "* clear boundaries between data, logic, and decisions\n",
        "* confidence that evaluations are repeatable and explainable\n",
        "\n",
        "This is exactly what’s required when AI systems move beyond experimentation and into operational use.\n"
      ],
      "metadata": {
        "id": "bN4MI2IOyJet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Score evaluations by comparing actual outputs to expected outcomes.\"\"\"\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "    scenarios = state.get('journey_scenarios', [])\n",
        "    scenario_lookup = build_scenario_lookup(scenarios)\n",
        "\n",
        "    evaluation_scores = []\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    for evaluation in evaluations:\n",
        "        scenario_id = evaluation.get('scenario_id')\n",
        "        scenario = scenario_lookup.get(scenario_id)\n",
        "\n",
        "        if not scenario:\n",
        "            errors.append(f\"Scenario {scenario_id} not found for scoring\")\n",
        "            continue\n",
        "\n",
        "        expected_outcome = scenario.get('expected_outcome')\n",
        "        expected_resolution_path = scenario.get('expected_resolution_path', [])\n",
        "\n",
        "        try:\n",
        "            score = score_evaluation(\n",
        "                evaluation,\n",
        "                expected_outcome,\n",
        "                expected_resolution_path,\n",
        "                config.scoring_weights,\n",
        "                config.pass_threshold\n",
        "            )\n",
        "\n",
        "            score['scenario_id'] = scenario_id\n",
        "            score['target_agent_id'] = evaluation.get('target_agent_id')\n",
        "\n",
        "            evaluation_scores.append(score)\n",
        "\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Scoring error for scenario {scenario_id}: {str(e)}\")\n",
        "\n",
        "    return {\n",
        "        \"evaluation_scores\": evaluation_scores,\n",
        "        \"errors\": errors\n",
        "    }"
      ],
      "metadata": {
        "id": "rakLRPAuo-6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Scoring Node: Where Evidence Becomes Judgment\n",
        "\n",
        "The `scoring_node` is responsible for converting executed evaluations into **explicit performance assessments**.\n",
        "\n",
        "At this point in the workflow:\n",
        "\n",
        "* execution has already happened\n",
        "* inputs and outputs are known\n",
        "* timing has been measured\n",
        "\n",
        "What remains is to answer a critical question:\n",
        "\n",
        "**Did the agent meet expectations?**\n",
        "\n",
        "---\n",
        "\n",
        "## Scoring Is Context-Aware, Not Isolated\n",
        "\n",
        "Rather than scoring evaluations in isolation, the scoring node reconnects each evaluation to its original scenario.\n",
        "\n",
        "By building a `scenario_lookup`, the system ensures that:\n",
        "\n",
        "* expected outcomes are retrieved reliably\n",
        "* scoring logic is grounded in scenario intent\n",
        "* evaluations are judged against the correct expectations\n",
        "\n",
        "This avoids generic or one-size-fits-all scoring.\n",
        "\n",
        "---\n",
        "\n",
        "## One Evaluation, One Scorecard\n",
        "\n",
        "For each evaluation record, the node:\n",
        "\n",
        "* retrieves the relevant scenario\n",
        "* extracts expected outcomes and resolution paths\n",
        "* applies the scoring function using configured weights and thresholds\n",
        "\n",
        "Each evaluation produces a **complete scorecard** that includes:\n",
        "\n",
        "* correctness\n",
        "* response time\n",
        "* output quality\n",
        "* overall score\n",
        "* pass/fail status\n",
        "* recorded issues\n",
        "\n",
        "This makes every judgment explainable and traceable.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuration Drives Standards\n",
        "\n",
        "Scoring behavior is controlled entirely through configuration:\n",
        "\n",
        "* scoring weights reflect business priorities\n",
        "* pass thresholds define acceptable performance\n",
        "\n",
        "The scoring node does not decide *what good means* — it enforces the standards that were defined earlier.\n",
        "\n",
        "This separation ensures that:\n",
        "\n",
        "* evaluation rules are consistent\n",
        "* changes are intentional\n",
        "* results remain comparable over time\n",
        "\n",
        "---\n",
        "\n",
        "## Deterministic by Design\n",
        "\n",
        "The scoring node applies deterministic logic to deterministic inputs.\n",
        "\n",
        "Given the same:\n",
        "\n",
        "* evaluation record\n",
        "* scenario expectations\n",
        "* scoring configuration\n",
        "\n",
        "the outcome will always be the same.\n",
        "\n",
        "This is what allows scores to be:\n",
        "\n",
        "* trusted\n",
        "* tracked over time\n",
        "* used for trend and drift analysis\n",
        "* relied upon in reports and dashboards\n",
        "\n",
        "---\n",
        "\n",
        "## Failures Are Explicit, Not Silent\n",
        "\n",
        "If a scenario cannot be found or scoring fails:\n",
        "\n",
        "* the issue is recorded\n",
        "* scoring continues for other evaluations\n",
        "* partial results remain usable\n",
        "\n",
        "This ensures robustness without hiding problems.\n",
        "\n",
        "Transparent error handling is essential for governance and operational confidence.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Node Produces\n",
        "\n",
        "The output of this node is a list of **evaluation scores**, each tied to:\n",
        "\n",
        "* a specific scenario\n",
        "* a specific agent\n",
        "* a specific execution\n",
        "\n",
        "These scores become the foundation for:\n",
        "\n",
        "* agent performance summaries\n",
        "* health classifications\n",
        "* system-wide metrics\n",
        "* executive reporting\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Layer Matters So Much\n",
        "\n",
        "This node represents a key architectural boundary:\n",
        "\n",
        "> **Before this point, the system observes behavior.\n",
        "> After this point, the system evaluates it.**\n",
        "\n",
        "By keeping that boundary explicit and deterministic, the orchestrator avoids the ambiguity that undermines trust in many AI systems.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Pattern Continues\n",
        "\n",
        "This node follows the same pattern used throughout the system:\n",
        "\n",
        "* state provides context\n",
        "* utilities implement logic\n",
        "* nodes coordinate workflow\n",
        "* outputs are written back to state\n",
        "\n",
        "That consistency makes the system easier to reason about and easier to extend.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FkxzUM-UzXGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhHgJU0BoqKp"
      },
      "outputs": [],
      "source": [
        "def performance_analysis_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Analyze agent performance and generate summaries.\"\"\"\n",
        "    agents = state.get('specialist_agents', {})\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "    scores = state.get('evaluation_scores', [])\n",
        "\n",
        "    agent_performance_summaries = []\n",
        "\n",
        "    # Calculate performance for each agent\n",
        "    for agent_id in agents.keys():\n",
        "        summary = calculate_agent_performance_summary(\n",
        "            agent_id,\n",
        "            evaluations,\n",
        "            scores,\n",
        "            config.health_thresholds\n",
        "        )\n",
        "        agent_performance_summaries.append(summary)\n",
        "\n",
        "    # Calculate overall evaluation summary\n",
        "    total_scenarios = len(state.get('journey_scenarios', []))\n",
        "    total_evaluations = len(evaluations)\n",
        "    total_passed = sum(1 for s in scores if s.get('passed', False))\n",
        "    total_failed = len(scores) - total_passed\n",
        "    overall_pass_rate = total_passed / len(scores) if scores else 0.0\n",
        "    average_score = sum(s.get('overall_score', 0.0) for s in scores) / len(scores) if scores else 0.0\n",
        "\n",
        "    healthy_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'healthy')\n",
        "    degraded_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'degraded')\n",
        "    critical_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'critical')\n",
        "\n",
        "    evaluation_summary = {\n",
        "        \"total_scenarios\": total_scenarios,\n",
        "        \"total_evaluations\": total_evaluations,\n",
        "        \"total_passed\": total_passed,\n",
        "        \"total_failed\": total_failed,\n",
        "        \"overall_pass_rate\": overall_pass_rate,\n",
        "        \"average_score\": average_score,\n",
        "        \"agents_evaluated\": len(agents),\n",
        "        \"healthy_agents\": healthy_agents,\n",
        "        \"degraded_agents\": degraded_agents,\n",
        "        \"critical_agents\": critical_agents\n",
        "    }\n",
        "\n",
        "    # Workflow analysis (using toolshed)\n",
        "    workflow_analysis = []\n",
        "    if config.enable_workflow_analysis:\n",
        "        for summary in agent_performance_summaries:\n",
        "            # Use failure rate as metric for workflow health\n",
        "            failure_rate = (summary.get('failed_count', 0) / summary.get('total_evaluations', 1)) * 100\n",
        "\n",
        "            workflow = {\n",
        "                \"workflow_id\": f\"eval_{summary.get('agent_id')}\",\n",
        "                \"agent_id\": summary.get('agent_id'),\n",
        "                \"failure_rate_7d\": failure_rate\n",
        "            }\n",
        "\n",
        "            # Use workflow health analysis\n",
        "            thresholds = {\n",
        "                \"healthy\": 10.0,    # <= 10% failure rate\n",
        "                \"degraded\": 30.0,   # 10-30% failure rate\n",
        "                \"critical\": 30.0    # > 30% failure rate\n",
        "            }\n",
        "\n",
        "            analysis = analyze_workflow_health(workflow, thresholds)\n",
        "            workflow_analysis.append(analysis)\n",
        "\n",
        "    # Performance metrics (using toolshed)\n",
        "    performance_metrics = {}\n",
        "    if config.enable_performance_tracking:\n",
        "        metrics_config = create_metrics_config(\n",
        "            metrics={\n",
        "                \"evaluation_time\": {\"threshold\": 2.0, \"unit\": \"seconds\"},\n",
        "                \"pass_rate\": {\"threshold\": 0.80, \"unit\": \"ratio\"}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        avg_eval_time = sum(e.get('execution_time_seconds', 0.0) for e in evaluations) / len(evaluations) if evaluations else 0.0\n",
        "\n",
        "        performance_metrics = {\n",
        "            \"average_evaluation_time\": avg_eval_time,\n",
        "            \"overall_pass_rate\": overall_pass_rate,\n",
        "            \"metrics_config\": metrics_config\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"agent_performance_summary\": agent_performance_summaries,\n",
        "        \"evaluation_summary\": evaluation_summary,\n",
        "        \"workflow_analysis\": workflow_analysis,\n",
        "        \"performance_metrics\": performance_metrics\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Performance Analysis: From Scores to System Insight\n",
        "\n",
        "The `performance_analysis_node` is responsible for turning individual evaluation scores into **meaningful summaries** at both the agent level and the system level.\n",
        "\n",
        "This is the point where raw metrics become insight.\n",
        "\n",
        "---\n",
        "\n",
        "## Agent-Level Performance: Clear Accountability\n",
        "\n",
        "The node begins by calculating a performance summary for each agent using previously scored evaluations.\n",
        "\n",
        "Each summary answers practical questions:\n",
        "\n",
        "* How many times was this agent evaluated?\n",
        "* How often did it pass or fail?\n",
        "* What is its average score?\n",
        "* How fast does it typically respond?\n",
        "* What is its overall health status?\n",
        "\n",
        "These summaries provide a **clean, consistent performance profile** for every agent in the system.\n",
        "\n",
        "Because health classification is driven by explicit thresholds, agent health is:\n",
        "\n",
        "* deterministic\n",
        "* comparable across agents\n",
        "* stable over time\n",
        "\n",
        "This makes it easy to identify which agents are reliable and which require attention.\n",
        "\n",
        "---\n",
        "\n",
        "## System-Level Evaluation Summary\n",
        "\n",
        "Beyond individual agents, the node computes a **system-wide evaluation summary** that captures overall performance at a glance.\n",
        "\n",
        "This includes:\n",
        "\n",
        "* total scenarios evaluated\n",
        "* total evaluation runs\n",
        "* pass and fail counts\n",
        "* overall pass rate\n",
        "* average score across all evaluations\n",
        "* distribution of agent health states\n",
        "\n",
        "This summary is designed to answer executive-level questions quickly, without digging into details.\n",
        "\n",
        "---\n",
        "\n",
        "## Translating Failures into Workflow Health\n",
        "\n",
        "When workflow analysis is enabled, the node goes one step further.\n",
        "\n",
        "Instead of treating failures as isolated events, it:\n",
        "\n",
        "* computes failure rates per agent\n",
        "* interprets those rates using health thresholds\n",
        "* classifies workflows as healthy, degraded, or critical\n",
        "\n",
        "This reframes performance issues as **workflow health signals**, which are easier to reason about and easier to act on.\n",
        "\n",
        "---\n",
        "\n",
        "## Performance Metrics as Operational Signals\n",
        "\n",
        "The node also computes performance metrics that describe how the evaluation system itself is behaving.\n",
        "\n",
        "These include:\n",
        "\n",
        "* average evaluation execution time\n",
        "* overall pass rate\n",
        "* defined performance thresholds\n",
        "\n",
        "These metrics help ensure that the evaluation process remains efficient and predictable, not just accurate.\n",
        "\n",
        "---\n",
        "\n",
        "## Determinism Preserved at Every Level\n",
        "\n",
        "Importantly, this node does not introduce new judgment logic.\n",
        "\n",
        "It:\n",
        "\n",
        "* aggregates deterministic scores\n",
        "* applies deterministic thresholds\n",
        "* produces deterministic summaries\n",
        "\n",
        "As a result, performance summaries remain reproducible and trustworthy. Changes in metrics reflect real changes in behavior, not analytical drift.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Node Matters\n",
        "\n",
        "This node is where the orchestrator becomes truly useful beyond engineering teams.\n",
        "\n",
        "It enables:\n",
        "\n",
        "* rapid assessment of agent reliability\n",
        "* early detection of performance degradation\n",
        "* clear communication of risk\n",
        "* confidence in scaling AI systems\n",
        "\n",
        "It transforms evaluation from a technical exercise into an **operational capability**.\n",
        "\n",
        "---\n",
        "\n",
        "## The Architectural Pattern Holds\n",
        "\n",
        "This node continues the system’s core pattern:\n",
        "\n",
        "* state provides context\n",
        "* utilities perform calculations\n",
        "* configuration defines standards\n",
        "* nodes coordinate flow\n",
        "* outputs are written back to state\n",
        "\n",
        "That consistency is what allows insights to scale without adding complexity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s0tl8nGH0FVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_generation_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
        "    from toolshed.reporting import generate_mission_report, save_report\n",
        "\n",
        "    summary = state.get('evaluation_summary', {})\n",
        "    agent_summaries = state.get('agent_performance_summary', [])\n",
        "    scores = state.get('evaluation_scores', [])\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "\n",
        "    # Build report sections\n",
        "    report_sections = []\n",
        "\n",
        "    # Executive Summary\n",
        "    report_sections.append(\"## Executive Summary\\n\\n\")\n",
        "    report_sections.append(f\"- **Total Scenarios Evaluated:** {summary.get('total_scenarios', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Total Evaluations:** {summary.get('total_evaluations', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Overall Pass Rate:** {summary.get('overall_pass_rate', 0.0):.1%}\\n\")\n",
        "    report_sections.append(f\"- **Average Score:** {summary.get('average_score', 0.0):.2f}\\n\")\n",
        "    report_sections.append(f\"- **Healthy Agents:** {summary.get('healthy_agents', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Degraded Agents:** {summary.get('degraded_agents', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Critical Agents:** {summary.get('critical_agents', 0)}\\n\\n\")\n",
        "\n",
        "    # Agent Performance Details\n",
        "    report_sections.append(\"## Agent Performance Details\\n\\n\")\n",
        "    for agent_summary in agent_summaries:\n",
        "        report_sections.append(f\"### {agent_summary.get('agent_id')}\\n\\n\")\n",
        "        report_sections.append(f\"- **Status:** {agent_summary.get('health_status', 'unknown')}\\n\")\n",
        "        report_sections.append(f\"- **Total Evaluations:** {agent_summary.get('total_evaluations', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Passed:** {agent_summary.get('passed_count', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Failed:** {agent_summary.get('failed_count', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Average Score:** {agent_summary.get('average_score', 0.0):.2f}\\n\")\n",
        "        report_sections.append(f\"- **Average Response Time:** {agent_summary.get('average_response_time', 0.0):.2f}s\\n\\n\")\n",
        "\n",
        "    # Evaluation Results\n",
        "    report_sections.append(\"## Evaluation Results\\n\\n\")\n",
        "    report_sections.append(\"| Scenario | Agent | Score | Passed | Issues |\\n\")\n",
        "    report_sections.append(\"|----------|-------|-------|--------|--------|\\n\")\n",
        "\n",
        "    for score in scores[:20]:  # Limit to first 20 for readability\n",
        "        scenario_id = score.get('scenario_id', 'N/A')\n",
        "        agent_id = score.get('target_agent_id', 'N/A')\n",
        "        overall_score = score.get('overall_score', 0.0)\n",
        "        passed = \"✓\" if score.get('passed', False) else \"✗\"\n",
        "        issues = \", \".join(score.get('issues', []))[:50]  # Truncate long issues\n",
        "        report_sections.append(f\"| {scenario_id} | {agent_id} | {overall_score:.2f} | {passed} | {issues} |\\n\")\n",
        "\n",
        "    if len(scores) > 20:\n",
        "        report_sections.append(f\"\\n*Showing first 20 of {len(scores)} evaluations*\\n\\n\")\n",
        "\n",
        "    # Combine report\n",
        "    report = \"# Evaluation-as-a-Service Report\\n\\n\"\n",
        "    report += f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
        "    report += \"\".join(report_sections)\n",
        "\n",
        "    # Save report if enabled\n",
        "    report_file_path = None\n",
        "    if config.enable_reporting:\n",
        "        try:\n",
        "            report_file_path = save_report(\n",
        "                report_content=report,\n",
        "                reports_dir=config.reports_dir,\n",
        "                report_name=\"evaluation_report\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            state.get('errors', []).append(f\"Report saving error: {str(e)}\")\n",
        "\n",
        "    return {\n",
        "        \"evaluation_report\": report,\n",
        "        \"report_file_path\": report_file_path\n",
        "    }"
      ],
      "metadata": {
        "id": "YWHCYGY1o3XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Report Generation: Turning Metrics into Meaning\n",
        "\n",
        "The `report_generation_node` is responsible for transforming evaluation data into a **clear, readable report** that can be shared with business leaders, managers, or clients.\n",
        "\n",
        "This is the moment where the system stops thinking like a machine and starts **communicating like an analyst**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Reporting Is a Separate Node\n",
        "\n",
        "Report generation is intentionally isolated from:\n",
        "\n",
        "* execution\n",
        "* scoring\n",
        "* analysis\n",
        "\n",
        "By the time this node runs:\n",
        "\n",
        "* all decisions have already been made\n",
        "* all metrics are finalized\n",
        "* all classifications are deterministic\n",
        "\n",
        "The report does not influence outcomes — it **explains them**.\n",
        "\n",
        "This separation is critical for trust.\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary Comes First (On Purpose)\n",
        "\n",
        "The report opens with a concise **Executive Summary** that answers the most important questions immediately:\n",
        "\n",
        "* How much was evaluated?\n",
        "* How well did the system perform?\n",
        "* How many agents are healthy, degraded, or critical?\n",
        "\n",
        "This mirrors how leaders actually consume information:\n",
        "\n",
        "* high-level first\n",
        "* details only if needed\n",
        "\n",
        "Nothing in this section is interpretive — it is a direct reflection of computed metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## Agent-Level Transparency\n",
        "\n",
        "The next section breaks performance down **agent by agent**.\n",
        "\n",
        "For each agent, the report shows:\n",
        "\n",
        "* health status\n",
        "* evaluation volume\n",
        "* pass/fail counts\n",
        "* average score\n",
        "* average response time\n",
        "\n",
        "This creates clear ownership and accountability.\n",
        "Each agent has a visible performance profile, grounded in deterministic evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Evidence Is Still Available\n",
        "\n",
        "For readers who want to go deeper, the report includes a table of individual evaluation results:\n",
        "\n",
        "* scenario\n",
        "* agent\n",
        "* score\n",
        "* pass/fail\n",
        "* issues encountered\n",
        "\n",
        "This preserves traceability without overwhelming the report.\n",
        "\n",
        "The report summarizes — but it never hides the evidence.\n",
        "\n",
        "---\n",
        "\n",
        "## Deterministic Data, Human-Friendly Presentation\n",
        "\n",
        "One of the most important aspects of this node is **what it does not do**.\n",
        "\n",
        "It does not:\n",
        "\n",
        "* reinterpret scores\n",
        "* adjust thresholds\n",
        "* “smooth” results\n",
        "* make new judgments\n",
        "\n",
        "It simply presents what the system already knows in a format humans can understand.\n",
        "\n",
        "This aligns perfectly with your guiding principle:\n",
        "\n",
        "> **Deterministic systems earn trust.\n",
        "> Probabilistic systems add insight.**\n",
        "\n",
        "Here, the report is informational — not authoritative.\n",
        "\n",
        "---\n",
        "\n",
        "## Where an LLM Fits (Optional, Not Required)\n",
        "\n",
        "This node is an ideal place to *optionally* introduce an LLM **without compromising consistency**.\n",
        "\n",
        "For example, an LLM could:\n",
        "\n",
        "* summarize trends across runs\n",
        "* explain why certain agents are degraded\n",
        "* highlight unusual patterns\n",
        "* suggest investigation areas\n",
        "\n",
        "Crucially:\n",
        "\n",
        "* the LLM would not change scores\n",
        "* the LLM would not decide pass/fail\n",
        "* the LLM would not alter health status\n",
        "\n",
        "It would act as a **commentator**, not a judge.\n",
        "\n",
        "---\n",
        "\n",
        "## Reports as Durable Artifacts\n",
        "\n",
        "When enabled, the report is saved as a persistent artifact:\n",
        "\n",
        "* timestamped\n",
        "* reproducible\n",
        "* auditable\n",
        "\n",
        "This allows reports to be:\n",
        "\n",
        "* shared with stakeholders\n",
        "* stored for compliance\n",
        "* compared across time\n",
        "* used as inputs to dashboards\n",
        "\n",
        "Reports become part of the system’s memory, not just transient output.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Node Completes the System\n",
        "\n",
        "This node completes a full, end-to-end loop:\n",
        "\n",
        "1. Data is loaded consistently\n",
        "2. Behavior is executed predictably\n",
        "3. Performance is scored deterministically\n",
        "4. Health is classified explicitly\n",
        "5. Results are communicated clearly\n",
        "\n",
        "Nothing is hidden.\n",
        "Nothing is subjective.\n",
        "Nothing is left unexplained.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Takeaway\n",
        "\n",
        "This report generation step is where your architectural philosophy fully pays off.\n",
        "\n",
        "Because everything upstream is:\n",
        "\n",
        "* structured\n",
        "* deterministic\n",
        "* standardized\n",
        "\n",
        "the reporting layer can stay:\n",
        "\n",
        "* simple\n",
        "* readable\n",
        "* trustworthy\n",
        "\n",
        "That is exactly what business leaders want from AI systems — not magic, but **clarity**.\n",
        "\n",
        "---\n",
        "\n",
        "At this point, your orchestrator is complete:\n",
        "\n",
        "* utilities define rules\n",
        "* nodes orchestrate flow\n",
        "* state carries truth\n",
        "* reports communicate outcomes\n",
        "\n",
        "What you’ve built is not just an agent — it’s a **governance-ready evaluation system**.\n",
        "\n"
      ],
      "metadata": {
        "id": "CPpWwKi14MHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UB7GrEuP4QSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}