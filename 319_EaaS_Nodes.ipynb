{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ/l2jAWGeEgUrz14ow2M/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/319_EaaS_Nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nodes for EaaS Orchestrator Agent"
      ],
      "metadata": {
        "id": "ypDROhtDpSfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Nodes for EaaS Orchestrator Agent\n",
        "\n",
        "Orchestration logic for the evaluation workflow.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any\n",
        "from datetime import datetime\n",
        "from toolshed.progress import calculate_progress, calculate_elapsed_time, estimate_remaining_time\n",
        "from toolshed.performance import create_metrics_config, track_execution_time\n",
        "from toolshed.workflows import analyze_workflow_health\n",
        "from toolshed.validation import validate_data_structure\n",
        "from agents.eval_as_service.utilities import (\n",
        "    load_journey_scenarios,\n",
        "    load_specialist_agents,\n",
        "    load_supporting_data,\n",
        "    load_decision_rules,\n",
        "    build_agent_lookup,\n",
        "    build_scenario_lookup,\n",
        "    simulate_agent_execution,\n",
        "    score_evaluation,\n",
        "    calculate_agent_performance_summary\n",
        ")\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "\n",
        "\n",
        "def goal_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"Define the goal for evaluation.\"\"\"\n",
        "    scenario_id = state.get('scenario_id')\n",
        "    target_agent_id = state.get('target_agent_id')\n",
        "\n",
        "    if scenario_id:\n",
        "        goal_description = f\"Evaluate agent performance for scenario {scenario_id}\"\n",
        "    elif target_agent_id:\n",
        "        goal_description = f\"Evaluate agent {target_agent_id} across all scenarios\"\n",
        "    else:\n",
        "        goal_description = \"Evaluate all agents across all scenarios\"\n",
        "\n",
        "    return {\n",
        "        \"goal\": {\n",
        "            \"description\": goal_description,\n",
        "            \"type\": \"evaluation\",\n",
        "            \"scope\": {\n",
        "                \"scenario_id\": scenario_id,\n",
        "                \"target_agent_id\": target_agent_id\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def planning_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"Create execution plan for evaluation.\"\"\"\n",
        "    plan = [\n",
        "        {\n",
        "            \"step\": 1,\n",
        "            \"task\": \"Load evaluation data\",\n",
        "            \"description\": \"Load scenarios, agents, and supporting data\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 2,\n",
        "            \"task\": \"Execute evaluations\",\n",
        "            \"description\": \"Run scenarios through target agents\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 3,\n",
        "            \"task\": \"Score evaluations\",\n",
        "            \"description\": \"Compare actual outputs to expected outcomes\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 4,\n",
        "            \"task\": \"Analyze performance\",\n",
        "            \"description\": \"Calculate agent performance summaries\"\n",
        "        },\n",
        "        {\n",
        "            \"step\": 5,\n",
        "            \"task\": \"Generate report\",\n",
        "            \"description\": \"Create comprehensive evaluation report\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "\n",
        "def data_loading_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Load all required data for evaluation.\"\"\"\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    try:\n",
        "        # Load scenarios\n",
        "        scenarios = load_journey_scenarios(config.data_dir, config.journey_scenarios_file)\n",
        "\n",
        "        # Filter by scenario_id if specified\n",
        "        scenario_id = state.get('scenario_id')\n",
        "        if scenario_id:\n",
        "            scenarios = [s for s in scenarios if s.get('scenario_id') == scenario_id]\n",
        "\n",
        "        # Load agents\n",
        "        agents_dict = load_specialist_agents(config.data_dir, config.specialist_agents_file)\n",
        "\n",
        "        # Filter by target_agent_id if specified\n",
        "        target_agent_id = state.get('target_agent_id')\n",
        "        if target_agent_id:\n",
        "            agents_dict = {k: v for k, v in agents_dict.items() if k == target_agent_id}\n",
        "\n",
        "        # Load supporting data\n",
        "        supporting_data = load_supporting_data(\n",
        "            config.data_dir,\n",
        "            config.customers_file,\n",
        "            config.orders_file,\n",
        "            config.logistics_file,\n",
        "            config.marketing_signals_file\n",
        "        )\n",
        "\n",
        "        # Load decision rules\n",
        "        decision_rules = load_decision_rules(config.data_dir, config.decision_rules_file)\n",
        "\n",
        "        # Build lookups\n",
        "        agent_lookup = build_agent_lookup(agents_dict)\n",
        "        scenario_lookup = build_scenario_lookup(scenarios)\n",
        "\n",
        "        # Validate data structure if enabled\n",
        "        if config.enable_validation:\n",
        "            try:\n",
        "                validate_data_structure(scenarios, required_fields=['scenario_id', 'customer_id', 'order_id'])\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Validation warning: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            \"journey_scenarios\": scenarios,\n",
        "            \"specialist_agents\": agents_dict,\n",
        "            \"supporting_data\": supporting_data,\n",
        "            \"decision_rules\": decision_rules,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Data loading error: {str(e)}\")\n",
        "        return {\"errors\": errors}\n"
      ],
      "metadata": {
        "id": "GgeNtfQuoybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_execution_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Execute evaluations by running scenarios through agents.\"\"\"\n",
        "    scenarios = state.get('journey_scenarios', [])\n",
        "    agents = state.get('specialist_agents', {})\n",
        "    supporting_data = state.get('supporting_data', {})\n",
        "\n",
        "    executed_evaluations = []\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    # Track start time for progress\n",
        "    start_time = state.get('evaluation_start_time')\n",
        "    if not start_time:\n",
        "        start_time = datetime.now().isoformat()\n",
        "\n",
        "    # Determine which agents to evaluate for each scenario\n",
        "    for scenario in scenarios:\n",
        "        expected_resolution_path = scenario.get('expected_resolution_path', [])\n",
        "\n",
        "        # Evaluate each agent in the expected resolution path\n",
        "        for agent_id in expected_resolution_path:\n",
        "            if agent_id not in agents:\n",
        "                errors.append(f\"Agent {agent_id} not found for scenario {scenario.get('scenario_id')}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Track execution time\n",
        "                execution_start = datetime.now()\n",
        "\n",
        "                # Simulate agent execution\n",
        "                result = simulate_agent_execution(\n",
        "                    agent_id,\n",
        "                    scenario,\n",
        "                    supporting_data,\n",
        "                    agents\n",
        "                )\n",
        "\n",
        "                execution_end = datetime.now()\n",
        "                execution_time = (execution_end - execution_start).total_seconds()\n",
        "\n",
        "                evaluation = {\n",
        "                    \"scenario_id\": scenario.get('scenario_id'),\n",
        "                    \"target_agent_id\": agent_id,\n",
        "                    \"input\": {\n",
        "                        \"customer_message\": scenario.get('customer_message'),\n",
        "                        \"customer_id\": scenario.get('customer_id'),\n",
        "                        \"order_id\": scenario.get('order_id')\n",
        "                    },\n",
        "                    \"actual_output\": result.get('output'),\n",
        "                    \"expected_output\": {\n",
        "                        \"expected_issue_type\": scenario.get('expected_issue_type'),\n",
        "                        \"expected_resolution_path\": expected_resolution_path,\n",
        "                        \"expected_outcome\": scenario.get('expected_outcome')\n",
        "                    },\n",
        "                    \"execution_time_seconds\": execution_time,\n",
        "                    \"status\": result.get('status', 'failed'),\n",
        "                    \"error\": result.get('error')\n",
        "                }\n",
        "\n",
        "                executed_evaluations.append(evaluation)\n",
        "\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Evaluation error for scenario {scenario.get('scenario_id')}, agent {agent_id}: {str(e)}\")\n",
        "                executed_evaluations.append({\n",
        "                    \"scenario_id\": scenario.get('scenario_id'),\n",
        "                    \"target_agent_id\": agent_id,\n",
        "                    \"status\": \"failed\",\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "    # Update progress\n",
        "    total = len(executed_evaluations)\n",
        "    completed = len([e for e in executed_evaluations if e.get('status') == 'completed'])\n",
        "\n",
        "    progress = calculate_progress(completed=completed, total=total) if total > 0 else 0.0\n",
        "\n",
        "    elapsed = calculate_elapsed_time(start_time)\n",
        "    remaining = estimate_remaining_time(\n",
        "        completed=completed,\n",
        "        total=total,\n",
        "        elapsed_minutes=elapsed / 60.0\n",
        "    ) if total > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"executed_evaluations\": executed_evaluations,\n",
        "        \"evaluations_completed\": completed,\n",
        "        \"evaluations_total\": total,\n",
        "        \"progress_percentage\": progress,\n",
        "        \"elapsed_time_seconds\": elapsed,\n",
        "        \"estimated_remaining_seconds\": remaining * 60.0,\n",
        "        \"evaluation_start_time\": start_time,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "mWYuqg3Uo8ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Score evaluations by comparing actual outputs to expected outcomes.\"\"\"\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "    scenarios = state.get('journey_scenarios', [])\n",
        "    scenario_lookup = build_scenario_lookup(scenarios)\n",
        "\n",
        "    evaluation_scores = []\n",
        "    errors = state.get('errors', [])\n",
        "\n",
        "    for evaluation in evaluations:\n",
        "        scenario_id = evaluation.get('scenario_id')\n",
        "        scenario = scenario_lookup.get(scenario_id)\n",
        "\n",
        "        if not scenario:\n",
        "            errors.append(f\"Scenario {scenario_id} not found for scoring\")\n",
        "            continue\n",
        "\n",
        "        expected_outcome = scenario.get('expected_outcome')\n",
        "        expected_resolution_path = scenario.get('expected_resolution_path', [])\n",
        "\n",
        "        try:\n",
        "            score = score_evaluation(\n",
        "                evaluation,\n",
        "                expected_outcome,\n",
        "                expected_resolution_path,\n",
        "                config.scoring_weights,\n",
        "                config.pass_threshold\n",
        "            )\n",
        "\n",
        "            score['scenario_id'] = scenario_id\n",
        "            score['target_agent_id'] = evaluation.get('target_agent_id')\n",
        "\n",
        "            evaluation_scores.append(score)\n",
        "\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Scoring error for scenario {scenario_id}: {str(e)}\")\n",
        "\n",
        "    return {\n",
        "        \"evaluation_scores\": evaluation_scores,\n",
        "        \"errors\": errors\n",
        "    }"
      ],
      "metadata": {
        "id": "rakLRPAuo-6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhHgJU0BoqKp"
      },
      "outputs": [],
      "source": [
        "def performance_analysis_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Analyze agent performance and generate summaries.\"\"\"\n",
        "    agents = state.get('specialist_agents', {})\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "    scores = state.get('evaluation_scores', [])\n",
        "\n",
        "    agent_performance_summaries = []\n",
        "\n",
        "    # Calculate performance for each agent\n",
        "    for agent_id in agents.keys():\n",
        "        summary = calculate_agent_performance_summary(\n",
        "            agent_id,\n",
        "            evaluations,\n",
        "            scores,\n",
        "            config.health_thresholds\n",
        "        )\n",
        "        agent_performance_summaries.append(summary)\n",
        "\n",
        "    # Calculate overall evaluation summary\n",
        "    total_scenarios = len(state.get('journey_scenarios', []))\n",
        "    total_evaluations = len(evaluations)\n",
        "    total_passed = sum(1 for s in scores if s.get('passed', False))\n",
        "    total_failed = len(scores) - total_passed\n",
        "    overall_pass_rate = total_passed / len(scores) if scores else 0.0\n",
        "    average_score = sum(s.get('overall_score', 0.0) for s in scores) / len(scores) if scores else 0.0\n",
        "\n",
        "    healthy_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'healthy')\n",
        "    degraded_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'degraded')\n",
        "    critical_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'critical')\n",
        "\n",
        "    evaluation_summary = {\n",
        "        \"total_scenarios\": total_scenarios,\n",
        "        \"total_evaluations\": total_evaluations,\n",
        "        \"total_passed\": total_passed,\n",
        "        \"total_failed\": total_failed,\n",
        "        \"overall_pass_rate\": overall_pass_rate,\n",
        "        \"average_score\": average_score,\n",
        "        \"agents_evaluated\": len(agents),\n",
        "        \"healthy_agents\": healthy_agents,\n",
        "        \"degraded_agents\": degraded_agents,\n",
        "        \"critical_agents\": critical_agents\n",
        "    }\n",
        "\n",
        "    # Workflow analysis (using toolshed)\n",
        "    workflow_analysis = []\n",
        "    if config.enable_workflow_analysis:\n",
        "        for summary in agent_performance_summaries:\n",
        "            # Use failure rate as metric for workflow health\n",
        "            failure_rate = (summary.get('failed_count', 0) / summary.get('total_evaluations', 1)) * 100\n",
        "\n",
        "            workflow = {\n",
        "                \"workflow_id\": f\"eval_{summary.get('agent_id')}\",\n",
        "                \"agent_id\": summary.get('agent_id'),\n",
        "                \"failure_rate_7d\": failure_rate\n",
        "            }\n",
        "\n",
        "            # Use workflow health analysis\n",
        "            thresholds = {\n",
        "                \"healthy\": 10.0,    # <= 10% failure rate\n",
        "                \"degraded\": 30.0,   # 10-30% failure rate\n",
        "                \"critical\": 30.0    # > 30% failure rate\n",
        "            }\n",
        "\n",
        "            analysis = analyze_workflow_health(workflow, thresholds)\n",
        "            workflow_analysis.append(analysis)\n",
        "\n",
        "    # Performance metrics (using toolshed)\n",
        "    performance_metrics = {}\n",
        "    if config.enable_performance_tracking:\n",
        "        metrics_config = create_metrics_config(\n",
        "            metrics={\n",
        "                \"evaluation_time\": {\"threshold\": 2.0, \"unit\": \"seconds\"},\n",
        "                \"pass_rate\": {\"threshold\": 0.80, \"unit\": \"ratio\"}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        avg_eval_time = sum(e.get('execution_time_seconds', 0.0) for e in evaluations) / len(evaluations) if evaluations else 0.0\n",
        "\n",
        "        performance_metrics = {\n",
        "            \"average_evaluation_time\": avg_eval_time,\n",
        "            \"overall_pass_rate\": overall_pass_rate,\n",
        "            \"metrics_config\": metrics_config\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"agent_performance_summary\": agent_performance_summaries,\n",
        "        \"evaluation_summary\": evaluation_summary,\n",
        "        \"workflow_analysis\": workflow_analysis,\n",
        "        \"performance_metrics\": performance_metrics\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def report_generation_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
        "    from toolshed.reporting import generate_mission_report, save_report\n",
        "\n",
        "    summary = state.get('evaluation_summary', {})\n",
        "    agent_summaries = state.get('agent_performance_summary', [])\n",
        "    scores = state.get('evaluation_scores', [])\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "\n",
        "    # Build report sections\n",
        "    report_sections = []\n",
        "\n",
        "    # Executive Summary\n",
        "    report_sections.append(\"## Executive Summary\\n\\n\")\n",
        "    report_sections.append(f\"- **Total Scenarios Evaluated:** {summary.get('total_scenarios', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Total Evaluations:** {summary.get('total_evaluations', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Overall Pass Rate:** {summary.get('overall_pass_rate', 0.0):.1%}\\n\")\n",
        "    report_sections.append(f\"- **Average Score:** {summary.get('average_score', 0.0):.2f}\\n\")\n",
        "    report_sections.append(f\"- **Healthy Agents:** {summary.get('healthy_agents', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Degraded Agents:** {summary.get('degraded_agents', 0)}\\n\")\n",
        "    report_sections.append(f\"- **Critical Agents:** {summary.get('critical_agents', 0)}\\n\\n\")\n",
        "\n",
        "    # Agent Performance Details\n",
        "    report_sections.append(\"## Agent Performance Details\\n\\n\")\n",
        "    for agent_summary in agent_summaries:\n",
        "        report_sections.append(f\"### {agent_summary.get('agent_id')}\\n\\n\")\n",
        "        report_sections.append(f\"- **Status:** {agent_summary.get('health_status', 'unknown')}\\n\")\n",
        "        report_sections.append(f\"- **Total Evaluations:** {agent_summary.get('total_evaluations', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Passed:** {agent_summary.get('passed_count', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Failed:** {agent_summary.get('failed_count', 0)}\\n\")\n",
        "        report_sections.append(f\"- **Average Score:** {agent_summary.get('average_score', 0.0):.2f}\\n\")\n",
        "        report_sections.append(f\"- **Average Response Time:** {agent_summary.get('average_response_time', 0.0):.2f}s\\n\\n\")\n",
        "\n",
        "    # Evaluation Results\n",
        "    report_sections.append(\"## Evaluation Results\\n\\n\")\n",
        "    report_sections.append(\"| Scenario | Agent | Score | Passed | Issues |\\n\")\n",
        "    report_sections.append(\"|----------|-------|-------|--------|--------|\\n\")\n",
        "\n",
        "    for score in scores[:20]:  # Limit to first 20 for readability\n",
        "        scenario_id = score.get('scenario_id', 'N/A')\n",
        "        agent_id = score.get('target_agent_id', 'N/A')\n",
        "        overall_score = score.get('overall_score', 0.0)\n",
        "        passed = \"✓\" if score.get('passed', False) else \"✗\"\n",
        "        issues = \", \".join(score.get('issues', []))[:50]  # Truncate long issues\n",
        "        report_sections.append(f\"| {scenario_id} | {agent_id} | {overall_score:.2f} | {passed} | {issues} |\\n\")\n",
        "\n",
        "    if len(scores) > 20:\n",
        "        report_sections.append(f\"\\n*Showing first 20 of {len(scores)} evaluations*\\n\\n\")\n",
        "\n",
        "    # Combine report\n",
        "    report = \"# Evaluation-as-a-Service Report\\n\\n\"\n",
        "    report += f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
        "    report += \"\".join(report_sections)\n",
        "\n",
        "    # Save report if enabled\n",
        "    report_file_path = None\n",
        "    if config.enable_reporting:\n",
        "        try:\n",
        "            report_file_path = save_report(\n",
        "                report_content=report,\n",
        "                reports_dir=config.reports_dir,\n",
        "                report_name=\"evaluation_report\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            state.get('errors', []).append(f\"Report saving error: {str(e)}\")\n",
        "\n",
        "    return {\n",
        "        \"evaluation_report\": report,\n",
        "        \"report_file_path\": report_file_path\n",
        "    }"
      ],
      "metadata": {
        "id": "YWHCYGY1o3XS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}