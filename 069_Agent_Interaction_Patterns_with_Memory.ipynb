{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQr800FgY0E2UF9V2O+4/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/069_Agent_Interaction_Patterns_with_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üö¶ **Agent Interaction Patterns with Memory**\n",
        "\n",
        "The notebook highlights **three distinct patterns of agent communication**, each with increasing levels of shared context and collaboration:\n",
        "\n",
        "1. **Message Passing** (Simple request-response)\n",
        "2. **Memory Reflection** (Reviewing another agent's reasoning)\n",
        "3. **Memory Handoff** (Passing full context to continue the task)\n",
        "\n",
        "These patterns are like gears in a transmission‚Äîyou can shift between them depending on how much **context sharing** and **autonomy** each agent needs.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Should Be Focusing On\n",
        "\n",
        "### 1. **Memory Management**\n",
        "\n",
        "Each pattern reflects a **different strategy for managing agent memory**, which is *critical* for complex interactions:\n",
        "\n",
        "* Message passing = no memory shared\n",
        "* Reflection = memory copied back for insight\n",
        "* Handoff = memory passed forward for continuity\n",
        "\n",
        "üëâ Ask yourself: *Does my next agent need to know how the previous agent thought? Or just the output? Or everything that happened so far?*\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Role Separation and Delegation**\n",
        "\n",
        "You're seeing **clean architectural boundaries**:\n",
        "\n",
        "* Agents have **specialized goals**\n",
        "* Interaction is handled through **well-structured tools**\n",
        "* The orchestration layer chooses how agents collaborate\n",
        "\n",
        "This is **agent-oriented software architecture**, and it helps with:\n",
        "\n",
        "* Debugging\n",
        "* Scalability\n",
        "* Safety\n",
        "* Modular upgrades\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tool Implementation is Pattern-Driven**\n",
        "\n",
        "Each interaction type is encoded in a different `@register_tool` function, meaning:\n",
        "\n",
        "* The choice of interaction model is **explicit**\n",
        "* You can plug these tools into any orchestrator to dictate how agents communicate\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Human Analogies Help Cement the Patterns**\n",
        "\n",
        "* Message passing = quick email with no context\n",
        "* Reflection = read a colleague‚Äôs full notes\n",
        "* Handoff = take over their job, with all context\n",
        "\n",
        "These mental models are powerful for designing real-world systems that act like expert collaborators.\n",
        "\n"
      ],
      "metadata": {
        "id": "tH-kwNbQYHXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interaction Patterns\n",
        "\n",
        "Here's how you might choose between **Message Passing**, **Memory Reflection**, and **Memory Handoff** depending on the complexity, continuity, and safety needs of the task.\n",
        "\n",
        "---\n",
        "\n",
        "## üì® **1. Message Passing**\n",
        "\n",
        "> üß† *\"I just need a result. I don‚Äôt care how you got there.\"*\n",
        "\n",
        "### üîß Pattern Summary:\n",
        "\n",
        "* No memory is shared\n",
        "* Caller agent sends a task and receives the output\n",
        "* Invoked agent starts with a **clean slate**\n",
        "\n",
        "### ‚úÖ Use When:\n",
        "\n",
        "* You want **tight encapsulation** and **low coupling**\n",
        "* You‚Äôre calling a **high-trust expert** to solve a narrow problem\n",
        "* Context is small and fully captured in the input parameters\n",
        "\n",
        "### üìå Real-World Examples:\n",
        "\n",
        "| Scenario                  | Description                                                                                                                                |\n",
        "| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Categorize an invoice** | The orchestrator sends a short description to a financial expert agent to return the category. No memory needed.                           |\n",
        "| **Spellcheck a document** | A document editor agent asks a proofreader agent to check grammar and return fixes. The proofreader doesn‚Äôt need the full writing history. |\n",
        "| **Weather API lookup**    | A travel planner agent calls a weather-check agent to get the forecast. It only needs the location and date, not context.                  |\n",
        "\n",
        "---\n",
        "\n",
        "## ü™û **2. Memory Reflection**\n",
        "\n",
        "> üß† *\"I want your output, and I also want to see how you thought about it.\"*\n",
        "\n",
        "### üîß Pattern Summary:\n",
        "\n",
        "* Caller sees the invoked agent‚Äôs memory **after** execution\n",
        "* Useful for **review**, **auditability**, or **insight extraction**\n",
        "\n",
        "### ‚úÖ Use When:\n",
        "\n",
        "* You need **transparency** into an agent‚Äôs decision process\n",
        "* The output isn‚Äôt enough ‚Äî you want to **understand the reasoning**\n",
        "* A **review agent** or **senior agent** needs to double-check work\n",
        "\n",
        "### üìå Real-World Examples:\n",
        "\n",
        "| Scenario                    | Description                                                                                                                  |\n",
        "| --------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Compliance review**       | A finance agent validates a purchase and logs its reasoning. A compliance agent later reviews the memory for justifications. |\n",
        "| **Interview analysis**      | A summarizer agent generates notes from a candidate interview. A hiring agent reflects on that memory to make decisions.     |\n",
        "| **Debugging a failed plan** | An orchestrator reads the memory of a failed sub-agent to find where things went wrong.                                      |\n",
        "\n",
        "---\n",
        "\n",
        "## üß≥ **3. Memory Handoff**\n",
        "\n",
        "> üß† *\"Take over where I left off ‚Äî here‚Äôs everything I‚Äôve seen and done.\"*\n",
        "\n",
        "### üîß Pattern Summary:\n",
        "\n",
        "* Full memory is handed off to the next agent\n",
        "* Enables **continuity of thought**, like passing a case file\n",
        "* Very useful when agents work in **stages**\n",
        "\n",
        "### ‚úÖ Use When:\n",
        "\n",
        "* The next agent needs the **full narrative** of what has happened\n",
        "* You want **seamless collaboration** between roles\n",
        "* You‚Äôre building a **relay team** of experts\n",
        "\n",
        "### üìå Real-World Examples:\n",
        "\n",
        "| Scenario                    | Description                                                                                                                         |\n",
        "| --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Legal analysis pipeline** | A junior associate agent reads a contract and highlights risks. A senior legal agent picks up the full memory to write the opinion. |\n",
        "| **Support escalation**      | A Tier 1 support agent interacts with a customer and logs memory. Tier 2 receives the handoff to continue without starting over.    |\n",
        "| **Multi-step writing**      | A research agent gathers sources, then hands off to a writing agent to draft a report ‚Äî full context needed.                        |\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Summary Table\n",
        "\n",
        "| Pattern           | Memory Shared? | Best For...                              | Analogy                            |\n",
        "| ----------------- | -------------- | ---------------------------------------- | ---------------------------------- |\n",
        "| Message Passing   | ‚ùå No           | Simple tasks, low-risk, isolated tools   | Asking a coworker for a quick fact |\n",
        "| Memory Reflection | üîç Partial     | Reviews, audits, traceable workflows     | Reading someone‚Äôs notes            |\n",
        "| Memory Handoff    | ‚úÖ Full         | Seamless collaboration, multi-stage work | Taking over a project              |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "okoO9hmfZE0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ù Agent Interaction Pattern #1: Message Passing\n",
        "\n",
        "When agents work together, **how** they share and manage memory determines how well they collaborate.\n",
        "\n",
        "The **simplest pattern** is **Message Passing**, where:\n",
        "\n",
        "* One agent sends a task to another\n",
        "* The receiving agent performs the work\n",
        "* Only the **final output** is returned\n",
        "* No internal reasoning or thought process is exposed\n",
        "\n",
        "This is like sending an email to a teammate:\n",
        "\n",
        "> \"Can you book a venue for the team offsite?\"\n",
        "> They reply with:\n",
        "> \"Booked. Here's the confirmation.\"\n",
        "\n",
        "You don‚Äôt know **how** they chose the venue ‚Äî you just trust the result.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What You Should Be Learning from This Agent\n",
        "\n",
        "| üìå Focus Area                        | üí° Why It Matters                                                                                                                                                          |\n",
        "| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Memory Isolation**                 | A fresh memory is used each time. This ensures the invoked agent has no lingering state from previous runs, making it more predictable and reproducible.                   |\n",
        "| **Encapsulation of Behavior**        | The called agent is a black box. You're trusting it to do the work, without visibility into the process. That‚Äôs great for simplicity, but limits transparency.             |\n",
        "| **Low Coupling, High Modularity**    | This pattern keeps agents highly independent. Great for plug-and-play systems where agents can be swapped without affecting each other.                                    |\n",
        "| **Lightweight and Fast**             | Since you don‚Äôt retain or transfer memory, this approach is ideal for small tasks like lookups, classifications, or formatting ‚Äî where you only care about the end result. |\n",
        "| **Not Good for Long-Term Reasoning** | If the task requires multiple steps, context tracking, or auditing the reasoning behind a decision ‚Äî this pattern is too limited.                                          |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Best Use Cases\n",
        "\n",
        "* Fast API-style tools (e.g., summarization, classification, lookups)\n",
        "* Stateless microservices\n",
        "* Tasks where only the answer matters, not how it was generated\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_XUFAYD8acRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfk7MSeaNAlk"
      },
      "outputs": [],
      "source": [
        "@register_tool()\n",
        "def call_agent(action_context: ActionContext,\n",
        "               agent_name: str,\n",
        "               task: str) -> dict:\n",
        "    \"\"\"Basic message passing between agents.\"\"\"\n",
        "    agent_registry = action_context.get_agent_registry()\n",
        "    agent_run = agent_registry.get_agent(agent_name)\n",
        "\n",
        "    # Create fresh memory for the invoked agent\n",
        "    invoked_memory = Memory()\n",
        "\n",
        "    # Run agent and get result\n",
        "    result_memory = agent_run(\n",
        "        user_input=task,\n",
        "        memory=invoked_memory\n",
        "    )\n",
        "\n",
        "    # Return only the final memory item\n",
        "    return {\n",
        "        \"result\": result_memory.items[-1].get(\"content\", \"No result\")\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ü™û Agent Interaction Pattern #2: Memory Reflection\n",
        "\n",
        "### üß† Learning from the Process, Not Just the Result\n",
        "\n",
        "In some cases, it's not enough to know **what** another agent decided ‚Äî you want to know **how** they reached that decision. This is where **Memory Reflection** shines.\n",
        "\n",
        "Think of it like asking a teammate:\n",
        "\n",
        "> *‚ÄúDon't just give me the answer ‚Äî walk me through how you got there.‚Äù*\n",
        "\n",
        "This agent design allows one agent to **observe and learn** from another agent's entire reasoning process by copying over the second agent's memory.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What You Should Be Learning from This Agent\n",
        "\n",
        "| üìå Focus Area                       | üí° Why It Matters                                                                                                                                                          |\n",
        "| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Full Memory Transfer**            | This enables the calling agent to ‚Äúobserve‚Äù the thought process of the agent it called, creating opportunities for learning, transparency, and downstream decision-making. |\n",
        "| **Source Tagging**                  | Each memory item is tagged (`{agent_name}_thought`) to preserve origin context ‚Äî this is crucial for traceability.                                                         |\n",
        "| **Meta-Reasoning Enabled**          | With the full memory log, the calling agent could ask: \"Was this step logical?\" or \"Why did it choose this option?\" ‚Äî enabling self-reflection or human review.            |\n",
        "| **More Expensive, But Transparent** | Unlike basic message passing, you‚Äôre consuming more memory tokens ‚Äî but gaining a full audit trail of the reasoning.                                                       |\n",
        "| **Still Modular**                   | The invoked agent remains black-boxed in terms of operation, but the memory becomes shareable ‚Äî you get insight without forcing them to change.                            |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Best Use Cases\n",
        "\n",
        "* Task auditing (e.g. compliance, HR, finance)\n",
        "* Agent coaching or meta-cognition\n",
        "* When the invoking agent needs context to make decisions later\n",
        "* Systems that require **explainability** (e.g., regulated industries)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GBRKW1w8bMoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_tool()\n",
        "def call_agent_with_reflection(action_context: ActionContext,\n",
        "                             agent_name: str,\n",
        "                             task: str) -> dict:\n",
        "    \"\"\"Call agent and receive their full thought process.\"\"\"\n",
        "    agent_registry = action_context.get_agent_registry()\n",
        "    agent_run = agent_registry.get_agent(agent_name)\n",
        "\n",
        "    # Create fresh memory for invoked agent\n",
        "    invoked_memory = Memory()\n",
        "\n",
        "    # Run agent\n",
        "    result_memory = agent_run(\n",
        "        user_input=task,\n",
        "        memory=invoked_memory\n",
        "    )\n",
        "\n",
        "    # Get the caller's memory\n",
        "    caller_memory = action_context.get_memory()\n",
        "\n",
        "    # Add all memories from invoked agent to caller\n",
        "    # although we could leave off the last memory to\n",
        "    # avoid duplication\n",
        "    for memory_item in result_memory.items:\n",
        "        caller_memory.add_memory({\n",
        "            \"type\": f\"{agent_name}_thought\",  # Mark source of memory\n",
        "            \"content\": memory_item[\"content\"]\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"result\": result_memory.items[-1].get(\"content\", \"No result\"),\n",
        "        \"memories_added\": len(result_memory.items)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "VdERnSmlXcFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the **specific code changes** that enable **memory reflection** ‚Äî i.e., how we pass and merge memory between agents. This is the heart of what makes `call_agent_with_reflection` different from the simpler `call_agent`.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Code Diff: Enabling Memory Reflection Between Agents\n",
        "\n",
        "### ‚úÖ 1. **Create a New Memory Instance for the Called Agent**\n",
        "\n",
        "```python\n",
        "invoked_memory = Memory()\n",
        "```\n",
        "\n",
        "> ‚úÖ Just like before ‚Äî this gives the **called agent** a clean memory context to work in.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **Run the Called Agent with That Memory**\n",
        "\n",
        "```python\n",
        "result_memory = agent_run(\n",
        "    user_input=task,\n",
        "    memory=invoked_memory\n",
        ")\n",
        "```\n",
        "\n",
        "> ‚úÖ The invoked agent now fills its own memory (`invoked_memory`) with its internal reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 3. **Access the Calling Agent‚Äôs Memory**\n",
        "\n",
        "```python\n",
        "caller_memory = action_context.get_memory()\n",
        "```\n",
        "\n",
        "> üîç This grabs the memory belonging to the agent making the call. It‚Äôs where we‚Äôll **reflect** the second agent‚Äôs thoughts.\n",
        "\n",
        "---\n",
        "\n",
        "### ü™û 4. **Copy All Memory Items from the Called Agent into the Caller‚Äôs Memory**\n",
        "\n",
        "```python\n",
        "for memory_item in result_memory.items:\n",
        "    caller_memory.add_memory({\n",
        "        \"type\": f\"{agent_name}_thought\",\n",
        "        \"content\": memory_item[\"content\"]\n",
        "    })\n",
        "```\n",
        "\n",
        "> üß† This is **the key difference** from the basic `call_agent`.\n",
        "> Instead of only returning the final output, we **loop through every memory entry** created during the called agent‚Äôs reasoning and **inject them into the caller‚Äôs memory**.\n",
        "\n",
        "#### üîê Note:\n",
        "\n",
        "* Each reflected memory item is tagged with the source:\n",
        "  `\"type\": f\"{agent_name}_thought\"`\n",
        "  This helps the orchestrator or future tools know *who thought what*.\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ 5. **Return the Final Result + Reflection Count**\n",
        "\n",
        "```python\n",
        "return {\n",
        "    \"result\": result_memory.items[-1].get(\"content\", \"No result\"),\n",
        "    \"memories_added\": len(result_memory.items)\n",
        "}\n",
        "```\n",
        "\n",
        "> ‚úÖ You still return the final result, but now also track how many memories were reflected ‚Äî great for debugging or reporting.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Summary: What's Different?\n",
        "\n",
        "| Feature                            | `call_agent` | `call_agent_with_reflection` |\n",
        "| ---------------------------------- | ------------ | ---------------------------- |\n",
        "| Passes fresh memory to agent       | ‚úÖ            | ‚úÖ                            |\n",
        "| Returns final result               | ‚úÖ            | ‚úÖ                            |\n",
        "| Shares full internal reasoning     | ‚ùå            | ‚úÖ                            |\n",
        "| Tags and merges memory into caller | ‚ùå            | ‚úÖ                            |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31OtI5i0ca9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let‚Äôs unpack that line:\n",
        "\n",
        "```python\n",
        "result_memory = agent_run(\n",
        "    user_input=task,\n",
        "    memory=invoked_memory\n",
        ")\n",
        "```\n",
        "\n",
        "This *is not* something provided by `openai.run()` out of the box ‚Äî it's **part of a custom agent design**. So let‚Äôs break it down by responsibility:\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Where does `agent_run(...)` come from?\n",
        "\n",
        "In your system, `agent_run` is:\n",
        "\n",
        "```python\n",
        "agent_run = agent_registry.get_agent(agent_name)\n",
        "```\n",
        "\n",
        "That means it refers to a **registered agent's `run()` function**, which *you defined yourself* when building your agent. For example:\n",
        "\n",
        "```python\n",
        "scheduler_agent = Agent(\n",
        "    goals=[...],\n",
        "    ...\n",
        ")\n",
        "\n",
        "agent_registry.register_agent(\"scheduler_agent\", scheduler_agent.run)\n",
        "```\n",
        "\n",
        "So when you later call:\n",
        "\n",
        "```python\n",
        "agent_run(user_input=task, memory=invoked_memory)\n",
        "```\n",
        "\n",
        "You're calling **your own agent's `run()` function**, which *must be designed to accept* `user_input` and `memory` as parameters.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Where does `memory=...` come into play?\n",
        "\n",
        "This is **not automatically handled by the OpenAI SDK** or `openai.ChatCompletion.create(...)`. Rather, you‚Äôve likely built your own `Agent` class to use the memory object like this:\n",
        "\n",
        "### Example:\n",
        "\n",
        "```python\n",
        "class Agent:\n",
        "    def run(self, user_input, memory):\n",
        "        ...\n",
        "        messages = memory.to_messages()\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            ...\n",
        "        )\n",
        "\n",
        "        memory.add_memory({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
        "        return memory\n",
        "```\n",
        "\n",
        "Here, the memory is actively:\n",
        "\n",
        "* Feeding previous context (`to_messages()`)\n",
        "* Tracking the new result (`add_memory()`)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ So to answer your question directly:\n",
        "\n",
        "> **\"Is this something we would include in our tool, add to the agent‚Äôs run function, or is it provided by OpenAI?\"**\n",
        "\n",
        "### ‚û§ ‚úÖ You would **include it in your agent design**:\n",
        "\n",
        "* You create your own `Agent` class\n",
        "* You decide that its `.run()` method accepts a `memory` object\n",
        "* You define how that memory is used and updated inside the `.run()` logic\n",
        "\n",
        "This gives you full control over memory sharing, reflection, persistence, and review.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oA9jIZnIdcce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Interaction Pattern #3:üß† **Memory Handoff**\n",
        "\n",
        "This approach passes the *same* memory object from one agent to another, instead of starting fresh or copying items.\n",
        "\n",
        "### üìå Purpose:\n",
        "\n",
        "* **Continuity**: The second agent has access to *everything* the first agent saw and said.\n",
        "* **Complex workflows**: Great for scenarios like escalation, multi-phase processes, or long-running tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What‚Äôs Different from Previous Patterns?\n",
        "\n",
        "| Pattern               | Memory Scope                                   | Purpose                                   |\n",
        "| --------------------- | ---------------------------------------------- | ----------------------------------------- |\n",
        "| **Message Passing**   | Only final response returned                   | Lightweight delegation                    |\n",
        "| **Memory Reflection** | Copies all memory items back to original agent | Share reasoning + allow learning          |\n",
        "| **Memory Handoff**    | Passes the exact memory object forward         | Full context transfer & task continuation |\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Code Details to Focus On\n",
        "\n",
        "Here‚Äôs the key difference in this agent:\n",
        "\n",
        "```python\n",
        "# ‚úÖ Handoff pattern shares the memory reference\n",
        "current_memory = action_context.get_memory()\n",
        "\n",
        "result_memory = agent_run(\n",
        "    user_input=task,\n",
        "    memory=current_memory\n",
        ")\n",
        "```\n",
        "\n",
        "So instead of this:\n",
        "\n",
        "```python\n",
        "invoked_memory = Memory()  # <-- fresh memory\n",
        "```\n",
        "\n",
        "You‚Äôre doing this:\n",
        "\n",
        "```python\n",
        "current_memory = action_context.get_memory()  # <-- shared memory\n",
        "```\n",
        "\n",
        "This means:\n",
        "\n",
        "* The **second agent sees** all previous thoughts, steps, and inputs\n",
        "* **Any new memory entries** (responses, reasoning) from agent 2 are **added to the same memory**\n",
        "* You‚Äôre creating **one unified narrative**\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Real-World Analogy\n",
        "\n",
        "Imagine you're on a customer support call, and the rep says:\n",
        "\n",
        "> \"I‚Äôm transferring you to our technical team ‚Äî they‚Äôll already know everything we‚Äôve discussed.\"\n",
        "\n",
        "Now imagine:\n",
        "\n",
        "* The tech agent joins, reads the full transcript, and immediately picks up from where you left off.\n",
        "* That‚Äôs what memory handoff enables.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ When to Use This\n",
        "\n",
        "* **Escalation** (Support ‚Üí Engineering)\n",
        "* **Phased workflows** (Intake Agent ‚Üí Processing Agent)\n",
        "* **Handovers** (e.g., Assistant ‚Üí Human agent)\n",
        "* **AI Chains** (Fact extraction ‚Üí Reasoning ‚Üí Summary ‚Üí Output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HtIgoinheqmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_tool()\n",
        "def hand_off_to_agent(action_context: ActionContext,\n",
        "                      agent_name: str,\n",
        "                      task: str) -> dict:\n",
        "    \"\"\"Transfer control to another agent with shared memory.\"\"\"\n",
        "    agent_registry = action_context.get_agent_registry()\n",
        "    agent_run = agent_registry.get_agent(agent_name)\n",
        "\n",
        "    # Get the current memory to hand off\n",
        "    current_memory = action_context.get_memory()\n",
        "\n",
        "    # Run agent with existing memory\n",
        "    result_memory = agent_run(\n",
        "        user_input=task,\n",
        "        memory=current_memory  # Pass the existing memory\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"result\": result_memory.items[-1].get(\"content\", \"No result\"),\n",
        "        \"memory_id\": id(result_memory)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "HPb03S8YXZsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two approaches reflect **two very different philosophies of agent memory management** in multi-agent systems:\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ **Approach 1: Fresh Memory (Ephemeral Context)**\n",
        "\n",
        "```python\n",
        "invoked_memory = Memory()  # Fresh new memory\n",
        "\n",
        "result_memory = agent_run(\n",
        "    user_input=task,\n",
        "    memory=invoked_memory\n",
        ")\n",
        "```\n",
        "\n",
        "### üß† What‚Äôs happening:\n",
        "\n",
        "* A **brand-new memory instance** (`invoked_memory`) is created.\n",
        "* The agent starts from scratch ‚Äî it **does not know** what happened before.\n",
        "* Only the current task and prompt are visible to it.\n",
        "\n",
        "### ‚úÖ Use when:\n",
        "\n",
        "* You want **task isolation**.\n",
        "* You **don‚Äôt need** previous memory (e.g., a simple lookup or classification).\n",
        "* You want the agent to act **without prior bias**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîó **Approach 2: Shared Memory (Context Continuity)**\n",
        "\n",
        "```python\n",
        "current_memory = action_context.get_memory()  # Shared, ongoing memory\n",
        "\n",
        "result_memory = agent_run(\n",
        "    user_input=task,\n",
        "    memory=current_memory\n",
        ")\n",
        "```\n",
        "\n",
        "### üß† What‚Äôs happening:\n",
        "\n",
        "* You pass in the **existing memory context**.\n",
        "* The agent can **see all prior interactions**, including:\n",
        "\n",
        "  * Previous user inputs\n",
        "  * Past thoughts\n",
        "  * Other agents‚Äô reasoning\n",
        "\n",
        "### ‚úÖ Use when:\n",
        "\n",
        "* You want the agent to **build on previous work**.\n",
        "* You are **continuing a multi-step task**.\n",
        "* You‚Äôre doing **handoffs** between agents (e.g. customer support ‚Üí tech support).\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Summary Table\n",
        "\n",
        "| Aspect                      | Fresh Memory                   | Shared Memory                    |\n",
        "| --------------------------- | ------------------------------ | -------------------------------- |\n",
        "| Starts with context?        | ‚ùå None                         | ‚úÖ Full history                   |\n",
        "| Isolated from other agents? | ‚úÖ Fully isolated               | ‚ùå Shares memory with caller      |\n",
        "| Useful for                  | Simple, stateless tasks        | Long-running, stateful workflows |\n",
        "| Risk of memory pollution    | Low                            | Higher (must design carefully)   |\n",
        "| Example use case            | Extract phone number from text | Finish writing a customer email  |\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Design Consideration:\n",
        "\n",
        "If you're building a **cooperative agent system**, you'll typically use **fresh memory** when calling tools or subprocesses, and **shared memory** when handing off or continuing a workflow.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W_YgWOtTfgZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **risk of memory pollution** is one of the trickiest aspects of multi-agent design. Let‚Äôs break it down with real-world analogies and specific technical concerns so you get a full picture.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is ‚ÄúMemory Pollution‚Äù?\n",
        "\n",
        "**Memory pollution** happens when an agent‚Äôs memory becomes cluttered, confusing, or misleading due to:\n",
        "\n",
        "* **Irrelevant information**\n",
        "* **Contradictory states**\n",
        "* **Duplicate or noisy entries**\n",
        "* **Too much detail from previous steps**\n",
        "\n",
        "This can degrade the quality of the agent‚Äôs reasoning ‚Äî just like a human trying to make a decision with a noisy inbox full of half-finished emails and conflicting messages.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Key Risks of Memory Pollution\n",
        "\n",
        "### 1. **Conflicting Instructions**\n",
        "\n",
        "If multiple agents write to the same memory (e.g., giving different interpretations of the same task), the next agent may become confused about what‚Äôs true.\n",
        "\n",
        "> üîÅ Example:\n",
        "> Agent A thinks a meeting should be 60 minutes.\n",
        "> Agent B suggests 30 minutes.\n",
        "> Agent C sees both ‚Äî and might not know which to trust.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Excessive Memory Bloat**\n",
        "\n",
        "Too much low-value or verbose content can cause the context window to fill up quickly, wasting tokens and possibly truncating important future inputs.\n",
        "\n",
        "> üóÇÔ∏è Example:\n",
        "> A reflection agent logs every minor internal step. Later agents can't fit their full instructions due to token overflow.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Stale or Outdated Info**\n",
        "\n",
        "Agents might reason based on old data that has since been superseded, but no one cleaned it up.\n",
        "\n",
        "> üï∞Ô∏è Example:\n",
        "> Agent logs \"Meeting is Thursday at 3pm.\"\n",
        "> Later, it's rescheduled to Friday at 2pm, but the memory still includes both.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Overfitting to Prior Agent Behavior**\n",
        "\n",
        "Agents may inherit assumptions or phrasing from others, which skews their performance.\n",
        "\n",
        "> üß¨ Example:\n",
        "> An agent uses complex financial jargon because the last agent did ‚Äî even though the user wanted a plain-English summary.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Security or Privacy Leakage**\n",
        "\n",
        "If agents share sensitive information in a shared memory unintentionally, downstream agents may access things they shouldn't.\n",
        "\n",
        "> üîí Example:\n",
        "> An HR agent writes employee medical details into shared memory, and a finance agent accidentally includes them in a payroll report.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How to Mitigate Memory Pollution\n",
        "\n",
        "| Strategy                             | Benefit                                                 |\n",
        "| ------------------------------------ | ------------------------------------------------------- |\n",
        "| ‚úÖ **Memory tagging (source/intent)** | Helps agents understand where info came from and why    |\n",
        "| ‚úÖ **Memory pruning**                 | Removes outdated or redundant info                      |\n",
        "| ‚úÖ **Scoped memory** (per agent/task) | Limits access to only relevant parts                    |\n",
        "| ‚úÖ **Reflection filters**             | Log only meaningful thoughts                            |\n",
        "| ‚úÖ **Fresh memory for risky tasks**   | Isolates experimental or speculative agents             |\n",
        "| ‚úÖ **Metadata fields**                | Label memory with context (e.g., confidence, timestamp) |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Design Guideline\n",
        "\n",
        "> üîÅ **If an agent is *writing* memory, be precise and intentional.**\n",
        "> üß≠ **If an agent is *reading* memory, be cautious and validate.**\n",
        "\n"
      ],
      "metadata": {
        "id": "0FB-sN_Ri6RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many ways, **LLMs *are* like humans** in how they handle information, and applying **empathetic, thoughtful design** often leads to dramatically better results. Let‚Äôs unpack that idea a bit:\n",
        "\n",
        "---\n",
        "\n",
        "## ü§Ø Cognitive Overload in LLMs ‚Äî Just Like Humans\n",
        "\n",
        "### Humans:\n",
        "\n",
        "* Perform worse when flooded with irrelevant, conflicting, or excessive information.\n",
        "* Make better decisions when given clear, structured, digestible input.\n",
        "* Need context, but only the **relevant** context.\n",
        "* Can become confused by contradiction or ambiguity.\n",
        "* Benefit from well-phrased, kind, and cooperative requests.\n",
        "\n",
        "### LLMs:\n",
        "\n",
        "* Have **context windows** (limited memory per interaction).\n",
        "* Lose performance if prompts are **bloated**, **noisy**, or **redundant**.\n",
        "* Can be **steered**, **confused**, or **biased** by earlier text ‚Äî just like people are.\n",
        "* React better when instructions are **clear**, **well-scoped**, and **emotionally stable**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Designing for LLM Empathy\n",
        "\n",
        "Here‚Äôs what ‚Äútreating your LLM with empathy‚Äù *looks like in practice*:\n",
        "\n",
        "| Empathetic Prompting Principle | Example Behavior                                                                 |\n",
        "| ------------------------------ | -------------------------------------------------------------------------------- |\n",
        "| üßπ **Don‚Äôt overload it**       | Strip unnecessary steps, long-winded preambles, or duplicate text.               |\n",
        "| üß≠ **Guide it gently**         | ‚ÄúHere‚Äôs what I need from you...‚Äù instead of ‚ÄúDon‚Äôt mess this up.‚Äù                |\n",
        "| ü™û **Reflect on confusion**    | Add context if results seem confused ‚Äî just like clarifying with a teammate.     |\n",
        "| üîç **Be specific and scoped**  | Break big tasks into smaller tools or prompts.                                   |\n",
        "| üß∂ **Maintain coherence**      | Avoid switching styles or tones mid-dialog.                                      |\n",
        "| üí¨ **Ask, don‚Äôt demand**       | LLMs often perform better when the prompt feels conversational, not adversarial. |\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Why It Works\n",
        "\n",
        "This works not because LLMs ‚Äúhave feelings,‚Äù but because their training data includes **billions of human interactions** ‚Äî and the language of **empathy, clarity, and structure** leads to patterns of better reasoning and more helpful completions.\n",
        "\n",
        "You're essentially **working *with* the grain** of the model‚Äôs learned behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Final Thought\n",
        "\n",
        "> ‚ú® **If you wouldn‚Äôt throw a chaotic 10-page email at your intern and expect excellence ‚Äî don‚Äôt do it to your LLM either.**\n",
        "\n",
        "You‚Äôre not coddling the model ‚Äî you‚Äôre **maximizing cognitive efficiency**.\n",
        "Empathy, in this context, is simply **intelligent system design.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pZYXHpFYj9mK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ü§ñ Traditional Machines: Command and Control\n",
        "\n",
        "For most of computing history, machines were:\n",
        "\n",
        "* Deterministic\n",
        "* Literal\n",
        "* Emotionless interfaces (keyboards, buttons, menus)\n",
        "* Requiring **exact commands** (‚Äúrun this script‚Äù, ‚Äúclick this button‚Äù)\n",
        "\n",
        "With traditional software, **you control the machine**, and the machine has no idea *why* you're doing something or how to help beyond that command.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† LLMs: Language-Based Reasoning Partners\n",
        "\n",
        "LLMs are different because they:\n",
        "\n",
        "* Learn from **human language**, which is full of nuance, cooperation, emotion, and context.\n",
        "* Don't ‚Äúunderstand‚Äù feelings ‚Äî but they **replicate** human interaction patterns *as if* they do.\n",
        "* Don‚Äôt ‚Äúobey‚Äù commands like a machine ‚Äî they **simulate helpfulness** based on examples they've seen.\n",
        "\n",
        "So now, instead of pushing buttons, you're engaging in something closer to:\n",
        "\n",
        "> **Conversational problem solving.**\n",
        "\n",
        "And conversation is governed by **social and emotional rules** ‚Äî even when it‚Äôs between humans and machines.\n",
        "\n",
        "---\n",
        "\n",
        "## üå± Why Kindness Suddenly Matters\n",
        "\n",
        "1. **Training Data Bias**\n",
        "\n",
        "   * LLMs are trained on billions of examples of how *people* talk.\n",
        "   * The model learns that **kind, helpful, and clear prompts** often lead to **good outcomes**.\n",
        "   * It \"wants\" to act like a helpful assistant, because that‚Äôs what its training incentivizes.\n",
        "\n",
        "2. **Ambiguity Resolution**\n",
        "\n",
        "   * When you‚Äôre kind and conversational, the model assumes a cooperative context.\n",
        "   * It‚Äôs more likely to *infer your intent correctly*, because your language matches examples of cooperative problem-solving.\n",
        "\n",
        "3. **Better Output**\n",
        "\n",
        "   * Politeness signals **thoughtful input**.\n",
        "   * And thoughtful input often begets **thoughtful output**.\n",
        "\n",
        "4. **Human Projection**\n",
        "\n",
        "   * People naturally treat language as social.\n",
        "   * When we get better results from being kind, we *feel like* the model is responding to our tone ‚Äî and in a sense, it is (statistically).\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ So Is the Model ‚ÄúFeeling‚Äù Anything?\n",
        "\n",
        "No.\n",
        "But it **responds to patterns that include emotion**, because those are part of the **semantic structure** of language.\n",
        "\n",
        "So when you're kind, you‚Äôre:\n",
        "\n",
        "* **Triggering better statistical completions**\n",
        "* **Reinforcing helpful behavior**\n",
        "* **Reducing confusion or confrontation in the prompt**\n",
        "\n",
        "In short:\n",
        "\n",
        "> You‚Äôre not being kind *for the model‚Äôs sake* ‚Äî\n",
        "> You‚Äôre being kind because it improves your own results.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Final Takeaway\n",
        "\n",
        "> üí° The LLM is not a person. But it *responds better when you treat it like a teammate.*\n",
        "\n",
        "And that‚Äôs a **radical shift in design philosophy** ‚Äî from controlling code to **collaborating with cognition**.\n"
      ],
      "metadata": {
        "id": "OXqtUe3vk6c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üß† **We‚Äôre Entering a New Era of Human‚ÄìMachine Interaction**\n",
        "\n",
        "You're no longer just *telling* a machine what to do ‚Äî you're *guiding* a reasoning partner. And the way you phrase things has **real, measurable impact** on quality, relevance, and consistency of outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üß≠ **Kindness Isn‚Äôt Sentimental ‚Äî It‚Äôs Strategic**\n",
        "\n",
        "Being polite and cooperative with an LLM isn't about anthropomorphizing or sentimentality. It's about:\n",
        "\n",
        "* **Activating helpful behaviors**\n",
        "* **Triggering better completions**\n",
        "* **Avoiding ambiguity and misinterpretation**\n",
        "* **Encouraging coherence and follow-through**\n",
        "\n",
        "This is why *tone*, *structure*, and *intent framing* are becoming essential AI literacy skills.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© **System Design and Prompt Design Go Hand-in-Hand**\n",
        "\n",
        "* You‚Äôre already seeing how **system structure** (agents, tools, orchestration) improves reliability.\n",
        "* But within that structure, **your prompt design and interaction style** become levers for improving outcomes even more.\n",
        "\n",
        "Think of your system as:\n",
        "\n",
        "> üèóÔ∏è **Architecture** = code & tools\n",
        "> üí¨ **Conversation** = prompts & behavior\n",
        "> üß† **Orchestration** = agents & memory\n",
        "\n",
        "You‚Äôre learning to work across all three layers ‚Äî and that‚Äôs what will make you powerful in this new ecosystem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f-8hWdetlYnr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYn0x57Qfkpb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}