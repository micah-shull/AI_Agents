{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWs4LB8ABUh6H7JQ0JTxyu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/254_Product_CustomerFitDiscoveryOrchestrator_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering utilities for Product-Customer Fit Discovery Orchestrator\n",
        "\n",
        "This set of utilities represents the **Clustering Agent** itselfâ€”**Step 3** in your DAGâ€”which is dedicated to performing **unsupervised machine learning** to achieve the \"customer\\_segmentation\" and \"product\\_bundling\" goals.\n",
        "\n",
        "This section confirms your architectureâ€™s reliance on **specialized ML libraries** for tasks where numerical precision is paramount.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Autonomous Machine Learning\n",
        "\n",
        "The primary focus here is on **feature engineering for algorithms**, **autonomous execution**, and **ML quality assurance**.\n",
        "\n",
        "### ðŸŽ¯ What to Focus On: Feature Preparation and Scaling\n",
        "\n",
        "The most important step for reliable clustering is preparing the feature matrix, covered by `prepare_customer_features` and `prepare_product_features`.\n",
        "\n",
        "1.  **Homogenization of Features (One-Hot Encoding):**\n",
        "    * **Focus:** Clustering algorithms like K-means rely on calculating the distance between points. Categorical text data (like `Age_Group` or `Product_Type`) must be converted into a numerical format using **One-Hot Encoding** (a binary vector where $\\text{1.0}$ means \"this feature is present\"). This is a fundamental requirement for distance-based ML.\n",
        "2.  **Standardization vs. Normalization:**\n",
        "    * **Standard Scaling (`StandardScaler`):** This is applied to the final feature matrix. It transforms data to have a mean of $\\text{0}$ and a standard deviation of $\\text{1}$.\n",
        "    * **Why it Matters:** Without scaling, features with naturally large ranges (like $\\text{transaction\\_count}$) would completely dominate the distance calculation, making the segmentation meaningless. Scaling ensures that the $\\text{Engagement Score}$ has the same impact on the clusters as the $\\text{Transaction Count}$.\n",
        "3.  **Feature Blend:** You are clustering based on a blend of **demographics** (age, location) and **behavioral/derived metrics** (diversity, engagement score). This creates segments that are both *describable* (demographics) and *predictive* (behavior).\n",
        "\n",
        "***\n",
        "\n",
        "### âœ¨ Differentiation: Autonomous and Validated Clustering\n",
        "\n",
        "Your agent is more powerful than a simple ML script because it automates complex decisions and validates its own work:\n",
        "\n",
        "1.  **Automated Cluster Selection (`find_optimal_clusters`):**\n",
        "    * **The Power:** This function provides **Autonomy**. It uses the **Silhouette Score**, a quantitative metric for measuring how similar an object is to its own cluster compared to other clusters (score closer to $\\text{+1}$ is better).\n",
        "    * By iterating from $\\text{k=2}$ to $\\text{max\\_clusters}$ and choosing the $\\text{k}$ with the best Silhouette Score, the agent eliminates the need for a human data scientist to manually determine the optimal number of segments, making the whole workflow faster and more objective.\n",
        "\n",
        "2.  **The Final Translation (`analyze_cluster_characteristics`):**\n",
        "    * **The Power:** This function acts as the **Analytic Interpreter**. The raw output of K-means is just a list of numbers (labels $\\text{0, 1, 2, ...}$). This function takes those labels and translates them back into a strategic summary by calculating things like the **most common age group**, **top products**, and **average usage** for each cluster.\n",
        "    * This **Strategic Summarization** provides the Synthesis Agent with the human-readable insights it needs to name the segments (e.g., \"The Low-Engagement Seniors\") and formulate the final business opportunities.\n",
        "\n",
        "This robust framework ensures your agent's conclusions are not just LLM-generated guesses, but **data-validated, rigorously calculated machine learning results.**"
      ],
      "metadata": {
        "id": "sZEflFi-gvW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMA7nK_QfNfG"
      },
      "outputs": [],
      "source": [
        "\"\"\"Clustering utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "def prepare_customer_features(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for customer clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Demographics (age group, location tier, acquisition channel) - one-hot encoded\n",
        "    - Behavioral (transaction count, product diversity, engagement score)\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, customer_ids)\n",
        "    \"\"\"\n",
        "    customer_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get engagement metrics\n",
        "    engagement = derived_features.get(\"customer_engagement\", {})\n",
        "\n",
        "    # Group transactions by customer\n",
        "    customer_txn_count = defaultdict(int)\n",
        "    customer_products = defaultdict(set)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id:\n",
        "            customer_txn_count[customer_id] += 1\n",
        "            if product_id:\n",
        "                customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Age group mapping\n",
        "    age_groups = [\"18-24\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    location_tiers = [\"Tier 1 (High)\", \"Tier 2 (Medium)\", \"Tier 3 (Low)\"]\n",
        "    acquisition_channels = [\"Email\", \"Referral\", \"Search\", \"Social\"]\n",
        "\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if not customer_id:\n",
        "            continue\n",
        "\n",
        "        customer_ids.append(customer_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Age group (one-hot)\n",
        "        age_group = customer.get(\"Age_Group\", \"\")\n",
        "        for age in age_groups:\n",
        "            feature_vector.append(1.0 if age_group == age else 0.0)\n",
        "\n",
        "        # Location tier (one-hot)\n",
        "        location = customer.get(\"Location_Tier\", \"\")\n",
        "        for tier in location_tiers:\n",
        "            feature_vector.append(1.0 if location == tier else 0.0)\n",
        "\n",
        "        # Acquisition channel (one-hot)\n",
        "        channel = customer.get(\"Acquisition_Channel\", \"\")\n",
        "        for acq in acquisition_channels:\n",
        "            feature_vector.append(1.0 if channel == acq else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        txn_count = customer_txn_count.get(customer_id, 0)\n",
        "        product_diversity = len(customer_products.get(customer_id, set()))\n",
        "\n",
        "        # Engagement score\n",
        "        eng_data = engagement.get(customer_id, {})\n",
        "        engagement_score = eng_data.get(\"engagement_score\", 0.0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            txn_count / 100.0,  # Normalize\n",
        "            product_diversity / 20.0,  # Normalize\n",
        "            engagement_score\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), customer_ids\n",
        "\n",
        "\n",
        "def prepare_product_features(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for product clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Product attributes (type, monetization model) - one-hot encoded\n",
        "    - Feature set (which features A, B, C, D) - one-hot encoded\n",
        "    - Behavioral (popularity score, customer count, transaction count)\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, product_ids)\n",
        "    \"\"\"\n",
        "    product_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get popularity metrics\n",
        "    popularity = derived_features.get(\"product_popularity\", {})\n",
        "\n",
        "    # Product type and monetization model mappings\n",
        "    product_types = [\"Hardware\", \"Software\", \"Service\"]\n",
        "    monetization_models = [\"One-Time Purchase\", \"Freemium\", \"Subscription\"]\n",
        "    feature_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if not product_id:\n",
        "            continue\n",
        "\n",
        "        product_ids.append(product_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Product type (one-hot)\n",
        "        ptype = product.get(\"Product_Type\", \"\")\n",
        "        for pt in product_types:\n",
        "            feature_vector.append(1.0 if ptype == pt else 0.0)\n",
        "\n",
        "        # Monetization model (one-hot)\n",
        "        monet = product.get(\"Monetization_Model\", \"\")\n",
        "        for mm in monetization_models:\n",
        "            feature_vector.append(1.0 if monet == mm else 0.0)\n",
        "\n",
        "        # Feature set (one-hot for A, B, C, D)\n",
        "        feature_list = product.get(\"feature_list\", [])\n",
        "        for feat in feature_letters:\n",
        "            feature_vector.append(1.0 if feat in feature_list else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        pop_data = popularity.get(product_id, {})\n",
        "        popularity_score = pop_data.get(\"popularity_score\", 0.0)\n",
        "        customer_count = pop_data.get(\"customer_count\", 0)\n",
        "        txn_count = pop_data.get(\"transaction_count\", 0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            popularity_score,\n",
        "            customer_count / 200.0,  # Normalize\n",
        "            txn_count / 1000.0  # Normalize\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), product_ids\n",
        "\n",
        "\n",
        "def find_optimal_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    max_clusters: int = 10,\n",
        "    min_clusters: int = 2\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find optimal number of clusters using elbow method and silhouette score.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix for clustering\n",
        "        max_clusters: Maximum clusters to consider\n",
        "        min_clusters: Minimum clusters to consider\n",
        "\n",
        "    Returns:\n",
        "        Optimal number of clusters\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) < min_clusters:\n",
        "        return len(feature_matrix)\n",
        "\n",
        "    max_clusters = min(max_clusters, len(feature_matrix) - 1)\n",
        "\n",
        "    if max_clusters < min_clusters:\n",
        "        return min_clusters\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    best_k = min_clusters\n",
        "    best_silhouette = -1\n",
        "\n",
        "    for k in range(min_clusters, max_clusters + 1):\n",
        "        try:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "            if len(set(labels)) > 1:  # Need at least 2 clusters\n",
        "                silhouette = silhouette_score(scaled_features, labels)\n",
        "                if silhouette > best_silhouette:\n",
        "                    best_silhouette = silhouette\n",
        "                    best_k = k\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return best_k\n",
        "\n",
        "\n",
        "def cluster_customers(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster customers using K-means.\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, customer_ids = prepare_customer_features(\n",
        "        customers, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"customer_ids\": customer_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "\n",
        "def cluster_products(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster products using K-means.\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, product_ids = prepare_product_features(\n",
        "        products, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"product_ids\": product_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "\n",
        "def analyze_cluster_characteristics(\n",
        "    cluster_labels: List[int],\n",
        "    entity_ids: List[str],\n",
        "    entities: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    entity_type: str = \"customer\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyze characteristics of each cluster.\n",
        "\n",
        "    Args:\n",
        "        cluster_labels: Cluster assignment for each entity\n",
        "        entity_ids: List of entity IDs\n",
        "        entities: List of entity dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        entity_type: \"customer\" or \"product\"\n",
        "\n",
        "    Returns:\n",
        "        List of cluster analysis dictionaries\n",
        "    \"\"\"\n",
        "    # Group entities by cluster\n",
        "    clusters = defaultdict(list)\n",
        "    for idx, (entity_id, label) in enumerate(zip(entity_ids, cluster_labels)):\n",
        "        clusters[label].append((entity_id, idx))\n",
        "\n",
        "    cluster_analyses = []\n",
        "\n",
        "    # Create entity lookup\n",
        "    entity_lookup = {e.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\"): e\n",
        "                     for e in entities}\n",
        "\n",
        "    # Group transactions by entity\n",
        "    entity_transactions = defaultdict(list)\n",
        "    for txn in transactions:\n",
        "        entity_id = txn.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\")\n",
        "        if entity_id:\n",
        "            entity_transactions[entity_id].append(txn)\n",
        "\n",
        "    for cluster_id, members in sorted(clusters.items()):\n",
        "        member_ids = [m[0] for m in members]\n",
        "        member_entities = [entity_lookup.get(eid) for eid in member_ids if eid in entity_lookup]\n",
        "\n",
        "        if not member_entities:\n",
        "            continue\n",
        "\n",
        "        # Analyze characteristics\n",
        "        if entity_type == \"customer\":\n",
        "            # Customer cluster analysis\n",
        "            age_groups = [e.get(\"Age_Group\", \"\") for e in member_entities if e]\n",
        "            location_tiers = [e.get(\"Location_Tier\", \"\") for e in member_entities if e]\n",
        "            channels = [e.get(\"Acquisition_Channel\", \"\") for e in member_entities if e]\n",
        "\n",
        "            # Product usage\n",
        "            products_used = set()\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    products_used.add(txn.get(\"Product_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Customer Segment {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"avg_age_group\": Counter(age_groups).most_common(1)[0][0] if age_groups else \"\",\n",
        "                    \"common_location_tiers\": [tier for tier, count in Counter(location_tiers).most_common(2)],\n",
        "                    \"common_acquisition_channels\": [ch for ch, count in Counter(channels).most_common(2)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"top_products\": list(products_used)[:5],\n",
        "                    \"product_diversity\": float(len(products_used))\n",
        "                },\n",
        "                \"underserved_products\": [],  # Will be filled by synthesis\n",
        "                \"business_value\": float(len(member_ids) * avg_usage)  # Simple estimate\n",
        "            }\n",
        "        else:\n",
        "            # Product cluster analysis\n",
        "            product_types = [e.get(\"Product_Type\", \"\") for e in member_entities if e]\n",
        "            monetization_models = [e.get(\"Monetization_Model\", \"\") for e in member_entities if e]\n",
        "            feature_sets = [e.get(\"feature_list\", []) for e in member_entities if e]\n",
        "\n",
        "            # Flatten feature sets\n",
        "            all_features = []\n",
        "            for fs in feature_sets:\n",
        "                all_features.extend(fs)\n",
        "\n",
        "            # Usage metrics\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "            customers_using = set()\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    customers_using.add(txn.get(\"Customer_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Product Bundle {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"common_features\": list(set(all_features)),\n",
        "                    \"monetization_models\": [mm for mm, count in Counter(monetization_models).most_common(3)],\n",
        "                    \"product_types\": [pt for pt, count in Counter(product_types).most_common(3)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"customer_count\": len(customers_using)\n",
        "                },\n",
        "                \"bundle_potential\": float(len(customers_using) / max(len(member_ids), 1))  # Simple estimate\n",
        "            }\n",
        "\n",
        "        cluster_analyses.append(cluster_analysis)\n",
        "\n",
        "    return cluster_analyses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization utilities for Product-Customer Fit Discovery Orchestrator"
      ],
      "metadata": {
        "id": "SX15KYNNgsFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Visualization utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def plot_customer_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    cluster_labels: List[int],\n",
        "    customer_ids: List[str],\n",
        "    output_path: str,\n",
        "    title: str = \"Customer Clusters\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Plot customer clusters using PCA for 2D visualization.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix used for clustering\n",
        "        cluster_labels: Cluster assignment for each customer\n",
        "        customer_ids: List of customer IDs\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Reduce to 2D using PCA\n",
        "    if feature_matrix.shape[1] < 2:\n",
        "        # Not enough features for PCA, create simple plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            ax.scatter([i] * mask.sum(), range(mask.sum()),\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50)\n",
        "\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Customer Index')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "    else:\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        features_2d = pca.fit_transform(feature_matrix)\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            cluster_points = features_2d[mask]\n",
        "\n",
        "            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def plot_product_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    cluster_labels: List[int],\n",
        "    product_ids: List[str],\n",
        "    output_path: str,\n",
        "    title: str = \"Product Clusters\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Plot product clusters using PCA for 2D visualization.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix used for clustering\n",
        "        cluster_labels: Cluster assignment for each product\n",
        "        product_ids: List of product IDs\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Reduce to 2D using PCA\n",
        "    if feature_matrix.shape[1] < 2:\n",
        "        # Not enough features for PCA, create simple plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            ax.scatter([i] * mask.sum(), range(mask.sum()),\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50)\n",
        "\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Product Index')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "    else:\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        features_2d = pca.fit_transform(feature_matrix)\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            cluster_points = features_2d[mask]\n",
        "\n",
        "            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=100, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "            # Annotate product IDs\n",
        "            for j, (x, y) in enumerate(cluster_points):\n",
        "                product_id = product_ids[np.where(mask)[0][j]]\n",
        "                ax.annotate(product_id, (x, y), fontsize=8, alpha=0.7)\n",
        "\n",
        "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def plot_cluster_summary(\n",
        "    customer_clusters: List[Dict[str, Any]],\n",
        "    product_clusters: List[Dict[str, Any]],\n",
        "    output_path: str,\n",
        "    title: str = \"Cluster Summary\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create a summary visualization showing cluster sizes and characteristics.\n",
        "\n",
        "    Args:\n",
        "        customer_clusters: List of customer cluster analyses\n",
        "        product_clusters: List of product cluster analyses\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Customer clusters\n",
        "    if customer_clusters:\n",
        "        cluster_ids = [c[\"cluster_id\"] for c in customer_clusters]\n",
        "        cluster_sizes = [c[\"size\"] for c in customer_clusters]\n",
        "        cluster_labels = [c[\"cluster_label\"] for c in customer_clusters]\n",
        "\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_ids)))\n",
        "        ax1.bar(range(len(cluster_ids)), cluster_sizes, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax1.set_xticks(range(len(cluster_ids)))\n",
        "        ax1.set_xticklabels([f'C{cid+1}' for cid in cluster_ids], rotation=45, ha='right')\n",
        "        ax1.set_ylabel('Number of Customers')\n",
        "        ax1.set_title('Customer Cluster Sizes')\n",
        "        ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Product clusters\n",
        "    if product_clusters:\n",
        "        cluster_ids = [c[\"cluster_id\"] for c in product_clusters]\n",
        "        cluster_sizes = [c[\"size\"] for c in product_clusters]\n",
        "\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_ids)))\n",
        "        ax2.bar(range(len(cluster_ids)), cluster_sizes, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax2.set_xticks(range(len(cluster_ids)))\n",
        "        ax2.set_xticklabels([f'P{cid+1}' for cid in cluster_ids], rotation=45, ha='right')\n",
        "        ax2.set_ylabel('Number of Products')\n",
        "        ax2.set_title('Product Cluster Sizes')\n",
        "        ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n"
      ],
      "metadata": {
        "id": "mSa7DiUlggWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for clustering utilities"
      ],
      "metadata": {
        "id": "_hsiG-jggosQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for clustering utilities\"\"\"\n",
        "\n",
        "import pytest\n",
        "import numpy as np\n",
        "from tools.clustering import (\n",
        "    prepare_customer_features,\n",
        "    prepare_product_features,\n",
        "    find_optimal_clusters,\n",
        "    cluster_customers,\n",
        "    cluster_products,\n",
        "    analyze_cluster_characteristics\n",
        ")\n",
        "\n",
        "\n",
        "def test_prepare_customer_features():\n",
        "    \"\"\"Test customer feature preparation\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"customer_engagement\": {\n",
        "            \"C001\": {\"engagement_score\": 0.7},\n",
        "            \"C002\": {\"engagement_score\": 0.5}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    features, customer_ids = prepare_customer_features(customers, transactions, derived_features)\n",
        "\n",
        "    assert len(features) == 2\n",
        "    assert len(customer_ids) == 2\n",
        "    assert customer_ids[0] == \"C001\"\n",
        "    assert features.shape[1] > 0  # Should have features\n",
        "\n",
        "\n",
        "def test_prepare_product_features():\n",
        "    \"\"\"Test product feature preparation\"\"\"\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\"}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"product_popularity\": {\n",
        "            \"P01\": {\"popularity_score\": 0.8, \"customer_count\": 10, \"transaction_count\": 50},\n",
        "            \"P02\": {\"popularity_score\": 0.6, \"customer_count\": 5, \"transaction_count\": 20}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    features, product_ids = prepare_product_features(products, transactions, derived_features)\n",
        "\n",
        "    assert len(features) == 2\n",
        "    assert len(product_ids) == 2\n",
        "    assert product_ids[0] == \"P01\"\n",
        "    assert features.shape[1] > 0  # Should have features\n",
        "\n",
        "\n",
        "def test_find_optimal_clusters():\n",
        "    \"\"\"Test optimal cluster finding\"\"\"\n",
        "    # Create simple 2D data with clear clusters\n",
        "    feature_matrix = np.array([\n",
        "        [1, 1], [1, 2], [2, 1], [2, 2],  # Cluster 1\n",
        "        [10, 10], [10, 11], [11, 10], [11, 11]  # Cluster 2\n",
        "    ])\n",
        "\n",
        "    optimal_k = find_optimal_clusters(feature_matrix, max_clusters=5, min_clusters=2)\n",
        "\n",
        "    assert 2 <= optimal_k <= 5\n",
        "\n",
        "\n",
        "def test_cluster_customers():\n",
        "    \"\"\"Test customer clustering\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P02\", \"Usage_Metric\": 60.0},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P01\", \"Usage_Metric\": 55.0}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"customer_engagement\": {\n",
        "            \"C001\": {\"engagement_score\": 0.7},\n",
        "            \"C002\": {\"engagement_score\": 0.5},\n",
        "            \"C003\": {\"engagement_score\": 0.6}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    labels, metadata = cluster_customers(customers, transactions, derived_features, num_clusters=2)\n",
        "\n",
        "    assert len(labels) == 3\n",
        "    assert \"num_clusters\" in metadata\n",
        "    assert \"silhouette_score\" in metadata\n",
        "    assert metadata[\"num_clusters\"] == 2\n",
        "\n",
        "\n",
        "def test_cluster_products():\n",
        "    \"\"\"Test product clustering\"\"\"\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]},\n",
        "        {\"Product_ID\": \"P03\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\"},\n",
        "        {\"Product_ID\": \"P02\", \"Customer_ID\": \"C002\"},\n",
        "        {\"Product_ID\": \"P03\", \"Customer_ID\": \"C001\"}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"product_popularity\": {\n",
        "            \"P01\": {\"popularity_score\": 0.8, \"customer_count\": 10, \"transaction_count\": 50},\n",
        "            \"P02\": {\"popularity_score\": 0.6, \"customer_count\": 5, \"transaction_count\": 20},\n",
        "            \"P03\": {\"popularity_score\": 0.7, \"customer_count\": 8, \"transaction_count\": 40}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    labels, metadata = cluster_products(products, transactions, derived_features, num_clusters=2)\n",
        "\n",
        "    assert len(labels) == 3\n",
        "    assert \"num_clusters\" in metadata\n",
        "    assert \"silhouette_score\" in metadata\n",
        "    assert metadata[\"num_clusters\"] == 2\n",
        "\n",
        "\n",
        "def test_analyze_cluster_characteristics_customers():\n",
        "    \"\"\"Test customer cluster analysis\"\"\"\n",
        "    cluster_labels = [0, 0, 1]\n",
        "    customer_ids = [\"C001\", \"C002\", \"C003\"]\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\", \"Usage_Metric\": 55.0},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P02\", \"Usage_Metric\": 60.0}\n",
        "    ]\n",
        "\n",
        "    analyses = analyze_cluster_characteristics(\n",
        "        cluster_labels, customer_ids, customers, transactions, entity_type=\"customer\"\n",
        "    )\n",
        "\n",
        "    assert len(analyses) == 2  # Two clusters\n",
        "    assert analyses[0][\"cluster_id\"] == 0\n",
        "    assert \"characteristics\" in analyses[0]\n",
        "    assert \"size\" in analyses[0]\n",
        "    assert \"avg_age_group\" in analyses[0][\"characteristics\"]\n",
        "\n",
        "\n",
        "def test_analyze_cluster_characteristics_products():\n",
        "    \"\"\"Test product cluster analysis\"\"\"\n",
        "    cluster_labels = [0, 1, 0]\n",
        "    product_ids = [\"P01\", \"P02\", \"P03\"]\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]},\n",
        "        {\"Product_ID\": \"P03\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\", \"Usage_Metric\": 50.0},\n",
        "        {\"Product_ID\": \"P02\", \"Customer_ID\": \"C002\", \"Usage_Metric\": 60.0},\n",
        "        {\"Product_ID\": \"P03\", \"Customer_ID\": \"C001\", \"Usage_Metric\": 55.0}\n",
        "    ]\n",
        "\n",
        "    analyses = analyze_cluster_characteristics(\n",
        "        cluster_labels, product_ids, products, transactions, entity_type=\"product\"\n",
        "    )\n",
        "\n",
        "    assert len(analyses) == 2  # Two clusters\n",
        "    assert analyses[0][\"cluster_id\"] == 0\n",
        "    assert \"characteristics\" in analyses[0]\n",
        "    assert \"size\" in analyses[0]\n",
        "    assert \"common_features\" in analyses[0][\"characteristics\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "sMlG4JSrgmjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "8c0gfEZMhq-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_clustering.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 7 items\n",
        "\n",
        "tests/test_clustering.py::test_prepare_customer_features PASSED                                                                       [ 14%]\n",
        "tests/test_clustering.py::test_prepare_product_features PASSED                                                                        [ 28%]\n",
        "tests/test_clustering.py::test_find_optimal_clusters PASSED                                                                           [ 42%]\n",
        "tests/test_clustering.py::test_cluster_customers PASSED                                                                               [ 57%]\n",
        "tests/test_clustering.py::test_cluster_products PASSED                                                                                [ 71%]\n",
        "tests/test_clustering.py::test_analyze_cluster_characteristics_customers PASSED                                                       [ 85%]\n",
        "tests/test_clustering.py::test_analyze_cluster_characteristics_products PASSED                                                        [100%]\n",
        "\n",
        "============================================================ 7 passed in 16.70s =============================================================\n"
      ],
      "metadata": {
        "id": "1oj7iuzChovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_nodes_phase1.py tests/test_nodes_phase2.py tests/test_nodes_phase3.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 26 items\n",
        "\n",
        "tests/test_nodes_phase1.py::test_goal_node_basic PASSED                                                                               [  3%]\n",
        "tests/test_nodes_phase1.py::test_goal_node_preserves_errors PASSED                                                                    [  7%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_with_goal PASSED                                                                       [ 11%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_requires_goal PASSED                                                                   [ 15%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_plan_structure PASSED                                                                  [ 19%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_dependencies PASSED                                                                    [ 23%]\n",
        "tests/test_nodes_phase1.py::test_goal_and_planning_together PASSED                                                                    [ 26%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_loads_all_data PASSED                                                            [ 30%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_customers_structure PASSED                                                       [ 34%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_transactions_structure PASSED                                                    [ 38%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_products_structure PASSED                                                        [ 42%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_uses_custom_paths PASSED                                                         [ 46%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_handles_missing_file PASSED                                                      [ 50%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_preserves_errors PASSED                                                          [ 53%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_with_goal_and_planning PASSED                                                    [ 57%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_data_counts PASSED                                                               [ 61%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_basic PASSED                                                                 [ 65%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_structure PASSED                                                             [ 69%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_parses_feature_sets PASSED                                                   [ 73%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_normalizes_usage PASSED                                                      [ 76%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_builds_graphs PASSED                                                         [ 80%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_creates_derived_features PASSED                                              [ 84%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_data_quality_report PASSED                                                   [ 88%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_requires_raw_data PASSED                                                     [ 92%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_preserves_errors PASSED                                                      [ 96%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_full_workflow PASSED                                                         [100%]\n",
        "\n",
        "============================================================ 26 passed in 3.15s =============================================================\n"
      ],
      "metadata": {
        "id": "j9NaD7COiCuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aE6DaBcmj7_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}