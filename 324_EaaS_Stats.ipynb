{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvNCttAm7EWT3BX3NnDkk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/324_EaaS_Stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_historical_evaluations(\n",
        "    reports_dir: str,\n",
        "    max_history: int = 10\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load historical evaluation summaries from previous reports.\n",
        "\n",
        "    Returns list of historical summaries, most recent first.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    from datetime import datetime\n",
        "\n",
        "    history_dir = Path(reports_dir) / \"history\"\n",
        "    if not history_dir.exists():\n",
        "        return []\n",
        "\n",
        "    historical_summaries = []\n",
        "\n",
        "    # Look for summary JSON files\n",
        "    summary_files = sorted(history_dir.glob(\"summary_*.json\"), reverse=True)\n",
        "\n",
        "    for summary_file in summary_files[:max_history]:\n",
        "        try:\n",
        "            with open(summary_file, 'r') as f:\n",
        "                summary = json.load(f)\n",
        "                historical_summaries.append(summary)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return historical_summaries\n",
        "\n",
        "\n",
        "def save_evaluation_summary(\n",
        "    summary: Dict[str, Any],\n",
        "    reports_dir: str\n",
        ") -> str:\n",
        "    \"\"\"Save current evaluation summary to history for future comparisons.\"\"\"\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    from datetime import datetime\n",
        "\n",
        "    history_dir = Path(reports_dir) / \"history\"\n",
        "    history_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    summary_file = history_dir / f\"summary_{timestamp}.json\"\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    return str(summary_file)\n"
      ],
      "metadata": {
        "id": "YJEYFukg4adS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR7Lodc64Gls"
      },
      "outputs": [],
      "source": [
        "def performance_analysis_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Analyze agent performance and generate summaries with statistical significance.\"\"\"\n",
        "    print(\"  [Performance Analysis Node] Starting...\")\n",
        "    agents = state.get('specialist_agents', {})\n",
        "    evaluations = state.get('executed_evaluations', [])\n",
        "    scores = state.get('evaluation_scores', [])\n",
        "\n",
        "    # Load historical data for statistical significance testing\n",
        "    print(\"    Loading historical data for statistical analysis...\")\n",
        "    historical_data = load_historical_evaluations(config.reports_dir)\n",
        "    if historical_data:\n",
        "        print(f\"    Loaded {len(historical_data)} historical evaluation(s)\")\n",
        "    else:\n",
        "        print(\"    No historical data available (this is normal for first run)\")\n",
        "\n",
        "    agent_performance_summaries = []\n",
        "    statistical_assessments = {}\n",
        "\n",
        "    # Calculate performance for each agent (with ROI and statistical significance)\n",
        "    for agent_id in agents.keys():\n",
        "        summary = calculate_agent_performance_summary(\n",
        "            agent_id,\n",
        "            evaluations,\n",
        "            scores,\n",
        "            config.health_thresholds,\n",
        "            include_roi=True,\n",
        "            historical_data=historical_data\n",
        "        )\n",
        "\n",
        "        # Add statistical significance testing if historical data exists\n",
        "        if historical_data and summary.get('statistical_assessment', {}).get('has_historical_data'):\n",
        "            try:\n",
        "                # Extract historical scores for this agent\n",
        "                historical_scores = summary['statistical_assessment']['historical_scores']\n",
        "                current_score = summary.get('average_score', 0.0)\n",
        "\n",
        "                # KPI significance test\n",
        "                kpi_assessment = assess_kpi_with_significance(\n",
        "                    current_value=current_score,\n",
        "                    historical_values=historical_scores,\n",
        "                    target_value=config.health_thresholds.get('healthy', 0.85),\n",
        "                    confidence_level=0.95\n",
        "                )\n",
        "\n",
        "                # ROI significance test (if we have historical ROI data)\n",
        "                if 'revenue_impact' in summary and 'total_cost' in summary:\n",
        "                    historical_roi = []\n",
        "                    for hist_summary in historical_data:\n",
        "                        agent_summaries = hist_summary.get('agent_performance_summary', [])\n",
        "                        for hist_agent in agent_summaries:\n",
        "                            if hist_agent.get('agent_id') == agent_id:\n",
        "                                hist_roi = hist_agent.get('net_roi', 0.0)\n",
        "                                if hist_roi is not None:\n",
        "                                    historical_roi.append(hist_roi)\n",
        "                                break\n",
        "\n",
        "                    if historical_roi:\n",
        "                        roi_assessment = assess_roi_with_significance(\n",
        "                            roi_estimate=summary.get('net_roi', 0.0),\n",
        "                            cost=summary.get('total_cost', 0.0),\n",
        "                            historical_roi=historical_roi,\n",
        "                            confidence_level=0.95,\n",
        "                            positive_threshold=0.0\n",
        "                        )\n",
        "                        summary['roi_statistical_assessment'] = roi_assessment\n",
        "\n",
        "                summary['kpi_statistical_assessment'] = kpi_assessment\n",
        "                statistical_assessments[agent_id] = {\n",
        "                    'kpi': kpi_assessment,\n",
        "                    'has_roi': 'roi_statistical_assessment' in summary\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Warning: Statistical assessment failed for {agent_id}: {str(e)}\")\n",
        "                summary['statistical_assessment']['error'] = str(e)\n",
        "\n",
        "        agent_performance_summaries.append(summary)\n",
        "\n",
        "    # Calculate overall evaluation summary\n",
        "    total_scenarios = len(state.get('journey_scenarios', []))\n",
        "    total_evaluations = len(evaluations)\n",
        "    total_passed = sum(1 for s in scores if s.get('passed', False))\n",
        "    total_failed = len(scores) - total_passed\n",
        "    overall_pass_rate = total_passed / len(scores) if scores else 0.0\n",
        "    average_score = sum(s.get('overall_score', 0.0) for s in scores) / len(scores) if scores else 0.0\n",
        "\n",
        "    healthy_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'healthy')\n",
        "    degraded_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'degraded')\n",
        "    critical_agents = sum(1 for s in agent_performance_summaries if s.get('health_status') == 'critical')\n",
        "\n",
        "    # Calculate total costs and ROI\n",
        "    total_cost = sum(s.get('total_cost', 0.0) for s in agent_performance_summaries)\n",
        "    total_revenue_impact = sum(s.get('revenue_impact', 0.0) for s in agent_performance_summaries)\n",
        "    total_net_roi = sum(s.get('net_roi', 0.0) for s in agent_performance_summaries)\n",
        "    overall_roi_percent = ((total_revenue_impact - total_cost) / total_cost * 100) if total_cost > 0 else 0.0\n",
        "    agents_with_positive_roi = sum(1 for s in agent_performance_summaries if s.get('roi_percent', 0) > 0)\n",
        "    agents_needing_optimization = sum(1 for s in agent_performance_summaries if s.get('roi_ratio', 0) < 2.0 and s.get('roi_ratio', 0) != float('inf'))\n",
        "\n",
        "    evaluation_summary = {\n",
        "        \"total_scenarios\": total_scenarios,\n",
        "        \"total_evaluations\": total_evaluations,\n",
        "        \"total_passed\": total_passed,\n",
        "        \"total_failed\": total_failed,\n",
        "        \"overall_pass_rate\": overall_pass_rate,\n",
        "        \"average_score\": average_score,\n",
        "        \"agents_evaluated\": len(agents),\n",
        "        \"healthy_agents\": healthy_agents,\n",
        "        \"degraded_agents\": degraded_agents,\n",
        "        \"critical_agents\": critical_agents,\n",
        "        \"total_cost\": round(total_cost, 2),\n",
        "        \"total_revenue_impact\": round(total_revenue_impact, 2),\n",
        "        \"total_net_roi\": round(total_net_roi, 2),\n",
        "        \"overall_roi_percent\": round(overall_roi_percent, 2),\n",
        "        \"agents_with_positive_roi\": agents_with_positive_roi,\n",
        "        \"agents_needing_optimization\": agents_needing_optimization,\n",
        "        \"cost_per_evaluation\": round(total_cost / total_evaluations, 2) if total_evaluations > 0 else 0.0\n",
        "    }\n",
        "\n",
        "    # Workflow analysis (using toolshed)\n",
        "    workflow_analysis = []\n",
        "    if config.enable_workflow_analysis:\n",
        "        for summary in agent_performance_summaries:\n",
        "            # Use failure rate as metric for workflow health\n",
        "            failure_rate = (summary.get('failed_count', 0) / summary.get('total_evaluations', 1)) * 100\n",
        "\n",
        "            workflow = {\n",
        "                \"workflow_id\": f\"eval_{summary.get('agent_id')}\",\n",
        "                \"agent_id\": summary.get('agent_id'),\n",
        "                \"failure_rate_7d\": failure_rate\n",
        "            }\n",
        "\n",
        "            # Use workflow health analysis\n",
        "            thresholds = {\n",
        "                \"healthy\": 10.0,    # <= 10% failure rate\n",
        "                \"degraded\": 30.0,   # 10-30% failure rate\n",
        "                \"critical\": 30.0    # > 30% failure rate\n",
        "            }\n",
        "\n",
        "            analysis = analyze_workflow_health(workflow, thresholds)\n",
        "            workflow_analysis.append(analysis)\n",
        "\n",
        "    # Performance metrics (using toolshed)\n",
        "    performance_metrics = {}\n",
        "    if config.enable_performance_tracking:\n",
        "        metrics_definitions = [\n",
        "            {\n",
        "                \"name\": \"evaluation_time\",\n",
        "                \"description\": \"Average evaluation execution time\",\n",
        "                \"unit\": \"seconds\",\n",
        "                \"thresholds\": {\n",
        "                    \"healthy\": 1.0,      # <= 1 second\n",
        "                    \"degraded\": 2.0,    # 1-2 seconds\n",
        "                    \"critical\": 2.0     # > 2 seconds\n",
        "                },\n",
        "                \"weight\": 0.5\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"pass_rate\",\n",
        "                \"description\": \"Overall evaluation pass rate\",\n",
        "                \"unit\": \"ratio\",\n",
        "                \"thresholds\": {\n",
        "                    \"healthy\": 0.90,    # >= 90%\n",
        "                    \"degraded\": 0.70,   # 70-90%\n",
        "                    \"critical\": 0.0    # < 70%\n",
        "                },\n",
        "                \"weight\": 0.5\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        metrics_config = create_metrics_config(metrics_definitions)\n",
        "\n",
        "        avg_eval_time = sum(e.get('execution_time_seconds', 0.0) for e in evaluations) / len(evaluations) if evaluations else 0.0\n",
        "\n",
        "        performance_metrics = {\n",
        "            \"average_evaluation_time\": avg_eval_time,\n",
        "            \"overall_pass_rate\": overall_pass_rate,\n",
        "            \"metrics_config\": metrics_config\n",
        "        }\n",
        "\n",
        "    # Save current summary to history for future comparisons\n",
        "    try:\n",
        "        summary_to_save = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"evaluation_summary\": evaluation_summary,\n",
        "            \"agent_performance_summary\": agent_performance_summaries\n",
        "        }\n",
        "        save_evaluation_summary(summary_to_save, config.reports_dir)\n",
        "        print(\"    Saved evaluation summary to history\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Warning: Could not save evaluation summary: {str(e)}\")\n",
        "\n",
        "    print(f\"  [Performance Analysis Node] Analyzed {len(agent_performance_summaries)} agents\")\n",
        "    if statistical_assessments:\n",
        "        print(f\"    Statistical significance calculated for {len(statistical_assessments)} agent(s)\")\n",
        "\n",
        "    return {\n",
        "        \"agent_performance_summary\": agent_performance_summaries,\n",
        "        \"evaluation_summary\": evaluation_summary,\n",
        "        \"workflow_analysis\": workflow_analysis,\n",
        "        \"performance_metrics\": performance_metrics,\n",
        "        \"statistical_assessments\": statistical_assessments\n",
        "    }\n",
        "\n"
      ]
    }
  ]
}