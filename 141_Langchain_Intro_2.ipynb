{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPaj8rAp2aVE256Jnq/3Z9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/141_Langchain_Intro_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Memory + Context Persistence**\n",
        "\n",
        "* Your orchestrator tracks state with enums and dataclasses.\n",
        "* LangChain has **Memory modules** to carry context across steps or conversations automatically.\n",
        "* That‚Äôs useful if you want multi-session or human-in-the-loop review with history intact.\n",
        "\n",
        "let‚Äôs zoom out and tackle **Memory + Context Persistence**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In Your Current Orchestrator\n",
        "\n",
        "You‚Äôve already seen how your system handles **state**:\n",
        "\n",
        "* **Enums** (`WorkflowStatus`, `AgentStatus`) ‚Üí track *where you are* in the pipeline.\n",
        "* **Dataclasses** (`WorkflowStep`, `WorkflowState`) ‚Üí record *what happened* at each step, with retries, errors, and timestamps.\n",
        "\n",
        "That‚Äôs explicit, structured, and very production-grade ‚Äî but it‚Äôs also **per run**. Once the workflow finishes, the context doesn‚Äôt persist unless you log/store it somewhere external.\n",
        "\n",
        "So if you wanted the next run to *remember* what happened last time (e.g., ‚Äúwe already emailed Jane Doe at Acme Corp‚Äù), you‚Äôd have to code the persistence layer yourself (write to a DB, reload before next run).\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ In LangChain\n",
        "\n",
        "LangChain adds **Memory modules** on top of orchestration.\n",
        "\n",
        "Think of Memory as:\n",
        "\n",
        "* A **buffer** that stores conversation or workflow context.\n",
        "* Something the agent automatically *remembers and injects* into the LLM prompt on future runs.\n",
        "\n",
        "Types of memory you get out of the box:\n",
        "\n",
        "| Memory Type                        | What It Does                                                             | Sales Example                                                                                                   |\n",
        "| ---------------------------------- | ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |\n",
        "| **ConversationBufferMemory**       | Stores running transcript of past interactions.                          | Remembering which contacts were discussed earlier in a prospecting session.                                     |\n",
        "| **ConversationSummaryMemory**      | Uses the LLM to compress history into a summary to keep token usage low. | Instead of re-pasting all research, store ‚ÄúJane Doe is Head of Ops at Acme, interested in AI efficiency tools.‚Äù |\n",
        "| **ConversationBufferWindowMemory** | Keeps only the last *N* exchanges for short-term memory.                 | Keep last 3 emails in context, ignore older ones.                                                               |\n",
        "| **EntityMemory**                   | Tracks facts about specific entities across sessions.                    | Remember everything learned about ‚ÄúAcme Corp‚Äù (employees, size, industry) and reuse it in later workflows.      |\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Why This Matters for Sales Agents\n",
        "\n",
        "* **Multi-session** ‚Üí If you run your pipeline on Monday and again on Friday, it can *remember* past outreach steps (instead of re-researching from scratch).\n",
        "* **Human-in-the-loop review** ‚Üí A sales rep could pause, edit a draft, and resume ‚Äî with history intact.\n",
        "* **Context continuity** ‚Üí When you re-encounter ‚ÄúJane Doe at Acme Corp,‚Äù the agent knows it already messaged her last week.\n",
        "* **Smarter personalization** ‚Üí Instead of treating every run as fresh, memory gives continuity like a human SDR has in their notes.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Key Difference\n",
        "\n",
        "* **Your orchestrator**: State = execution log (structured, explicit, per run).\n",
        "* **LangChain**: Memory = contextual glue (persistent, automatic, across runs).\n",
        "\n",
        "Together, they‚Äôre complementary:\n",
        "\n",
        "* Orchestrator ensures predictability, retries, and structured state.\n",
        "* Memory ensures conversations and agent reasoning don‚Äôt reset each time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMMar0L8ZLxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs make this concrete. I‚Äôll show you **side-by-side code** for:\n",
        "\n",
        "1. **Your orchestrator style** (explicit state with dataclasses & enums).\n",
        "2. **LangChain style** (automatic memory modules).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 1. Your Orchestrator (explicit state tracking)\n",
        "\n",
        "Here you keep a detailed log of steps, retries, and errors. But context only exists for *this run*.\n",
        "\n",
        "```python\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "\n",
        "class WorkflowStatus(Enum):\n",
        "    PENDING = \"pending\"\n",
        "    IN_PROGRESS = \"in_progress\"\n",
        "    COMPLETED = \"completed\"\n",
        "    FAILED = \"failed\"\n",
        "\n",
        "class AgentStatus(Enum):\n",
        "    READY = \"ready\"\n",
        "    RUNNING = \"running\"\n",
        "    COMPLETED = \"completed\"\n",
        "    FAILED = \"failed\"\n",
        "\n",
        "@dataclass\n",
        "class WorkflowStep:\n",
        "    step_id: str\n",
        "    agent_name: str\n",
        "    status: AgentStatus = AgentStatus.READY\n",
        "    start_time: Optional[datetime] = None\n",
        "    end_time: Optional[datetime] = None\n",
        "    error_message: Optional[str] = None\n",
        "    retries: int = 0\n",
        "    input_data: dict = field(default_factory=dict)\n",
        "    output_data: dict = field(default_factory=dict)\n",
        "\n",
        "@dataclass\n",
        "class WorkflowState:\n",
        "    workflow_id: str\n",
        "    company_name: str\n",
        "    status: WorkflowStatus = WorkflowStatus.PENDING\n",
        "    steps: List[WorkflowStep] = field(default_factory=list)\n",
        "    start_time: datetime = field(default_factory=datetime.utcnow)\n",
        "    end_time: Optional[datetime] = None\n",
        "\n",
        "# ‚Üí This is precise, auditable, great for debugging.\n",
        "# But: once the run ends, you‚Äôd have to save WorkflowState somewhere\n",
        "# if you want memory across sessions.\n",
        "```\n"
      ],
      "metadata": {
        "id": "CBu2tyx2ekaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üß† 2. LangChain Memory (automatic context persistence)\n",
        "\n",
        "Here, LangChain handles context injection into prompts across runs.\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Add memory ‚Äî this will keep all prior messages in context\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# First run\n",
        "print(conversation.predict(input=\"We‚Äôre targeting Acme Corp this week.\"))\n",
        "\n",
        "# Second run (later in the same session)\n",
        "print(conversation.predict(input=\"Who was the main contact again?\"))\n",
        "# ‚Üí Memory injects: \"We‚Äôre targeting Acme Corp...\" into the LLM prompt\n",
        "# So the LLM can answer without you re-supplying Acme‚Äôs name.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üü© Key Contrast\n",
        "\n",
        "* **Dataclass State** = structured, explicit, one-off execution log (great for orchestration, retries, debugging).\n",
        "* **LangChain Memory** = implicit, automatic, multi-session conversational memory (great for continuity, personalization).\n",
        "\n",
        "You could *combine* them:\n",
        "\n",
        "* Orchestrator manages workflow control + logs.\n",
        "* LangChain memory keeps track of *conversation context* for agents (e.g. research results, past outreach).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-9UtkBqedBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great catch üôå ‚Äî and here‚Äôs the subtle but *very important* truth:\n",
        "\n",
        "**No ‚Äî LangChain‚Äôs `ConversationBufferMemory` isn‚Äôt replacing everything your orchestrator does.**\n",
        "It‚Äôs solving a *different* problem. Let me unpack it:\n",
        "\n",
        "---\n",
        "\n",
        "### üü¶ What Your Orchestrator Does\n",
        "\n",
        "Your custom code with **dataclasses + enums** handles things like:\n",
        "\n",
        "* ‚úÖ Tracking **workflow status** (pending, in-progress, completed, failed).\n",
        "* ‚úÖ Logging **steps, retries, and errors** with timestamps.\n",
        "* ‚úÖ Keeping an **audit trail** (what agent ran, with what input/output).\n",
        "* ‚úÖ Orchestrating **control flow** (what runs next, what to skip, when to retry).\n",
        "\n",
        "üëâ That‚Äôs heavy-duty *workflow management*.\n",
        "It‚Äôs explicit, structured, deterministic, and essential for **production-grade pipelines**.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ What LangChain Memory Does\n",
        "\n",
        "LangChain memory modules handle:\n",
        "\n",
        "* ‚úÖ Remembering **conversational context** for an LLM across steps/sessions.\n",
        "* ‚úÖ Automatically re-injecting history into prompts so the LLM doesn‚Äôt ‚Äúforget.‚Äù\n",
        "* ‚úÖ Summarizing or windowing context to control token usage.\n",
        "* ‚úÖ Storing **facts about entities** (e.g. ‚ÄúJane Doe = VP Ops at Acme Corp‚Äù).\n",
        "\n",
        "üëâ That‚Äôs lightweight *context persistence*.\n",
        "It‚Äôs focused on **what the LLM sees** when generating responses, not on auditing execution.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ The Big Difference\n",
        "\n",
        "Your orchestrator = üõ†Ô∏è **Control system** (like flight control + black box log).\n",
        "LangChain memory = üß† **Short/long-term memory** (like the pilot remembering instructions).\n",
        "\n",
        "That‚Äôs why the LangChain code snippet looks tiny ‚Äî it‚Äôs solving only the *memory* piece, not the full orchestration.\n",
        "\n",
        "If you only used `ConversationBufferMemory`, you‚Äôd have:\n",
        "\n",
        "* Great continuity of context (‚Äúwe‚Äôre targeting Acme Corp‚Äù),\n",
        "* But no way to **retry** on API failure,\n",
        "* No audit log of step-by-step execution,\n",
        "* No structured state to debug which agent failed.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ So: LangChain memory doesn‚Äôt *replace* your orchestrator.\n",
        "It *complements* it, by letting agents ‚Äúremember‚Äù across sessions, while your orchestrator keeps the workflow **predictable and debuggable**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n_SHc1PKfRJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üèóÔ∏è Hybrid Orchestrator + LangChain Memory\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    subgraph Pipeline[Sales Pipeline Run]\n",
        "        A[Research Agent] --> B[Analysis Agent]\n",
        "        B --> C[Personalization Agent]\n",
        "    end\n",
        "\n",
        "    subgraph Orchestrator[Sales Orchestrator]\n",
        "        S1[WorkflowState (dataclass)]\n",
        "        S2[WorkflowStep logs]\n",
        "        S3[Enums: WorkflowStatus & AgentStatus]\n",
        "    end\n",
        "\n",
        "    subgraph Memory[LangChain Memory Module]\n",
        "        M1[ConversationBufferMemory]\n",
        "        M2[EntityMemory (Acme, Jane Doe)]\n",
        "    end\n",
        "\n",
        "    Pipeline --> Orchestrator\n",
        "    Pipeline --> Memory\n",
        "\n",
        "    Orchestrator -->|Logs status/errors| Dev[Developer / Dashboard]\n",
        "    Memory -->|Injects context| Pipeline\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ How It Works\n",
        "\n",
        "* **Pipeline (Agents)** ‚Üí Executes steps: Research ‚Üí Analysis ‚Üí Personalization.\n",
        "* **Orchestrator (State Layer)** ‚Üí Tracks structured execution:\n",
        "\n",
        "  * Start/end times\n",
        "  * Status (pending, failed, retrying)\n",
        "  * Error logs\n",
        "  * Audit trail\n",
        "* **Memory (Context Layer)** ‚Üí Feeds agents with continuity:\n",
        "\n",
        "  * Past research on Acme Corp\n",
        "  * Last contact person emailed\n",
        "  * Prior personalization strategies\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Key Takeaway\n",
        "\n",
        "* **Orchestrator = Control tower** üõ†Ô∏è\n",
        "\n",
        "  * Manages execution, retries, observability, workflow state.\n",
        "* **Memory = Working memory / notes** üß†\n",
        "\n",
        "  * Keeps conversation + context alive across steps and sessions.\n",
        "\n",
        "Together ‚Üí you get **predictable pipelines** *and* **contextually smart agents**.\n",
        "\n"
      ],
      "metadata": {
        "id": "csTtTpSwhBFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Observability**\n",
        "\n",
        "* LangSmith (companion tool) gives you **logs, traces, metrics, and dashboards** without writing custom monitoring.\n",
        "* In your demo pipeline, you printed out metrics manually ‚Äî LangChain gives you observability as a first-class citizen.\n",
        "---\n",
        "\n",
        "Let‚Äôs dig into **Observability** üîé ‚Äî because this is where the difference between your orchestrator and LangChain really pops.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Your Current Demo Pipeline\n",
        "\n",
        "In your pipeline script you had things like:\n",
        "\n",
        "```python\n",
        "print(f\"Workflow completed in {duration} seconds\")\n",
        "print(f\"Total retries: {retry_count}\")\n",
        "print(f\"Errors: {errors}\")\n",
        "```\n",
        "\n",
        "That‚Äôs **manual observability**: you decide what to log, you format the printouts, and if you want metrics over time, you‚Äôd need to export them somewhere (DB, Prometheus, Grafana, etc.).\n",
        "\n",
        "‚úÖ Pro: Full control, lightweight.\n",
        "‚ùå Con: No automatic traces, no nice UI, hard to compare runs over time without building extra infra.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ LangChain + LangSmith\n",
        "\n",
        "LangChain‚Äôs companion tool, **LangSmith**, makes **observability a first-class citizen**.\n",
        "\n",
        "* **Traces**: Each agent call, each LLM prompt, each tool invocation is logged automatically.\n",
        "* **Metrics**: You get latency, token counts, success/failure rates without writing code.\n",
        "* **Dashboards**: Web UI shows runs, inputs/outputs, errors, retry paths.\n",
        "* **Replay**: You can re-run a past input through your pipeline to debug.\n",
        "* **Feedback**: You can attach ratings or labels to outputs for fine-tuning or evals.\n",
        "\n",
        "Example setup is often just:\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tracing=True  # <-- sends traces to LangSmith\n",
        ")\n",
        "```\n",
        "\n",
        "That‚Äôs it ‚Äî every LLM call and agent chain gets tracked.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Why It Matters for Sales Agents\n",
        "\n",
        "* **Debugging** ‚Üí If outreach personalization fails, you can trace *which prompt* or *which agent* caused the bad output.\n",
        "* **Performance monitoring** ‚Üí Spot if ResearchAgent starts timing out more often, or if PersonalizationAgent drifts.\n",
        "* **A/B Testing** ‚Üí Compare two personalization templates in the dashboard, see which converts better.\n",
        "* **Team collaboration** ‚Üí Non-developers (sales ops, PMs) can look at dashboards without digging into code logs.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Big Picture Contrast\n",
        "\n",
        "* **Your pipeline** ‚Üí Observability = ad hoc print statements + manual metrics.\n",
        "* **LangChain + LangSmith** ‚Üí Observability = structured traces, metrics, dashboards built-in, no custom infra needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "-1Qa4b_5ZSzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Think of **LangSmith** as the **‚ÄúDatadog + Postman + Grafana‚Äù** for LLM apps:\n",
        "\n",
        "---\n",
        "\n",
        "### üü¶ Primary Use Case: Debugging\n",
        "\n",
        "* **Trace every step**: See each LLM call, the raw prompt sent, the exact output.\n",
        "* **Replay**: Take a failing run and replay it step by step.\n",
        "* **Drill down**: Was the bug caused by the ResearchAgent‚Äôs prompt? A bad tool call? A malformed JSON return?\n",
        "\n",
        "üëâ This is *debugging heaven* compared to just reading `print()` logs.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ But Also: Monitoring & Optimization\n",
        "\n",
        "* **Metrics**: Latency, token usage, error rates, retries.\n",
        "* **Dashboards**: Aggregate view of how pipelines perform over time.\n",
        "* **Comparison**: Run A vs. Run B, side by side.\n",
        "* **Feedback**: Add thumbs-up/down or labels for outputs ‚Üí useful for evals or fine-tuning.\n",
        "\n",
        "üëâ This makes it **observability**, not just debugging.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ In Your Pipeline Context\n",
        "\n",
        "* Today: If the AnalysisAgent crashes, you only see `print(\"Error in AnalysisAgent...\")`.\n",
        "* With LangSmith: You‚Äôd see\n",
        "\n",
        "  * The **exact prompt** AnalysisAgent got,\n",
        "  * The **LLM raw output**,\n",
        "  * The **error message**,\n",
        "  * The **stack of calls** leading there.\n",
        "\n",
        "That‚Äôs debugging.\n",
        "But you‚Äôd also see:\n",
        "\n",
        "* 30% of runs are timing out at ResearchAgent,\n",
        "* Average personalization takes 2.3s and 300 tokens,\n",
        "* Outreach messages drifted in tone after a prompt change.\n",
        "\n",
        "That‚Äôs monitoring + optimization.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ So yes, **LangSmith is great for debugging** ‚Äî but it really shines as a full **observability platform** for LLM systems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9azuYSUMhmAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. **Composability**\n",
        "\n",
        "* LangChain uses the `Runnable` interface, so you can treat agents, chains, or even a whole pipeline as **lego blocks**.\n",
        "* Swap out your ResearchAgent for a new one without breaking the orchestrator.\n",
        "* In your current hand-built orchestrator, you had to code those connections manually.\n",
        "\n",
        "---\n",
        "\n",
        "Let‚Äôs wrap this with **Composability** üß© ‚Äî the ‚Äúlego block‚Äù principle.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Your Current Orchestrator\n",
        "\n",
        "In your hand-built orchestrator, wiring looks like:\n",
        "\n",
        "```python\n",
        "research_output = research_agent.run(company)\n",
        "analysis_output = analysis_agent.run(research_output)\n",
        "personalization_output = personalization_agent.run(analysis_output)\n",
        "```\n",
        "\n",
        "* The **order** is hard-coded.\n",
        "* If you wanted to swap ResearchAgent v2, you‚Äôd have to edit the orchestrator logic.\n",
        "* If you wanted to branch (e.g., run *both* AnalysisAgent and a new SWOTAgent), you‚Äôd add custom code.\n",
        "\n",
        "‚úÖ Explicit and transparent.\n",
        "‚ùå Brittle ‚Äî changes require editing orchestrator logic.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ LangChain Composability with `Runnable`\n",
        "\n",
        "LangChain standardizes everything (LLMs, tools, chains, custom agents) under a **`Runnable` interface**:\n",
        "\n",
        "* `.invoke()` ‚Üí run once.\n",
        "* `.batch()` ‚Üí run on many inputs.\n",
        "* `.stream()` ‚Üí stream outputs.\n",
        "\n",
        "This makes composition simple and modular.\n",
        "\n",
        "```python\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "pipeline = (\n",
        "    research_agent\n",
        "    | analysis_agent\n",
        "    | personalization_agent\n",
        ")\n",
        "```\n",
        "\n",
        "Now:\n",
        "\n",
        "* Swapping `research_agent` with `new_research_agent` = one-line change.\n",
        "* Adding branches = trivial with `RunnableParallel`.\n",
        "* Wrapping the entire pipeline = just another `Runnable` that can be composed higher up.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Why This Matters\n",
        "\n",
        "* **Modularity**: You can upgrade agents independently.\n",
        "* **Experimentation**: A/B test two different ResearchAgents with minimal glue code.\n",
        "* **Nested Pipelines**: Treat your whole sales pipeline as one `Runnable` that plugs into a bigger system (say, a multi-market campaign orchestrator).\n",
        "* **Standard interface**: Whether it‚Äôs a function, LLM, chain, or custom agent ‚Äî it all behaves like lego blocks.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Big Picture Contrast\n",
        "\n",
        "* **Hand-built orchestrator** ‚Üí Manual wiring, lots of custom code to change connections.\n",
        "* **LangChain `Runnable`** ‚Üí Declarative pipelines, easy swaps, easy composition.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A0Xt4hkUZPM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs rebuild your **Research ‚Üí Analysis ‚Üí Personalization** pipeline using **LangChain Runnables** to show how composability makes it cleaner and more modular.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Step 1: Define Agents as `Runnable`s\n",
        "\n",
        "We‚Äôll wrap your existing agent logic (research, analysis, personalization) as functions or LangChain-compatible runnables.\n",
        "\n",
        "```python\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "# Wrap your custom agents as Runnables\n",
        "research_agent = RunnableLambda(lambda company: {\"research\": f\"Research on {company}\"})\n",
        "analysis_agent = RunnableLambda(lambda inputs: {\"analysis\": f\"SWOT for {inputs['research']}\"})\n",
        "personalization_agent = RunnableLambda(lambda inputs: {\"personalization\": f\"Email pitch based on {inputs['analysis']}\"})\n",
        "```\n",
        "\n",
        "Here, each agent accepts structured input and returns structured output.\n",
        "Notice how we **wrap each one** ‚Äî this is what makes them lego blocks.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Step 2: Compose the Pipeline\n",
        "\n",
        "Now we just ‚Äúsnap‚Äù them together with `RunnableSequence`.\n",
        "\n",
        "```python\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "\n",
        "pipeline = RunnableSequence(\n",
        "    steps=[\n",
        "        research_agent,\n",
        "        analysis_agent,\n",
        "        personalization_agent,\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "That‚Äôs it üéâ\n",
        "\n",
        "* One pipeline = one object.\n",
        "* Easy to **invoke, batch, or stream**.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Step 3: Run It\n",
        "\n",
        "```python\n",
        "result = pipeline.invoke(\"Acme Corp\")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"research\": \"Research on Acme Corp\",\n",
        "  \"analysis\": \"SWOT for Research on Acme Corp\",\n",
        "  \"personalization\": \"Email pitch based on SWOT for Research on Acme Corp\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üü¶ Step 4: Swap or Extend\n",
        "\n",
        "* Swap in a new ResearchAgent:\n",
        "\n",
        "  ```python\n",
        "  pipeline = new_research_agent | analysis_agent | personalization_agent\n",
        "  ```\n",
        "* Add a SWOTAgent branch:\n",
        "\n",
        "  ```python\n",
        "  from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "  parallel_analysis = RunnableParallel({\n",
        "      \"swot\": swot_agent,\n",
        "      \"standard\": analysis_agent\n",
        "  })\n",
        "\n",
        "  pipeline = research_agent | parallel_analysis | personalization_agent\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Takeaway\n",
        "\n",
        "With **Runnables**:\n",
        "\n",
        "* Agents, chains, or whole pipelines are all *lego blocks*.\n",
        "* You can **swap, extend, or compose** with almost no code changes.\n",
        "* Your pipeline is just another Runnable ‚Üí can plug into larger systems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8j0UKG9qiIp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When you see your pipeline as a `RunnableSequence` (or a parallel of them), you get:\n",
        "\n",
        "* **At-a-glance clarity** ‚Üí each block is a self-contained unit.\n",
        "* **Separation of concerns** ‚Üí swap or debug one agent without touching the rest.\n",
        "* **Declarative design** ‚Üí instead of writing *how* to wire things, you just describe the *flow*.\n",
        "\n",
        "It‚Äôs like moving from spaghetti code with `if/else` and manual wiring‚Ä¶\n",
        "üëâ to a **visual block diagram written in code**.\n",
        "\n",
        "---\n",
        "\n",
        "Here‚Äôs the key mindset shift:\n",
        "\n",
        "* In your **hand-built orchestrator**, you ask: *‚ÄúWhat‚Äôs the next step, and how do I pass outputs forward?‚Äù*\n",
        "* In **LangChain composability**, you ask: *‚ÄúWhat‚Äôs the overall flow of blocks?‚Äù* ‚Äî and the system takes care of the plumbing.\n",
        "\n",
        "---\n",
        "\n",
        "üí° Pro tip: Once you‚Äôre comfortable with this, you can even use **Mermaid diagrams** or LangSmith‚Äôs trace visualizations to see these flows *literally* drawn out ‚Äî so your mental model matches what‚Äôs happening under the hood.\n",
        "\n"
      ],
      "metadata": {
        "id": "_NYDTZHxiZ3G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnONF6drZLNA"
      },
      "outputs": [],
      "source": []
    }
  ]
}