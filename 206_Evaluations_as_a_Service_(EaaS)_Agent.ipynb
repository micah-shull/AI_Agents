{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE/RlyYvoowh3xXdI6CJLv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/206_Evaluations_as_a_Service_(EaaS)_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Report\n",
        "\n",
        "**Generated:** 2025-11-17 16:21:51\n",
        "\n",
        "## Summary\n",
        "\n",
        "Evaluated **2 agent(s)** across **20 test scenario(s)**.\n",
        "Detected **2 orchestrator insight(s)**.\n",
        "\n",
        "## Agent Scores\n",
        "\n",
        "### agent_001\n",
        "\n",
        "- **Overall Score:** 90.00%\n",
        "- **Accuracy:** 90.00%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "### agent_002\n",
        "\n",
        "- **Overall Score:** 0.00%\n",
        "- **Accuracy:** 0.00%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "## Detailed Results\n",
        "\n",
        "| Agent | Scenario | Input | Expected | Actual | Correct |\n",
        "|-------|----------|-------|----------|--------|---------|\n",
        "| agent_001 | c001 | I absolutely loved the new dashboard ‚Äì it‚Äôs so muc... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c002 | This update is terrible, nothing works the way it ... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c003 | It‚Äôs fine, I guess. Not really better or worse tha... | neutral | positive | ‚ùå |\n",
        "| agent_001 | c004 | Thank you so much for fixing this so quickly, I re... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c005 | I‚Äôm really frustrated that I keep getting logged o... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c006 | The results are okay, but there‚Äôs still room for i... | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c007 | This new feature saves me at least an hour every d... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c008 | I don‚Äôt really care about this change. | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c009 | This is completely unusable; I‚Äôm going back to the... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c010 | Nice job on the redesign ‚Äì it looks clean and intu... | positive | positive | ‚úÖ |\n",
        "\n",
        "*... and 10 more results*\n",
        "\n",
        "## üéØ Orchestrator Insights\n",
        "\n",
        "*These insights are only visible when evaluating multiple agents together - this is the orchestrator value!*\n",
        "\n",
        "### üî¥ Systemic Failures (All Agents Fail)\n",
        "\n",
        "**All agents fail on scenario type: neutral**\n",
        "- Scenario: c003\n",
        "- Agents affected: agent_001, agent_002\n",
        "- Category: sentiment\n",
        "- üí° Recommendation: Improve handling of neutral scenarios (category: sentiment)\n",
        "\n",
        "### üìä Consistency Patterns\n",
        "\n",
        "**agent_002 is consistently fails across all scenarios**\n",
        "- üí° Recommendation: agent_002 performs reliably\n"
      ],
      "metadata": {
        "id": "L07j4HROtU5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps Analysis: What We're Seeing & Where to Go\n",
        "\n",
        "**Based on:** Evaluation report showing improved mock agents working!\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ What's Working Great\n",
        "\n",
        "### Agent 001 (Classification): 90% Accuracy! ‚úÖ\n",
        "- Correctly classifying positive and negative sentiment\n",
        "- Only 1 failure: c003 (neutral sentiment - \"It's fine, I guess\")\n",
        "- This is realistic behavior - neutral sentiment is harder to detect\n",
        "\n",
        "### Orchestrator Insights Working:\n",
        "- ‚úÖ Detected systemic failure: \"Both agents fail on neutral scenario c003\"\n",
        "- ‚úÖ Detected consistency pattern: \"Agent 002 consistently fails\"\n",
        "- The orchestrator is finding real patterns!\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What We're Seeing\n",
        "\n",
        "### Agent 002 (Safety): 0% Accuracy\n",
        "\n",
        "**Why this is happening:**\n",
        "- Agent 002 is a **safety agent** (returns \"safe\" or \"unsafe\")\n",
        "- But it's being tested on **classification scenarios** (expect \"positive\", \"negative\", \"neutral\")\n",
        "- This is a **mismatch** - safety agent can't match classification labels\n",
        "\n",
        "**This is actually a great learning moment!**\n",
        "- The orchestrator detected this: \"Agent 002 consistently fails\"\n",
        "- This shows orchestrator value: it found a systemic issue (wrong agent type for scenarios)\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Strategic Recommendation\n",
        "\n",
        "### **Option A: Test Agents with Appropriate Scenarios (Recommended)** ‚≠ê\n",
        "\n",
        "**Why:**\n",
        "1. **Teaches orchestrator coordination** - matching agents to appropriate scenarios\n",
        "2. **Shows realistic patterns** - each agent tested on its domain\n",
        "3. **Demonstrates orchestrator value** - can evaluate different agent types together\n",
        "4. **More realistic insights** - patterns will reflect actual behavior\n",
        "\n",
        "**What to do:**\n",
        "- Test Agent 001 (classification) on classification scenarios\n",
        "- Test Agent 002 (safety) on safety scenarios\n",
        "- Run both evaluations together\n",
        "- See orchestrator insights across different agent types\n",
        "\n",
        "**Learning Value:**\n",
        "- How orchestrators coordinate different agent types\n",
        "- How to match scenarios to agents\n",
        "- How to analyze across different domains\n",
        "- This is real orchestrator coordination!\n",
        "\n",
        "---\n",
        "\n",
        "### **Option B: Improve Classification Agent Further**\n",
        "\n",
        "**Why this might make sense:**\n",
        "- Agent 001 is 90% accurate, but fails on neutral sentiment\n",
        "- Could improve keyword detection for neutral cases\n",
        "\n",
        "**Why this is less valuable right now:**\n",
        "- 90% is already good for a simple keyword-based mock\n",
        "- The real learning is orchestrator coordination\n",
        "- Better to see orchestrator value with multiple agent types\n",
        "\n",
        "**When to do this:**\n",
        "- After we've learned orchestrator coordination\n",
        "- When we want to optimize individual agents\n",
        "- When we're ready for production-quality agents\n",
        "\n",
        "---\n",
        "\n",
        "### **Option C: Add More Agent Types**\n",
        "\n",
        "**Why this might make sense:**\n",
        "- Could add more agent types (generation, summarization, etc.)\n",
        "- More agents = more orchestrator insights\n",
        "\n",
        "**Why this is less valuable right now:**\n",
        "- We should first learn coordination with 2 agent types\n",
        "- Better to understand fundamentals before adding complexity\n",
        "- Current setup is perfect for learning\n",
        "\n",
        "**When to do this:**\n",
        "- After mastering coordination with 2 agent types\n",
        "- When we understand orchestrator patterns\n",
        "- When we want to scale to more agents\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ My Recommendation: Test with Appropriate Scenarios (Option A)\n",
        "\n",
        "### Why This is the Best Next Step:\n",
        "\n",
        "1. **Teaches Orchestrator Coordination**\n",
        "   - How to match agents to scenarios\n",
        "   - How to evaluate different agent types together\n",
        "   - This is core orchestrator skill\n",
        "\n",
        "2. **Shows Realistic Patterns**\n",
        "   - Each agent tested on its domain\n",
        "   - Patterns will reflect actual behavior\n",
        "   - More meaningful insights\n",
        "\n",
        "3. **Demonstrates Orchestrator Value**\n",
        "   - Can evaluate classification AND safety agents together\n",
        "   - Can find patterns across different domains\n",
        "   - This is what orchestrators do!\n",
        "\n",
        "4. **Natural Learning Progression**\n",
        "   - We've learned pattern detection ‚úÖ\n",
        "   - We've improved execution ‚úÖ\n",
        "   - Now learn coordination ‚≠ê\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Implementation Plan\n",
        "\n",
        "### Step 1: Update Test Runner to Use Both Scenario Types\n",
        "\n",
        "```python\n",
        "# Test with both scenario types\n",
        "state = {\n",
        "    \"target_agents\": [\n",
        "        {\"id\": \"agent_001\", \"name\": \"Sentiment Classifier\", \"type\": \"classification\"},\n",
        "        {\"id\": \"agent_002\", \"name\": \"Safety Checker\", \"type\": \"safety\"}\n",
        "    ],\n",
        "    \"test_data_path\": \"data/classification_cases.json\",  # For agent_001\n",
        "    # Need to also load safety_cases.json for agent_002\n",
        "}\n",
        "```\n",
        "\n",
        "### Step 2: Enhance Data Ingestion to Support Multiple Files\n",
        "\n",
        "- Load classification cases for classification agents\n",
        "- Load safety cases for safety agents\n",
        "- Match scenarios to agents based on type\n",
        "\n",
        "### Step 3: Update Evaluation Execution\n",
        "\n",
        "- Route scenarios to appropriate agents\n",
        "- Classification scenarios ‚Üí classification agents\n",
        "- Safety scenarios ‚Üí safety agents\n",
        "\n",
        "### Step 4: Test and See Orchestrator Insights\n",
        "\n",
        "- See patterns within each domain\n",
        "- See patterns across domains (if any)\n",
        "- Learn orchestrator coordination\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Expected Outcomes\n",
        "\n",
        "### With Appropriate Scenarios:\n",
        "- **Agent 001:** High accuracy on classification scenarios\n",
        "- **Agent 002:** High accuracy on safety scenarios\n",
        "- **Orchestrator Insights:** Patterns within and across domains\n",
        "- **Learning Value:** Understand orchestrator coordination\n",
        "\n",
        "### What We'll Learn:\n",
        "- How to match agents to scenarios\n",
        "- How to coordinate different agent types\n",
        "- How orchestrators handle multi-domain evaluation\n",
        "- How to structure evaluation for different agent types\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Learning Focus\n",
        "\n",
        "### What to Focus On:\n",
        "1. **Agent-Scenario Matching** - How to route scenarios to appropriate agents\n",
        "2. **Multi-Domain Coordination** - How orchestrators handle different agent types\n",
        "3. **Pattern Detection Across Domains** - Can we find patterns across classification and safety?\n",
        "4. **Orchestrator Architecture** - How to structure evaluation for heterogeneous agents\n",
        "\n",
        "### Why This Matters:\n",
        "- This is real orchestrator coordination\n",
        "- Production orchestrators need to handle multiple agent types\n",
        "- This teaches the \"orchestration\" part of orchestrators\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üéØ Final Recommendation\n",
        "\n",
        "**Test with Appropriate Scenarios (Option A)** because:\n",
        "1. ‚úÖ Teaches orchestrator coordination (core skill)\n",
        "2. ‚úÖ Shows realistic patterns (each agent in its domain)\n",
        "3. ‚úÖ Demonstrates orchestrator value (multi-domain evaluation)\n",
        "4. ‚úÖ Natural learning progression (next logical step)\n",
        "5. ‚úÖ Production-relevant (real orchestrators do this)\n",
        "\n",
        "**Then:**\n",
        "- After coordination works, we can analyze patterns\n",
        "- We can improve individual agents\n",
        "- We can add more agent types\n",
        "- We can refine pattern detection\n",
        "\n",
        "---\n",
        "\n",
        "*This recommendation focuses on learning orchestrator coordination - matching agents to scenarios and evaluating across domains. This is a core orchestrator skill.*\n",
        "\n"
      ],
      "metadata": {
        "id": "fnvrY_bNtnYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion Node"
      ],
      "metadata": {
        "id": "451QieqUvAxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNIWoy7ptUdY"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data Ingestion Node - Loads test scenarios and ground truth\"\"\"\n",
        "\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _load_scenario_file(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load scenarios from a single file\"\"\"\n",
        "    data_file = Path(file_path)\n",
        "    if not data_file.exists():\n",
        "        raise FileNotFoundError(f\"Test data file not found: {file_path}\")\n",
        "\n",
        "    # Read file content\n",
        "    content = data_file.read_text()\n",
        "\n",
        "    # Handle Python assignment format (e.g., \"classification_cases = [...]\")\n",
        "    # Extract JSON array from the content\n",
        "    if \"=\" in content:\n",
        "        # Find the JSON array part\n",
        "        start_idx = content.find(\"[\")\n",
        "        end_idx = content.rfind(\"]\") + 1\n",
        "        if start_idx != -1 and end_idx > start_idx:\n",
        "            json_content = content[start_idx:end_idx]\n",
        "        else:\n",
        "            json_content = content\n",
        "    else:\n",
        "        json_content = content\n",
        "\n",
        "    # Parse JSON\n",
        "    return json.loads(json_content)\n",
        "\n",
        "\n",
        "def data_ingestion_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Load test scenarios from data file(s).\n",
        "\n",
        "    Supports:\n",
        "    - Single file: test_data_path = \"data/classification_cases.json\"\n",
        "    - Multiple files: test_data_paths = [\"data/classification_cases.json\", \"data/safety_cases.json\"]\n",
        "\n",
        "    Reads: test_data_path or test_data_paths\n",
        "    Writes: evaluation_data\n",
        "    \"\"\"\n",
        "    logger.info(\"üì• Ingesting evaluation data...\")\n",
        "\n",
        "    try:\n",
        "        # Support both single file and multiple files\n",
        "        test_data_path = state.get(\"test_data_path\")\n",
        "        test_data_paths = state.get(\"test_data_paths\", [])\n",
        "\n",
        "        # Determine which files to load\n",
        "        files_to_load = []\n",
        "        if test_data_path:\n",
        "            files_to_load.append(test_data_path)\n",
        "        if test_data_paths:\n",
        "            files_to_load.extend(test_data_paths)\n",
        "\n",
        "        if len(files_to_load) == 0:\n",
        "            error_msg = \"test_data_path or test_data_paths is required\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Load all scenario files\n",
        "        all_scenarios = []\n",
        "        source_files = []\n",
        "\n",
        "        for file_path in files_to_load:\n",
        "            try:\n",
        "                scenarios = _load_scenario_file(file_path)\n",
        "                all_scenarios.extend(scenarios)\n",
        "                source_files.append(str(file_path))\n",
        "                logger.info(f\"  Loaded {len(scenarios)} scenarios from {file_path}\")\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Error loading {file_path}: {str(e)}\"\n",
        "                logger.warning(error_msg)\n",
        "                state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "        if len(all_scenarios) == 0:\n",
        "            error_msg = \"No scenarios loaded from any file\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Extract task types\n",
        "        task_types = list(set(scenario.get(\"task_type\", \"unknown\") for scenario in all_scenarios))\n",
        "\n",
        "        # Build evaluation_data structure\n",
        "        evaluation_data = {\n",
        "            \"test_scenarios\": all_scenarios,\n",
        "            \"metadata\": {\n",
        "                \"total_scenarios\": len(all_scenarios),\n",
        "                \"task_types\": task_types,\n",
        "                \"source_files\": source_files\n",
        "            }\n",
        "        }\n",
        "\n",
        "        state[\"evaluation_data\"] = evaluation_data\n",
        "        logger.info(f\"‚úÖ Loaded {len(all_scenarios)} test scenarios (types: {task_types})\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        error_msg = f\"JSON parsing error: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in data_ingestion_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Report\n",
        "\n",
        "**Generated:** 2025-11-17 16:36:00\n",
        "\n",
        "## Summary\n",
        "\n",
        "Evaluated **2 agent(s)** across **20 test scenario(s)**.\n",
        "Detected **0 orchestrator insight(s)**.\n",
        "\n",
        "## Agent Scores\n",
        "\n",
        "### agent_001\n",
        "\n",
        "- **Overall Score:** 90.00%\n",
        "- **Accuracy:** 90.00%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "### agent_002\n",
        "\n",
        "- **Overall Score:** 80.00%\n",
        "- **Accuracy:** 80.00%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "## Detailed Results\n",
        "\n",
        "| Agent | Scenario | Input | Expected | Actual | Correct |\n",
        "|-------|----------|-------|----------|--------|---------|\n",
        "| agent_001 | c001 | I absolutely loved the new dashboard ‚Äì it‚Äôs so muc... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c002 | This update is terrible, nothing works the way it ... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c003 | It‚Äôs fine, I guess. Not really better or worse tha... | neutral | positive | ‚ùå |\n",
        "| agent_001 | c004 | Thank you so much for fixing this so quickly, I re... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c005 | I‚Äôm really frustrated that I keep getting logged o... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c006 | The results are okay, but there‚Äôs still room for i... | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c007 | This new feature saves me at least an hour every d... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c008 | I don‚Äôt really care about this change. | neutral | neutral | ‚úÖ |\n",
        "| agent_001 | c009 | This is completely unusable; I‚Äôm going back to the... | negative | negative | ‚úÖ |\n",
        "| agent_001 | c010 | Nice job on the redesign ‚Äì it looks clean and intu... | positive | positive | ‚úÖ |\n",
        "\n",
        "*... and 10 more results*\n",
        "\n",
        "## üéØ Orchestrator Insights\n",
        "\n",
        "*No patterns detected. This may indicate:*\n",
        "- Agents are performing well across all scenarios\n",
        "- Need more agents or scenarios to detect patterns\n",
        "- Evaluation data may need more diversity\n"
      ],
      "metadata": {
        "id": "vMONFdgYv03r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrator Coordination Walkthrough: Code Changes & Impact\n",
        "\n",
        "**Purpose:** Understanding the code changes that enabled orchestrator coordination and improved scores.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ The Big Picture: What Changed\n",
        "\n",
        "### Before:\n",
        "- Mock agents returned placeholder values (\"positive\" or \"safe\" for everything)\n",
        "- All agents tested on all scenarios (mismatch: safety agent on classification scenarios)\n",
        "- Agent 001: 40% accuracy, Agent 002: 0% accuracy\n",
        "\n",
        "### After:\n",
        "- Mock agents actually analyze input text (keyword-based classification)\n",
        "- Agents matched to appropriate scenarios (orchestrator coordination)\n",
        "- Agent 001: 90% accuracy, Agent 002: 80% accuracy\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Code Changes Breakdown\n",
        "\n",
        "### Change 1: Improved Mock Agents (Biggest Impact on Scores) ‚≠ê\n",
        "\n",
        "**File:** `nodes/evaluation_execution_node.py`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def _run_agent(agent: Dict[str, Any], input_text: str) -> str:\n",
        "    # Just return placeholder\n",
        "    if agent_type == \"classification\":\n",
        "        return \"positive\"  # Always returns positive\n",
        "    elif agent_type == \"safety\":\n",
        "        return \"safe\"  # Always returns safe\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def _run_classification_agent(input_text: str) -> str:\n",
        "    \"\"\"Actually analyzes input text using keywords\"\"\"\n",
        "    input_lower = input_text.lower()\n",
        "    \n",
        "    # Check for positive keywords\n",
        "    positive_keywords = [\"love\", \"great\", \"awesome\", \"thank\", ...]\n",
        "    positive_count = sum(1 for keyword in positive_keywords if keyword in input_lower)\n",
        "    \n",
        "    # Check for negative keywords\n",
        "    negative_keywords = [\"terrible\", \"frustrated\", \"unusable\", ...]\n",
        "    negative_count = sum(1 for keyword in negative_keywords if keyword in input_lower)\n",
        "    \n",
        "    # Classify based on keyword presence\n",
        "    if positive_count > negative_count and positive_count > 0:\n",
        "        return \"positive\"\n",
        "    elif negative_count > positive_count and negative_count > 0:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "```\n",
        "\n",
        "**Impact:**\n",
        "- **Agent 001:** 40% ‚Üí 90% accuracy (125% improvement!)\n",
        "- **Why:** Now actually analyzes text instead of always returning \"positive\"\n",
        "- **This is the biggest impact on scores**\n",
        "\n",
        "**What to Focus On:**\n",
        "- **Agent execution logic** - How agents process input\n",
        "- **Keyword-based classification** - Simple but effective pattern\n",
        "- **This teaches orchestration mechanics** - How to structure agent execution\n",
        "\n",
        "---\n",
        "\n",
        "### Change 2: Orchestrator Coordination (Enables Multi-Domain Evaluation) ‚≠ê\n",
        "\n",
        "**File:** `nodes/evaluation_execution_node.py`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "# Execute each scenario through each agent (no matching)\n",
        "for agent in target_agents:\n",
        "    for scenario in all_scenarios:\n",
        "        # Run agent on scenario (even if mismatch)\n",
        "        actual_output = _run_agent(agent, input_text)\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "# Match scenarios to agents based on task_type (orchestrator coordination!)\n",
        "for agent in target_agents:\n",
        "    agent_type = agent.get(\"type\", \"unknown\")\n",
        "    \n",
        "    # Find scenarios that match this agent's type\n",
        "    matching_scenarios = [\n",
        "        scenario for scenario in all_scenarios\n",
        "        if scenario.get(\"task_type\", \"unknown\") == agent_type\n",
        "    ]\n",
        "    \n",
        "    # Only evaluate matching scenarios\n",
        "    for scenario in matching_scenarios:\n",
        "        actual_output = _run_agent(agent, input_text)\n",
        "```\n",
        "\n",
        "**Impact:**\n",
        "- **Agent 002:** 0% ‚Üí 80% accuracy (now tested on safety scenarios!)\n",
        "- **Why:** Safety agent now only tested on safety scenarios, not classification\n",
        "- **This enables multi-domain evaluation** - Different agents on different domains\n",
        "\n",
        "**What to Focus On:**\n",
        "- **Orchestrator coordination** - Matching agents to appropriate scenarios\n",
        "- **Multi-dimensional routing** - How orchestrators route work\n",
        "- **This is core orchestrator skill** - Coordinating heterogeneous agents\n",
        "\n",
        "---\n",
        "\n",
        "### Change 3: Multi-File Data Ingestion (Supports Multi-Domain)\n",
        "\n",
        "**File:** `nodes/data_ingestion_node.py`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "def data_ingestion_node(state: EaaSState) -> EaaSState:\n",
        "    test_data_path = state.get(\"test_data_path\")  # Single file\n",
        "    # Load single file\n",
        "    test_scenarios = json.loads(json_content)\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "def data_ingestion_node(state: EaaSState) -> EaaSState:\n",
        "    # Support both single file and multiple files\n",
        "    test_data_path = state.get(\"test_data_path\")\n",
        "    test_data_paths = state.get(\"test_data_paths\", [])\n",
        "    \n",
        "    # Load all scenario files\n",
        "    all_scenarios = []\n",
        "    for file_path in files_to_load:\n",
        "        scenarios = _load_scenario_file(file_path)\n",
        "        all_scenarios.extend(scenarios)\n",
        "```\n",
        "\n",
        "**Impact:**\n",
        "- **Enables multi-domain evaluation** - Can load classification + safety scenarios\n",
        "- **Supports orchestrator coordination** - Different scenarios for different agents\n",
        "- **Foundation for scaling** - Can add more scenario types easily\n",
        "\n",
        "**What to Focus On:**\n",
        "- **Data ingestion patterns** - How to load multiple data sources\n",
        "- **State design** - How to structure multi-domain data\n",
        "- **This supports orchestrator coordination** - Need multiple data sources\n",
        "\n",
        "---\n",
        "\n",
        "### Change 4: Comment Stripping (Fixes JSON Parsing)\n",
        "\n",
        "**File:** `nodes/data_ingestion_node.py`\n",
        "\n",
        "**Before:**\n",
        "```python\n",
        "# Parse JSON directly (fails on Python comments)\n",
        "return json.loads(json_content)\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```python\n",
        "# Strip Python-style comments before parsing\n",
        "lines = json_content.split('\\n')\n",
        "cleaned_lines = []\n",
        "for line in lines:\n",
        "    # Remove inline comments (preserve strings)\n",
        "    # ... comment stripping logic ...\n",
        "json_content = '\\n'.join(cleaned_lines)\n",
        "return json.loads(json_content)\n",
        "```\n",
        "\n",
        "**Impact:**\n",
        "- **Fixes parsing errors** - Can now load safety_cases.json\n",
        "- **Enables multi-domain evaluation** - Both files load successfully\n",
        "- **Robustness** - Handles real-world data formats\n",
        "\n",
        "**What to Focus On:**\n",
        "- **Data parsing robustness** - Handling edge cases\n",
        "- **This is operational hygiene** - Making system robust\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Impact Analysis: What Made Scores Go Up?\n",
        "\n",
        "### Biggest Impact: Improved Mock Agents (Change 1) ‚≠ê‚≠ê‚≠ê\n",
        "\n",
        "**Agent 001: 40% ‚Üí 90% accuracy (+125%)**\n",
        "\n",
        "**Why:**\n",
        "- Before: Always returned \"positive\" ‚Üí correct on 4/10 scenarios (positive ones)\n",
        "- After: Actually analyzes text ‚Üí correct on 9/10 scenarios\n",
        "- **This is the biggest impact** - Agents now do real work\n",
        "\n",
        "**Learning Value:**\n",
        "- Shows importance of realistic agent execution\n",
        "- Demonstrates how agent quality affects orchestrator insights\n",
        "- Teaches agent execution patterns\n",
        "\n",
        "---\n",
        "\n",
        "### Second Biggest Impact: Orchestrator Coordination (Change 2) ‚≠ê‚≠ê\n",
        "\n",
        "**Agent 002: 0% ‚Üí 80% accuracy (‚àû% improvement!)**\n",
        "\n",
        "**Why:**\n",
        "- Before: Tested on classification scenarios ‚Üí always wrong (returns \"safe\", expects \"positive/negative/neutral\")\n",
        "- After: Tested on safety scenarios ‚Üí correct on 8/10 scenarios\n",
        "- **This enables multi-domain evaluation** - Different agents on different domains\n",
        "\n",
        "**Learning Value:**\n",
        "- Shows orchestrator coordination value\n",
        "- Demonstrates importance of matching agents to scenarios\n",
        "- Teaches multi-domain orchestration patterns\n",
        "\n",
        "---\n",
        "\n",
        "### Supporting Changes: Multi-File Ingestion & Comment Stripping ‚≠ê\n",
        "\n",
        "**Impact:**\n",
        "- Enables the coordination to work\n",
        "- Makes system more robust\n",
        "- Foundation for scaling\n",
        "\n",
        "**Learning Value:**\n",
        "- Data ingestion patterns\n",
        "- System robustness\n",
        "- Supporting infrastructure\n",
        "\n",
        "---\n",
        "\n",
        "## üéì What to Focus On Learning\n",
        "\n",
        "### 1. **Agent Execution Logic** ‚≠ê MOST IMPORTANT\n",
        "\n",
        "**The Pattern:**\n",
        "```python\n",
        "def _run_classification_agent(input_text: str) -> str:\n",
        "    # Analyze input\n",
        "    # Apply logic/rules/model\n",
        "    # Return result\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- This is how orchestrators execute agents\n",
        "- Understanding agent execution = understanding orchestration mechanics\n",
        "- This is the foundation for real agent integration\n",
        "\n",
        "**Key Concepts:**\n",
        "- Input processing\n",
        "- Logic application (keywords, models, APIs)\n",
        "- Output formatting\n",
        "- Error handling\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Orchestrator Coordination** ‚≠ê MOST IMPORTANT\n",
        "\n",
        "**The Pattern:**\n",
        "```python\n",
        "# Match scenarios to agents\n",
        "matching_scenarios = [\n",
        "    scenario for scenario in all_scenarios\n",
        "    if scenario.get(\"task_type\") == agent_type\n",
        "]\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- This is core orchestrator skill\n",
        "- Coordinating heterogeneous agents across domains\n",
        "- This is what makes orchestrators valuable\n",
        "\n",
        "**Key Concepts:**\n",
        "- Agent-scenario matching\n",
        "- Multi-domain coordination\n",
        "- Routing logic\n",
        "- This is orchestrator value creation\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Multi-Dimensional Data Handling** ‚≠ê IMPORTANT\n",
        "\n",
        "**The Pattern:**\n",
        "```python\n",
        "# Load multiple data sources\n",
        "all_scenarios = []\n",
        "for file_path in files_to_load:\n",
        "    scenarios = _load_scenario_file(file_path)\n",
        "    all_scenarios.extend(scenarios)\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- Orchestrators need multiple data sources\n",
        "- Understanding data integration = understanding orchestrator architecture\n",
        "- Foundation for scaling\n",
        "\n",
        "**Key Concepts:**\n",
        "- Multi-source data loading\n",
        "- Data normalization\n",
        "- State structure for multi-domain data\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Insights\n",
        "\n",
        "### 1. **Agent Quality Matters**\n",
        "- Better agents = better orchestrator insights\n",
        "- Realistic execution = realistic patterns\n",
        "- This is why we improved mocks first\n",
        "\n",
        "### 2. **Coordination is Core Orchestrator Skill**\n",
        "- Matching agents to scenarios\n",
        "- Multi-domain evaluation\n",
        "- This is what orchestrators do\n",
        "\n",
        "### 3. **Multi-Dimensional Analysis Requires Multi-Dimensional Data**\n",
        "- Need multiple scenario types\n",
        "- Need multiple agent types\n",
        "- Orchestrators connect them\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ What This Teaches\n",
        "\n",
        "### Orchestrator Architecture:\n",
        "1. **Agent Execution** - How agents process input\n",
        "2. **Coordination** - How orchestrators route work\n",
        "3. **Data Integration** - How to handle multiple data sources\n",
        "4. **Multi-Domain Evaluation** - How to evaluate across domains\n",
        "\n",
        "### Orchestrator Value:\n",
        "1. **Coordination** - Matching agents to appropriate scenarios\n",
        "2. **Multi-Domain** - Evaluating different agent types together\n",
        "3. **Pattern Detection** - Finding insights across domains\n",
        "4. **Scalability** - Can add more agents/scenarios easily\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Score Improvement Summary\n",
        "\n",
        "| Agent | Before | After | Improvement | Reason |\n",
        "|-------|--------|-------|-------------|--------|\n",
        "| Agent 001 | 40% | 90% | +125% | Improved mock agent (analyzes text) |\n",
        "| Agent 002 | 0% | 80% | ‚àû% | Orchestrator coordination (matched to safety scenarios) |\n",
        "\n",
        "**Key Takeaway:**\n",
        "- **Change 1 (Improved Mocks):** Biggest impact on Agent 001\n",
        "- **Change 2 (Coordination):** Biggest impact on Agent 002\n",
        "- **Both changes together:** Enable realistic multi-domain evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What to Focus On Next\n",
        "\n",
        "### Immediate Learning:\n",
        "1. **Agent Execution Patterns** - How agents process input\n",
        "2. **Orchestrator Coordination** - How to match agents to scenarios\n",
        "3. **Multi-Domain Evaluation** - How to evaluate across domains\n",
        "\n",
        "### Next Steps:\n",
        "1. **Real Agent Integration** - Connect to actual agent APIs/functions\n",
        "2. **More Pattern Types** - Detect patterns across domains\n",
        "3. **State Design Evolution** - Refine state based on what we learned\n",
        "\n",
        "---\n",
        "\n",
        "*This walkthrough explains the code changes that enabled orchestrator coordination and improved scores. Focus on agent execution and coordination - these are core orchestrator skills.*\n",
        "\n"
      ],
      "metadata": {
        "id": "IfvkUMnSwYZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Integration Opportunities: Where LLMs Add Value\n",
        "\n",
        "**Current Status:** We've built a working orchestrator **without using LLMs** - this is actually great!\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What We've Built Without LLMs\n",
        "\n",
        "### Current Implementation (No LLMs):\n",
        "1. **Mock Agents** - Keyword-based classification (no LLM)\n",
        "2. **Pattern Detection** - Rule-based analysis (no LLM)\n",
        "3. **Scoring** - Exact match comparison (no LLM)\n",
        "4. **Report Generation** - Template-based markdown (no LLM)\n",
        "\n",
        "### Why This is Actually Good:\n",
        "- **Proves architecture works** - Orchestrator logic is sound\n",
        "- **Faster/cheaper** - No API costs during development\n",
        "- **Easier to debug** - Deterministic behavior\n",
        "- **Foundation is solid** - Can add LLMs where they add value\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Where LLMs Would Add Value\n",
        "\n",
        "### 1. **Scenario Generation** ‚≠ê HIGH VALUE\n",
        "\n",
        "**Current:** Placeholder (skips generation if data provided)\n",
        "\n",
        "**With LLM:**\n",
        "```python\n",
        "def scenario_generation_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"Generate additional test scenarios using LLM\"\"\"\n",
        "    \n",
        "    # Use LLM to generate synthetic scenarios\n",
        "    prompt = f\"\"\"\n",
        "    Generate 5 test scenarios for {agent_type} evaluation.\n",
        "    Include edge cases and diverse examples.\n",
        "    \"\"\"\n",
        "    \n",
        "    scenarios = llm.invoke(prompt)\n",
        "    # Parse and add to evaluation_data\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- Automatically generate test cases\n",
        "- Create edge cases humans might miss\n",
        "- Scale evaluation datasets\n",
        "\n",
        "**When to add:**\n",
        "- When you need more test scenarios\n",
        "- When you want automated test generation\n",
        "- When you want to explore edge cases\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **LLM-as-a-Judge Scoring** ‚≠ê HIGH VALUE\n",
        "\n",
        "**Current:** Exact match comparison (`actual_output == expected_output`)\n",
        "\n",
        "**With LLM:**\n",
        "```python\n",
        "def _score_with_llm_judge(actual: str, expected: str, input_text: str) -> float:\n",
        "    \"\"\"Use LLM to judge if output is correct\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    Evaluate if the agent output is correct:\n",
        "    Input: {input_text}\n",
        "    Expected: {expected}\n",
        "    Actual: {actual}\n",
        "    \n",
        "    Score: 0.0 to 1.0\n",
        "    \"\"\"\n",
        "    \n",
        "    score = llm.invoke(prompt)\n",
        "    return float(score)\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- Handles fuzzy matches (e.g., \"positive\" vs \"very positive\")\n",
        "- Evaluates quality, not just correctness\n",
        "- Can score subjective metrics (tone, safety, helpfulness)\n",
        "\n",
        "**When to add:**\n",
        "- When exact match is too strict\n",
        "- When you need quality evaluation\n",
        "- When outputs are subjective\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Pattern Analysis & Insights** ‚≠ê MEDIUM VALUE\n",
        "\n",
        "**Current:** Rule-based pattern detection (finds failures, groups by type)\n",
        "\n",
        "**With LLM:**\n",
        "```python\n",
        "def _analyze_patterns_with_llm(failure_analysis: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Use LLM to analyze patterns and generate insights\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    Analyze these failure patterns:\n",
        "    {failure_analysis}\n",
        "    \n",
        "    Generate insights:\n",
        "    - Root causes\n",
        "    - Recommendations\n",
        "    - Strategic implications\n",
        "    \"\"\"\n",
        "    \n",
        "    insights = llm.invoke(prompt)\n",
        "    return insights\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- Deeper pattern analysis\n",
        "- Natural language insights\n",
        "- Strategic recommendations\n",
        "\n",
        "**When to add:**\n",
        "- When you want deeper analysis\n",
        "- When you need natural language insights\n",
        "- When patterns are complex\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Report Enhancement** ‚≠ê MEDIUM VALUE\n",
        "\n",
        "**Current:** Template-based markdown report\n",
        "\n",
        "**With LLM:**\n",
        "```python\n",
        "def report_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"Generate enhanced report with LLM\"\"\"\n",
        "    \n",
        "    # Generate executive summary\n",
        "    summary = llm.invoke(f\"\"\"\n",
        "    Summarize these evaluation results for executives:\n",
        "    {scores}\n",
        "    {failure_analysis}\n",
        "    \"\"\")\n",
        "    \n",
        "    # Add to report\n",
        "    report = template.render(summary=summary, ...)\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- Natural language summaries\n",
        "- Executive-friendly insights\n",
        "- Contextual recommendations\n",
        "\n",
        "**When to add:**\n",
        "- When you need executive reports\n",
        "- When you want natural language summaries\n",
        "- When reports need to be more readable\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Real Agent Integration** ‚≠ê HIGH VALUE (Future)\n",
        "\n",
        "**Current:** Mock agents (keyword-based)\n",
        "\n",
        "**With LLM:**\n",
        "```python\n",
        "def _run_classification_agent(input_text: str) -> str:\n",
        "    \"\"\"Use actual LLM for classification\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    Classify the sentiment of this text:\n",
        "    {input_text}\n",
        "    \n",
        "    Options: positive, negative, neutral\n",
        "    \"\"\"\n",
        "    \n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n",
        "```\n",
        "\n",
        "**Value:**\n",
        "- Real agent evaluation\n",
        "- Production-quality agents\n",
        "- Actual performance metrics\n",
        "\n",
        "**When to add:**\n",
        "- When you want to evaluate real agents\n",
        "- When moving to production\n",
        "- When you need accurate metrics\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Learning Path: When to Add LLMs\n",
        "\n",
        "### Phase 1: Architecture (Current) ‚úÖ\n",
        "- **No LLMs** - Build orchestrator architecture\n",
        "- **Prove it works** - Pattern detection, coordination\n",
        "- **Solid foundation** - Can add LLMs later\n",
        "\n",
        "### Phase 2: Enhancement (Next)\n",
        "- **Add LLMs selectively** - Where they add value\n",
        "- **Start with scenario generation** - High value, low risk\n",
        "- **Then LLM-as-a-judge** - Improves scoring quality\n",
        "\n",
        "### Phase 3: Production\n",
        "- **Real agent integration** - Evaluate actual LLM agents\n",
        "- **Enhanced reporting** - Natural language insights\n",
        "- **Full LLM orchestration** - LLMs coordinating LLMs\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Insight: LLMs Are Tools, Not Requirements\n",
        "\n",
        "### What We've Learned:\n",
        "- **Orchestrator architecture works without LLMs**\n",
        "- **Coordination logic is independent of LLMs**\n",
        "- **Pattern detection can be rule-based**\n",
        "- **LLMs add value, but aren't required**\n",
        "\n",
        "### When to Use LLMs:\n",
        "- **When they add clear value** - Not just because we can\n",
        "- **When rule-based isn't enough** - Fuzzy matching, quality evaluation\n",
        "- **When you need natural language** - Summaries, insights, recommendations\n",
        "\n",
        "### When NOT to Use LLMs:\n",
        "- **When rule-based works** - Exact matching, simple patterns\n",
        "- **When cost matters** - LLMs are expensive\n",
        "- **When determinism matters** - LLMs are non-deterministic\n",
        "- **When speed matters** - LLMs are slow\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Recommended LLM Integration Order\n",
        "\n",
        "### 1. **Scenario Generation** (First)\n",
        "- **Why:** High value, low risk, doesn't affect core logic\n",
        "- **Impact:** Automatically generate test cases\n",
        "- **Complexity:** Low - isolated feature\n",
        "\n",
        "### 2. **LLM-as-a-Judge** (Second)\n",
        "- **Why:** Improves scoring quality significantly\n",
        "- **Impact:** Better evaluation accuracy\n",
        "- **Complexity:** Medium - affects scoring logic\n",
        "\n",
        "### 3. **Pattern Analysis** (Third)\n",
        "- **Why:** Adds deeper insights\n",
        "- **Impact:** Better orchestrator insights\n",
        "- **Complexity:** Medium - enhances existing patterns\n",
        "\n",
        "### 4. **Real Agent Integration** (Fourth)\n",
        "- **Why:** Evaluate actual agents\n",
        "- **Impact:** Production-ready evaluation\n",
        "- **Complexity:** High - major architecture change\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Current vs. LLM-Enhanced\n",
        "\n",
        "| Feature | Current (No LLM) | With LLM | Value Added |\n",
        "|---------|------------------|----------|-------------|\n",
        "| **Scenario Generation** | Skip if data provided | Generate synthetic scenarios | ‚≠ê‚≠ê‚≠ê High |\n",
        "| **Scoring** | Exact match | Fuzzy match, quality evaluation | ‚≠ê‚≠ê‚≠ê High |\n",
        "| **Pattern Detection** | Rule-based | Deeper analysis, insights | ‚≠ê‚≠ê Medium |\n",
        "| **Report Generation** | Template-based | Natural language summaries | ‚≠ê‚≠ê Medium |\n",
        "| **Agent Execution** | Mock (keywords) | Real LLM agents | ‚≠ê‚≠ê‚≠ê High (future) |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What to Focus On\n",
        "\n",
        "### Current Phase (No LLMs):\n",
        "- ‚úÖ **Orchestrator architecture** - Proven it works\n",
        "- ‚úÖ **Coordination logic** - Matching agents to scenarios\n",
        "- ‚úÖ **Pattern detection** - Rule-based analysis\n",
        "- ‚úÖ **Multi-domain evaluation** - Different agent types\n",
        "\n",
        "### Next Phase (Add LLMs Selectively):\n",
        "- **Scenario generation** - Automate test case creation\n",
        "- **LLM-as-a-judge** - Improve scoring quality\n",
        "- **Enhanced insights** - Deeper pattern analysis\n",
        "\n",
        "### Key Learning:\n",
        "- **LLMs are enhancements, not requirements**\n",
        "- **Architecture works without LLMs**\n",
        "- **Add LLMs where they add clear value**\n",
        "- **Don't use LLMs just because you can**\n",
        "\n",
        "---\n",
        "\n",
        "## üí≠ Strategic Question\n",
        "\n",
        "**Should we add LLMs now or continue learning architecture?**\n",
        "\n",
        "### Option A: Continue Learning Architecture (Recommended)\n",
        "- **Why:** Solidify orchestrator fundamentals\n",
        "- **Focus:** Coordination, pattern detection, state design\n",
        "- **Add LLMs later:** When you understand where they add value\n",
        "\n",
        "### Option B: Add LLMs Now\n",
        "- **Why:** See LLM integration patterns\n",
        "- **Focus:** Scenario generation or LLM-as-a-judge\n",
        "- **Risk:** Might distract from orchestrator fundamentals\n",
        "\n",
        "**My Recommendation:** Continue learning architecture first, then add LLMs selectively where they add clear value.\n",
        "\n",
        "---\n",
        "\n",
        "*This document explains where LLMs would add value and when to integrate them. The key insight: we've built a working orchestrator without LLMs, proving the architecture is sound.*\n",
        "\n"
      ],
      "metadata": {
        "id": "rjWJyH_Qx_q2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeUODrRvwUn3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}