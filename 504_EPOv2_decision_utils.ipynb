{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPIhDiARwN0J5SCBkg4vcg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/504_EPOv2_decision_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent ‚Äî this node is **structurally sound** and very much in line with the architecture you‚Äôve been building. I‚Äôll review it the same way we‚Äôve been doing the others: crisp, practical, and focused on correctness + future resilience.\n",
        "\n",
        "I‚Äôll cover:\n",
        "\n",
        "1. ‚úÖ What this node already does *very well*\n",
        "2. üîç One subtle but important state-management issue\n",
        "3. ‚ö†Ô∏è Two small fixes that will prevent future bugs\n",
        "4. üß† Why this node completes your ‚Äúanalysis ‚Üí decision‚Äù contract cleanly\n",
        "\n",
        "No rewrites yet ‚Äî just review and guidance while you‚Äôre mid-revision.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ‚úÖ What This Node Already Nails\n",
        "\n",
        "### üîπ Correct separation of concerns\n",
        "\n",
        "This node **does not re-analyze data** and **does not compute stats**. It strictly:\n",
        "\n",
        "* Consumes analysis\n",
        "* Applies policy\n",
        "* Emits decisions\n",
        "\n",
        "That‚Äôs exactly right.\n",
        "\n",
        "You avoided the common mistake of letting ‚Äúdecision logic creep into analysis logic.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Proper merge of calculated + loaded analyses\n",
        "\n",
        "This is *very good*:\n",
        "\n",
        "```python\n",
        "analysis_lookup = state.get(\"analysis_lookup\", {})\n",
        "calculated_analyses = state.get(\"calculated_analyses\", [])\n",
        "\n",
        "for analysis in calculated_analyses:\n",
        "    exp_id = analysis.get(\"experiment_id\")\n",
        "    if exp_id:\n",
        "        analysis_lookup[exp_id] = analysis\n",
        "```\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* Fresh calculations override stale stored analysis\n",
        "* Downstream nodes see a unified view\n",
        "\n",
        "This is **state-centric orchestration done right**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Symmetric handling of single vs portfolio scope\n",
        "\n",
        "Both branches behave consistently:\n",
        "\n",
        "| Scope     | Behavior                              |\n",
        "| --------- | ------------------------------------- |\n",
        "| Single    | Evaluate exactly one experiment       |\n",
        "| Portfolio | Evaluate only those needing decisions |\n",
        "| Fallback  | Safe default for partial pipelines    |\n",
        "\n",
        "This symmetry makes the system predictable.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. üîç Subtle but Important State Issue\n",
        "\n",
        "### ‚ùó You are mutating `analysis_lookup` in-place\n",
        "\n",
        "This line is the culprit:\n",
        "\n",
        "```python\n",
        "analysis_lookup = state.get(\"analysis_lookup\", {})\n",
        "```\n",
        "\n",
        "If `analysis_lookup` is a **reference** to the state object (which it usually is), you are mutating shared state without explicitly returning it.\n",
        "\n",
        "Right now this *works* because:\n",
        "\n",
        "* You don‚Äôt rely on the original `analysis_lookup` later\n",
        "* Python dicts are mutable\n",
        "\n",
        "But this can cause **silent coupling** between nodes later.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Recommended (safe) pattern\n",
        "\n",
        "Make the merge explicit and return it:\n",
        "\n",
        "```python\n",
        "analysis_lookup = dict(state.get(\"analysis_lookup\", {}))\n",
        "\n",
        "for analysis in calculated_analyses:\n",
        "    exp_id = analysis.get(\"experiment_id\")\n",
        "    if exp_id:\n",
        "        analysis_lookup[exp_id] = analysis\n",
        "```\n",
        "\n",
        "And then **return it**:\n",
        "\n",
        "```python\n",
        "return {\n",
        "    \"analysis_lookup\": analysis_lookup,\n",
        "    \"generated_decisions\": generated_decisions,\n",
        "    \"errors\": errors\n",
        "}\n",
        "```\n",
        "\n",
        "This keeps your orchestrator **pure, explicit, and testable**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ‚ö†Ô∏è Two Small Fixes You Should Make\n",
        "\n",
        "These are easy and worth doing now.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó Fix 1: Require `analysis_lookup` explicitly\n",
        "\n",
        "Right now you check:\n",
        "\n",
        "```python\n",
        "if not definitions_lookup:\n",
        "    return error\n",
        "```\n",
        "\n",
        "But you don‚Äôt check for `analysis_lookup`.\n",
        "\n",
        "In single-experiment mode, this can silently do nothing if:\n",
        "\n",
        "* Definitions exist\n",
        "* Analysis is missing\n",
        "\n",
        "#### ‚úÖ Add this guard:\n",
        "\n",
        "```python\n",
        "if not analysis_lookup and not calculated_analyses:\n",
        "    return {\n",
        "        \"errors\": errors + [\"decision_evaluation_node: analysis required. Run statistical_analysis_node first.\"]\n",
        "    }\n",
        "```\n",
        "\n",
        "This prevents ‚Äúempty success.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó Fix 2: Align error messages with other nodes\n",
        "\n",
        "Most of your nodes follow this pattern:\n",
        "\n",
        "```python\n",
        "\"<node_name>: <clear actionable message>\"\n",
        "```\n",
        "\n",
        "This one already mostly does ‚Äî just make it consistent everywhere.\n",
        "\n",
        "Example tweak:\n",
        "\n",
        "```python\n",
        "\"decision_evaluation_node: definitions_lookup required. Run data_loading_node first.\"\n",
        "```\n",
        "\n",
        "is perfect.\n",
        "Apply the same clarity for missing analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. üß† Why This Node Completes the System\n",
        "\n",
        "This node is where your architecture **crosses the line** from:\n",
        "\n",
        "> ‚ÄúA data analysis pipeline‚Äù\n",
        "\n",
        "to\n",
        "\n",
        "> **‚ÄúAn organizational decision system‚Äù**\n",
        "\n",
        "Because now you have:\n",
        "\n",
        "* Evidence ‚Üí Analysis\n",
        "* Analysis ‚Üí Policy\n",
        "* Policy ‚Üí Action\n",
        "* Action ‚Üí Review date\n",
        "* Review date ‚Üí Reversal triggers\n",
        "\n",
        "That‚Äôs a *full governance loop*.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Verdict\n",
        "\n",
        "### This node is:\n",
        "\n",
        "* ‚úÖ Correctly scoped\n",
        "* ‚úÖ State-aware\n",
        "* ‚úÖ Policy-driven\n",
        "* ‚úÖ Deterministic\n",
        "* ‚úÖ CEO-defensible\n",
        "\n",
        "And with **two small safety tweaks**, it becomes production-grade.\n"
      ],
      "metadata": {
        "id": "s7PelEzuki1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEVPNoLFhHGL"
      },
      "outputs": [],
      "source": [
        "\"\"\"Decision Evaluation Utilities for Experimentation Portfolio Orchestrator\n",
        "\n",
        "Functions to evaluate experiments and generate decision recommendations\n",
        "based on statistical analysis results and decision policies.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "def evaluate_decision_confidence(\n",
        "    analysis: Dict[str, Any],\n",
        "    p_value_threshold: float = 0.05\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Evaluate decision confidence based on statistical analysis.\n",
        "\n",
        "    Args:\n",
        "        analysis: Experiment analysis result\n",
        "        p_value_threshold: P-value threshold for significance\n",
        "\n",
        "    Returns:\n",
        "        \"high\", \"medium\", or \"low\"\n",
        "    \"\"\"\n",
        "    statistical_test = analysis.get(\"statistical_test\", {})\n",
        "    p_value = statistical_test.get(\"p_value\")\n",
        "\n",
        "    if p_value is None:\n",
        "        return \"low\"\n",
        "\n",
        "    if p_value < 0.01:\n",
        "        return \"high\"\n",
        "    elif p_value < p_value_threshold:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"low\"\n",
        "\n",
        "\n",
        "def evaluate_decision_risk(\n",
        "    definition: Dict[str, Any],\n",
        "    analysis: Dict[str, Any],\n",
        "    portfolio_entry: Optional[Dict[str, Any]] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Evaluate decision risk based on experiment characteristics and results.\n",
        "\n",
        "    Args:\n",
        "        definition: Experiment definition\n",
        "        analysis: Experiment analysis result\n",
        "        portfolio_entry: Optional portfolio entry for additional context\n",
        "\n",
        "    Returns:\n",
        "        \"low\", \"medium\", or \"high\"\n",
        "    \"\"\"\n",
        "    # Start with risk tier from portfolio\n",
        "    risk_tier = portfolio_entry.get(\"risk_tier\", \"medium\") if portfolio_entry else \"medium\"\n",
        "\n",
        "    # Check guardrails\n",
        "    guardrails_passed = analysis.get(\"guardrails_passed\", True)\n",
        "    if not guardrails_passed:\n",
        "        return \"high\"\n",
        "\n",
        "    # Check for data quality flags\n",
        "    metrics = analysis.get(\"metrics\", [])\n",
        "    has_quality_flags = any(\n",
        "        metric.get(\"data_quality_flags\", [])\n",
        "        for metric in metrics\n",
        "    )\n",
        "    if has_quality_flags:\n",
        "        if risk_tier == \"low\":\n",
        "            risk_tier = \"medium\"\n",
        "        else:\n",
        "            risk_tier = \"high\"\n",
        "\n",
        "    # Check segment consistency\n",
        "    segment_consistency = analysis.get(\"segment_consistency\", \"consistent\")\n",
        "    if segment_consistency != \"consistent\":\n",
        "        if risk_tier == \"low\":\n",
        "            risk_tier = \"medium\"\n",
        "\n",
        "    # Check statistical significance\n",
        "    statistical_test = analysis.get(\"statistical_test\", {})\n",
        "    is_significant = statistical_test.get(\"is_statistically_significant\", False) or statistical_test.get(\"is_significant\", False)\n",
        "    if not is_significant and risk_tier == \"low\":\n",
        "        risk_tier = \"medium\"\n",
        "\n",
        "    return risk_tier\n",
        "\n",
        "\n",
        "def determine_decision(\n",
        "    analysis: Dict[str, Any],\n",
        "    definition: Dict[str, Any],\n",
        "    config: Any,\n",
        "    decision_signal: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Determine decision recommendation based on analysis and config thresholds.\n",
        "\n",
        "    Args:\n",
        "        analysis: Experiment analysis result\n",
        "        definition: Experiment definition\n",
        "        config: Config with decision thresholds\n",
        "        decision_signal: Optional pre-calculated decision signal from analysis\n",
        "\n",
        "    Returns:\n",
        "        \"scale\", \"iterate\", \"retire\", \"pause\", or \"do_not_start\"\n",
        "    \"\"\"\n",
        "    # Use decision_signal from analysis if available\n",
        "    if decision_signal:\n",
        "        if decision_signal == \"strong_scale\":\n",
        "            return \"scale\"\n",
        "        elif decision_signal == \"cautious_scale\":\n",
        "            return \"iterate\"  # Cautious scale = iterate first\n",
        "        elif decision_signal == \"iterate\":\n",
        "            return \"iterate\"\n",
        "        elif decision_signal == \"retire\":\n",
        "            return \"retire\"\n",
        "\n",
        "    # Fallback to rule-based decision\n",
        "    status = definition.get(\"status\", \"unknown\")\n",
        "\n",
        "    # Planned experiments that shouldn't start\n",
        "    if status == \"planned\":\n",
        "        risk_notes = definition.get(\"risk_notes\", \"\").lower()\n",
        "        if \"bias\" in risk_notes or \"compliance\" in risk_notes or \"regulatory\" in risk_notes:\n",
        "            return \"do_not_start\"\n",
        "\n",
        "    # Get lift metrics\n",
        "    relative_lift_percent = analysis.get(\"relative_lift_percent\")\n",
        "    if relative_lift_percent is None:\n",
        "        relative_lift_percent = analysis.get(\"relative_change_percent\", 0)\n",
        "        # For decrease metrics, make it positive\n",
        "        if definition.get(\"expected_direction\") == \"decrease\":\n",
        "            relative_lift_percent = abs(relative_lift_percent)\n",
        "\n",
        "    meets_minimum_effect = analysis.get(\"meets_minimum_effect\", False)\n",
        "    statistical_test = analysis.get(\"statistical_test\", {})\n",
        "    is_significant = statistical_test.get(\"is_statistically_significant\", False) or statistical_test.get(\"is_significant\", False)\n",
        "\n",
        "    # Decision logic\n",
        "    if not meets_minimum_effect:\n",
        "        return \"retire\"\n",
        "\n",
        "    if is_significant and relative_lift_percent >= config.scale_threshold_lift:\n",
        "        return \"scale\"\n",
        "    elif is_significant and relative_lift_percent >= config.iterate_threshold_lift:\n",
        "        return \"iterate\"\n",
        "    elif relative_lift_percent < config.retire_threshold_lift:\n",
        "        return \"retire\"\n",
        "    else:\n",
        "        return \"iterate\"\n",
        "\n",
        "\n",
        "def generate_decision_rationale(\n",
        "    analysis: Dict[str, Any],\n",
        "    definition: Dict[str, Any],\n",
        "    decision: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate human-readable rationale for decision.\n",
        "\n",
        "    Args:\n",
        "        analysis: Experiment analysis result\n",
        "        definition: Experiment definition\n",
        "        decision: Decision recommendation\n",
        "\n",
        "    Returns:\n",
        "        Rationale string\n",
        "    \"\"\"\n",
        "    primary_metric = analysis.get(\"primary_metric\", \"metric\")\n",
        "\n",
        "    if decision == \"scale\":\n",
        "        lift = analysis.get(\"relative_lift_percent\", 0)\n",
        "        p_value = analysis.get(\"statistical_test\", {}).get(\"p_value\")\n",
        "        if p_value:\n",
        "            return (\n",
        "                f\"{primary_metric} improved by {lift:.1f}% with statistical significance \"\n",
        "                f\"(p={p_value:.4f}). Effect exceeds minimum threshold and meets scale criteria.\"\n",
        "            )\n",
        "        else:\n",
        "            return f\"{primary_metric} improved by {lift:.1f}%. Effect exceeds minimum threshold.\"\n",
        "\n",
        "    elif decision == \"iterate\":\n",
        "        lift = analysis.get(\"relative_lift_percent\", 0)\n",
        "        p_value = analysis.get(\"statistical_test\", {}).get(\"p_value\")\n",
        "        if p_value and p_value >= 0.05:\n",
        "            return (\n",
        "                f\"{primary_metric} improved by {lift:.1f}% but statistical significance is uncertain \"\n",
        "                f\"(p={p_value:.4f}). Continue experiment with refinements to increase confidence.\"\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                f\"{primary_metric} improved by {lift:.1f}% but effect size is below scale threshold. \"\n",
        "                f\"Continue experiment with optimizations.\"\n",
        "            )\n",
        "\n",
        "    elif decision == \"retire\":\n",
        "        lift = analysis.get(\"relative_lift_percent\", 0)\n",
        "        return (\n",
        "            f\"{primary_metric} change ({lift:.1f}%) does not meet minimum effect threshold. \"\n",
        "            f\"Experiment should be retired.\"\n",
        "        )\n",
        "\n",
        "    elif decision == \"do_not_start\":\n",
        "        risk_notes = definition.get(\"risk_notes\", \"\")\n",
        "        return (\n",
        "            f\"Experiment design presents elevated risk: {risk_notes}. \"\n",
        "            f\"Refine design and add guardrails before proceeding.\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        return \"Decision evaluation completed.\"\n",
        "\n",
        "\n",
        "def generate_recommended_action(\n",
        "    decision: str,\n",
        "    definition: Dict[str, Any],\n",
        "    analysis: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate recommended action based on decision.\n",
        "\n",
        "    Args:\n",
        "        decision: Decision recommendation\n",
        "        definition: Experiment definition\n",
        "        analysis: Experiment analysis result\n",
        "\n",
        "    Returns:\n",
        "        Recommended action string\n",
        "    \"\"\"\n",
        "    experiment_name = definition.get(\"hypothesis\", \"experiment\")\n",
        "\n",
        "    if decision == \"scale\":\n",
        "        return f\"Roll out {experiment_name} to full population.\"\n",
        "    elif decision == \"iterate\":\n",
        "        return f\"Continue experiment with refinements and expanded monitoring.\"\n",
        "    elif decision == \"retire\":\n",
        "        return f\"End experiment and document learnings.\"\n",
        "    elif decision == \"do_not_start\":\n",
        "        return f\"Refine hypothesis, add guardrails, and re-submit for review.\"\n",
        "    else:\n",
        "        return \"Review experiment status and determine next steps.\"\n",
        "\n",
        "\n",
        "def estimate_expected_impact(\n",
        "    analysis: Dict[str, Any],\n",
        "    definition: Dict[str, Any],\n",
        "    portfolio_entry: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimate expected impact for decision.\n",
        "\n",
        "    Args:\n",
        "        analysis: Experiment analysis result\n",
        "        definition: Experiment definition\n",
        "        portfolio_entry: Optional portfolio entry\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with expected impact estimates\n",
        "    \"\"\"\n",
        "    primary_metric = analysis.get(\"primary_metric\")\n",
        "    relative_lift_percent = analysis.get(\"relative_lift_percent\", 0)\n",
        "\n",
        "    # Conservative estimate: use 70% of observed lift for scaling\n",
        "    estimated_lift_percent = relative_lift_percent * 0.7 if relative_lift_percent > 0 else relative_lift_percent\n",
        "\n",
        "    # Annual value would need business context - placeholder for now\n",
        "    annual_value_usd = None\n",
        "\n",
        "    return {\n",
        "        \"kpi\": primary_metric,\n",
        "        \"estimated_lift_percent\": round(estimated_lift_percent, 1) if estimated_lift_percent else None,\n",
        "        \"annual_value_usd\": annual_value_usd\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_reversal_triggers(\n",
        "    decision: str,\n",
        "    definition: Dict[str, Any],\n",
        "    analysis: Dict[str, Any]\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Generate reversal triggers for decision.\n",
        "\n",
        "    Args:\n",
        "        decision: Decision recommendation\n",
        "        definition: Experiment definition\n",
        "        analysis: Experiment analysis result\n",
        "\n",
        "    Returns:\n",
        "        List of reversal trigger strings\n",
        "    \"\"\"\n",
        "    primary_metric = analysis.get(\"primary_metric\", \"metric\")\n",
        "    guardrail_metrics = definition.get(\"guardrail_metrics\", [])\n",
        "\n",
        "    triggers = []\n",
        "\n",
        "    if decision == \"scale\":\n",
        "        # If scaling, monitor primary metric\n",
        "        triggers.append(f\"{primary_metric} falls below control baseline for two consecutive weeks\")\n",
        "\n",
        "        # Add guardrail triggers\n",
        "        for guardrail in guardrail_metrics:\n",
        "            triggers.append(f\"{guardrail} degrades below acceptable threshold\")\n",
        "\n",
        "    elif decision == \"iterate\":\n",
        "        # If iterating, monitor for degradation\n",
        "        triggers.append(f\"{primary_metric} shows negative trend\")\n",
        "        for guardrail in guardrail_metrics:\n",
        "            triggers.append(f\"{guardrail} drops below baseline\")\n",
        "\n",
        "    return triggers\n",
        "\n",
        "\n",
        "def evaluate_experiment_decision(\n",
        "    experiment_id: str,\n",
        "    definition: Dict[str, Any],\n",
        "    analysis: Dict[str, Any],\n",
        "    portfolio_entry: Optional[Dict[str, Any]],\n",
        "    config: Any\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluate experiment and generate complete decision recommendation.\n",
        "\n",
        "    Args:\n",
        "        experiment_id: Experiment ID\n",
        "        definition: Experiment definition\n",
        "        analysis: Experiment analysis result\n",
        "        portfolio_entry: Optional portfolio entry\n",
        "        config: Config with thresholds\n",
        "\n",
        "    Returns:\n",
        "        Complete decision dictionary\n",
        "    \"\"\"\n",
        "    # Get decision signal from analysis if available\n",
        "    decision_signal = analysis.get(\"decision_signal\")\n",
        "\n",
        "    # Determine decision\n",
        "    decision = determine_decision(analysis, definition, config, decision_signal)\n",
        "\n",
        "    # Evaluate confidence and risk\n",
        "    decision_confidence = evaluate_decision_confidence(analysis, config.confidence_threshold)\n",
        "    decision_risk = evaluate_decision_risk(definition, analysis, portfolio_entry)\n",
        "\n",
        "    # Generate rationale and action\n",
        "    rationale = generate_decision_rationale(analysis, definition, decision)\n",
        "    recommended_action = generate_recommended_action(decision, definition, analysis)\n",
        "\n",
        "    # Get decision owner from definition\n",
        "    decision_owner = definition.get(\"decision_owner\", \"unknown\")\n",
        "\n",
        "    # Estimate impact\n",
        "    expected_impact = estimate_expected_impact(analysis, definition, portfolio_entry)\n",
        "\n",
        "    # Generate reversal triggers\n",
        "    reversal_triggers = generate_reversal_triggers(decision, definition, analysis)\n",
        "\n",
        "    # Calculate next review date (30 days from now for scale, 60 for iterate, etc.)\n",
        "    days_until_review = {\n",
        "        \"scale\": 30,\n",
        "        \"iterate\": 60,\n",
        "        \"retire\": 90,\n",
        "        \"do_not_start\": 90,\n",
        "        \"pause\": 30\n",
        "    }.get(decision, 60)\n",
        "\n",
        "    next_review_date = (datetime.now() + timedelta(days=days_until_review)).strftime(\"%Y-%m-%d\")\n",
        "    decision_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    return {\n",
        "        \"experiment_id\": experiment_id,\n",
        "        \"decision\": decision,\n",
        "        \"decision_confidence\": decision_confidence,\n",
        "        \"decision_risk\": decision_risk,\n",
        "        \"rationale\": rationale,\n",
        "        \"recommended_action\": recommended_action,\n",
        "        \"decision_owner\": decision_owner,\n",
        "        \"expected_impact\": expected_impact,\n",
        "        \"reversal_triggers\": reversal_triggers,\n",
        "        \"next_review_date\": next_review_date,\n",
        "        \"decision_date\": decision_date\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_experiments_needing_decisions(\n",
        "    analyzed_experiments: list,\n",
        "    definitions_lookup: Dict[str, Dict[str, Any]],\n",
        "    analysis_lookup: Dict[str, Dict[str, Any]],\n",
        "    portfolio_lookup: Dict[str, Dict[str, Any]],\n",
        "    decisions_lookup: Dict[str, Dict[str, Any]],\n",
        "    config: Any\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Evaluate experiments that need decision recommendations.\n",
        "\n",
        "    Args:\n",
        "        analyzed_experiments: List of experiment status analyses\n",
        "        definitions_lookup: Definitions lookup\n",
        "        analysis_lookup: Analysis lookup (includes calculated_analyses)\n",
        "        portfolio_lookup: Portfolio lookup\n",
        "        decisions_lookup: Existing decisions lookup\n",
        "        config: Config with thresholds\n",
        "\n",
        "    Returns:\n",
        "        List of newly generated decisions\n",
        "    \"\"\"\n",
        "    generated_decisions = []\n",
        "\n",
        "    for exp_status in analyzed_experiments:\n",
        "        if not exp_status.get(\"needs_decision\", False):\n",
        "            continue\n",
        "\n",
        "        experiment_id = exp_status.get(\"experiment_id\")\n",
        "        if not experiment_id:\n",
        "            continue\n",
        "\n",
        "        # Skip if decision already exists\n",
        "        if experiment_id in decisions_lookup:\n",
        "            continue\n",
        "\n",
        "        definition = definitions_lookup.get(experiment_id)\n",
        "        analysis = analysis_lookup.get(experiment_id)\n",
        "        portfolio_entry = portfolio_lookup.get(experiment_id)\n",
        "\n",
        "        if not definition or not analysis:\n",
        "            continue\n",
        "\n",
        "        # Generate decision\n",
        "        decision = evaluate_experiment_decision(\n",
        "            experiment_id=experiment_id,\n",
        "            definition=definition,\n",
        "            analysis=analysis,\n",
        "            portfolio_entry=portfolio_entry,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        generated_decisions.append(decision)\n",
        "\n",
        "    return generated_decisions\n"
      ]
    }
  ]
}