{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMotA8ARP9phX9J/74Oqg/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/367_EFIA_Summarization_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## LLM Summarization\n",
        "\n",
        "### Using AI to Explain Insight — Not Create It\n",
        "\n",
        "This code block defines how **LLMs are used inside the Employee Feedback Intelligence Agent** — and just as importantly, **where they are not used**.\n",
        "\n",
        "The purpose of this layer is simple and tightly scoped:\n",
        "\n",
        "> **Translate structured analysis into clear, executive-ready language.**\n",
        "\n",
        "The LLM never decides priorities, detects patterns, or drives outcomes.\n",
        "It only explains results that have already been determined by transparent, rule-based logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Summarization Is a Separate Layer\n",
        "\n",
        "In many AI systems, analysis and explanation are blended together.\n",
        "That makes results hard to trust and even harder to audit.\n",
        "\n",
        "This agent intentionally separates them:\n",
        "\n",
        "* Rules and statistics determine *what matters*\n",
        "* LLMs explain *what that means* in natural language\n",
        "\n",
        "This preserves:\n",
        "\n",
        "* Accountability\n",
        "* Explainability\n",
        "* Executive confidence\n",
        "\n",
        "---\n",
        "\n",
        "## Department Summaries\n",
        "\n",
        "### Turning Local Feedback Into Clear Insight\n",
        "\n",
        "#### `generate_department_summary`\n",
        "\n",
        "This function produces a short, executive-ready summary for each department.\n",
        "\n",
        "It takes as input:\n",
        "\n",
        "* Actual employee feedback samples\n",
        "* Precomputed sentiment statistics\n",
        "* A known department context\n",
        "\n",
        "The prompt instructs the LLM to:\n",
        "\n",
        "* Focus on patterns, not anecdotes\n",
        "* Highlight sentiment direction\n",
        "* Call out areas needing attention\n",
        "* Stay concise and actionable\n",
        "\n",
        "The result is a **2–3 sentence summary** a leader can absorb in seconds.\n",
        "\n",
        "---\n",
        "\n",
        "### Built-In Safety: Rule-Based Fallback\n",
        "\n",
        "If the LLM fails for any reason:\n",
        "\n",
        "* The system gracefully falls back to a rule-based summary\n",
        "* No part of the workflow breaks\n",
        "* No report section is silently missing\n",
        "\n",
        "This ensures the agent is **operationally reliable**, not brittle.\n",
        "\n",
        "---\n",
        "\n",
        "## Theme Summaries\n",
        "\n",
        "### Explaining Patterns, Not Inventing Them\n",
        "\n",
        "#### `generate_theme_summary`\n",
        "\n",
        "Themes are detected earlier using explicit rules and frequency thresholds.\n",
        "This function simply explains those themes in plain language.\n",
        "\n",
        "Inputs include:\n",
        "\n",
        "* Theme name\n",
        "* Frequency of occurrence\n",
        "* Real employee examples\n",
        "\n",
        "The LLM is asked to:\n",
        "\n",
        "* Describe what employees are saying\n",
        "* Focus on the core issue or opportunity\n",
        "* Avoid speculation or recommendation creep\n",
        "\n",
        "This keeps summaries grounded in evidence.\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "### A CEO-Ready Synthesis\n",
        "\n",
        "#### `generate_executive_summary`\n",
        "\n",
        "This is the highest-level narrative output of the agent.\n",
        "\n",
        "It is intentionally constrained:\n",
        "\n",
        "* Limited to top issues and ideas\n",
        "* Driven by prior prioritization\n",
        "* Anchored in concrete statistics\n",
        "\n",
        "The LLM is guided to:\n",
        "\n",
        "* Describe the overall state\n",
        "* Highlight critical risks\n",
        "* Surface key opportunities\n",
        "* Offer a high-level directional recommendation\n",
        "\n",
        "The result is **clarity, not verbosity**.\n",
        "\n",
        "---\n",
        "\n",
        "## Strong Guardrails on AI Behavior\n",
        "\n",
        "Across all summarization functions:\n",
        "\n",
        "* Temperature is kept low\n",
        "* Token limits are enforced\n",
        "* Prompts are narrowly scoped\n",
        "* Inputs are structured and curated\n",
        "\n",
        "The LLM never sees:\n",
        "\n",
        "* Raw, unfiltered data\n",
        "* Internal scoring logic\n",
        "* Decision thresholds\n",
        "\n",
        "This prevents hallucination, drift, and overreach.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Leaders Can Trust These Summaries\n",
        "\n",
        "From a governance perspective, this design ensures:\n",
        "\n",
        "* AI outputs are explainable\n",
        "* Conclusions are reproducible\n",
        "* Summaries align with underlying data\n",
        "* AI cannot “change the story”\n",
        "\n",
        "If leadership challenges a summary, the system can always point to:\n",
        "\n",
        "* The data\n",
        "* The rules\n",
        "* The prioritization logic\n",
        "\n",
        "The LLM is simply the narrator.\n",
        "\n",
        "---\n",
        "\n",
        "## Architectural Takeaway\n",
        "\n",
        "This summarization layer reflects the core philosophy of the agent:\n",
        "\n",
        "> **The LLM does not decide what the business should do — it explains what the system has already proven.**\n",
        "\n",
        "By isolating AI to a communication role and backing it with deterministic logic and fallbacks, the agent achieves something rare:\n",
        "\n",
        "* Natural language insight\n",
        "* Without sacrificing control\n",
        "\n",
        "That’s how AI earns a seat in executive decision-making.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mw4lkDiGvkbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization Utilities for Employee Feedback Intelligence Agent"
      ],
      "metadata": {
        "id": "ZSmWWnZ8uanv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvdIqMR2uX_V"
      },
      "outputs": [],
      "source": [
        "\"\"\"Summarization Utilities for Employee Feedback Intelligence Agent\n",
        "\n",
        "LLM-based summarization of feedback themes, departments, and categories.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "def generate_department_summary(\n",
        "    department: str,\n",
        "    feedback_entries: List[Dict[str, Any]],\n",
        "    sentiment_summary: Dict[str, Any],\n",
        "    llm_model: str = \"gpt-4o-mini\",\n",
        "    temperature: float = 0.3,\n",
        "    max_tokens: int = 500\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate LLM summary for a department's feedback.\n",
        "\n",
        "    Args:\n",
        "        department: Department name\n",
        "        feedback_entries: List of feedback entries for this department\n",
        "        sentiment_summary: Sentiment summary for this department\n",
        "        llm_model: LLM model to use\n",
        "        temperature: LLM temperature\n",
        "        max_tokens: Max tokens for summary\n",
        "\n",
        "    Returns:\n",
        "        Summary text\n",
        "    \"\"\"\n",
        "    # Prepare feedback text samples (up to 10 entries)\n",
        "    feedback_samples = [e.get(\"free_text_feedback\", \"\") for e in feedback_entries[:10]]\n",
        "    feedback_text = \"\\n\".join(f\"- {sample}\" for sample in feedback_samples)\n",
        "\n",
        "    # Get sentiment info\n",
        "    dept_sentiment = sentiment_summary.get(\"sentiment_by_department\", {}).get(department, {})\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an organizational intelligence analyst.\n",
        "        Your job is to create concise, executive-ready summaries of employee feedback.\n",
        "        Focus on patterns, themes, and actionable insights. Use clear, professional language.\"\"\"),\n",
        "        (\"human\", \"\"\"Summarize the employee feedback for the {department} department.\n",
        "\n",
        "Feedback samples:\n",
        "{feedback_text}\n",
        "\n",
        "Sentiment breakdown:\n",
        "- Positive: {positive_count}\n",
        "- Neutral: {neutral_count}\n",
        "- Negative: {negative_count}\n",
        "\n",
        "Create a 2-3 sentence summary that highlights:\n",
        "1. Main themes or patterns\n",
        "2. Overall sentiment\n",
        "3. Key areas needing attention (if any)\n",
        "\n",
        "Keep it concise and actionable.\"\"\")\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        llm = ChatOpenAI(model=llm_model, temperature=temperature, max_tokens=max_tokens)\n",
        "        chain = prompt | llm\n",
        "        response = chain.invoke({\n",
        "            \"department\": department,\n",
        "            \"feedback_text\": feedback_text,\n",
        "            \"positive_count\": dept_sentiment.get(\"positive\", 0),\n",
        "            \"neutral_count\": dept_sentiment.get(\"neutral\", 0),\n",
        "            \"negative_count\": dept_sentiment.get(\"negative\", 0)\n",
        "        })\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        # Fallback to rule-based summary\n",
        "        return (f\"{department}: {len(feedback_entries)} feedback entries. \"\n",
        "                f\"Sentiment: {dept_sentiment.get('positive', 0)} positive, \"\n",
        "                f\"{dept_sentiment.get('negative', 0)} negative, \"\n",
        "                f\"{dept_sentiment.get('neutral', 0)} neutral.\")\n",
        "\n",
        "\n",
        "def generate_theme_summary(\n",
        "    theme: Dict[str, Any],\n",
        "    feedback_entries: List[Dict[str, Any]],\n",
        "    llm_model: str = \"gpt-4o-mini\",\n",
        "    temperature: float = 0.3,\n",
        "    max_tokens: int = 300\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate LLM summary for a feedback theme.\n",
        "\n",
        "    Args:\n",
        "        theme: Theme dictionary\n",
        "        feedback_entries: List of feedback entries in this theme\n",
        "        llm_model: LLM model to use\n",
        "        temperature: LLM temperature\n",
        "        max_tokens: Max tokens for summary\n",
        "\n",
        "    Returns:\n",
        "        Summary text\n",
        "    \"\"\"\n",
        "    theme_name = theme.get(\"theme_name\", \"Unknown Theme\")\n",
        "    frequency = theme.get(\"frequency\", 0)\n",
        "    examples = theme.get(\"example_feedback\", [])\n",
        "\n",
        "    examples_text = \"\\n\".join(f\"- {ex}\" for ex in examples[:5])\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an organizational intelligence analyst.\n",
        "        Create concise summaries of recurring feedback themes.\"\"\"),\n",
        "        (\"human\", \"\"\"Summarize this feedback theme:\n",
        "\n",
        "Theme: {theme_name}\n",
        "Frequency: {frequency} occurrences\n",
        "\n",
        "Example feedback:\n",
        "{examples_text}\n",
        "\n",
        "Create a 1-2 sentence summary explaining what employees are saying about this theme.\n",
        "Focus on the core issue or opportunity.\"\"\")\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        llm = ChatOpenAI(model=llm_model, temperature=temperature, max_tokens=max_tokens)\n",
        "        chain = prompt | llm\n",
        "        response = chain.invoke({\n",
        "            \"theme_name\": theme_name,\n",
        "            \"frequency\": frequency,\n",
        "            \"examples_text\": examples_text\n",
        "        })\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        # Fallback to rule-based summary\n",
        "        return (f\"{theme_name}: {frequency} occurrences. \"\n",
        "                f\"Employees report issues/ideas related to {theme_name.lower()}.\")\n",
        "\n",
        "\n",
        "def generate_executive_summary(\n",
        "    feedback_summary: Dict[str, Any],\n",
        "    prioritized_issues: List[Dict[str, Any]],\n",
        "    prioritized_ideas: List[Dict[str, Any]],\n",
        "    sentiment_summary: Dict[str, Any],\n",
        "    llm_model: str = \"gpt-4o-mini\",\n",
        "    temperature: float = 0.3,\n",
        "    max_tokens: int = 500\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate executive summary of all feedback.\n",
        "\n",
        "    Args:\n",
        "        feedback_summary: Overall feedback summary\n",
        "        prioritized_issues: Top prioritized issues\n",
        "        prioritized_ideas: Top prioritized ideas\n",
        "        sentiment_summary: Sentiment summary\n",
        "        llm_model: LLM model to use\n",
        "        temperature: LLM temperature\n",
        "        max_tokens: Max tokens for summary\n",
        "\n",
        "    Returns:\n",
        "        Executive summary text\n",
        "    \"\"\"\n",
        "    top_3_issues = [i[\"feedback_text\"][:100] for i in prioritized_issues[:3]]\n",
        "    top_3_ideas = [i[\"feedback_text\"][:100] for i in prioritized_ideas[:3]]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an executive intelligence analyst.\n",
        "        Create a concise, high-level summary for senior leadership.\n",
        "        Focus on patterns, priorities, and actionable insights.\"\"\"),\n",
        "        (\"human\", \"\"\"Create an executive summary of employee feedback analysis.\n",
        "\n",
        "Overall Statistics:\n",
        "- Total feedback: {total_feedback}\n",
        "- Issues: {total_issues}\n",
        "- Ideas: {total_ideas}\n",
        "- Overall sentiment: {overall_sentiment}\n",
        "\n",
        "Top 3 Priority Issues:\n",
        "{top_issues}\n",
        "\n",
        "Top 3 Priority Ideas:\n",
        "{top_ideas}\n",
        "\n",
        "Create a 3-4 sentence executive summary that:\n",
        "1. Highlights the overall state\n",
        "2. Identifies the most critical issues\n",
        "3. Notes key opportunities (ideas)\n",
        "4. Provides a high-level recommendation\n",
        "\n",
        "Keep it concise and CEO-friendly.\"\"\")\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        llm = ChatOpenAI(model=llm_model, temperature=temperature, max_tokens=max_tokens)\n",
        "        chain = prompt | llm\n",
        "        response = chain.invoke({\n",
        "            \"total_feedback\": feedback_summary.get(\"total_feedback\", 0),\n",
        "            \"total_issues\": feedback_summary.get(\"total_issues\", 0),\n",
        "            \"total_ideas\": feedback_summary.get(\"total_ideas\", 0),\n",
        "            \"overall_sentiment\": sentiment_summary.get(\"overall_sentiment\", \"neutral\"),\n",
        "            \"top_issues\": \"\\n\".join(f\"- {issue}\" for issue in top_3_issues),\n",
        "            \"top_ideas\": \"\\n\".join(f\"- {idea}\" for idea in top_3_ideas)\n",
        "        })\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        # Fallback to rule-based summary\n",
        "        return (f\"Analysis of {feedback_summary.get('total_feedback', 0)} feedback entries: \"\n",
        "                f\"{feedback_summary.get('total_issues', 0)} issues and \"\n",
        "                f\"{feedback_summary.get('total_ideas', 0)} ideas. \"\n",
        "                f\"Overall sentiment: {sentiment_summary.get('overall_sentiment', 'neutral')}. \"\n",
        "                f\"Top priority areas identified for leadership attention.\")\n",
        "\n"
      ]
    }
  ]
}