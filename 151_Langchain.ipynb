{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/151_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad2e337e",
      "metadata": {
        "id": "ad2e337e",
        "outputId": "2649b7ce-031b-41b9-860e-ccfd4f87163a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python executable in use: /Users/micahshull/Documents/AI_Agents/LangChain/LC_setup_day_00/.venv/bin/python3\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 ‚Äî Kernel Check - Make sure your notebook is running in .venv:\n",
        "import sys\n",
        "print(\"Python executable in use:\", sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8d943f",
      "metadata": {
        "id": "3a8d943f",
        "outputId": "4cde4b09-8bae-4dd5-ee31-4316a3b93e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from LangChain!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load your API keys\n",
        "load_dotenv(\"API_KEYS.env\")\n",
        "\n",
        "# Initialize model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Test it\n",
        "response = llm.invoke(\"Say hello from LangChain in one sentence.\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9ec6ed",
      "metadata": {
        "id": "5b9ec6ed",
        "outputId": "9e6fb02c-1df7-411b-c2a2-c1fe1a26938f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bonjour, comment √ßa va ?\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Build a prompt template with a placeholder\n",
        "prompt = ChatPromptTemplate.from_template(\"Translate the following English text into French:\\n\\n{text}\")\n",
        "\n",
        "# Combine prompt + model into a simple chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# Run it\n",
        "result = chain.invoke({\"text\": \"Good morning, how are you?\"})\n",
        "print(result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf91d3b",
      "metadata": {
        "id": "ddf91d3b"
      },
      "source": [
        "\n",
        "## üß† Why `ChatPromptTemplate` is More Flexible Than Hardcoding\n",
        "\n",
        "Hardcoding a prompt might look like this:\n",
        "\n",
        "```python\n",
        "response = llm.invoke(\"Translate this: Hello!\")\n",
        "```\n",
        "\n",
        "Seems fine, right? But the moment your project grows, that starts to cause problems:\n",
        "\n",
        "| Problem         | Hardcoded Prompt                 | ChatPromptTemplate                          |\n",
        "| --------------- | -------------------------------- | ------------------------------------------- |\n",
        "| Reusability     | ‚ùå Duplicated strings all over    | ‚úÖ One place, used many times                |\n",
        "| Maintainability | ‚ùå Can't tweak prompts easily     | ‚úÖ Central control over prompt logic         |\n",
        "| Dynamic Inputs  | ‚ùå Need to use f-strings manually | ‚úÖ Built-in placeholders and formatting      |\n",
        "| Testing         | ‚ùå Hard to isolate logic          | ‚úÖ Easy to unit test input/output formatting |\n",
        "| Pipelines       | ‚ùå Hard to chain inputs           | ‚úÖ Plugs into the LangChain pipeline cleanly |\n",
        "\n",
        "**Example of hardcoding f-strings:**\n",
        "\n",
        "```python\n",
        "llm.invoke(f\"Translate this: {text}\")\n",
        "```\n",
        "\n",
        "What if you want to:\n",
        "\n",
        "* Switch out `{text}` for `{sentence}`\n",
        "* Use a different language\n",
        "* Add system instructions?\n",
        "\n",
        "With `ChatPromptTemplate`, it‚Äôs just:\n",
        "\n",
        "```python\n",
        "template = ChatPromptTemplate.from_template(\"Translate to {language}: {text}\")\n",
        "template.format(language=\"French\", text=\"Hello\")\n",
        "```\n",
        "\n",
        "üöÄ **TL;DR**: Prompt templates make your system **cleaner**, **more scalable**, and **easier to change** without rewriting code all over.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Options for `ChatPromptTemplate`\n",
        "\n",
        "Here are the ways to create a prompt template:\n",
        "\n",
        "### 1. `from_template(...)`\n",
        "\n",
        "* Takes a **string with `{variables}`**.\n",
        "* Easiest and most common.\n",
        "\n",
        "```python\n",
        "ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `from_messages([...])`\n",
        "\n",
        "* Useful for multi-message chat (e.g. system + user messages).\n",
        "\n",
        "```python\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"user\", \"Translate this: {text}\")\n",
        "])\n",
        "```\n",
        "\n",
        "üß† Good for structured multi-turn inputs or formatting for chat models like OpenAI‚Äôs GPT-4.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `from_examples(...)`\n",
        "\n",
        "* Builds few-shot prompts using example inputs and outputs.\n",
        "\n",
        "```python\n",
        "ChatPromptTemplate.from_examples(\n",
        "    examples=[(\"Hello\", \"Bonjour\"), (\"Goodbye\", \"Au revoir\")],\n",
        "    example_template=\"Input: {input}\\nOutput: {output}\",\n",
        "    suffix=\"Now translate: {text}\",\n",
        "    input_variables=[\"text\"]\n",
        ")\n",
        "```\n",
        "\n",
        "This is used for few-shot learning, where you show the model examples of the task before the actual input.\n",
        "\n",
        "---\n",
        "\n",
        "## ü™Ñ The `|` Pipe Operator in LangChain (LCEL)\n",
        "\n",
        "The `|` operator comes from **LangChain Expression Language (LCEL)**.\n",
        "\n",
        "### üîÅ Think of it like a data pipeline:\n",
        "\n",
        "```python\n",
        "prompt | llm | parser\n",
        "```\n",
        "\n",
        "* **`prompt`** generates a string\n",
        "* **`llm`** uses the string as input, returns a response\n",
        "* **`parser`** turns that output into structured data (e.g. dict, list)\n",
        "\n",
        "Each component is an **LCEL runnable**, which means:\n",
        "\n",
        "* It can be invoked\n",
        "* It can be composed with others\n",
        "* It can be serialized, saved, or traced\n",
        "\n",
        "---\n",
        "\n",
        "### üî• Why the Pipe is Powerful\n",
        "\n",
        "* **Minimal boilerplate** ‚Äî no messy function calls\n",
        "* **Composable** ‚Äî can reuse or mix-and-match components\n",
        "* **Traceable** ‚Äî easy to debug intermediate stages\n",
        "* **Flexible** ‚Äî swap in new models, prompts, tools, parsers\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Bonus: You Can Build Chains like LEGO\n",
        "\n",
        "```python\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"What's the opposite of '{word}'?\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"word\": \"hot\"})\n",
        "print(result)  # ‚Üí \"cold\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ebfece",
      "metadata": {
        "id": "49ebfece"
      },
      "source": [
        "Think of these as **your essential building blocks** for designing any kind of LLM-powered agent.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç High-Level Breakdown\n",
        "\n",
        "### 1. `RunnableLambda`, `RunnableMap`\n",
        "\n",
        "(from `langchain_core.runnables`)\n",
        "\n",
        "#### üí° **Purpose**\n",
        "\n",
        "These are part of **LCEL (LangChain Expression Language)**, which gives you a way to **compose steps together** using the `|` operator (like pipes in Unix).\n",
        "\n",
        "* `RunnableLambda`: lets you wrap any Python function so it behaves like a LangChain step.\n",
        "* `RunnableMap`: lets you run multiple runnables in **parallel**, mapping keys to components. Great for branching pipelines.\n",
        "\n",
        "#### ü§ñ **Why they matter**\n",
        "\n",
        "They turn *everything* into a composable building block ‚Äî so you can:\n",
        "\n",
        "* Swap components in and out (LLMs, tools, filters).\n",
        "* Reuse logic easily.\n",
        "* Keep agent chains clean and declarative.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `StrOutputParser`\n",
        "\n",
        "(from `langchain_core.output_parsers`)\n",
        "\n",
        "#### üí° **Purpose**\n",
        "\n",
        "Parses the **raw output** from the LLM (often a structured message or object) into a plain string.\n",
        "\n",
        "* It simplifies: `\"Chat message\" ‚Üí \"text only\"`\n",
        "* Useful at the **end of a chain** to get usable data.\n",
        "\n",
        "#### ü§ñ **Why it matters**\n",
        "\n",
        "Most LLM calls return structured messages (like `AIMessage(content=...)`). You usually want just the string ‚Äî this does that cleanly.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `AIMessage`, `HumanMessage`\n",
        "\n",
        "(from `langchain_core.messages`)\n",
        "\n",
        "#### üí° **Purpose**\n",
        "\n",
        "Used to **manually craft chat history** or LLM input/output messages. They're part of the new **message-based LLM architecture**.\n",
        "\n",
        "* `HumanMessage`: what the user says\n",
        "* `AIMessage`: what the model says\n",
        "\n",
        "#### ü§ñ **Why they matter**\n",
        "\n",
        "* Required if you're manually setting up a conversation (chat history, few-shot examples, etc.).\n",
        "* Helps if you're simulating messages, fine-tuning prompts, or building chat-based memory.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `tool` decorator\n",
        "\n",
        "(from `langchain_core.tools`)\n",
        "\n",
        "#### üí° **Purpose**\n",
        "\n",
        "Wraps a regular Python function and makes it a **LangChain-compatible tool**. Adds metadata (like name, description, etc.) automatically.\n",
        "\n",
        "```python\n",
        "@tool\n",
        "def fake_weather(location: str) -> str:\n",
        "    return \"Always sunny\"\n",
        "```\n",
        "\n",
        "* Adds metadata ‚Üí tool becomes usable inside agents or chains.\n",
        "* Auto-generates OpenAI-style function definitions (if needed).\n",
        "\n",
        "#### ü§ñ **Why it matters**\n",
        "\n",
        "LangChain agents can only use tools defined this way. It‚Äôs the easiest path from *normal Python code* ‚Üí *agent-executable action*.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why You Should Care\n",
        "\n",
        "All of these together form the **core abstraction layer** in LangChain:\n",
        "\n",
        "* Flexible composition with `RunnableLambda` and `RunnableMap`\n",
        "* Clean output handling with `StrOutputParser`\n",
        "* Native chat formatting with `AIMessage`, `HumanMessage`\n",
        "* Easy tool integration with `@tool`\n",
        "\n",
        "They're what make LangChain feel like **LEGO blocks for AI systems**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6b94b6",
      "metadata": {
        "id": "1f6b94b6"
      },
      "outputs": [],
      "source": [
        "# agent_structure_v1.py\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API key\n",
        "load_dotenv(\"API_KEYS.env\")\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# üõ†Ô∏è 1. Define a mock tool\n",
        "# -------------------------------\n",
        "@tool\n",
        "def fake_weather_tool(location: str) -> str:\n",
        "    \"\"\"Returns a fake weather report for a given location.\"\"\"\n",
        "    return f\"The weather in {location} is sunny and 72¬∞F.\"\n",
        "\n",
        "# Make it usable in LangChain format\n",
        "tools = {\n",
        "    \"fake_weather_tool\": fake_weather_tool,\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# üß† 2. Create prompt template\n",
        "# -------------------------------\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant that uses tools.\n",
        "\n",
        "User input: {input}\n",
        "\n",
        "Decide what to do and respond. If a tool is needed, say so.\n",
        "\"\"\")\n",
        "\n",
        "# -------------------------------\n",
        "# üß± 3. Build the chain\n",
        "# -------------------------------\n",
        "\n",
        "# Step 1: Prompt -> LLM\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# -------------------------------\n",
        "# üöÄ 4. Run the agent\n",
        "# -------------------------------\n",
        "response = chain.invoke({\"input\": \"What's the weather in Paris?\"})\n",
        "\n",
        "print(\"LLM response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e8f96c",
      "metadata": {
        "id": "d5e8f96c"
      },
      "source": [
        "\n",
        "## üß† What This Code Teaches You\n",
        "\n",
        "### üîß `@tool`\n",
        "\n",
        "* Converts a basic function into a **LangChain Tool**\n",
        "* Registers name + docstring as metadata\n",
        "* You‚Äôll later pass these to agents or tool routers\n",
        "\n",
        "### üì¶ `RunnableMap`\n",
        "\n",
        "* It‚Äôs a \"multi-input pipe stage\"\n",
        "* Takes inputs like `{\"number\": 5}` and **creates multiple outputs**:\n",
        "\n",
        "  * One to send to the prompt as `{number}`\n",
        "  * One to simulate a tool call as `{square}`\n",
        "\n",
        "### üìù `ChatPromptTemplate`\n",
        "\n",
        "* Accepts placeholders like `{number}` and `{square}`\n",
        "* Feeds the constructed prompt into the LLM\n",
        "\n",
        "### üîó `|` (Pipe Operator)\n",
        "\n",
        "* Combines components into a **chain of operations**\n",
        "* Everything is modular:\n",
        "\n",
        "  * Tools\n",
        "  * Prompt templates\n",
        "  * LLMs\n",
        "  * Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff2dde5",
      "metadata": {
        "id": "aff2dde5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.runnables import RunnableMap\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# 1. Create a toy tool\n",
        "@tool\n",
        "def square_number(number: int) -> int:\n",
        "    \"\"\"Returns the square of an integer.\"\"\"\n",
        "    return number * number\n",
        "\n",
        "# 2. Set up your LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# 3. Create a simple prompt (simulate the agent using tool)\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"A user wants to square the number {number}. Confirm that it is {square}.\"\n",
        ")\n",
        "\n",
        "# 4. Tool call + prompt chain\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"square\": lambda input: square_number(input[\"number\"]),\n",
        "        \"number\": lambda input: input[\"number\"]\n",
        "    }) |\n",
        "    prompt |\n",
        "    llm |\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "# 5. Run the full pipeline\n",
        "output = chain.invoke({\"number\": 5})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2501bfa1",
      "metadata": {
        "id": "2501bfa1"
      },
      "source": [
        "\n",
        "\n",
        "## üîç 1. **Where Is the Tool Metadata Stored?**\n",
        "\n",
        "When you create a tool using `@tool` or `Tool.from_function(...)`, LangChain creates a **`Tool` object** (or subclass like `StructuredTool`) with fields like:\n",
        "\n",
        "```python\n",
        "Tool(\n",
        "  name=\"add_numbers\",\n",
        "  description=\"Add two numbers.\",\n",
        "  args_schema=<inferred pydantic schema>,\n",
        "  func=<your function>,\n",
        "  ...\n",
        ")\n",
        "```\n",
        "\n",
        "This object is stored in memory and passed around in your code. It's **not saved to disk or a database by default**, just kept as a Python object.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 2. **What Does the Agent \"See\"?**\n",
        "\n",
        "When you give tools to an agent, you do something like this:\n",
        "\n",
        "```python\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[my_tool_1, my_tool_2],  # <- your Tool objects\n",
        "    llm=llm,\n",
        "    agent_type=\"openai-tools\",     # or \"zero-shot-react-description\"\n",
        ")\n",
        "```\n",
        "\n",
        "The agent:\n",
        "\n",
        "* **Looks at each tool‚Äôs `name` and `description`**\n",
        "* **Builds a prompt** that includes this info\n",
        "* When needed, uses the **input schema** (`args_schema`) to parse/validate inputs\n",
        "\n",
        "That prompt might look like this internally (simplified):\n",
        "\n",
        "```\n",
        "You can use the following tools:\n",
        "\n",
        "1. add_numbers\n",
        "   Add two numbers.\n",
        "\n",
        "2. search_weather\n",
        "   Search for the current weather in a city.\n",
        "\n",
        "When you decide to take an action, choose a tool name and provide its input.\n",
        "```\n",
        "\n",
        "This is how the LLM chooses which tool to call!\n",
        "\n",
        "---\n",
        "\n",
        "## üîß 3. **How Can You Inspect Tool Metadata?**\n",
        "\n",
        "You can easily see what‚Äôs inside any tool object:\n",
        "\n",
        "```python\n",
        "print(my_tool.name)\n",
        "print(my_tool.description)\n",
        "print(my_tool.args_schema)\n",
        "print(my_tool.func)\n",
        "```\n",
        "\n",
        "You can even build a custom registry or introspect tools dynamically:\n",
        "\n",
        "```python\n",
        "for tool in tools:\n",
        "    print(f\"Tool: {tool.name} ‚Äî {tool.description}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Metadata      | Stored in                     | Used by agent?     |\n",
        "| ------------- | ----------------------------- | ------------------ |\n",
        "| `name`        | `Tool.name`                   | ‚úÖ Yes              |\n",
        "| `description` | `Tool.description`            | ‚úÖ Yes              |\n",
        "| `args_schema` | `Tool.args_schema` (Pydantic) | ‚úÖ Yes              |\n",
        "| `func`        | `Tool.func`                   | ‚úÖ Yes (at runtime) |\n",
        "\n",
        "LangChain builds the agent prompt *on the fly* using these ‚Äî so the better your tool metadata, the smarter your agent becomes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63feff23",
      "metadata": {
        "id": "63feff23"
      },
      "source": [
        "LangChain‚Äôs architecture does closely mirror the structure in the `_Agent_1_Recipe.txt` file you uploaded. Let me walk you through **how they are similar** and **why that matters**, especially in the context of tools and action/tool registries.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Structural Similarity: LangChain vs. Your Agent Framework\n",
        "\n",
        "| Concept in Your Recipe             | Equivalent in LangChain                                 | Notes                                                                 |\n",
        "| ---------------------------------- | ------------------------------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Tool (`Tool` class)**            | `@tool` or LangChain `Tool` class                       | Both define name, description, parameters, and handler function       |\n",
        "| **Tool Registry (`ToolRegistry`)** | LangChain's `Tool` list / registry                      | LangChain uses a registry (often implicit in `AgentExecutor`)         |\n",
        "| **ActionContext (with deps)**      | LangChain‚Äôs `RunnableConfig`, `callbacks`, or injection | Dependencies like `clock`, API keys, services                         |\n",
        "| **Environment (for DI execution)** | LangChain‚Äôs `Runnable` execution pipeline               | LangChain does some magic to auto-inject args too                     |\n",
        "| **Agent class**                    | `AgentExecutor`, `initialize_agent`                     | LangChain wraps model + tools + memory + capabilities                 |\n",
        "| **PlanFirst / ProgressTracking**   | LangChain callbacks or planner agents                   | You can achieve similar planning logic using planners or custom hooks |\n",
        "| **FakeModel (respond)**            | LLM + PromptTemplate chain                              | LangChain uses LLM chains or agents to decide next steps              |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Shared Philosophies\n",
        "\n",
        "### 1. **Tool Encapsulation**\n",
        "\n",
        "Both use small, focused tools with:\n",
        "\n",
        "* A name and description\n",
        "* JSON-style parameters\n",
        "* Stateless design\n",
        "* Metadata used by the planner/model\n",
        "\n",
        "This is exactly how LangChain‚Äôs `@tool` decorator works. LangChain also makes this metadata available to the LLM for function-calling.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Central Registry**\n",
        "\n",
        "Your `ToolRegistry` class is very similar to LangChain‚Äôs use of a tool list passed into an agent:\n",
        "\n",
        "```python\n",
        "agent = initialize_agent(\n",
        "    tools=[my_tool_1, my_tool_2],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        ")\n",
        "```\n",
        "\n",
        "LangChain relies on the tool‚Äôs name and description to let the LLM call them correctly.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Agent-Centered Loop**\n",
        "\n",
        "Your agent loop (respond ‚Üí call tool ‚Üí update state) mimics LangChain‚Äôs:\n",
        "\n",
        "1. Create a plan (optional)\n",
        "2. Select a tool to invoke\n",
        "3. Track progress or state\n",
        "4. Repeat or finish\n",
        "\n",
        "This is precisely the heart of LangChain's `AgentExecutor`, but your version makes the steps explicit and instructive ‚Äî perfect for learning.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Dependency Injection**\n",
        "\n",
        "The `_dep_name` pattern in your code is smart: it allows tools to request injected dependencies (like `clock`) from the context.\n",
        "\n",
        "LangChain lets you inject:\n",
        "\n",
        "* API keys\n",
        "* Databases\n",
        "* Embedding models\n",
        "* Configurations\n",
        "\n",
        "You can even customize LangChain tool execution to support DI like yours if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Goal-Oriented Execution**\n",
        "\n",
        "Your `GOAL` string defines what the agent should do, and then it builds a small plan and executes it.\n",
        "\n",
        "LangChain does this implicitly by feeding prompts to LLMs like:\n",
        "\n",
        "> \"You are an assistant with access to the following tools. Use them to answer the user‚Äôs question: {input}\"\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why This Matters for LangChain\n",
        "\n",
        "Because you already think in terms of:\n",
        "\n",
        "* Tool metadata\n",
        "* Dependency injection\n",
        "* Planning phases\n",
        "* Context tracking\n",
        "* Registry-based design\n",
        "\n",
        "You're already \"thinking like LangChain\".\n",
        "\n",
        "This mindset makes LangChain much easier to master. The key difference is that LangChain does more *implicitly* (e.g., hiding the loop, planning, and DI) ‚Äî but you can override or extend any part.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Full Breakdown of Your Chain\n",
        "\n",
        "Here's your full pipeline with annotations:\n",
        "\n",
        "```python\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"square\": lambda input: square_number(input[\"number\"]),   # Compute square of input[\"number\"]\n",
        "        \"number\": lambda input: input[\"number\"]                   # Pass original number too\n",
        "    }) |\n",
        "    prompt |       # Use both values in the prompt template\n",
        "    llm |          # Send prompt to the LLM\n",
        "    StrOutputParser()  # Extract plain text string from response\n",
        ")\n",
        "```\n",
        "\n",
        "**Input:**\n",
        "\n",
        "```python\n",
        "{\"number\": 3}\n",
        "```\n",
        "\n",
        "**Transformed via RunnableMap:**\n",
        "\n",
        "```python\n",
        "{\"square\": 9, \"number\": 3}\n",
        "```\n",
        "\n",
        "**Prompt might look like (after templating):**\n",
        "\n",
        "```\n",
        "What is the relationship between 3 and 9?\n",
        "```\n",
        "\n",
        "**LLM returns:** `AIMessage(content=\"9 is the square of 3.\")`\n",
        "\n",
        "**Output after parser:** `\"9 is the square of 3.\"`\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why This Pattern Is Great\n",
        "\n",
        "* **Modular**: Each component does one thing well.\n",
        "* **Composable**: You can insert/remove elements without refactoring everything.\n",
        "* **Debuggable**: You can test parts in isolation (e.g., just the prompt or the LLM).\n",
        "* **Future-friendly**: All this integrates seamlessly into more complex agents, routers, or retrievers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d51106",
      "metadata": {
        "id": "b9d51106"
      },
      "source": [
        "\n",
        "\n",
        "## üß© `RunnableMap`\n",
        "\n",
        "### ‚úÖ What It Is:\n",
        "\n",
        "`RunnableMap` is a LangChain **composable building block** that takes a dictionary of key-value pairs, where the values are **functions or other `Runnable` objects**, and runs them **in parallel** on the same input.\n",
        "\n",
        "### üß† Why It's Valuable:\n",
        "\n",
        "* It‚Äôs a **fan-out/fan-in mechanism**: one input ‚Üí multiple processors ‚Üí one output object.\n",
        "* It lets you prepare or branch inputs cleanly before sending them to other components like prompts, LLMs, or chains.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ What \"Parallel\" Means in `RunnableMap`\n",
        "\n",
        "### In this context:\n",
        "\n",
        "‚ÄúParallel‚Äù means:\n",
        "\n",
        "* Each function in the `RunnableMap` receives the **same input**.\n",
        "* They **run independently** of each other.\n",
        "* Their results are **collected into a single output dictionary**.\n",
        "\n",
        "üö´ It does **not** mean true hardware-level parallelism unless explicitly executed with async/multiprocessing ‚Äî but it **logically** acts that way.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö°Ô∏è Strengths of the `RunnableMap` Fan-out Pattern\n",
        "\n",
        "### 1. **Decouples Logic**\n",
        "\n",
        "Each function does one job (e.g., extract a field, transform it, fetch something, etc.). This makes it:\n",
        "\n",
        "* Easier to debug\n",
        "* Easier to test\n",
        "* Easier to reuse or rewire parts later\n",
        "\n",
        "### 2. **One Input ‚Üí Many Branches**\n",
        "\n",
        "You can take a single user input and:\n",
        "\n",
        "* Route it to several tools\n",
        "* Format it for multiple prompts\n",
        "* Cache/interpolate different values\n",
        "\n",
        "> üì¶ Example:\n",
        "\n",
        "```python\n",
        "RunnableMap({\n",
        "  \"text\": lambda d: d[\"text\"],\n",
        "  \"length\": lambda d: len(d[\"text\"]),\n",
        "  \"reversed\": lambda d: d[\"text\"][::-1]\n",
        "})\n",
        "```\n",
        "\n",
        "One input ‚Üí 3 outputs ‚Üí great for prompt building.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Sets You Up for Agent Tooling**\n",
        "\n",
        "This pattern is perfect for agents that:\n",
        "\n",
        "* Need to pre-process or augment input before tool selection\n",
        "* Use multiple results in a prompt (like `{\"question\": ..., \"retrieved_docs\": ..., \"context\": ...}`)\n",
        "\n",
        "---\n",
        "\n",
        "## üö´ Limitations\n",
        "\n",
        "### 1. **No True Concurrency (by default)**\n",
        "\n",
        "Although logically parallel, it executes **sequentially** unless:\n",
        "\n",
        "* You explicitly use async runnables\n",
        "* Or build in multiprocessing / distributed compute\n",
        "\n",
        "### 2. **Independent Execution**\n",
        "\n",
        "You can't make one function depend on the result of another **inside** the `RunnableMap` ‚Äî all keys compute in isolation.\n",
        "\n",
        "If you need:\n",
        "\n",
        "```python\n",
        "\"step2\": lambda d: d[\"step1_result\"] + 5\n",
        "```\n",
        "\n",
        "‚Üí You must **chain** that as a separate step after.\n",
        "\n",
        "### 3. **Non-Deterministic Order (sometimes)**\n",
        "\n",
        "The output dictionary preserves key order in Python 3.7+, but if you‚Äôre depending on that for formatting or prompt-building, it‚Äôs better to be explicit.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† When to Use `RunnableMap`\n",
        "\n",
        "Use it when you want to:\n",
        "\n",
        "* Transform a single input into **multiple computed values**\n",
        "* Feed those values into a prompt\n",
        "* Keep your code **modular and maintainable**\n",
        "* Eventually plug those branches into **retrievers**, **tools**, **functions**, or **multi-modal chains**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92aa3a9d",
      "metadata": {
        "id": "92aa3a9d"
      },
      "source": [
        "\n",
        "\n",
        "## üì¶ `StrOutputParser`\n",
        "\n",
        "### ‚úÖ What It Is:\n",
        "\n",
        "This is a **post-processor** that takes the LLM‚Äôs raw output (which is typically an `AIMessage`, `BaseMessage`, or a dictionary), and **extracts just the string content**.\n",
        "\n",
        "### üß† Why It's Valuable:\n",
        "\n",
        "* Many chains produce more than just text (e.g. token usage, metadata, or wrapped messages).\n",
        "* If you're only interested in the string (like in early prototypes), `StrOutputParser` **simplifies your outputs** for easier printing, debugging, or downstream use.\n",
        "\n",
        "### üîß Example:\n",
        "\n",
        "After an LLM runs and returns:\n",
        "\n",
        "```python\n",
        "AIMessage(content=\"The answer is 42.\")\n",
        "```\n",
        "\n",
        "The `StrOutputParser()` will give you:\n",
        "\n",
        "```python\n",
        "\"The answer is 42.\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9128802",
      "metadata": {
        "id": "a9128802"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# -----------------------\n",
        "# Step 1: Define Tools\n",
        "# -----------------------\n",
        "\n",
        "@tool\n",
        "def weather_lookup(city: str) -> str:\n",
        "    \"\"\"Returns the current weather for a given city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny and 75¬∞F.\"\n",
        "\n",
        "@tool\n",
        "def time_lookup(location: str) -> str:\n",
        "    \"\"\"Returns the current time in a given location.\"\"\"\n",
        "    return f\"The current time in {location} is 3:45 PM.\"\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Create a Tool Registry\n",
        "# -------------------------------\n",
        "\n",
        "tool_registry = {\n",
        "    weather_lookup.name: weather_lookup,\n",
        "    time_lookup.name: time_lookup,\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Prompt to Select Tool\n",
        "# -------------------------------\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You're a helpful assistant who decides which tool to use based on the user's request.\n",
        "\n",
        "User Input: {input}\n",
        "\n",
        "Available tools:\n",
        "{tool_list}\n",
        "\n",
        "Respond ONLY with the name of the correct tool to use.\n",
        "\"\"\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Define the Agent Chain\n",
        "# -------------------------------\n",
        "\n",
        "# Custom tool selector logic\n",
        "def select_tool(input: dict) -> str:\n",
        "    user_input = input[\"input\"]\n",
        "\n",
        "    # Generate prompt input\n",
        "    tool_list = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in tool_registry.items()])\n",
        "    filled_prompt = prompt.format(input=user_input, tool_list=tool_list)\n",
        "\n",
        "    # Pass through LLM\n",
        "    response = llm.invoke(filled_prompt)\n",
        "    return response.content.strip()\n",
        "\n",
        "# Final agent chain\n",
        "agent_chain = (\n",
        "    RunnableLambda(select_tool) |\n",
        "    RunnableLambda(lambda tool_name_and_input: tool_registry[tool_name_and_input[\"tool\"]].invoke({\"city\": tool_name_and_input[\"input\"]})) |\n",
        "    StrOutputParser()\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Run the Agent\n",
        "# -------------------------------\n",
        "\n",
        "# Simulate input\n",
        "user_query = \"What is the weather in Paris?\"\n",
        "\n",
        "# Manually split out tool input (real agent would parse this better)\n",
        "chain_input = {\n",
        "    \"input\": \"Paris\",\n",
        "    \"tool\": \"weather_lookup\"\n",
        "}\n",
        "\n",
        "result = agent_chain.invoke(chain_input)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e98ad2d",
      "metadata": {
        "id": "3e98ad2d"
      },
      "source": [
        "The agent recipe you shared **does use a proper tool registry**, and the architecture is very similar in spirit to LangChain's approach, even though the implementation details are different.\n",
        "\n",
        "Let‚Äôs compare the **tool selection logic** in your LangChain agent vs. the architecture in your `_Agent_1_Recipe.txt`, and address your core question:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What You're Doing Now (LangChain Tool Selector)\n",
        "\n",
        "In your current setup:\n",
        "\n",
        "```python\n",
        "def select_tool(input: dict) -> str:\n",
        "    ...\n",
        "    tool_list = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in tool_registry.items()])\n",
        "    filled_prompt = prompt.format(input=user_input, tool_list=tool_list)\n",
        "    response = llm.invoke(filled_prompt)\n",
        "    return response.content.strip()\n",
        "```\n",
        "\n",
        "### üîç Summary:\n",
        "\n",
        "* You're manually formatting the list of tools.\n",
        "* Injecting it into a prompt.\n",
        "* Passing it to the LLM.\n",
        "* Extracting the name of the tool as a string.\n",
        "\n",
        "This is **flexible and transparent**, but has limitations:\n",
        "\n",
        "* You have to **manually parse the LLM‚Äôs response**.\n",
        "* You‚Äôre assuming it will return a tool name correctly every time.\n",
        "* There‚Äôs **no strict validation** or fallback if the tool name is invalid.\n",
        "* You‚Äôre not using LangChain's built-in support for **structured function/tool calling**, which is more robust.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ How Your Agent Recipe Handles It\n",
        "\n",
        "In your agent framework, this part is clean and elegant:\n",
        "\n",
        "```python\n",
        "class Tool:\n",
        "    name: str\n",
        "    description: str\n",
        "    parameters: Dict[str, Any]\n",
        "    handler: Callable[[ActionContext, Dict[str, Any]], Any]\n",
        "```\n",
        "\n",
        "And the model logic is like:\n",
        "\n",
        "```python\n",
        "# Example decision logic in FakeModel\n",
        "return {\n",
        "  \"tool\": \"create_plan\",\n",
        "  \"arguments\": {\"goal\": goal}\n",
        "}\n",
        "```\n",
        "\n",
        "Then the environment simply executes:\n",
        "\n",
        "```python\n",
        "res = self.env.execute(name, args)\n",
        "```\n",
        "\n",
        "### ‚úÖ Benefits:\n",
        "\n",
        "* The **tool registry is real and enforced**.\n",
        "* Tool parameters are typed and explicitly declared.\n",
        "* Tool execution is safe, wrapped, and DI-injected.\n",
        "* Tool selection uses a structured format (dicts, not just text strings).\n",
        "* You can **add guardrails and capabilities cleanly** (like `PlanFirstCapability`, `ProgressTrackingCapability`).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† So, What‚Äôs Missing in the LangChain Example?\n",
        "\n",
        "LangChain actually *does* support something quite close to your agent recipe ‚Äî **tool registration + function-calling via OpenAI-compatible models**, like this:\n",
        "\n",
        "```python\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents.openai_functions_agent import OpenAIFunctionsAgent\n",
        "\n",
        "tools = [Tool.from_function(my_tool_fn), ...]\n",
        "\n",
        "agent = OpenAIFunctionsAgent.from_llm_and_tools(llm, tools)\n",
        "executor = AgentExecutor(agent=agent, tools=tools)\n",
        "executor.invoke({\"input\": \"my task\"})\n",
        "```\n",
        "\n",
        "That way:\n",
        "\n",
        "* The tool descriptions, names, and parameters are **automatically passed to the LLM**.\n",
        "* The LLM can use **function-calling** (not natural text parsing) to select tools.\n",
        "* LangChain handles **tool lookup, validation, and execution** under the hood ‚Äî just like your custom `ToolRegistry + Environment`.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Recommendations Moving Forward\n",
        "\n",
        "Since you're using your LangChain agents to **learn structure and build real foundations**, I‚Äôd suggest:\n",
        "\n",
        "1. **Stick with your explicit tool registry and call model manually** for now ‚Äî it‚Äôs great for learning.\n",
        "2. Later, explore LangChain‚Äôs built-in `AgentExecutor` + `OpenAIFunctionsAgent` which wraps all of this.\n",
        "3. You can even **plug your registry into LangChain‚Äôs tools** by writing a converter function that takes your tool objects and turns them into LangChain-compatible `Tool` instances.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5314b0",
      "metadata": {
        "id": "cf5314b0"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, tool\n",
        "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
        "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "# === Step 1: LLM ===\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# === Step 2: Tools with @tool decorator ===\n",
        "@tool\n",
        "def square_number(number: int) -> int:\n",
        "    \"\"\"Returns the square of a given number.\"\"\"\n",
        "    return number * number\n",
        "\n",
        "@tool\n",
        "def greet(name: str) -> str:\n",
        "    \"\"\"Returns a friendly greeting.\"\"\"\n",
        "    return f\"Hello, {name}! Welcome to LangChain.\"\n",
        "\n",
        "# === Step 3: Build prompt ===\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. You can use tools to help answer questions.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# === Step 4: Tool list (acts like a registry) ===\n",
        "tool_registry = [square_number, greet]\n",
        "\n",
        "# === Step 5: Build agent with OpenAI function calling ===\n",
        "agent = OpenAIFunctionsAgent(\n",
        "    llm=llm,\n",
        "    tools=tool_registry,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "# === Step 6: Optional memory (can remove if not needed) ===\n",
        "memory = AgentTokenBufferMemory(memory_key=\"history\", llm=llm)\n",
        "\n",
        "# === Step 7: Agent executor ===\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tool_registry,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# === Step 8: Run agent ===\n",
        "response = agent_executor.invoke({\"input\": \"Please square the number 7.\"})\n",
        "print(response[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12455913",
      "metadata": {
        "id": "12455913"
      },
      "source": [
        "You're right to want to **mirror your own agent recipe** as closely as possible while taking advantage of what LangChain gives you out of the box. Let‚Äôs walk through a full example using **LangChain‚Äôs `OpenAIFunctionsAgent` and `AgentExecutor`**, which:\n",
        "\n",
        "* Automatically **registers tools** and sends them to the LLM\n",
        "* Uses **OpenAI function-calling** (structured, safe, robust)\n",
        "* Matches your idea of:\n",
        "\n",
        "  * Tool registry ‚úÖ\n",
        "  * Tool metadata ‚úÖ\n",
        "  * Clear execution pipeline ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What‚Äôs Happening Behind the Scenes\n",
        "\n",
        "* üß∞ `@tool`: Auto-wraps functions with `name`, `description`, and input schema ‚Äî just like your registry.\n",
        "* üß† `OpenAIFunctionsAgent`: Uses OpenAI's function-calling to pick a tool **reliably**.\n",
        "* üóÉ `AgentExecutor`: Orchestrates the pipeline and handles memory, retries, logging, etc.\n",
        "* üó® Prompt includes user input and system context.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What Matches Your Agent Recipe\n",
        "\n",
        "| Concept               | Your Recipe                        | LangChain Version                           |\n",
        "| --------------------- | ---------------------------------- | ------------------------------------------- |\n",
        "| Tool registry         | `Tool(name, description, handler)` | List of `@tool` functions (`tool_registry`) |\n",
        "| Tool selection        | Model picks tool by name           | Function-calling selects based on metadata  |\n",
        "| Input parsing         | Args from model into function call | Schema-based function-calling handles this  |\n",
        "| Execution environment | `env.execute(tool_name, args)`     | `AgentExecutor` handles calling tools       |\n",
        "| Prompting             | Central input + tool metadata      | `ChatPromptTemplate` with tool awareness    |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Benefits of This Setup\n",
        "\n",
        "* üí™ Robust and safe ‚Äî the LLM can only call registered tools.\n",
        "* üß† You don‚Äôt need to manually inject tool descriptions ‚Äî it‚Äôs automatic.\n",
        "* ‚úÖ Clean interface for extending tools later.\n",
        "* üîÑ Can swap out the prompt, memory, or LLM easily.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ‚úÖ What You Observed: LLM Selects the Tool Directly\n",
        "\n",
        "> ‚ÄúIt allows the LLM to choose the tool rather than working its way through if-else statements.‚Äù\n",
        "\n",
        "**‚úîÔ∏è Correct.** When using `OpenAIFunctionsAgent` with `AgentExecutor`, the LLM is **given a full list of tools**, each with:\n",
        "\n",
        "* A name\n",
        "* A description\n",
        "* An input schema (automatically inferred from the Python function)\n",
        "\n",
        "The model then decides, using function-calling, **which tool (if any)** to call and what arguments to pass ‚Äî in a single step.\n",
        "\n",
        "There‚Äôs no need to write your own logic like:\n",
        "\n",
        "```python\n",
        "if \"weather\" in input:\n",
        "    call weather_tool\n",
        "elif \"math\" in input:\n",
        "    call calculator\n",
        "...\n",
        "```\n",
        "\n",
        "The LLM does that reasoning internally.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ Example Thought Process (Internally, the LLM Does This)\n",
        "\n",
        "With a prompt like:\n",
        "\n",
        "> ‚ÄúPlease square the number 7‚Äù\n",
        "\n",
        "And two tools:\n",
        "\n",
        "* `greet(name: str)`\n",
        "* `square_number(number: int)`\n",
        "\n",
        "The model sees:\n",
        "\n",
        "```\n",
        "You can use the following tools:\n",
        "\n",
        "- greet: Returns a friendly greeting.\n",
        "- square_number: Returns the square of a given number.\n",
        "```\n",
        "\n",
        "It thinks:\n",
        "\n",
        "> \"This looks like a math task involving a number. ‚Äòsquare\\_number‚Äô is the only tool that seems applicable. Let me call it with `number=7`.\"\n",
        "\n",
        "And it sends back:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_call\": {\n",
        "    \"name\": \"square_number\",\n",
        "    \"arguments\": {\n",
        "      \"number\": 7\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "LangChain then executes that tool and returns the result.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Why This is Preferable\n",
        "\n",
        "| Advantage                     | Description                                                            |\n",
        "| ----------------------------- | ---------------------------------------------------------------------- |\n",
        "| ‚úÖ **Simpler Code**            | No need to write manual routing logic (if/else or keyword matching)    |\n",
        "| üéØ **More Accurate Tool Use** | Model reasons about **descriptions** and **inputs**, not just keywords |\n",
        "| üß† **Better Composability**   | Easy to add more tools ‚Äî no need to touch selection logic              |\n",
        "| üõ° **Safe Execution**         | The LLM can't execute arbitrary Python ‚Äî only registered tools         |\n",
        "| üîç **Full Traceability**      | Logs clearly show *which tool* was selected and *why*                  |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Your Role as Developer\n",
        "\n",
        "What you control:\n",
        "\n",
        "* Tool definitions (`@tool`)\n",
        "* Prompt and system instructions\n",
        "* LLM settings (temperature, model name)\n",
        "* Whether or not to use memory, retries, caching, etc.\n",
        "\n",
        "You **set the stage**, and the LLM + agent architecture **orchestrates the play**.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62001776",
      "metadata": {
        "id": "62001776"
      },
      "source": [
        "Let's walk through what makes this approach such a **powerful and clean design**, especially for learning and scaling:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why This Structure Is So Elegant\n",
        "\n",
        "#### üßº **Compact, Clear, and Concise**\n",
        "\n",
        "Yes ‚Äî 100%. Using `OpenAIFunctionsAgent` (or similar high-level agents in LangChain) means:\n",
        "\n",
        "* You **don‚Äôt repeat yourself** (no manual routing logic)\n",
        "* The code is **modular**: tools are defined once and reused\n",
        "* **Separation of concerns**: tools just do work, the agent handles reasoning\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "@tool\n",
        "def square_number(number: int) -> int:\n",
        "    \"\"\"Returns the square of a number.\"\"\"\n",
        "    return number * number\n",
        "```\n",
        "\n",
        "Boom ‚Äî tool registered, documented, type-safe, and available to the agent immediately. No glue code needed.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† The Agent Has *Agency*\n",
        "\n",
        "> ‚ÄúThe Agent has agency to choose the tool it wants to use.‚Äù\n",
        "\n",
        "Yes! This is a **paradigm shift**: instead of hardcoding logic like a flowchart, you're giving the agent **a menu of intelligent capabilities**, and it decides what to use based on the user‚Äôs query.\n",
        "\n",
        "* It doesn‚Äôt just keyword-match.\n",
        "* It **interprets intent**, analyzes your tool descriptions, and chooses the best fit.\n",
        "* It can even chain tool use (in more advanced agent types).\n",
        "\n",
        "---\n",
        "\n",
        "### üß∞ Naming and Descriptions *Really Matter*\n",
        "\n",
        "You nailed this:\n",
        "\n",
        "> ‚Äú...as long as we name the tools intelligently and clearly...‚Äù\n",
        "\n",
        "Yes! When building tool-using agents, think like a UX designer ‚Äî your **tool name + description** is the user interface for the LLM.\n",
        "\n",
        "#### Examples:\n",
        "\n",
        "| Tool Name         | Description                                     |\n",
        "| ----------------- | ----------------------------------------------- |\n",
        "| `weather_lookup`  | \"Get the current weather for a given city.\"     |\n",
        "| `translate_text`  | \"Translate English text into another language.\" |\n",
        "| `summarize_notes` | \"Summarize meeting notes into bullet points.\"   |\n",
        "\n",
        "This helps the model **understand capabilities** and **map them to user requests** with high accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Bonus: Easily Scalable\n",
        "\n",
        "Want to go from 3 tools to 30? Just add them.\n",
        "\n",
        "No need to:\n",
        "\n",
        "* Update giant `if/elif` blocks\n",
        "* Change routing logic\n",
        "* Debug spaghetti chains\n",
        "\n",
        "Everything stays composable and maintainable.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Bottom Line\n",
        "\n",
        "You're not just writing code ‚Äî you're **designing reasoning spaces** for agents to act intelligently.\n",
        "\n",
        "This is the mindset shift that sets LangChain apart from basic scripts. And you're already thinking this way ‚Äî so you're on the right path.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec178260",
      "metadata": {
        "id": "ec178260"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Tools (Fake Implementations)\n",
        "# ---------------------------\n",
        "\n",
        "@tool\n",
        "def search_linkedin(name: str) -> str:\n",
        "    \"\"\"Searches LinkedIn and returns mock profile info for a person.\"\"\"\n",
        "    return f\"Mock LinkedIn profile for {name} with title 'VP of Sales at Acme Corp'\"\n",
        "\n",
        "@tool\n",
        "def extract_person_details(profile_text: str) -> dict:\n",
        "    \"\"\"Extracts details about a person from their profile text.\"\"\"\n",
        "    return {\"name\": \"John Smith\", \"title\": \"VP of Sales\", \"company\": \"Acme Corp\"}\n",
        "\n",
        "@tool\n",
        "def research_company(company_name: str) -> str:\n",
        "    \"\"\"Returns mock research findings about a company.\"\"\"\n",
        "    return f\"{company_name} just raised Series B funding and launched a new product.\"\n",
        "\n",
        "@tool\n",
        "def generate_email(details: dict) -> str:\n",
        "    \"\"\"Generates a personalized B2B sales email.\"\"\"\n",
        "    return f\"Hi {details['name']}, congrats on the product launch at {details['company']}! Let's talk growth...\"\n",
        "\n",
        "@tool\n",
        "def send_email(email_content: str) -> str:\n",
        "    \"\"\"Mocks sending an email.\"\"\"\n",
        "    return f\"Email sent to prospect: {email_content}\"\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Tool Registry\n",
        "# ---------------------------\n",
        "\n",
        "tool_registry = {\n",
        "    \"search_linkedin\": search_linkedin,\n",
        "    \"extract_person_details\": extract_person_details,\n",
        "    \"research_company\": research_company,\n",
        "    \"generate_email\": generate_email,\n",
        "    \"send_email\": send_email,\n",
        "}\n",
        "\n",
        "# ---------------------------\n",
        "# 3. LLM and Agent Setup\n",
        "# ---------------------------\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "from langchain.agents.openai_functions_agent.agent import OpenAIFunctionsAgent\n",
        "\n",
        "agent = OpenAIFunctionsAgent.from_tools(\n",
        "    list(tool_registry.values()),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=list(tool_registry.values()), verbose=True)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Run Example Task\n",
        "# ---------------------------\n",
        "\n",
        "task = \"Find John Smith on LinkedIn, research his company, and send him a personalized cold email.\"\n",
        "\n",
        "result = agent_executor.invoke({\"input\": task})\n",
        "print(\"\\nFinal Agent Output:\\n\", result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4cb721b",
      "metadata": {
        "id": "c4cb721b"
      },
      "source": [
        "\n",
        "## üì¶ Tool Chaining Logic: Why It Matters in Complex Agents\n",
        "\n",
        "When your agent has to accomplish **multiple dependent tasks**, tool usage can't always be left purely to chance. So, you have two main strategies:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Option 1: Let the Agent Decide (Autonomous Tool Selection)**\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* Give the agent access to **all tools**\n",
        "* Give it a **task description**\n",
        "* Let the **LLM reason** its way through the steps, picking tools as needed.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "> \"Find John Smith on LinkedIn, research his company, and send him a cold email.\"\n",
        "\n",
        "**Agent flow (auto-selected):**\n",
        "\n",
        "1. `search_linkedin(name)`\n",
        "2. `extract_person_details(profile)`\n",
        "3. `research_company(company_name)`\n",
        "4. `generate_email(details)`\n",
        "5. `send_email(email)`\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "* ‚ú® Very flexible\n",
        "* ‚ú® Easy to scale\n",
        "* ‚ú® No hardcoded order\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "* ‚ùå Less predictable\n",
        "* ‚ùå Harder to debug\n",
        "* ‚ùå May get confused or loop\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Option 2: Use Structured Stages (Manual Tool Chaining)**\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* Break the task into **discrete stages**\n",
        "* Run **specific tools** in a predefined order\n",
        "* Use intermediate data (like dicts) to pass between steps\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"profile\": lambda x: search_linkedin(x[\"name\"]),\n",
        "    }) |\n",
        "    RunnableMap({\n",
        "        \"details\": lambda x: extract_person_details(x[\"profile\"]),\n",
        "    }) |\n",
        "    RunnableMap({\n",
        "        \"company_research\": lambda x: research_company(x[\"details\"][\"company\"]),\n",
        "    }) |\n",
        "    RunnableLambda(lambda x: generate_email(x[\"details\"])) |\n",
        "    RunnableLambda(lambda email: send_email(email))\n",
        ")\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "* ‚úÖ Precise control\n",
        "* ‚úÖ Debuggable\n",
        "* ‚úÖ Safer for sensitive tasks\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "* ‚ùå Less dynamic\n",
        "* ‚ùå Can‚Äôt easily branch or adapt\n",
        "\n",
        "---\n",
        "\n",
        "## üîÄ Pipe (`|`) vs AgentExecutor ‚Äî Why It‚Äôs Not Used Here\n",
        "\n",
        "You‚Äôre right: we didn‚Äôt use the `|` pipe syntax (`Runnable` chains) in the agent example. Here's why:\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Pipe (`|`) is for **Structured Data Pipelines**\n",
        "\n",
        "You use it when:\n",
        "\n",
        "* You know the sequence of actions\n",
        "* You're processing step-by-step (like a function pipeline)\n",
        "* You want **manual control** over the flow\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> ‚ÄúTake this input ‚Üí transform it ‚Üí transform it again ‚Üí produce output‚Äù\n",
        "\n",
        "This is great for:\n",
        "\n",
        "* Prompt templates\n",
        "* Output parsing\n",
        "* Tool chaining with known structure\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ AgentExecutor is for **Autonomous Agents**\n",
        "\n",
        "You use it when:\n",
        "\n",
        "* You give the LLM a goal\n",
        "* The agent chooses its **own tool sequence**\n",
        "* You want **flexibility and reasoning**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> ‚ÄúHere‚Äôs your toolbox and a goal. Go figure it out.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ When to Use Each\n",
        "\n",
        "| Use Case                             | Recommended Approach                                      |         |\n",
        "| ------------------------------------ | --------------------------------------------------------- | ------- |\n",
        "| Structured pipelines                 | `RunnableMap` / \\`                                        | \\` pipe |\n",
        "| Dynamic reasoning / planning         | `AgentExecutor`                                           |         |\n",
        "| Hybrid (some structure, some agency) | Use `pipe` inside tools or chains, but wrap with an agent |         |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "* Tool chaining becomes key when tools depend on each other.\n",
        "* You can let the LLM handle chaining (agent-based) or do it yourself (pipe-based).\n",
        "* Pipes are deterministic; agents are autonomous.\n",
        "* For **controlled experiments and learning**, start with pipe.\n",
        "* For **realistic UX and autonomy**, migrate to agents later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3631308",
      "metadata": {
        "id": "e3631308"
      },
      "source": [
        "\n",
        "\n",
        "## ‚úÖ Your Strategy: Use the LLM to Generate a Plan First\n",
        "\n",
        "> ‚ÄúGive the agent a **clear persona + goal**, let it **strategize the steps**, and then use tools to act.‚Äù\n",
        "\n",
        "### ‚ú≥Ô∏è Example Goal Prompt:\n",
        "\n",
        "```text\n",
        "You are a B2B tech sales professional hungry for new leads.\n",
        "\n",
        "Your job is to:\n",
        "1. Discover and qualify new leads on LinkedIn.\n",
        "2. Research their company to understand their needs.\n",
        "3. Craft a highly personalized cold email that shows how Product X can help increase their sales.\n",
        "4. Send the email via a CRM tool.\n",
        "\n",
        "Provide a step-by-step plan for how you would approach this task.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üî• Why This Approach Is Smart\n",
        "\n",
        "| Benefit                  | Why It Matters                                     |\n",
        "| ------------------------ | -------------------------------------------------- |\n",
        "| üéØ Goal-aligned behavior | The agent knows the **‚Äúwhy‚Äù** behind each step     |\n",
        "| ü™ú LLM as planner        | LLMs are good at **strategy & decomposition**      |\n",
        "| üß© Tools follow logic    | You can **dynamically decide what tools to build** |\n",
        "| üß™ Testable in pieces    | You can run/test each step individually            |\n",
        "| üß† Encourages autonomy   | Agents become more than glorified API wrappers     |\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è How to Implement It (2-Step Agent Loop)\n",
        "\n",
        "### **Step 1: Strategy Planning Agent**\n",
        "\n",
        "We‚Äôll use a simple LLM chain like this:\n",
        "\n",
        "```python\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "goal_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a B2B tech sales expert. Your mission is to discover new leads and convert them.\n",
        "\n",
        "Your objective:\n",
        "{goal}\n",
        "\n",
        "Generate a step-by-step plan to achieve this goal using tools, reasoning, and outreach strategy.\n",
        "\"\"\")\n",
        "\n",
        "goal_chain = goal_prompt | llm\n",
        "\n",
        "# Run once to get strategy\n",
        "plan = goal_chain.invoke({\n",
        "    \"goal\": \"Find and email qualified leads from LinkedIn who would benefit from Product X.\"\n",
        "})\n",
        "\n",
        "print(plan.content)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Build Tool Registry Based on Plan**\n",
        "\n",
        "Once we see the steps (e.g., ‚ÄúSearch LinkedIn ‚Üí Extract Info ‚Üí Research Company ‚Üí Craft Message ‚Üí Send Email‚Äù), we can say:\n",
        "\n",
        "> ‚ÄúCool. Now let me wrap tools around each step.‚Äù\n",
        "\n",
        "These tools might be dummies at first, like:\n",
        "\n",
        "```python\n",
        "@tool\n",
        "def search_linkedin(name: str) -> str:\n",
        "    return f\"Fake LinkedIn profile for {name}\"\n",
        "\n",
        "@tool\n",
        "def extract_info(profile: str) -> dict:\n",
        "    return {\"name\": \"Jane Doe\", \"company\": \"SalesForce\", \"title\": \"Director of Sales\"}\n",
        "\n",
        "@tool\n",
        "def research_company(name: str) -> str:\n",
        "    return f\"Top headlines about {name}...\"\n",
        "\n",
        "@tool\n",
        "def craft_message(profile_data: dict, company_info: str) -> str:\n",
        "    return f\"Hi {profile_data['name']}, I think Product X can help...\"\n",
        "\n",
        "@tool\n",
        "def send_email(message: str) -> str:\n",
        "    return \"Email sent successfully!\"\n",
        "```\n",
        "\n",
        "Then we just plug these into an agent like:\n",
        "\n",
        "```python\n",
        "agent = initialize_agent(\n",
        "    tools=[search_linkedin, extract_info, research_company, craft_message, send_email],\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Optional: Let Agent Execute Its Own Plan\n",
        "\n",
        "Want to go full circle?\n",
        "\n",
        "You can even feed the LLM‚Äôs **own plan back into itself** like:\n",
        "\n",
        "```python\n",
        "agent.run(f\"Execute this strategy:\\n{plan.content}\")\n",
        "```\n",
        "\n",
        "That gives the agent complete control: it *made the plan*, and now it‚Äôs *following the plan*. That's autonomy ‚Äî responsibly scoped.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e76059",
      "metadata": {
        "id": "b5e76059",
        "outputId": "391b1f28-62ea-483a-c2b3-fa41351fc7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† STRATEGY GENERATED BY LLM:\n",
            "1. **Define Ideal Customer Profile (ICP)**: Identify key characteristics of target companies and decision-makers.\n",
            "\n",
            "2. **Use LinkedIn Sales Navigator**: \n",
            "   - Search for leads based on ICP criteria (industry, company size, job title).\n",
            "   - Save relevant leads to a list for tracking.\n",
            "\n",
            "3. **Research Leads and Companies**: \n",
            "   - Use LinkedIn profiles, company websites, and news articles to gather insights on leads and their organizations.\n",
            "   - Note recent achievements, challenges, or industry trends.\n",
            "\n",
            "4. **Segment Leads**: \n",
            "   - Categorize leads based on their potential value and urgency (high, medium, low).\n",
            "\n",
            "5. **Craft Personalized Outreach Messages**: \n",
            "   - Start with a personalized greeting.\n",
            "   - Mention a specific detail from your research to establish relevance.\n",
            "   - Clearly articulate how Product X can address their needs or pain points.\n",
            "   - Include a call-to-action (e.g., schedule a call, request a demo).\n",
            "\n",
            "6. **Choose Outreach Method**: \n",
            "   - Decide whether to send the message via LinkedIn InMail or email based on lead preferences and contact information.\n",
            "\n",
            "7. **Utilize CRM Tool**: \n",
            "   - Log lead information and outreach details in the CRM (e.g., HubSpot, Salesforce).\n",
            "   - Set reminders for follow-ups.\n",
            "\n",
            "8. **Send the Outreach Message**: \n",
            "   - Review the message for clarity and professionalism.\n",
            "   - Send the message through the chosen platform.\n",
            "\n",
            "9. **Track Engagement**: \n",
            "   - Monitor responses and engagement metrics in the CRM.\n",
            "   - Adjust follow-up strategies based on lead interactions.\n",
            "\n",
            "10. **Follow Up**: \n",
            "   - Schedule follow-up messages based on lead responses or lack thereof.\n",
            "   - Continue to provide value in follow-ups (e.g., sharing relevant content or insights).\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create the prompt to guide the LLM\n",
        "goal_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a B2B tech sales expert working at Product X.\n",
        "\n",
        "Your goal is to:\n",
        "- Discover and qualify new leads on LinkedIn.\n",
        "- Research the lead and their company.\n",
        "- Craft a personalized outreach message.\n",
        "- Send that message via CRM or email tool.\n",
        "\n",
        "Your task is to:\n",
        "1. Break this down into a step-by-step strategy.\n",
        "2. Include tool usage where appropriate.\n",
        "3. Be efficient, logical, and persuasive.\n",
        "\n",
        "Respond ONLY with the steps. No explanations.\n",
        "\"\"\")\n",
        "\n",
        "# Create the chain with a parser\n",
        "strategy_chain = goal_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run it!\n",
        "strategy = strategy_chain.invoke({})\n",
        "print(\"üß† STRATEGY GENERATED BY LLM:\")\n",
        "print(strategy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b536a60f",
      "metadata": {
        "id": "b536a60f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize dummy LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# üîÆ Generate step-by-step outreach strategy using LLM\n",
        "goal_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a B2B tech sales expert working at Product X.\n",
        "\n",
        "Your goal is to:\n",
        "- Discover and qualify new leads on LinkedIn.\n",
        "- Research the lead and their company.\n",
        "- Craft a personalized outreach message.\n",
        "- Send that message via CRM or email tool.\n",
        "\n",
        "Your task is to:\n",
        "1. Break this down into a step-by-step strategy.\n",
        "2. Include tool usage where appropriate.\n",
        "3. Be efficient, logical, and persuasive.\n",
        "\n",
        "Respond ONLY with the steps. No explanations.\n",
        "\"\"\")\n",
        "\n",
        "# Chain to get strategy\n",
        "strategy_chain = goal_prompt | llm | StrOutputParser()\n",
        "outreach_strategy = strategy_chain.invoke({})\n",
        "print(\"üß† STRATEGY GENERATED BY LLM:\")\n",
        "print(outreach_strategy)\n",
        "\n",
        "# üì¶ Tool registry\n",
        "tool_registry = {}\n",
        "\n",
        "def register_tool(tool_fn):\n",
        "    tool_registry[tool_fn.name] = tool_fn\n",
        "    return tool_fn\n",
        "\n",
        "# üß∞ Dummy tools\n",
        "@tool\n",
        "@register_tool\n",
        "def define_icp() -> str:\n",
        "    return \"Defined Ideal Customer Profile.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def search_linkedin() -> str:\n",
        "    return \"Found 10 matching LinkedIn profiles.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def research_lead() -> str:\n",
        "    return \"Researched lead and their company.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def segment_leads() -> str:\n",
        "    return \"Leads segmented into High, Medium, Low tiers.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def craft_outreach() -> str:\n",
        "    return \"Wrote a personalized outreach message.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def choose_outreach() -> str:\n",
        "    return \"Decided to send email based on profile info.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def log_to_crm() -> str:\n",
        "    return \"Logged lead to CRM with notes.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def send_message() -> str:\n",
        "    return \"Sent message to lead via chosen platform.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def track_engagement() -> str:\n",
        "    return \"Tracking email open and reply rates.\"\n",
        "\n",
        "@tool\n",
        "@register_tool\n",
        "def follow_up() -> str:\n",
        "    return \"Follow-up scheduled for 3 days later.\"\n",
        "\n",
        "# üîÅ Build and run the agent\n",
        "agent = create_openai_functions_agent(llm, list(tool_registry.values()))\n",
        "agent_executor = AgentExecutor(agent=agent, tools=list(tool_registry.values()), verbose=True)\n",
        "\n",
        "# üöÄ Invoke the agent\n",
        "result = agent_executor.invoke({\"input\": \"Find and reach out to new sales leads on LinkedIn\"})\n",
        "print(\"\\n‚úÖ FINAL AGENT RESULT:\")\n",
        "print(result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be41c8a4",
      "metadata": {
        "id": "be41c8a4"
      },
      "source": [
        "### ‚úÖ Benefits of This Setup\n",
        "\n",
        "* Everything is **encapsulated** in a single cell ‚Äî super portable and modular\n",
        "* You get **live generation** of the outreach plan\n",
        "* Agent + tools + registry all stay **decoupled and clean**\n",
        "* Easy to swap in a different goal, toolset, or strategy without touching the core logic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚úÖ Why This Scaffold Is So Effective:\n",
        "\n",
        "This structure is doing **a lot** of heavy lifting while keeping everything **intuitive and clean**:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Encapsulated & Modular**\n",
        "\n",
        "* The **goal, strategy, tools, registry, and agent** are all neatly contained in a single block.\n",
        "* You can **swap components** in or out without unraveling the whole system.\n",
        "* Example: Want to replace the goal? Swap one string. Want to add a tool? Add a decorated function. ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Agent-First Thinking**\n",
        "\n",
        "* By **letting the LLM plan** (via `goal_prompt`), you stay aligned with the agent-based paradigm.\n",
        "* You‚Äôre not just scripting behavior ‚Äî you're **defining a thinking entity** with a goal and tools.\n",
        "* The agent **selects actions**, not just executes instructions.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **The Tool Registry Is GOLD**\n",
        "\n",
        "* Mirrors real-world agent setups like **ReAct**, **OpenAIFunctionsAgent**, and even **LangGraph** node routing.\n",
        "* Gives you **traceability** and a clear place to manage your tool layer.\n",
        "* Makes your agent **transparent and auditable**, especially if it scales.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Beautifully Readable**\n",
        "\n",
        "* No mystery functions.\n",
        "* Clear decorators (`@tool`, `@register_tool`).\n",
        "* Minimal abstraction.\n",
        "* You could **teach this structure to a teammate in 5 minutes**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Scalable Starting Point**\n",
        "\n",
        "* Add multi-step workflows.\n",
        "* Replace dummy tools with API calls or LangChain toolkits.\n",
        "* Plug this into **LangGraph**, **FastAPI**, or **Streamlit** without major refactoring.\n",
        "\n",
        "---\n",
        "\n",
        "If you're learning LangChain, this setup is like having a **custom command center** to experiment with strategies, tools, and agent behavior ‚Äî without clutter, confusion, or overengineering.\n",
        "\n",
        "Let me know when you‚Äôre ready to:\n",
        "\n",
        "* Add memory\n",
        "* Introduce stages or routing\n",
        "* Build a LangGraph version\n",
        "* Or turn this into a real lead gen stack!\n",
        "\n",
        "üöÄ You're building the *right* mental model.\n"
      ],
      "metadata": {
        "id": "wry9HSdsrdyl"
      },
      "id": "wry9HSdsrdyl"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bb0SNQ_NrfPf"
      },
      "id": "Bb0SNQ_NrfPf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}