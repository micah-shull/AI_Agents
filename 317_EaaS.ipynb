{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJZQ8ao1aGfN5/4vFOZOEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/317_EaaS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ§© **Introduction to an Evaluations-as-a-Service (EaaS) Agent**\n",
        "\n",
        "An **Evaluations-as-a-Service (EaaS) Agent** is an AI system designed to **audit and evaluate the performance, reliability, and safety of other AI agents**. Think of it as the *quality assurance* layer of the AI ecosystem â€” the AI that evaluates other AIs.\n",
        "\n",
        "As organizations increasingly deploy agentic systems across workflows, the need for **continuous, automated oversight** becomes essential. Thatâ€™s exactly what an EaaS agent provides.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ§  **What Is an EaaS Agent?**\n",
        "\n",
        "An EaaS agent is a specialized agent that:\n",
        "\n",
        "### **âœ”ï¸ Generates evaluation scenarios**\n",
        "\n",
        "Synthetic or real-world test cases that simulate the tasks other agents must perform.\n",
        "\n",
        "### **âœ”ï¸ Produces ground-truth outputs**\n",
        "\n",
        "The correct answers, safe responses, or expected behaviors.\n",
        "\n",
        "### **âœ”ï¸ Runs test tasks through other agents**\n",
        "\n",
        "It acts as the orchestrator, sending inputs and retrieving outputs.\n",
        "\n",
        "### **âœ”ï¸ Scores and analyzes agent performance**\n",
        "\n",
        "Evaluates correctness, quality, tone, safety, reasoning, and consistency.\n",
        "\n",
        "### **âœ”ï¸ Detects drift and failures over time**\n",
        "\n",
        "Tracks when agent behavior changes â€” often before humans notice.\n",
        "\n",
        "### **âœ”ï¸ Outputs comprehensive evaluation reports**\n",
        "\n",
        "Summaries, metrics, dashboards, and alerts.\n",
        "\n",
        "This transforms AI agent testing from a manual chore into an automated, scalable service.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŽ¯ **What Does It Actually Do?**\n",
        "\n",
        "Hereâ€™s what an EaaS agent performs under the hood:\n",
        "\n",
        "### **1. Builds evaluation datasets**\n",
        "\n",
        "Synthetic or workflow-specific test cases.\n",
        "\n",
        "### **2. Defines evaluation criteria and scoring rules**\n",
        "\n",
        "Accuracy, safety, tone, hallucination detection, latency, etc.\n",
        "\n",
        "### **3. Routes tasks to target agents**\n",
        "\n",
        "Sends each test case to the agent being evaluated.\n",
        "\n",
        "### **4. Compares the actual output to ground truth**\n",
        "\n",
        "Using rule-based checks or LLM-as-a-judge scoring.\n",
        "\n",
        "### **5. Logs reasoning, output quality, and metrics**\n",
        "\n",
        "Versioned and timestamped for monitoring.\n",
        "\n",
        "### **6. Surfaces insights**\n",
        "\n",
        "* Where the agent is strong\n",
        "* Where it fails\n",
        "* What changed since the last version\n",
        "* What needs improvement\n",
        "\n",
        "### **7. Enables continuous monitoring**\n",
        "\n",
        "Running nightly, weekly, or on model updates.\n",
        "\n",
        "It becomes the **automated QA department for agents**.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ’° **What Makes It Valuable?**\n",
        "\n",
        "### **1. Every company deploying agents needs evaluation**\n",
        "\n",
        "As AI agents take over real workflows, they must be safe, correct, and reliable. Human-only QA is too slow and expensive.\n",
        "\n",
        "### **2. Agent behavior drifts rapidly**\n",
        "\n",
        "LLMs change, instructions evolve, and agent logic adapts. Without monitoring, outputs become:\n",
        "\n",
        "* inconsistent\n",
        "* unsafe\n",
        "* misaligned\n",
        "* inaccurate\n",
        "\n",
        "Evaluation agents catch these early.\n",
        "\n",
        "### **3. Needed for compliance and governance**\n",
        "\n",
        "Enterprises need proof of:\n",
        "\n",
        "* correctness\n",
        "* safety\n",
        "* explainability\n",
        "* policy alignment\n",
        "\n",
        "EaaS agents provide structured audit trails.\n",
        "\n",
        "### **4. Required for orchestration systems**\n",
        "\n",
        "Large agent systems rely on:\n",
        "\n",
        "* routing agents\n",
        "* memory agents\n",
        "* retrieval agents\n",
        "* task-specific specialist agents\n",
        "\n",
        "An evaluator agent is what keeps the entire ecosystem stable.\n",
        "\n",
        "### **5. Reduces human review load**\n",
        "\n",
        "A good evaluator performs *80%* of the checking automatically, leaving humans only for ambiguous or critical cases.\n",
        "\n",
        "### **6. Foundational for ROI measurement**\n",
        "\n",
        "Evaluation metrics connect AI agent behavior to business value.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸš€ **Why EaaS Agents Are the Future of Agent Development**\n",
        "\n",
        "We are entering a world where companies will run:\n",
        "\n",
        "* 50 agents\n",
        "* then 500 agents\n",
        "* eventually *thousands* of interoperable agents\n",
        "\n",
        "This creates new problems:\n",
        "\n",
        "### **1. How do you ensure every agent is performing well?**\n",
        "\n",
        "You need automated checks.\n",
        "\n",
        "### **2. How do you detect when an agent starts hallucinating more than usual?**\n",
        "\n",
        "You need drift monitoring.\n",
        "\n",
        "### **3. How do you know which agent is best for a task?**\n",
        "\n",
        "You need performance benchmarking.\n",
        "\n",
        "### **4. How do you build trust with non-technical stakeholders?**\n",
        "\n",
        "You need evaluation reports and dashboards.\n",
        "\n",
        "### **5. How do you orchestrate multi-agent systems safely?**\n",
        "\n",
        "You need a â€œmeta-agentâ€ supervising the ecosystem.\n",
        "\n",
        "Every mature AI ecosystem will have:\n",
        "\n",
        "* *Orchestrators* controlling the workflows\n",
        "* *Memory systems* storing state\n",
        "* *Tool agents* performing tasks\n",
        "* **Evaluation agents ensuring everything works**\n",
        "\n",
        "This agent class becomes as essential as CI/CD pipelines in modern software engineering.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŒŸ **In simple terms:**\n",
        "\n",
        "> **EaaS agents are the quality control, safety guardian, performance benchmarker, and governance layer for AI agent ecosystems.**\n",
        "\n",
        "They turn agentic systems from experimental prototypes into reliable production infrastructure.\n",
        "\n"
      ],
      "metadata": {
        "id": "MEniLBsajFgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation-as-a-Service (EaaS) Orchestrator Agent"
      ],
      "metadata": {
        "id": "i3wALWMNpZrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svrO9SyKjE1v"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Evaluation-as-a-Service (EaaS) Orchestrator Agent\n",
        "# ============================================================================\n",
        "\n",
        "class EvalAsServiceOrchestratorState(TypedDict, total=False):\n",
        "    \"\"\"State for Evaluation-as-a-Service Orchestrator Agent\"\"\"\n",
        "\n",
        "    # Input fields\n",
        "    scenario_id: Optional[str]              # Specific scenario to evaluate (None = evaluate all)\n",
        "    target_agent_id: Optional[str]          # Specific agent to evaluate (None = evaluate all)\n",
        "\n",
        "    # Goal & Planning fields (MVP: Fixed goal, template-based plan)\n",
        "    goal: Dict[str, Any]                    # Goal definition (from goal_node)\n",
        "    plan: List[Dict[str, Any]]              # Execution plan (from planning_node)\n",
        "\n",
        "    # Data Ingestion\n",
        "    journey_scenarios: List[Dict[str, Any]]  # Loaded test scenarios\n",
        "    # Structure per scenario:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"customer_id\": \"C001\",\n",
        "    #   \"order_id\": \"O1001\",\n",
        "    #   \"customer_message\": \"Hi, my order hasn't arrived yet...\",\n",
        "    #   \"expected_issue_type\": \"where_is_my_order\",\n",
        "    #   \"expected_resolution_path\": [\"shipping_update_agent\"],\n",
        "    #   \"expected_outcome\": \"provide_delivery_update\"\n",
        "    # }\n",
        "\n",
        "    specialist_agents: List[Dict[str, Any]]  # Loaded specialist agents to evaluate\n",
        "    # Structure per agent:\n",
        "    # {\n",
        "    #   \"agent_id\": \"refund_agent\",\n",
        "    #   \"description\": \"Issues refunds for lost or incorrect orders.\",\n",
        "    #   \"actions\": {...}\n",
        "    # }\n",
        "\n",
        "    supporting_data: Dict[str, Any]         # Supporting data (customers, orders, logistics, marketing)\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"customers\": [...],\n",
        "    #   \"orders\": [...],\n",
        "    #   \"logistics\": {...},\n",
        "    #   \"marketing_signals\": [...]\n",
        "    # }\n",
        "\n",
        "    decision_rules: Dict[str, Any]          # Orchestrator decision rules for validation\n",
        "\n",
        "    # Evaluation Execution\n",
        "    executed_evaluations: List[Dict[str, Any]]  # Completed evaluations\n",
        "    # Structure per evaluation:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"target_agent_id\": \"shipping_update_agent\",\n",
        "    #   \"input\": {...},\n",
        "    #   \"actual_output\": {...},\n",
        "    #   \"expected_output\": {...},\n",
        "    #   \"execution_time_seconds\": 0.5,\n",
        "    #   \"status\": \"completed\" | \"failed\" | \"timeout\",\n",
        "    #   \"error\": Optional[str]\n",
        "    # }\n",
        "\n",
        "    # Scoring & Analysis\n",
        "    evaluation_scores: List[Dict[str, Any]]  # Scores per evaluation\n",
        "    # Structure per score:\n",
        "    # {\n",
        "    #   \"scenario_id\": \"S001\",\n",
        "    #   \"target_agent_id\": \"shipping_update_agent\",\n",
        "    #   \"correctness_score\": 0.95,  # 0-1, matches expected outcome\n",
        "    #   \"response_time_score\": 0.90,  # 0-1, based on thresholds\n",
        "    #   \"output_quality_score\": 0.85,  # 0-1, based on structure/format\n",
        "    #   \"overall_score\": 0.90,  # Weighted average\n",
        "    #   \"passed\": True,  # Overall score >= threshold\n",
        "    #   \"issues\": [\"slight_format_deviation\"]\n",
        "    # }\n",
        "\n",
        "    agent_performance_summary: Dict[str, Any]  # Performance summary per agent\n",
        "    # Structure per agent:\n",
        "    # {\n",
        "    #   \"agent_id\": \"shipping_update_agent\",\n",
        "    #   \"total_evaluations\": 10,\n",
        "    #   \"passed_count\": 9,\n",
        "    #   \"failed_count\": 1,\n",
        "    #   \"average_score\": 0.88,\n",
        "    #   \"average_response_time\": 0.45,\n",
        "    #   \"health_status\": \"healthy\" | \"degraded\" | \"critical\"\n",
        "    # }\n",
        "\n",
        "    # Quality Control Metrics (using toolshed)\n",
        "    performance_metrics: Dict[str, Any]      # Performance tracking metrics\n",
        "    workflow_analysis: List[Dict[str, Any]]  # Workflow health analysis\n",
        "    validation_results: List[Dict[str, Any]]  # Data validation results\n",
        "\n",
        "    # Progress Tracking (using toolshed)\n",
        "    progress_percentage: float              # 0-100\n",
        "    evaluations_completed: int              # Count of completed evaluations\n",
        "    evaluations_total: int                  # Total evaluations to run\n",
        "    elapsed_time_seconds: float             # Time since evaluation start\n",
        "    estimated_remaining_seconds: float      # Estimated time to completion\n",
        "    evaluation_start_time: Optional[str]     # ISO timestamp when evaluation started\n",
        "\n",
        "    # Summary Metrics\n",
        "    evaluation_summary: Dict[str, Any]\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"total_scenarios\": 10,\n",
        "    #   \"total_evaluations\": 30,\n",
        "    #   \"total_passed\": 27,\n",
        "    #   \"total_failed\": 3,\n",
        "    #   \"overall_pass_rate\": 0.90,\n",
        "    #   \"average_score\": 0.87,\n",
        "    #   \"agents_evaluated\": 4,\n",
        "    #   \"healthy_agents\": 3,\n",
        "    #   \"degraded_agents\": 1,\n",
        "    #   \"critical_agents\": 0\n",
        "    # }\n",
        "\n",
        "    # Output\n",
        "    evaluation_report: str                  # Final markdown report\n",
        "    report_file_path: Optional[str]        # Path to saved report file\n",
        "\n",
        "    # Metadata\n",
        "    errors: Annotated[List[str], operator.add]  # Any errors encountered\n",
        "    processing_time: Optional[float]      # Time taken to process\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvalAsServiceOrchestratorConfig:\n",
        "    \"\"\"Configuration for Evaluation-as-a-Service Orchestrator Agent\"\"\"\n",
        "    llm_model: str = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
        "    temperature: float = 0.3\n",
        "    reports_dir: str = \"output/eval_as_service_reports\"  # Where to save reports\n",
        "\n",
        "    # Data file paths\n",
        "    data_dir: str = \"agents/data\"\n",
        "    journey_scenarios_file: str = \"journey_scenarios.json\"\n",
        "    specialist_agents_file: str = \"specialist_agents.json\"\n",
        "    customers_file: str = \"customers.json\"\n",
        "    orders_file: str = \"orders.json\"\n",
        "    logistics_file: str = \"logistics_api.json\"\n",
        "    marketing_signals_file: str = \"marketing_signals.json\"\n",
        "    decision_rules_file: str = \"orchestrator_decision_rules.json\"\n",
        "\n",
        "    # Evaluation Settings\n",
        "    pass_threshold: float = 0.80  # Minimum score to pass (0-1)\n",
        "    response_time_threshold_seconds: float = 2.0  # Max acceptable response time\n",
        "    enable_parallel_evaluation: bool = False  # MVP: Sequential only\n",
        "\n",
        "    # Scoring Weights\n",
        "    scoring_weights: Dict[str, float] = field(default_factory=lambda: {\n",
        "        \"correctness\": 0.50,      # Matches expected outcome\n",
        "        \"response_time\": 0.20,     # Response time performance\n",
        "        \"output_quality\": 0.30     # Output structure/format quality\n",
        "    })\n",
        "\n",
        "    # Health Status Thresholds\n",
        "    health_thresholds: Dict[str, float] = field(default_factory=lambda: {\n",
        "        \"healthy\": 0.85,      # >= 85% average score\n",
        "        \"degraded\": 0.70,     # 70-85% average score\n",
        "        \"critical\": 0.0       # < 70% average score\n",
        "    })\n",
        "\n",
        "    # Toolshed Integration\n",
        "    enable_progress_tracking: bool = True   # Use toolshed.progress\n",
        "    enable_performance_tracking: bool = True  # Use toolshed.performance\n",
        "    enable_workflow_analysis: bool = True    # Use toolshed.workflows\n",
        "    enable_validation: bool = True          # Use toolshed.validation\n",
        "    enable_kpi_tracking: bool = True        # Use toolshed.kpi\n",
        "    enable_reporting: bool = True          # Use toolshed.reporting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Evaluation-as-a-Service (EaaS) Orchestrator â€” How It Works and Why It Matters\n",
        "\n",
        "This notebook documents the design of an **Evaluation-as-a-Service (EaaS) Orchestrator Agent**. The goal is not just to show working code, but to explain how the system is structured, what each major component does, and why this architecture matters when AI systems are used in real business workflows.\n",
        "\n",
        "At a high level, the orchestrator acts as **quality control for AI agents**. Instead of trusting agents blindly, it evaluates their behavior, measures performance, and produces clear, repeatable evidence of how well they perform.\n",
        "\n",
        "---\n",
        "\n",
        "## The Big Idea\n",
        "\n",
        "As organizations deploy more AI agents, a new problem appears:\n",
        "\n",
        "**Who checks the AI?**\n",
        "\n",
        "The EaaS Orchestrator answers that question by acting as a supervising agent that:\n",
        "\n",
        "* runs realistic test scenarios\n",
        "* sends them to target agents\n",
        "* compares results to expected outcomes\n",
        "* scores performance\n",
        "* summarizes findings in plain language\n",
        "\n",
        "This turns AI evaluation from a manual, subjective process into a **systematic and scalable service**.\n",
        "\n",
        "---\n",
        "\n",
        "## One State, One Story\n",
        "\n",
        "The `EvalAsServiceOrchestratorState` is the backbone of the system. It serves as a shared workspace where everything related to an evaluation lives.\n",
        "\n",
        "Inputs, decisions, results, scores, and reports are all stored in one place. This makes it easy to answer questions like:\n",
        "\n",
        "* What was tested?\n",
        "* What was expected?\n",
        "* What actually happened?\n",
        "* How well did it perform?\n",
        "* Has performance changed over time?\n",
        "\n",
        "Nothing is hidden, and nothing is lost.\n",
        "\n",
        "---\n",
        "\n",
        "## What Gets Evaluated\n",
        "\n",
        "### Scenarios: Real-World Situations\n",
        "\n",
        "The `journey_scenarios` represent realistic business situations, such as customer support requests or operational tasks. Each scenario defines:\n",
        "\n",
        "* the input the agent receives\n",
        "* the path it is expected to take\n",
        "* the outcome it should produce\n",
        "\n",
        "This ensures agents are evaluated on **real problems**, not artificial benchmarks.\n",
        "\n",
        "---\n",
        "\n",
        "### Agents: Who Is Being Tested\n",
        "\n",
        "The `specialist_agents` define which agents are being evaluated and what each one is responsible for. By modeling agents explicitly, the system can:\n",
        "\n",
        "* compare agents objectively\n",
        "* track performance over time\n",
        "* identify weak points or regressions\n",
        "\n",
        "This is especially useful as agent ecosystems grow larger.\n",
        "\n",
        "---\n",
        "\n",
        "## Running the Evaluations\n",
        "\n",
        "When a scenario is executed, the orchestrator records everything in `executed_evaluations`:\n",
        "\n",
        "* the input sent to the agent\n",
        "* the output it produced\n",
        "* the expected output\n",
        "* how long it took\n",
        "* whether it succeeded or failed\n",
        "\n",
        "This creates a clear record that can be reviewed later for debugging, audits, or performance reviews.\n",
        "\n",
        "---\n",
        "\n",
        "## Scoring What Matters\n",
        "\n",
        "Performance is broken into simple, understandable dimensions:\n",
        "\n",
        "* **Correctness** â€“ did the agent solve the right problem?\n",
        "* **Response time** â€“ was it fast enough?\n",
        "* **Output quality** â€“ was the response structured and usable?\n",
        "\n",
        "Each evaluation receives a score, and those scores are combined into an overall result. This makes performance measurable instead of subjective.\n",
        "\n",
        "---\n",
        "\n",
        "## From Scores to Agent Health\n",
        "\n",
        "Individual scores are useful, but summaries are what decision-makers care about.\n",
        "\n",
        "The `agent_performance_summary` rolls up results into a clear health signal for each agent:\n",
        "\n",
        "* how often it passes or fails\n",
        "* its average score\n",
        "* its typical response time\n",
        "* whether it is considered healthy, degraded, or critical\n",
        "\n",
        "This allows teams to quickly identify which agents are safe to rely on and which ones need attention.\n",
        "\n",
        "---\n",
        "\n",
        "## Keeping an Eye on the System\n",
        "\n",
        "The orchestrator integrates with monitoring tools to track:\n",
        "\n",
        "* execution performance\n",
        "* workflow health\n",
        "* data validation results\n",
        "* progress and timing\n",
        "\n",
        "This ensures the evaluation system itself remains reliable and predictable, not just the agents it evaluates.\n",
        "\n",
        "---\n",
        "\n",
        "## Progress and Predictability\n",
        "\n",
        "Evaluation runs can take time, especially as the number of scenarios grows. Progress tracking fields provide visibility into:\n",
        "\n",
        "* how much work has been completed\n",
        "* how long the process has been running\n",
        "* how much time remains\n",
        "\n",
        "This makes evaluations easier to plan and easier to trust.\n",
        "\n",
        "---\n",
        "\n",
        "## Executive-Ready Results\n",
        "\n",
        "At the end of a run, the orchestrator produces a concise evaluation summary that answers key questions at a glance:\n",
        "\n",
        "* How many scenarios were tested?\n",
        "* How many evaluations passed or failed?\n",
        "* What is the overall pass rate?\n",
        "* How many agents are healthy, degraded, or critical?\n",
        "\n",
        "These summaries are designed to be read quickly and confidently by business leaders.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuration as Policy, Not Code\n",
        "\n",
        "The `EvalAsServiceOrchestratorConfig` defines how evaluations behave through clear settings:\n",
        "\n",
        "* minimum passing scores\n",
        "* scoring weights\n",
        "* health thresholds\n",
        "* monitoring features to enable or disable\n",
        "\n",
        "This separates **business standards** from implementation details, allowing expectations to change without rewriting logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Design Scales\n",
        "\n",
        "This architecture treats evaluation as first-class infrastructure. It supports:\n",
        "\n",
        "* trust and transparency\n",
        "* early detection of failures or drift\n",
        "* reduced reliance on manual review\n",
        "* clear links between AI behavior and business outcomes\n",
        "\n",
        "As AI systems grow more complex, this kind of evaluation layer becomes essential for moving from experimentation to production.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dvt_hGiqL043"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation-as-a-Service (EaaS) Orchestrator Agent\n",
        "\n",
        "**Purpose:** Automated evaluation and quality assurance for AI agents. This orchestrator runs test scenarios through target agents, scores their performance, and generates comprehensive evaluation reports.\n",
        "\n",
        "**Status:** MVP - Ready for testing and enhancement\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ What It Does\n",
        "\n",
        "The EaaS Orchestrator Agent:\n",
        "\n",
        "1. **Loads Test Scenarios** - Reads journey scenarios with expected outcomes\n",
        "2. **Executes Evaluations** - Runs scenarios through specialist agents (simulated in MVP)\n",
        "3. **Scores Performance** - Compares actual outputs to expected outcomes\n",
        "4. **Analyzes Results** - Calculates agent performance summaries and health status\n",
        "5. **Generates Reports** - Creates comprehensive evaluation reports with metrics\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ï¸ Architecture\n",
        "\n",
        "### **State Schema**\n",
        "Located in `config.py` as `EvalAsServiceOrchestratorState`:\n",
        "- Input: `scenario_id`, `target_agent_id` (optional filters)\n",
        "- Data: Scenarios, agents, supporting data, decision rules\n",
        "- Execution: Evaluations, scores, performance summaries\n",
        "- Output: Evaluation report, report file path\n",
        "\n",
        "### **Workflow**\n",
        "Linear orchestration flow:\n",
        "```\n",
        "Goal â†’ Planning â†’ Data Loading â†’ Evaluation Execution â†’ Scoring â†’ Performance Analysis â†’ Report Generation\n",
        "```\n",
        "\n",
        "### **Nodes**\n",
        "- `goal_node` - Defines evaluation goal\n",
        "- `planning_node` - Creates execution plan\n",
        "- `data_loading_node` - Loads scenarios, agents, and supporting data\n",
        "- `evaluation_execution_node` - Runs scenarios through agents\n",
        "- `scoring_node` - Scores evaluations against expected outcomes\n",
        "- `performance_analysis_node` - Calculates agent performance summaries\n",
        "- `report_generation_node` - Generates comprehensive evaluation report\n",
        "\n",
        "### **Utilities**\n",
        "Located in `utilities.py`:\n",
        "- Data loading functions\n",
        "- Agent execution simulation (MVP: rule-based)\n",
        "- Scoring logic\n",
        "- Performance calculation\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Usage\n",
        "\n",
        "### **Basic Usage**\n",
        "\n",
        "```python\n",
        "from config import EvalAsServiceOrchestratorConfig\n",
        "from agents.eval_as_service.orchestrator import create_orchestrator\n",
        "\n",
        "# Create orchestrator\n",
        "config = EvalAsServiceOrchestratorConfig()\n",
        "orchestrator = create_orchestrator(config)\n",
        "\n",
        "# Run evaluation (all scenarios, all agents)\n",
        "initial_state = {\n",
        "    \"scenario_id\": None,\n",
        "    \"target_agent_id\": None,\n",
        "    \"errors\": []\n",
        "}\n",
        "\n",
        "result = orchestrator.invoke(initial_state)\n",
        "\n",
        "# Access results\n",
        "print(result[\"evaluation_report\"])\n",
        "print(f\"Pass rate: {result['evaluation_summary']['overall_pass_rate']:.1%}\")\n",
        "```\n",
        "\n",
        "### **Evaluate Single Scenario**\n",
        "\n",
        "```python\n",
        "initial_state = {\n",
        "    \"scenario_id\": \"S001\",  # Single scenario\n",
        "    \"target_agent_id\": None,\n",
        "    \"errors\": []\n",
        "}\n",
        "result = orchestrator.invoke(initial_state)\n",
        "```\n",
        "\n",
        "### **Evaluate Single Agent**\n",
        "\n",
        "```python\n",
        "initial_state = {\n",
        "    \"scenario_id\": None,\n",
        "    \"target_agent_id\": \"shipping_update_agent\",  # Single agent\n",
        "    \"errors\": []\n",
        "}\n",
        "result = orchestrator.invoke(initial_state)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Testing\n",
        "\n",
        "Run the test file:\n",
        "\n",
        "```bash\n",
        "python test_eval_as_service.py\n",
        "```\n",
        "\n",
        "Or use pytest:\n",
        "\n",
        "```bash\n",
        "pytest test_eval_as_service.py -v\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Quality Control Tools Integration\n",
        "\n",
        "The agent integrates several toolshed quality control utilities:\n",
        "\n",
        "### **Performance Tracking** (`toolshed.performance`)\n",
        "- Tracks evaluation execution times\n",
        "- Configurable metrics with thresholds\n",
        "- Performance monitoring\n",
        "\n",
        "### **Workflow Analysis** (`toolshed.workflows`)\n",
        "- Analyzes workflow health based on failure rates\n",
        "- Generates recommendations\n",
        "- Multi-workflow analysis\n",
        "\n",
        "### **Validation** (`toolshed.validation`)\n",
        "- Validates data structure\n",
        "- Checks required fields\n",
        "- Data quality checks\n",
        "\n",
        "### **Progress Tracking** (`toolshed.progress`)\n",
        "- Calculates progress percentage\n",
        "- Estimates remaining time\n",
        "- Tracks elapsed time\n",
        "\n",
        "### **Reporting** (`toolshed.reporting`)\n",
        "- Generates markdown reports\n",
        "- Saves reports to file system\n",
        "- Structured report format\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Data Files\n",
        "\n",
        "The agent expects data files in `agents/data/`:\n",
        "- `journey_scenarios.json` - Test scenarios with expected outcomes\n",
        "- `specialist_agents.json` - Agent configurations\n",
        "- `customers.json` - Customer data\n",
        "- `orders.json` - Order data\n",
        "- `logistics_api.json` - Logistics data\n",
        "- `marketing_signals.json` - Marketing signals\n",
        "- `orchestrator_decision_rules.json` - Decision rules\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Configuration\n",
        "\n",
        "Configuration is in `config.py` as `EvalAsServiceOrchestratorConfig`:\n",
        "\n",
        "- **Pass Threshold**: 0.80 (minimum score to pass)\n",
        "- **Response Time Threshold**: 2.0 seconds\n",
        "- **Scoring Weights**:\n",
        "  - Correctness: 50%\n",
        "  - Response Time: 20%\n",
        "  - Output Quality: 30%\n",
        "- **Health Thresholds**:\n",
        "  - Healthy: >= 85% average score\n",
        "  - Degraded: 70-85% average score\n",
        "  - Critical: < 70% average score\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”„ MVP Limitations\n",
        "\n",
        "**Current MVP is rule-based and simulated:**\n",
        "\n",
        "1. **Agent Execution**: Simulated (not actual API calls)\n",
        "   - Future: Replace with actual agent API calls\n",
        "   - Future: Support async/parallel execution\n",
        "\n",
        "2. **Scoring**: Rule-based comparison\n",
        "   - Future: LLM-as-a-judge scoring\n",
        "   - Future: More sophisticated evaluation criteria\n",
        "\n",
        "3. **Sequential Execution**: Evaluations run one at a time\n",
        "   - Future: Parallel evaluation execution\n",
        "\n",
        "4. **Simple Metrics**: Basic correctness, response time, output quality\n",
        "   - Future: Advanced metrics (hallucination detection, safety, tone)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ˆ Future Enhancements\n",
        "\n",
        "1. **Real Agent Integration** - Replace simulation with actual agent API calls\n",
        "2. **LLM-as-a-Judge** - Use LLM for more sophisticated scoring\n",
        "3. **Parallel Execution** - Run multiple evaluations concurrently\n",
        "4. **Advanced Metrics** - Add hallucination detection, safety checks, tone analysis\n",
        "5. **Drift Detection** - Track performance changes over time\n",
        "6. **Dashboard** - Visual dashboard for evaluation results\n",
        "7. **Alerting** - Alert when agents fail or degrade\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› Troubleshooting\n",
        "\n",
        "**Issue: Data loading errors**\n",
        "- Check that data files exist in `agents/data/`\n",
        "- Verify JSON file format is correct\n",
        "\n",
        "**Issue: Agent not found**\n",
        "- Check `specialist_agents.json` for agent IDs\n",
        "- Verify agent_id matches expected format\n",
        "\n",
        "**Issue: Scoring issues**\n",
        "- Check that scenarios have expected outcomes\n",
        "- Verify scoring weights sum to 1.0\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Notes\n",
        "\n",
        "- This is an MVP implementation focused on getting the core workflow working\n",
        "- Agent execution is simulated for MVP (will be replaced with real calls)\n",
        "- Scoring is rule-based (can be enhanced with LLM-as-a-judge)\n",
        "- All quality control tools are integrated and ready to use\n",
        "\n",
        "---\n",
        "\n",
        "**Built with:** LangGraph, toolshed utilities, rule-based MVP approach\n",
        "\n"
      ],
      "metadata": {
        "id": "cBpt1bv6peGv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGADvYUopi9b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}