{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3mj4ekk/dMF4hgIgoj17O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/030_Output_Schema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **critical concept** in tool-using agents:\n",
        "üîë **LLMs speak natural language**, but tools require **structured input** ‚Äî usually **JSON**.\n",
        "\n",
        "---\n",
        "\n",
        "## Structured Output\n",
        "\n",
        "When an LLM is used inside an agent, its job isn‚Äôt just to generate human-readable text.\n",
        "Instead, it must **output structured data** that follows a **strict schema**, so the system (you, your code, or OpenAI‚Äôs API) can:\n",
        "\n",
        "* **Recognize**: What tool it wants to call\n",
        "* **Extract**: The tool name and arguments\n",
        "* **Execute**: A corresponding Python function or API call\n",
        "\n",
        "And that‚Äôs where the format comes in üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Two Modes of LLM Interaction\n",
        "\n",
        "| Type                            | Description                               | Format               |\n",
        "| ------------------------------- | ----------------------------------------- | -------------------- |\n",
        "| **Chat/Conversation**           | Normal assistant replies                  | üó£ Natural language  |\n",
        "| **Tool Use (Function Calling)** | Structured interaction with external code | ‚úÖ **JSON structure** |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ So What‚Äôs the Challenge?\n",
        "\n",
        "By default, LLMs love to generate beautiful English text. But when tools are involved, the output **must follow a specific structure**, such as:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_call\": {\n",
        "    \"name\": \"search_file_names\",\n",
        "    \"arguments\": {\n",
        "      \"keyword\": \"memory\",\n",
        "      \"case_sensitive\": false\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "If the LLM generated:\n",
        "\n",
        "> I think we should search for \"memory\" ‚Äî let‚Äôs use the tool for that!\n",
        "\n",
        "‚Ä¶it wouldn‚Äôt be usable by the tool router.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How OpenAI Helps With This\n",
        "\n",
        "When using the **`tools`** and **`tool_choice=\"auto\"`** features in `chat.completions.create()`, you're telling the LLM:\n",
        "\n",
        "> ‚ÄúDon't give me an English-language response ‚Äî I want you to return a tool call in structured JSON instead.‚Äù\n",
        "\n",
        "Then, OpenAI **forces** the model to return a `tool_calls` block, like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"function\": {\n",
        "        \"name\": \"search_file_names\",\n",
        "        \"arguments\": \"{ \\\"keyword\\\": \\\"memory\\\", \\\"case_sensitive\\\": false }\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "So your code can extract it like this:\n",
        "\n",
        "```python\n",
        "tool_calls = response.choices[0].message.tool_calls\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ What You Should Learn\n",
        "\n",
        "### 1. **Tool-enabled LLMs return JSON, not natural language**\n",
        "\n",
        "* But **only if** you use `tools` and `tool_choice=\"auto\"`.\n",
        "\n",
        "### 2. **Your agent code needs to parse the output**\n",
        "\n",
        "* Typically, you‚Äôll `json.loads(arguments)` to convert the string to a dictionary.\n",
        "\n",
        "### 3. **Well-structured schemas improve tool usage**\n",
        "\n",
        "* The better your schema (with descriptions, types, required fields), the better the LLM is at calling tools correctly.\n",
        "\n"
      ],
      "metadata": {
        "id": "chKKRdf53lcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† Big Picture: Why This Feature Exists\n",
        "\n",
        "Language models (like GPT-3.5 / GPT-4) are *masters* of generating natural language ‚Äî but tools (your code, APIs, databases) require **structured input**.\n",
        "\n",
        "The **`tools` + `tool_choice=\"auto\"`** setup lets you tell the LLM:\n",
        "\n",
        "> ‚ÄúInstead of just replying in English, *decide if a tool should be used*, and if so, return the tool call in a special JSON format that I can extract and run.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ What You Provide to OpenAI\n",
        "\n",
        "In your API call, you send:\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    tools=tools,               # ‚úÖ Tell the LLM what tools exist\n",
        "    tool_choice=\"auto\",        # ‚úÖ Let it decide which to use\n",
        ")\n",
        "```\n",
        "\n",
        "You also provide a list of **tools**, like:\n",
        "\n",
        "```python\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_file_names\",\n",
        "            \"description\": \"Searches filenames for a keyword.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"keyword\": { \"type\": \"string\" },\n",
        "                    \"case_sensitive\": { \"type\": \"boolean\" }\n",
        "                },\n",
        "                \"required\": [\"keyword\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "This schema defines what the LLM **can** call ‚Äî the tools, their names, argument names/types, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ What the LLM Does\n",
        "\n",
        "When it gets your user message, the model decides:\n",
        "\n",
        "> ‚ÄúAh! This request looks like it‚Äôs asking for a tool. I know a tool called `search_file_names`. Let me return a tool call.‚Äù\n",
        "\n",
        "Instead of responding with:\n",
        "\n",
        "> ‚ÄúSure, I can search the files for ‚Äòmemory‚Äô...‚Äù\n",
        "\n",
        "‚Ä¶it returns a **special structure** called `tool_calls`:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"id\": \"call_xyz\",             // optional internal ID\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "        \"name\": \"search_file_names\",\n",
        "        \"arguments\": \"{ \\\"keyword\\\": \\\"memory\\\", \\\"case_sensitive\\\": false }\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è **Important Note**: the `arguments` field is a **stringified JSON**. So you must run `json.loads()` on it to use it as a dictionary in Python.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example in Your Code\n",
        "\n",
        "If you inspect the response:\n",
        "\n",
        "```python\n",
        "choice = response.choices[0]\n",
        "tool_call = choice.message.tool_calls[0]\n",
        "tool_name = tool_call.function.name\n",
        "args = json.loads(tool_call.function.arguments)\n",
        "```\n",
        "\n",
        "Then you can route to the right function like:\n",
        "\n",
        "```python\n",
        "result = tool_router(tool_name, args)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Benefits of This System\n",
        "\n",
        "* **No custom prompt hacks** needed to extract JSON\n",
        "* LLM learns how to use tools just from the schema\n",
        "* OpenAI ensures the LLM follows the format\n",
        "* You can build **modular, extensible agent systems** easily\n",
        "\n",
        "---\n",
        "\n",
        "## üìé Summary\n",
        "\n",
        "| Step                     | Role                                                         |\n",
        "| ------------------------ | ------------------------------------------------------------ |\n",
        "| `tools=`                 | Tell the LLM what tools are available and how to call them   |\n",
        "| `tool_choice=\"auto\"`     | Let the model decide if/when to use a tool                   |\n",
        "| `tool_calls` in response | Structured output with name and args of the function to call |\n",
        "| `json.loads(arguments)`  | Parse the input string back to Python dict                   |\n",
        "| `tool_router()`          | Run the actual function in Python                            |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jff4-oLW6jTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The `tools` and `tool_choice=\"auto\"` features in the **OpenAI API** are **built-in conveniences** that handle all the tricky parts of tool calling for you. Here‚Äôs how it breaks down:\n",
        "\n",
        "---\n",
        "\n",
        "## üéÅ What OpenAI Gives You: A Tool-Calling Convenience Layer\n",
        "\n",
        "Without this special API support, you‚Äôd have to:\n",
        "\n",
        "### üõ†Ô∏è *Manual Method (Before Tool Support)*:\n",
        "\n",
        "1. **Prompt** the LLM to respond in *strict JSON*:\n",
        "\n",
        "   > \"Respond with JSON only, do not include any extra text...\"\n",
        "2. **Hope** it doesn't slip into natural language or hallucinate field names.\n",
        "3. **Regex** or `json.loads()` and catch formatting errors.\n",
        "4. **Manually decide** if the LLM response even implies a function call.\n",
        "5. **Route** the response to a tool yourself.\n",
        "\n",
        "This is fragile, error-prone, and often frustrating.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ *With Tool Calling Support (Modern Way)*:\n",
        "\n",
        "1. You register your tools using strict JSON Schema (standardized).\n",
        "\n",
        "2. You send user input and say:\n",
        "\n",
        "   ```python\n",
        "   tool_choice=\"auto\"\n",
        "   ```\n",
        "\n",
        "3. **OpenAI handles everything**:\n",
        "\n",
        "   * Detects if a tool is needed.\n",
        "   * Selects the right tool.\n",
        "   * Formats a clean `tool_calls` block.\n",
        "   * Validates the input matches your schema.\n",
        "   * Guarantees it's parseable JSON inside a string.\n",
        "\n",
        "4. You just extract and run:\n",
        "\n",
        "   ```python\n",
        "   tool_name = tool_call.function.name\n",
        "   args = json.loads(tool_call.function.arguments)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Bottom Line:\n",
        "\n",
        "Tool calling support in the API lets the LLM act like an **intelligent dispatcher** that:\n",
        "\n",
        "* Decides *what* to call\n",
        "* Follows strict *input format*\n",
        "* Returns *structured JSON* reliably\n",
        "\n",
        "So yes ‚Äî without this feature, you‚Äôd have to **engineer all of that behavior yourself**, which is hard, brittle, and not nearly as elegant.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aego9N_M7Gkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_W29GYz2bF1",
        "outputId": "300c6d79-7c34-4000-c853-72c4f87f5dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/765.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m337.9/765.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m757.8/765.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# üîπ Step 1: Imports and Setup\n",
        "source_dir = \"/content/docs_folder\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    raise FileNotFoundError(f\"üìÅ Directory not found: {source_dir}\")\n",
        "\n",
        "# List and build full file paths\n",
        "file_list = [\n",
        "    os.path.join(source_dir, f)\n",
        "    for f in os.listdir(source_dir)\n",
        "    if os.path.isfile(os.path.join(source_dir, f))\n",
        "]\n",
        "\n",
        "# Display the found files\n",
        "print(\"üìÇ Files found:\")\n",
        "for file in file_list:\n",
        "    print(\"  -\", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EBKro7O2g3U",
        "outputId": "8a106973-9ee6-4873-c41f-4e1b874c0de9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Files found:\n",
            "  - /content/docs_folder/004_AGENT_Tools.txt\n",
            "  - /content/docs_folder/001_PArse_the Response.txt\n",
            "  - /content/docs_folder/003_gent Feedback and Memory.txt\n",
            "  - /content/docs_folder/000_Prompting for Agents -GAIL.txt\n",
            "  - /content/docs_folder/002_Execute_the_Action.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can think of your code in two main **sections** that correspond to stages of building an agent:\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ **Part 1: Tool Creation & Declaration**\n",
        "\n",
        "This is everything *before* the agent does anything. It includes:\n",
        "\n",
        "### 1. **Python Tool Functions**\n",
        "\n",
        "You create Python functions like `list_files()`, `read_file()`, etc. ‚Äî these are real, working backend tools.\n",
        "\n",
        "### 2. **Tool Schema Definitions**\n",
        "\n",
        "You define JSON-like schemas for each tool, describing:\n",
        "\n",
        "* What the tool is called (`name`)\n",
        "* What it does (`description`)\n",
        "* What parameters it needs (`parameters`)\n",
        "* And the OpenAI-required `\"type\": \"function\"` and `\"function\": { ... }` format.\n",
        "\n",
        "These schemas are needed **so the LLM knows how to talk to your tools**, but the LLM doesn‚Äôt run them ‚Äî you do.\n",
        "\n",
        "### 3. **Master Tool List (`tools`)**\n",
        "\n",
        "This list is passed into `client.chat.completions.create()` so the LLM is aware that tools exist and knows what options it has.\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ **Part 2: Agent Operation & Orchestration**\n",
        "\n",
        "This is where your LLM-based agent comes alive and begins responding to user input.\n",
        "\n",
        "### 4. **`generate_agent_response()`**\n",
        "\n",
        "* Builds the message thread\n",
        "* Sends it to OpenAI with tools + `tool_choice=\"auto\"`\n",
        "* Returns either:\n",
        "\n",
        "  * a **tool call** (`type: \"tool\"`) with structured arguments, or\n",
        "  * a **text response** (`type: \"text\"`)\n",
        "\n",
        "### 5. **`handle_tool_call()`**\n",
        "\n",
        "* Inspects the choice\n",
        "* If it‚Äôs a tool, it extracts the name + args and routes the call\n",
        "* If it‚Äôs just text, it prints it\n",
        "\n",
        "### 6. **`tool_router()`**\n",
        "\n",
        "* Executes the correct Python function using the arguments\n",
        "* Returns the result back to `handle_tool_call()` to be printed\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "Your idea to split the notebook into:\n",
        "\n",
        "* **Notebook 1: Tool Building**\n",
        "* **Notebook 2: Agent Logic**\n",
        "\n",
        "...is an excellent learning strategy and mirrors real-world agent development.\n",
        "\n",
        "You‚Äôve built a fully functioning, modular, easy-to-debug agent architecture ‚Äî and you now understand:\n",
        "\n",
        "* The difference between tool logic and tool schema\n",
        "* The structure and purpose of OpenAI‚Äôs function calling interface\n",
        "* How agents decide between generating text vs. using tools\n",
        "\n"
      ],
      "metadata": {
        "id": "ClFA3Ur4ArxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Tool 1: List all .txt files\n",
        "def list_files():\n",
        "    return [os.path.basename(f) for f in file_list if f.endswith(\".txt\")]\n",
        "\n",
        "# ‚úÖ Tool 2: Read a specific file\n",
        "def read_file(filename):\n",
        "    path = os.path.join(source_dir, filename)\n",
        "    if not os.path.isfile(path):\n",
        "        return f\"‚ö†Ô∏è File not found: {filename}\"\n",
        "    with open(path, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# ‚úÖ Tool 3: Search for keyword in file names\n",
        "def search_file_names(keyword, case_sensitive=False):\n",
        "    matches = []\n",
        "    for f in file_list:\n",
        "        name = os.path.basename(f)\n",
        "        haystack = name if case_sensitive else name.lower()\n",
        "        needle = keyword if case_sensitive else keyword.lower()\n",
        "        if needle in haystack:\n",
        "            matches.append(name)\n",
        "    return matches\n",
        "\n",
        "# üîß Define tools individually\n",
        "list_files_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"list_files\",\n",
        "    \"description\": \"Lists all files in the source directory.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "read_file_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"read_file\",\n",
        "    \"description\": \"Reads the content of a specified file.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"filename\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the file to read.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"filename\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "search_file_names_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"search_file_names\",\n",
        "    \"description\": \"Searches for files whose names include the given keyword.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"keyword\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The keyword to search for in the file names.\"\n",
        "            },\n",
        "            \"case_sensitive\": {\n",
        "                \"type\": \"boolean\",\n",
        "                \"description\": \"Whether the search should be case sensitive.\",\n",
        "                \"default\": False\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"keyword\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "#=======================================\n",
        "# üß∞ This is the ‚ÄúTool Declaration‚Äù Step\n",
        "#=======================================\n",
        "\n",
        "# You're defining what tools exist ‚Äî the LLM needs to know:\n",
        "\n",
        "# What each tool is called\n",
        "# What it does\n",
        "# What inputs it expects (parameters, types, required fields, etc.)\n",
        "\n",
        "# This tools list is then passed to the LLM inside the generate_agent_response() function like so\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",               # ‚úÖ Declares this is a function-style tool\n",
        "        \"function\": list_files_tool       # ‚úÖ Embeds the tool definition\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": read_file_tool\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": search_file_names_tool\n",
        "    }\n",
        "]\n",
        "\n",
        "#=======================================\n",
        "# generate_agent_response() Runs\n",
        "#=======================================\n",
        "\n",
        "# Prepares a messages list that includes:\n",
        "# A system prompt (describes the assistant‚Äôs behavior)\n",
        "\n",
        "# The user‚Äôs input Sends the request to the OpenAI API using client.chat.completions.create(...)\n",
        "\n",
        "# Includes:\n",
        "\n",
        "# Your tools\n",
        "# tool_choice=\"auto\" ‚Üí LLM chooses if it wants to use a tool\n",
        "\n",
        "# Returns either:\n",
        "\n",
        "# A tool_call ‚Üí structured JSON telling you which tool to run\n",
        "# A regular text message\n",
        "\n",
        "def generate_agent_response(user_input):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that helps with managing files. Use tools when needed.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        tools=tools,            # ‚úÖ Tell the LLM what tools exist\n",
        "        tool_choice=\"auto\",     # ‚úÖ Let it decide which to use\n",
        "    )\n",
        "\n",
        "    choice = response.choices[0].message\n",
        "\n",
        "    if choice.tool_calls:\n",
        "        # The LLM decided to use a tool\n",
        "        tool_call = choice.tool_calls[0]\n",
        "        return {\"type\": \"tool\", \"tool_call\": tool_call}\n",
        "    else:\n",
        "        # Regular assistant reply\n",
        "        return {\"type\": \"text\", \"content\": choice.content}\n",
        "\n",
        "#=========================================\n",
        "# üß∞ handle_tool_call(choice) Runs\n",
        "#=========================================\n",
        "\n",
        "# Receives the LLM‚Äôs response (the choice)\n",
        "# Checks: Is it a tool call?\n",
        "\n",
        "# ‚úÖ Yes ‚Üí extract tool name and arguments\n",
        "\n",
        "# üëâ Send to tool_router() to run the actual Python function\n",
        "# üí¨ Print tool result\n",
        "\n",
        "# ‚ùå No ‚Üí just print the LLM‚Äôs response directly\n",
        "\n",
        "def handle_tool_call(choice):\n",
        "    if choice[\"type\"] == \"tool\":\n",
        "        tool_name = choice[\"tool_call\"].function.name\n",
        "        args = json.loads(choice[\"tool_call\"].function.arguments)\n",
        "        result = tool_router(tool_name, args)\n",
        "        print(f\"üõ†Ô∏è Used Tool: {tool_name}\")\n",
        "        print(f\"üì§ Args: {args}\")\n",
        "        print(f\"üì• Result:\\n{result}\")\n",
        "    elif choice[\"type\"] == \"text\":\n",
        "        print(f\"üí¨ Assistant Response:\\n{choice['content']}\")\n",
        "    else:\n",
        "        print(\"‚ùì Unrecognized response type.\")\n",
        "\n",
        "#=========================================\n",
        "# tool_router() Executes the Right Tool\n",
        "#=========================================\n",
        "\n",
        "# Gets called from inside handle_tool_call()\n",
        "# Based on the tool_name, it picks and runs the correct Python function\n",
        "# Returns the result\n",
        "\n",
        "def tool_router(tool_name, args):\n",
        "    if tool_name == \"list_files\":\n",
        "        return list_files()\n",
        "    elif tool_name == \"read_file\":\n",
        "        return read_file(args[\"filename\"])\n",
        "    elif tool_name == \"search_file_names\":\n",
        "        return search_file_names(\n",
        "            keyword=args[\"keyword\"],\n",
        "            case_sensitive=args.get(\"case_sensitive\", False)\n",
        "        )\n",
        "    else:\n",
        "        return f\"‚ùå Unknown tool: {tool_name}\"\n",
        "\n",
        "#================\n",
        "# User Input\n",
        "#================\n",
        "\n",
        "user_input = \"List all the files that contain the word 'memory'\"\n",
        "choice = generate_agent_response(user_input)\n",
        "handle_tool_call(choice)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMbtGS3T2k2I",
        "outputId": "7e6357b2-4eee-4b66-a1c1-6ec140341cc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Used Tool: search_file_names\n",
            "üì§ Args: {'keyword': 'memory'}\n",
            "üì• Result:\n",
            "['003_gent Feedback and Memory.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"List all the files that contain the word 'Agent'\"\n",
        "choice = generate_agent_response(user_input)\n",
        "handle_tool_call(choice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLdp7yr88UtU",
        "outputId": "15167efd-eebc-41c6-9324-bfcbaf87ad95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Used Tool: search_file_names\n",
            "üì§ Args: {'keyword': 'Agent'}\n",
            "üì• Result:\n",
            "['004_AGENT_Tools.txt', '000_Prompting for Agents -GAIL.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîß TOOL & AGENT DESIGN LESSONS\n",
        "\n",
        "### 1. **Tool Design Is an API Contract**\n",
        "\n",
        "Each tool you define (via the JSON schema) acts like a **mini-API endpoint**:\n",
        "\n",
        "* You specify the **interface** (what arguments it takes)\n",
        "* The LLM decides **when to call it** based on the user message\n",
        "* You write the **backend function** to actually perform the task\n",
        "\n",
        "üëâ Think of tools like Lego blocks: small, reusable, testable units that can be composed by the agent as needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **LLM Tool Usage Is Purely Declarative**\n",
        "\n",
        "The LLM does *not* run your code.\n",
        "\n",
        "* It merely **calls out what tool it thinks should be used**\n",
        "* It formats the request like:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"tool_calls\": [\n",
        "      {\n",
        "        \"function\": {\n",
        "          \"name\": \"read_file\",\n",
        "          \"arguments\": \"{ \\\"filename\\\": \\\"example.txt\\\" }\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "  ```\n",
        "* It's **your job** to run the Python code based on that request.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tool Invocation Format Is Strict**\n",
        "\n",
        "OpenAI‚Äôs API expects a very specific tool structure:\n",
        "\n",
        "* Outer list of tool objects\n",
        "* Each tool:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": { ... }\n",
        "  }\n",
        "  ```\n",
        "* The inner `function` must include: `name`, `description`, `parameters`\n",
        "\n",
        "‚ö†Ô∏è If you omit or misname one of these keys, you‚Äôll get cryptic 400 errors like:\n",
        "\n",
        "> `\"Missing required parameter: tools[0].function\"`\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **API Models Must Support Tools**\n",
        "\n",
        "Only some models support tool calling:\n",
        "\n",
        "* ‚úÖ `gpt-4`, `gpt-4-0613`, `gpt-4-1106-preview`, `gpt-4o`\n",
        "* ‚úÖ `gpt-3.5-turbo-0613`, `gpt-3.5-turbo-1106`\n",
        "* ‚ùå Older models (like `gpt-3.5-turbo` with no suffix) *don‚Äôt* support it.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **You‚Äôre Building an Agent Loop**\n",
        "\n",
        "A full agent doesn‚Äôt just use a tool once ‚Äî it loops:\n",
        "\n",
        "1. LLM chooses a tool\n",
        "2. You execute it\n",
        "3. You give back the result as a message\n",
        "4. LLM may use another tool‚Ä¶ or stop\n",
        "\n",
        "This can become a full **think-act-learn loop** with memory and planning if you add history or reflection.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Agent System Messages Matter**\n",
        "\n",
        "Your `\"system\"` prompt should:\n",
        "\n",
        "* Tell the model *what role* it's playing\n",
        "* Give it hints like:\n",
        "\n",
        "  > ‚ÄúYou are a file management assistant. Use tools to retrieve or read files. Only respond in plain language if a tool is not required.‚Äù\n",
        "\n",
        "This helps the LLM reason about *when* to use a tool.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Pro Tips\n",
        "\n",
        "| Tip                                                    | Why It Matters                                              |\n",
        "| ------------------------------------------------------ | ----------------------------------------------------------- |\n",
        "| ‚úÖ Always test tool functions standalone                | Confirms your backend works before involving the LLM        |\n",
        "| üß™ Use print statements or logging in the router       | Helps trace LLM decisions during development                |\n",
        "| üõë Be defensive: use `.get()` and try/except on `args` | Avoids runtime crashes if the LLM returns unexpected values |\n",
        "| üìé Keep your tools modular                             | Easier to test, debug, and evolve your agent                |\n",
        "| üß∞ Start small: 2‚Äì3 tools per agent                    | Keeps behavior predictable while you develop                |\n",
        "\n"
      ],
      "metadata": {
        "id": "wSp7xJQuCYO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is **one of the most important things to understand** when working with tools and agents.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **It is the job of the LLM to generate this `arguments` block.**\n",
        "\n",
        "When you define your tools like this:\n",
        "\n",
        "```python\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"read_file\",\n",
        "            \"description\": \"Reads the content of a specified file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"filename\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The name of the file to read.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"filename\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "You are telling the LLM:\n",
        "\n",
        "> \"If the user input seems to require reading a file, and you want to call a tool to do that, then use this `read_file` tool, and fill out the `filename` parameter appropriately.\"\n",
        "\n",
        "So, when the user says:\n",
        "\n",
        "> ‚ÄúCan you show me what's inside `tool_notes.txt`?‚Äù\n",
        "\n",
        "The **LLM parses this**, understands that `tool_notes.txt` is the value for `filename`, and **generates** this tool call:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"function\": {\n",
        "        \"name\": \"read_file\",\n",
        "        \"arguments\": \"{ \\\"filename\\\": \\\"tool_notes.txt\\\" }\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üö´ You do **not** write this yourself.\n",
        "\n",
        "If the model is given your `tools` definition and you use:\n",
        "\n",
        "```python\n",
        "tool_choice=\"auto\"\n",
        "```\n",
        "\n",
        "then OpenAI‚Äôs API *forces* the LLM to return a `tool_calls` block **instead of normal text**, and includes the arguments it parsed from the user‚Äôs message.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Your Job:\n",
        "\n",
        "Once you receive the response with:\n",
        "\n",
        "```python\n",
        "response.choices[0].message.tool_calls\n",
        "```\n",
        "\n",
        "You:\n",
        "\n",
        "1. Parse the arguments (with `json.loads(...)`)\n",
        "2. Route them to the right Python function\n",
        "3. Pass in the arguments\n",
        "4. Return the output back to the LLM (if you're looping)\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Recap Flow:\n",
        "\n",
        "1. **User Input**: `\"Show me what's inside 'report.txt'\"`\n",
        "2. **LLM ‚Üí tool\\_call**:\n",
        "\n",
        "   ```json\n",
        "   {\n",
        "     \"name\": \"read_file\",\n",
        "     \"arguments\": \"{ \\\"filename\\\": \\\"report.txt\\\" }\"\n",
        "   }\n",
        "   ```\n",
        "3. **You run**: `read_file(\"report.txt\")`\n",
        "4. **You return**: The file contents back to the LLM (optional, in looped agents)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ue4zrxlzCq82"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eH4bWooBCaZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}