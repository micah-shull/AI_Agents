{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMphltMMUZ6H1c6/JbprSSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/088_Programmatic_Prompting%2BMemory_Management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üõ†Ô∏è Sending Prompts & Managing Memory\n",
        "\n",
        "## üöÄ From Manual Prompts to Automated Agents\n",
        "\n",
        "**Programmatically sending prompts** is how we move from:\n",
        "\n",
        "> *‚ÄúA human types a prompt and reacts to the LLM‚Äôs response‚Äù*\n",
        "\n",
        "‚Ä¶to:\n",
        "\n",
        "> *‚ÄúAn agent automatically sends prompts and acts on the results.‚Äù*\n",
        "\n",
        "To start building agents, we need two **core capabilities**:\n",
        "\n",
        "### 1Ô∏è‚É£ Programmatic Prompting\n",
        "\n",
        "Automating the **prompt ‚Üí response** cycle that humans do manually.\n",
        "This forms the foundation of the **Agent Loop**.\n",
        "\n",
        "### 2Ô∏è‚É£ Memory Management\n",
        "\n",
        "Controlling what information *persists* between iterations (e.g., API calls + results) so the agent maintains **context** while making decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ Sending Prompts Programmatically & Managing Memory\n",
        "\n",
        "### üè∑ System Message ‚Äî *The Most Important Part*\n",
        "\n",
        "* Tells the model **how to behave**.\n",
        "* Sets the **ground rules** for the conversation.\n",
        "* **Models pay more attention** to this than to user messages.\n",
        "* You ‚Äú**program**‚Äù the agent through this message.\n",
        "\n",
        "### üë§ User Message ‚Äî *The Question*\n",
        "\n",
        "* Asks the model for a specific answer or action.\n",
        "\n",
        "**Example Prompt Structure**:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"role\": \"system\", \"content\": \"You are an expert software engineer...\"},\n",
        "  {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dict.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Why This Matters\n",
        "\n",
        "LLMs are **stateless**. Without explicit instructions, they forget everything between turns.\n",
        "\n",
        "To make **multi-turn conversations** work, you must *manually* manage context.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Takeaways\n",
        "\n",
        "> ### ‚ùå No Inherent Memory\n",
        "\n",
        "The LLM **has no knowledge** of past interactions unless you **include** them in the current prompt.\n",
        "\n",
        "> ### üìú Provide Full Context\n",
        "\n",
        "To simulate continuity, include **all relevant messages** (both `user` and `assistant`) in your `messages` parameter.\n",
        "\n",
        "> ### ü§ù Role of Assistant Messages\n",
        "\n",
        "Adding **previous responses** as `assistant` messages lets the model:\n",
        "\n",
        "* Maintain a **coherent conversation**\n",
        "* Build on earlier exchanges\n",
        "* ‚ÄúRemember‚Äù what actions (like API calls) it took in the past\n",
        "\n",
        "> ### üéØ Memory Management\n",
        "\n",
        "You control what the model *remembers* by deciding which messages to include in the conversation.\n",
        "Sometimes **forgetting** is powerful ‚Äî for example:\n",
        "\n",
        "* Breaking patterns of poor responses\n",
        "* Resetting an agent‚Äôs thinking mid-loop\n",
        "\n"
      ],
      "metadata": {
        "id": "TC9S9f1asbCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1) Programmatic prompting (vs. hand-typing)\n",
        "\n",
        "**Concept:** Agents call the model in code with a `messages` list (role + content). That‚Äôs the foundation of an agent loop.&#x20;\n",
        "**In your code:**\n",
        "\n",
        "* `messages = [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}]` is the programmatic prompt you‚Äôd send.\n",
        "* `step(...)` is the ‚Äúloop step‚Äù: choose visible context ‚Üí get reply ‚Üí append it.\n",
        "\n",
        "Why it matters: this is how you go from ‚Äúchatting‚Äù to **automating** the chat.&#x20;\n",
        "\n",
        "# 2) Roles (system, user, assistant)\n",
        "\n",
        "**Concept:** The **system** message sets behavior; **user** asks; **assistant** replies. Models weight the system message heavily.&#x20;\n",
        "**In your code:**\n",
        "\n",
        "* `system_instructions = \"...\"; messages = [{\"role\":\"system\", \"content\": system_instructions}]`\n",
        "* You append user messages via `add_message(\"user\", ...)`.\n",
        "* Assistant outputs are appended inside `step(...)` with `add_message(\"assistant\", reply)`.\n",
        "\n",
        "Takeaway: You ‚Äúprogram‚Äù the agent through the **system** message.&#x20;\n",
        "\n",
        "# 3) LLMs are stateless\n",
        "\n",
        "**Concept:** The model doesn‚Äôt remember anything between calls unless you include it again in `messages`.&#x20;\n",
        "**In your code:**\n",
        "\n",
        "* `step(memory_size=2)` may drop earlier turns; the mock replies ‚ÄúI need more context.‚Äù\n",
        "* That‚Äôs the ‚Äúmissing context‚Äù demo from your notes.&#x20;\n",
        "\n",
        "# 4) Memory = what you include in `messages`\n",
        "\n",
        "**Concept:** ‚ÄúMemory‚Äù is **explicit**: you keep past turns (including prior assistant outputs) in the next request. That‚Äôs how continuity and multi-step reasoning work.&#x20;\n",
        "**In your code:**\n",
        "\n",
        "* `window(messages, max_messages)` is your memory policy.\n",
        "* Small window ‚áí loses earlier assistant code ‚áí can‚Äôt ‚Äúupdate the function.‚Äù\n",
        "* Larger window (e.g., `memory_size=10`) ‚áí includes prior assistant code ‚áí the mock can add docs.\n",
        "\n",
        "Key takeaways match your notes: no inherent memory; provide full context; assistant messages are crucial for continuity.&#x20;\n",
        "\n",
        "# 5) Memory management (what to keep vs. forget)\n",
        "\n",
        "**Concept:** You control recall by choosing which turns to pass along. Sometimes **forgetting** is useful to break bad patterns.&#x20;\n",
        "**In your code:**\n",
        "\n",
        "* `window(...)` is a sliding-window policy by **count**.\n",
        "* Later you can swap this for token-budgeting or summarization without changing the rest.\n",
        "\n",
        "# 6) GAIL framing (preview for later)\n",
        "\n",
        "**Concept:** A practical way to think about agents: **G**oals, **A**ctions (tools), **I**nformation (memory/context), **L**anguage (prompts/instructions). Your Lecture 1 focuses on **I** and **L**; we‚Äôll layer **G** and **A** as you add tools/APIs.&#x20;\n",
        "\n",
        "---\n",
        "\n",
        "## Quick ‚Äúconcept ‚Üí code‚Äù checklist\n",
        "\n",
        "* **Programmatic prompt:** your `messages` list.&#x20;\n",
        "* **System grounding:** first `messages[0]`.&#x20;\n",
        "* **Statelessness:** second query fails when earlier context isn‚Äôt included.&#x20;\n",
        "* **Memory via assistant turns:** include previous assistant output to enable edits.&#x20;\n",
        "* **Memory policy:** your `window(...)` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFahDV6tpihe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup: roles and a starting system message ---\n",
        "system_instructions = \"You are an expert software engineer who explains simply.\"\n",
        "messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "\n",
        "def add_message(role, content):\n",
        "    messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def window(msgs, max_messages=None):\n",
        "    \"\"\"Return the last N messages (or all if None).\"\"\"\n",
        "    if max_messages is None:\n",
        "        return msgs\n",
        "    return msgs[-max_messages:]\n",
        "\n",
        "# --- Mock LLM (no network): reacts to what's in the visible window ---\n",
        "import textwrap\n",
        "\n",
        "def mock_llm(visible_messages):\n",
        "    # find latest user request\n",
        "    last_user = \"\"\n",
        "    for m in reversed(visible_messages):\n",
        "        if m[\"role\"] == \"user\":\n",
        "            last_user = m[\"content\"]\n",
        "            break\n",
        "    # find the most recent assistant message, if any\n",
        "    prev_assistant = \"\"\n",
        "    for m in reversed(visible_messages):\n",
        "        if m[\"role\"] == \"assistant\":\n",
        "            prev_assistant = m[\"content\"]\n",
        "            break\n",
        "\n",
        "    # very naive branching\n",
        "    if \"Update the function\" in last_user and \"def swap_keys_values\" in prev_assistant:\n",
        "        return textwrap.dedent(\"\"\"\\\n",
        "            \\\"\\\"\\\"Swap keys and values in a dictionary.\n",
        "\n",
        "            Args:\n",
        "                d (dict): Input dictionary with hashable keys and values.\n",
        "            Returns:\n",
        "                dict: A new dictionary with keys/values swapped.\n",
        "            \\\"\\\"\\\"\n",
        "            def swap_keys_values(d):\n",
        "                return {v: k for k, v in d.items()}\n",
        "        \"\"\")\n",
        "    if \"swap the keys and values\" in last_user:\n",
        "        return \"def swap_keys_values(d):\\n    return {v: k for k, v in d.items()}\"\n",
        "    return \"I need more context.\"\n",
        "\n",
        "def step(memory_size=None):\n",
        "    \"\"\"Build visible context -> get reply -> store it -> return it.\"\"\"\n",
        "    visible = window(messages, memory_size)\n",
        "    reply = mock_llm(visible)\n",
        "    add_message(\"assistant\", reply)\n",
        "    return reply\n"
      ],
      "metadata": {
        "id": "19-1ATZwos6S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## What each piece does (plain-English)\n",
        "\n",
        "* `messages`: the whole conversation as a list of `{role, content}` dicts.\n",
        "* `add_message(role, content)`: push a new turn.\n",
        "* `window(msgs, max_messages)`: picks the *last N* messages. This is your **memory policy**.\n",
        "* `mock_llm(visible_messages)`: a dumb stand-in for an LLM that:\n",
        "\n",
        "  * looks at the **latest user** request,\n",
        "  * optionally looks at the **previous assistant** code,\n",
        "  * returns either the base function, the documented version, or ‚ÄúI need more context.‚Äù\n",
        "* `step(memory_size)`: the ‚Äúagent loop‚Äù‚Äîchoose visible context ‚ûú get a reply ‚ûú store it.\n",
        "\n",
        "This mirrors the lecture‚Äôs core ideas: LLMs are stateless, so **whatever‚Äôs in `visible_messages` is what the model ‚Äúremembers.‚Äù** Small `memory_size` can make the follow-up fail; a bigger one makes it succeed.\n",
        "\n",
        "Want me to keep this minimal style for the next lecture too (and only add complexity when a concept demands it)?\n"
      ],
      "metadata": {
        "id": "LAQ0QmaQo64c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset conversation\n",
        "messages[:] = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "\n",
        "# 1) Ask for the function\n",
        "add_message(\"user\", \"Write a function to swap the keys and values in a dictionary.\")\n",
        "print(\"Initial response (small memory):\\n\", step(memory_size=2), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4DGlVoo3cm",
        "outputId": "9f24e35f-f0e8-456b-97f5-5ffb855bf2f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial response (small memory):\n",
            " def swap_keys_values(d):\n",
            "    return {v: k for k, v in d.items()} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Ask to update, but keep memory very small (likely loses the prior assistant code)\n",
        "add_message(\"user\", \"Update the function to include documentation.\")\n",
        "print(\"Update with small memory:\\n\", step(memory_size=2), \"\\n\")  # often fails"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn5amXEkqRJW",
        "outputId": "49de89ef-100f-45cb-dd20-d3d30addcacc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update with small memory:\n",
            " \"\"\"Swap keys and values in a dictionary.\n",
            "\n",
            "Args:\n",
            "    d (dict): Input dictionary with hashable keys and values.\n",
            "Returns:\n",
            "    dict: A new dictionary with keys/values swapped.\n",
            "\"\"\"\n",
            "def swap_keys_values(d):\n",
            "    return {v: k for k, v in d.items()}\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Try again with larger memory (keeps prior assistant output visible)\n",
        "add_message(\"user\", \"Update the function to include documentation.\")\n",
        "print(\"Update with larger memory:\\n\", step(memory_size=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjx6G8XrqTJK",
        "outputId": "6c4fc7e4-8877-4f09-f138-2393617c997b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update with larger memory:\n",
            " \"\"\"Swap keys and values in a dictionary.\n",
            "\n",
            "Args:\n",
            "    d (dict): Input dictionary with hashable keys and values.\n",
            "Returns:\n",
            "    dict: A new dictionary with keys/values swapped.\n",
            "\"\"\"\n",
            "def swap_keys_values(d):\n",
            "    return {v: k for k, v in d.items()}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat History & Memory\n",
        "\n",
        "## 1. **Chat history**\n",
        "\n",
        "That‚Äôs your full record of *everything* that‚Äôs happened in the conversation so far.\n",
        "In our code, it‚Äôs the `messages` list:\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"...\"},\n",
        "    {\"role\": \"user\", \"content\": \"...\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"...\"},\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "Every time you call:\n",
        "\n",
        "```python\n",
        "add_message(role, content)\n",
        "```\n",
        "\n",
        "‚Ä¶you‚Äôre **adding to the chat history**.\n",
        "The history grows forever unless you clear or reset it.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Memory** (in the LLM sense)\n",
        "\n",
        "LLMs are *stateless*, so they don‚Äôt actually ‚Äúremember‚Äù the history you‚Äôve stored.\n",
        "The **only ‚Äúmemory‚Äù they have** is the **messages you include in the next API call**.\n",
        "\n",
        "That‚Äôs why we have:\n",
        "\n",
        "```python\n",
        "def window(msgs, max_messages=None):\n",
        "    if max_messages is None:\n",
        "        return msgs  # send full history\n",
        "    return msgs[-max_messages:]  # send only the last N messages\n",
        "```\n",
        "\n",
        "* `max_messages=None` ‚áí you send *full* chat history (perfect recall).\n",
        "* Small `max_messages` ‚áí you send only a **slice** of the chat history ‚Äî that‚Äôs your *memory policy*.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Key difference**\n",
        "\n",
        "* **Chat history** = everything that has happened (local to your program).\n",
        "* **Memory** = what you *choose* to give the LLM this time.\n",
        "\n",
        "If you drop parts of the history from the `window(...)` result, the LLM has no awareness of them ‚Äî it‚Äôs as if they never happened.\n",
        "That‚Äôs how you can simulate forgetting or limit context for performance/relevance reasons.\n",
        "\n",
        "---\n",
        "\n",
        "A nice mental model:\n",
        "\n",
        "```\n",
        "chat history   = full notebook of notes\n",
        "memory         = what you flip open and show the model right now\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "* **Chat history** = your *entire transcript* stored in your program (perfect recall from your side).\n",
        "* **Memory** = the *subset of that transcript* you hand over to the model right now.\n",
        "\n",
        "If you pass the **entire chat history** to the LLM, then *memory = history*.\n",
        "If you pass only the **last N turns** (or a summary), then *memory ‚äÇ history*.\n",
        "\n",
        "This distinction matters because:\n",
        "\n",
        "* It‚Äôs the **memory** that shapes the model‚Äôs current response.\n",
        "* You can keep **history** for logging, analysis, or reloading later, even if you don‚Äôt send it every time.\n",
        "* Controlling **memory** lets you decide what the model ‚Äúknows‚Äù at any given turn ‚Äî which is key for performance, cost, and avoiding drift.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can show you in code how to:\n",
        "\n",
        "1. Keep **full history**.\n",
        "2. Send **small memory window** to model.\n",
        "3. See exactly how the model‚Äôs behavior changes when earlier assistant messages are *in* vs. *out* of the memory.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "He6jgrnHrJHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHAT HISTORY ---\n",
        "# This list holds the *entire* conversation so far.\n",
        "messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "\n",
        "def add_message(role, content):\n",
        "    # Adds a message to the *full* chat history.\n",
        "    messages.append({\"role\": role, \"content\": content})\n",
        "# --- END CHAT HISTORY ---\n",
        "\n",
        "\n",
        "# --- MEMORY ---\n",
        "def window(msgs, max_messages=None):\n",
        "    \"\"\"Return the last N messages (or all if None).\"\"\"\n",
        "    # Creates the *memory window* ‚Äî the slice of history\n",
        "    # you actually give to the model right now.\n",
        "    if max_messages is None:\n",
        "        return msgs  # full recall (memory == history)\n",
        "    return msgs[-max_messages:]  # limited recall (memory ‚äÇ history)\n",
        "# --- END MEMORY ---\n"
      ],
      "metadata": {
        "id": "VRCbRhWlrVuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Agent Code"
      ],
      "metadata": {
        "id": "pukqzx8axkXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install and import ---\n",
        "!pip install --quiet python-dotenv openai\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- 2. Load API key ---\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# --- 3. Start with a system message (chat history) ---\n",
        "system_instructions = \"You are an expert software engineer who explains simply.\"\n",
        "messages = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "\n",
        "def add_message(role, content):\n",
        "    \"\"\"Append a message to the full chat history.\"\"\"\n",
        "    messages.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def window(msgs, max_messages=None):\n",
        "    \"\"\"Pick which messages the model sees (memory).\"\"\"\n",
        "    if max_messages is None:\n",
        "        return msgs\n",
        "    return msgs[-max_messages:]\n",
        "\n",
        "def step(memory_size=None):\n",
        "    \"\"\"Send memory window to model, get reply, add to history.\"\"\"\n",
        "    visible = window(messages, memory_size)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=visible\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    add_message(\"assistant\", reply)\n",
        "    return reply\n",
        "\n",
        "# --- 4. Demo: small vs. large memory window ---\n",
        "\n",
        "# Reset chat history\n",
        "messages[:] = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "\n",
        "# Step 1: Ask for the base function\n",
        "add_message(\"user\", \"Write a Python function to swap the keys and values in a dictionary.\")\n",
        "print(\"\\n--- Initial response (small memory) ---\")\n",
        "print(step(memory_size=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQtfJ2WxLXf",
        "outputId": "0d6cdc62-ac38-4354-9f47-ac8b3af477df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initial response (small memory) ---\n",
            "Certainly! You can create a function that swaps the keys and values in a dictionary using a simple dictionary comprehension. Here's a function to do that:\n",
            "\n",
            "```python\n",
            "def swap_dict(d):\n",
            "    # Create a new dictionary by swapping keys and values\n",
            "    swapped = {value: key for key, value in d.items()}\n",
            "    return swapped\n",
            "\n",
            "# Example usage:\n",
            "original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped_dict = swap_dict(original_dict)\n",
            "print(swapped_dict)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "- The function `swap_dict` takes a dictionary `d` as input.\n",
            "- It uses a dictionary comprehension to create a new dictionary where each key is now the value from the original dictionary, and each value is now the key.\n",
            "- Finally, it returns the new dictionary with swapped keys and values.\n",
            "\n",
            "### Note:\n",
            "- If there are duplicate values in the original dictionary, this method will lose some data because dictionary keys must be unique. The last value for each original key will overwrite any previous ones in the swapped dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Ask to update, but with small memory (likely missing prior code)\n",
        "add_message(\"user\", \"Update the function to include a clear docstring.\")\n",
        "print(\"\\n--- Update with small memory ---\")\n",
        "print(step(memory_size=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Bo1f2Cx25U",
        "outputId": "90358af6-e65f-4965-c6d9-3ab9550d8367"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Update with small memory ---\n",
            "Certainly! Here is the updated function with a clear docstring that describes its purpose, parameters, return value, and any important notes:\n",
            "\n",
            "```python\n",
            "def swap_dict(d):\n",
            "    \"\"\"\n",
            "    Swap the keys and values of a given dictionary.\n",
            "\n",
            "    This function takes a dictionary as input and returns a new dictionary \n",
            "    where the keys and values are swapped. If there are duplicate values in \n",
            "    the original dictionary, the last corresponding key will overwrite \n",
            "    previous keys in the swapped dictionary.\n",
            "\n",
            "    Parameters:\n",
            "    d (dict): A dictionary with keys and values to be swapped.\n",
            "\n",
            "    Returns:\n",
            "    dict: A new dictionary with the keys and values swapped.\n",
            "\n",
            "    Example:\n",
            "    >>> original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "    >>> swapped_dict = swap_dict(original_dict)\n",
            "    >>> print(swapped_dict)\n",
            "    {1: 'a', 2: 'b', 3: 'c'}\n",
            "    \"\"\"\n",
            "    # Create a new dictionary by swapping keys and values\n",
            "    swapped = {value: key for key, value in d.items()}\n",
            "    return swapped\n",
            "\n",
            "# Example usage:\n",
            "original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped_dict = swap_dict(original_dict)\n",
            "print(swapped_dict)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Changes Made:\n",
            "- A docstring has been added that includes:\n",
            "  - **Summary**: A brief overview of what the function does.\n",
            "  - **Parameters**: Description of the input parameter.\n",
            "  - **Returns**: The type and description of the return value.\n",
            "  - **Example**: An example of how to use the function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Try again with larger memory (includes prior assistant output)\n",
        "add_message(\"user\", \"Update the function to include a clear docstring.\")\n",
        "print(\"\\n--- Update with larger memory ---\")\n",
        "print(step(memory_size=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU9WLNolyCGZ",
        "outputId": "91422a73-f645-484c-e623-bff7d4bdf045"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Update with larger memory ---\n",
            "It seems that I misunderstood your request initially, but I've already provided a clear docstring in the previous response. However, I'll go ahead and reaffirm that by structuring the code properly with additional clarity. Here‚Äôs the updated version with a refined docstring:\n",
            "\n",
            "```python\n",
            "def swap_dict(d):\n",
            "    \"\"\"\n",
            "    Swap the keys and values of a given dictionary.\n",
            "\n",
            "    This function takes a dictionary as input and returns a new dictionary \n",
            "    where the keys and values are swapped. If there are duplicate values in \n",
            "    the original dictionary, the last corresponding key will overwrite \n",
            "    earlier keys in the swapped dictionary.\n",
            "\n",
            "    Parameters:\n",
            "    d (dict): A dictionary with keys and values to be swapped.\n",
            "\n",
            "    Returns:\n",
            "    dict: A new dictionary with the keys and values swapped.\n",
            "\n",
            "    Example:\n",
            "    >>> original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "    >>> swapped_dict = swap_dict(original_dict)\n",
            "    >>> print(swapped_dict)\n",
            "    {1: 'a', 2: 'b', 3: 'c'}\n",
            "    \"\"\"\n",
            "    # Create a new dictionary by swapping keys and values\n",
            "    swapped = {value: key for key, value in d.items()}\n",
            "    return swapped\n",
            "\n",
            "# Example usage\n",
            "if __name__ == \"__main__\":\n",
            "    original_dict = {'a': 1, 'b': 2, 'c': 3}\n",
            "    swapped_dict = swap_dict(original_dict)\n",
            "    print(swapped_dict)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "### Key Features of the Docstring:\n",
            "- **Function Overview**: Explains what the function does.\n",
            "- **Parameters**: Specifies the expected input type and purpose.\n",
            "- **Return Value**: Describes what the function returns.\n",
            "- **Example**: Provides a usage example to help clarify how to call the function and what to expect.\n",
            "\n",
            "Let me know if you need any additional modifications or clarifications!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add External Data to Memory\n",
        "\n",
        "This is where *memory* isn‚Äôt just the running chat history, but also **external knowledge** you decide to include in the next prompt.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© How it fits the ‚Äúhistory vs. memory‚Äù idea\n",
        "\n",
        "Right now we have:\n",
        "\n",
        "```\n",
        "Chat history ‚Üí memory window ‚Üí model\n",
        "```\n",
        "\n",
        "We can expand it to:\n",
        "\n",
        "```\n",
        "Chat history\n",
        "+ External sources (PDF text, URL data, DB results, etc.)\n",
        "‚Üí merged into memory window ‚Üí model\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† How to do it in our Lecture-1 style\n",
        "\n",
        "We‚Äôll keep our **full chat history** the same, but when we build the `visible` messages list, we can also inject extra ‚Äúassistant‚Äù or ‚Äúsystem‚Äù messages containing external data.\n",
        "\n",
        "Here‚Äôs a minimal example:\n",
        "\n",
        "```python\n",
        "def add_external_memory(visible, content, role=\"system\"):\n",
        "    \"\"\"\n",
        "    Injects external info into the memory window.\n",
        "    This could be PDF content, URL scrape, database query, etc.\n",
        "    \"\"\"\n",
        "    visible.append({\"role\": role, \"content\": content})\n",
        "    return visible\n",
        "\n",
        "# Example: simulate loading from PDF or URL\n",
        "external_text = \"\"\"\n",
        "This is content from an external document:\n",
        "The swap_keys_values function should also handle empty dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "# Step with extra memory\n",
        "def step_with_external(memory_size=None, external=None):\n",
        "    visible = window(messages, memory_size)\n",
        "    if external:\n",
        "        visible = add_external_memory(visible, external, role=\"system\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=visible\n",
        "    )\n",
        "    reply = response.choices[0].message.content\n",
        "    add_message(\"assistant\", reply)\n",
        "    return reply\n",
        "\n",
        "# Usage:\n",
        "messages[:] = [{\"role\": \"system\", \"content\": system_instructions}]\n",
        "add_message(\"user\", \"Write a function to swap keys and values in a dictionary.\")\n",
        "print(step_with_external(memory_size=10))\n",
        "\n",
        "add_message(\"user\", \"Update the function to handle empty dictionaries.\")\n",
        "print(step_with_external(memory_size=10, external=external_text))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key points\n",
        "\n",
        "* **Chat history**: still tracks the conversation.\n",
        "* **Memory window**: now includes both the history slice *and* extra external data.\n",
        "* External info can come from:\n",
        "\n",
        "  * PDF extraction (`PyPDF2`, `pdfplumber`, etc.)\n",
        "  * Web scraping or API calls\n",
        "  * Database lookups\n",
        "  * Pre-computed summaries\n",
        "* You can label it as a `\"system\"` message (instructions) or `\"assistant\"` message (past outputs) depending on how you want the model to treat it.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can make you a **Colab-ready example** that:\n",
        "\n",
        "1. Loads a PDF,\n",
        "2. Extracts relevant text,\n",
        "3. Injects it into the memory window,\n",
        "   ‚Ä¶so GPT-4o-mini can use it in the same way it uses the chat history.\n",
        "\n",
        "Do you want me to build that next?\n"
      ],
      "metadata": {
        "id": "1iUn2Dh_y7kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîë Wrap-up Points for Lecture 1\n",
        "\n",
        "1. **Memory = control**\n",
        "\n",
        "   * *Chat history* is just storage in your program.\n",
        "   * *Memory* is what you choose to show the model.\n",
        "   * This choice directly controls model behavior.\n",
        "\n",
        "2. **Memory can come from anywhere**\n",
        "\n",
        "   * History of the conversation.\n",
        "   * External docs (PDFs, URLs, databases).\n",
        "   * Summaries or structured facts you prepare ahead of time.\n",
        "   * The model has no clue *where* the info came from ‚Äî it just sees it in `messages`.\n",
        "\n",
        "3. **Assistant messages are gold**\n",
        "\n",
        "   * If your follow-up depends on what the model previously generated (e.g., code), you must include that output in the memory window.\n",
        "   * Dropping them makes the model ‚Äúforget‚Äù its own work.\n",
        "\n",
        "4. **Forgetting is sometimes good**\n",
        "\n",
        "   * You can intentionally remove context to break bad patterns or reduce irrelevant info.\n",
        "   * Good agents know when to prune memory.\n",
        "\n",
        "5. **System message = ‚Äúpersonality‚Äù + rules**\n",
        "\n",
        "   * Use it to *program* the model‚Äôs role, constraints, and goals.\n",
        "   * Models heavily weight system instructions.\n",
        "\n",
        "6. **Costs & limits matter**\n",
        "\n",
        "   * More messages ‚Üí more tokens ‚Üí higher cost.\n",
        "   * That‚Äôs why smart memory policies (sliding windows, summarization, retrieval) are essential.\n",
        "\n",
        ".\n",
        "\n"
      ],
      "metadata": {
        "id": "XX_4HvmvzYmL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAmhyStFzFR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}