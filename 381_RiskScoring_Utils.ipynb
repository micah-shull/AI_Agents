{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPgAug7mFO0bGAn6eMfICx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/381_RiskScoring_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY7HBoYznre8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Risk scoring utilities for Governance & Compliance Orchestrator\n",
        "\n",
        "Calculates risk scores for agents and overall system based on violations,\n",
        "bias signals, and drift signals.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "from config import GovernanceComplianceOrchestratorConfig\n",
        "\n",
        "\n",
        "def calculate_agent_risk_score(\n",
        "    agent_name: str,\n",
        "    compliance_events: List[Dict[str, Any]],\n",
        "    bias_signals: List[Dict[str, Any]],\n",
        "    drift_signals: List[Dict[str, Any]],\n",
        "    events_lookup: Dict[str, Dict[str, Any]],\n",
        "    config: GovernanceComplianceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate risk score for a specific agent.\n",
        "\n",
        "    Args:\n",
        "        agent_name: Name of the agent\n",
        "        compliance_events: List of compliance events (violations)\n",
        "        bias_signals: List of bias signals\n",
        "        drift_signals: List of drift signals\n",
        "        events_lookup: Lookup dict for events by event_id\n",
        "        config: Configuration with severity weights\n",
        "\n",
        "    Returns:\n",
        "        Risk score dict for the agent\n",
        "    \"\"\"\n",
        "    # Filter violations for this agent by looking up the event\n",
        "    agent_violations = []\n",
        "    for event in compliance_events:\n",
        "        event_id = event.get(\"event_id\")\n",
        "        if event_id and event_id in events_lookup:\n",
        "            original_event = events_lookup[event_id]\n",
        "            if original_event.get(\"agent_name\") == agent_name:\n",
        "                agent_violations.append(event)\n",
        "\n",
        "    # Count violations by severity\n",
        "    severity_counts = {\"critical\": 0, \"high\": 0, \"medium\": 0, \"low\": 0}\n",
        "    for violation in agent_violations:\n",
        "        severity = violation.get(\"severity\", \"medium\")\n",
        "        if severity in severity_counts:\n",
        "            severity_counts[severity] += 1\n",
        "\n",
        "    # Count bias signals for this agent\n",
        "    agent_bias = [s for s in bias_signals if s.get(\"agent_name\") == agent_name]\n",
        "    bias_count = len(agent_bias)\n",
        "\n",
        "    # Count drift signals for this agent\n",
        "    agent_drift = [s for s in drift_signals if s.get(\"agent_name\") == agent_name]\n",
        "    drift_count = len(agent_drift)\n",
        "\n",
        "    # Calculate weighted risk score (0.0 to 1.0)\n",
        "    total_violations = sum(severity_counts.values())\n",
        "    weighted_violation_score = sum(\n",
        "        severity_counts[sev] * config.severity_weights.get(sev, 0.5)\n",
        "        for sev in severity_counts\n",
        "    )\n",
        "\n",
        "    # Normalize by total violations (if any)\n",
        "    if total_violations > 0:\n",
        "        violation_score = weighted_violation_score / total_violations\n",
        "    else:\n",
        "        violation_score = 0.0\n",
        "\n",
        "    # Add bias and drift contributions\n",
        "    bias_score = min(1.0, bias_count * 0.2)  # Each bias signal adds 0.2\n",
        "    drift_score = min(1.0, drift_count * 0.15)  # Each drift signal adds 0.15\n",
        "\n",
        "    # Combined risk score (weighted average)\n",
        "    risk_score = (\n",
        "        violation_score * 0.6 +\n",
        "        bias_score * 0.25 +\n",
        "        drift_score * 0.15\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"agent_name\": agent_name,\n",
        "        \"total_violations\": total_violations,\n",
        "        \"severity_counts\": severity_counts,\n",
        "        \"bias_signals_count\": bias_count,\n",
        "        \"drift_signals_count\": drift_count,\n",
        "        \"risk_score\": round(risk_score, 3)\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_overall_risk_scores(\n",
        "    compliance_events: List[Dict[str, Any]],\n",
        "    bias_signals: List[Dict[str, Any]],\n",
        "    drift_signals: List[Dict[str, Any]],\n",
        "    agent_action_logs: List[Dict[str, Any]],\n",
        "    events_lookup: Dict[str, Dict[str, Any]],\n",
        "    config: GovernanceComplianceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate risk scores for all agents and overall system.\n",
        "\n",
        "    Args:\n",
        "        compliance_events: List of compliance events (violations)\n",
        "        bias_signals: List of bias signals\n",
        "        drift_signals: List of drift signals\n",
        "        agent_action_logs: List of agent action log events\n",
        "        events_lookup: Lookup dict for events by event_id\n",
        "        config: Configuration with severity weights\n",
        "\n",
        "    Returns:\n",
        "        Risk scores dict with agent_scores and overall_risk_score\n",
        "    \"\"\"\n",
        "    # Get unique agent names\n",
        "    agent_names = set()\n",
        "    for event in agent_action_logs:\n",
        "        agent_name = event.get(\"agent_name\")\n",
        "        if agent_name:\n",
        "            agent_names.add(agent_name)\n",
        "\n",
        "    # Calculate risk score for each agent\n",
        "    agent_scores = {}\n",
        "    for agent_name in agent_names:\n",
        "        agent_score = calculate_agent_risk_score(\n",
        "            agent_name,\n",
        "            compliance_events,\n",
        "            bias_signals,\n",
        "            drift_signals,\n",
        "            events_lookup,\n",
        "            config\n",
        "        )\n",
        "        agent_scores[agent_name] = agent_score\n",
        "\n",
        "    # Calculate overall risk score (average of agent scores, weighted by violations)\n",
        "    if agent_scores:\n",
        "        total_violations_all = sum(\n",
        "            score.get(\"total_violations\", 0) for score in agent_scores.values()\n",
        "        )\n",
        "\n",
        "        if total_violations_all > 0:\n",
        "            # Weight by violations\n",
        "            weighted_sum = sum(\n",
        "                score.get(\"risk_score\", 0.0) * score.get(\"total_violations\", 0)\n",
        "                for score in agent_scores.values()\n",
        "            )\n",
        "            overall_risk_score = weighted_sum / total_violations_all\n",
        "        else:\n",
        "            # Simple average if no violations\n",
        "            overall_risk_score = sum(\n",
        "                score.get(\"risk_score\", 0.0) for score in agent_scores.values()\n",
        "            ) / len(agent_scores)\n",
        "    else:\n",
        "        overall_risk_score = 0.0\n",
        "\n",
        "    return {\n",
        "        \"agent_scores\": agent_scores,\n",
        "        \"overall_risk_score\": round(overall_risk_score, 3),\n",
        "        \"total_violations\": len(compliance_events),\n",
        "        \"total_bias_signals\": len(bias_signals),\n",
        "        \"total_drift_signals\": len(drift_signals)\n",
        "    }\n",
        "\n"
      ]
    }
  ]
}