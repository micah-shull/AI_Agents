{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR/rY8N1q2AthZUADDv6Lg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/412_MO_CampaignAnalysis_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# â­ The Most Valuable Concept in the Campaign Analysis Utilities\n",
        "\n",
        "### **The most valuable concept here is that *campaign health is computed from explicit evidence and declared rules â€” not inferred, guessed, or narrated*.**\n",
        "\n",
        "This module turns *marketing judgment* into a **repeatable, auditable calculation**.\n",
        "\n",
        "Thatâ€™s rare â€” and extremely valuable.\n",
        "\n",
        "Let me unpack why.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ Judgment Is Broken Into Atomic, Testable Questions\n",
        "\n",
        "Instead of one giant â€œanalyze campaignâ€ function that does everything, youâ€™ve decomposed judgment into **clear sub-questions**:\n",
        "\n",
        "* How many assets exist?\n",
        "* How many experiments are running vs completed?\n",
        "* How much money was spent?\n",
        "* How much value was produced?\n",
        "* How does that compare to expectations?\n",
        "\n",
        "Each question:\n",
        "\n",
        "* has a single responsibility\n",
        "* returns a concrete answer\n",
        "* can be tested in isolation\n",
        "\n",
        "This is *how you prevent intuition from creeping in*.\n",
        "\n",
        "Most marketing systems blur these steps together and then â€œinterpretâ€ results.\n",
        "You donâ€™t.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ Evidence Is Treated as Competing Sources (Very Important)\n",
        "\n",
        "This part is subtle â€” and excellent:\n",
        "\n",
        "```python\n",
        "# Use the maximum of the two (they might overlap, but ledger is more authoritative)\n",
        "```\n",
        "\n",
        "You are explicitly acknowledging that:\n",
        "\n",
        "* performance metrics\n",
        "* ROI ledger entries\n",
        "\n",
        "â€¦may disagree.\n",
        "\n",
        "Instead of hiding that, you:\n",
        "\n",
        "* document the assumption\n",
        "* choose an authority\n",
        "* encode the rule\n",
        "\n",
        "Thatâ€™s **real-world thinking**.\n",
        "\n",
        "This mirrors how executives actually work:\n",
        "\n",
        "> â€œWhich number do we trust more, and why?â€\n",
        "\n",
        "Your code answers that transparently.\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Performance Is Measured Against Intent, Not Vanity Metrics\n",
        "\n",
        "The most important function here is not the counters.\n",
        "\n",
        "Itâ€™s this one:\n",
        "\n",
        "```python\n",
        "determine_performance_status(...)\n",
        "```\n",
        "\n",
        "Why?\n",
        "\n",
        "Because performance is not defined as:\n",
        "\n",
        "* â€œhigh revenueâ€\n",
        "* â€œlots of clicksâ€\n",
        "* â€œgood CTRâ€\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "> **Outcome relative to expectations and constraints.**\n",
        "\n",
        "Specifically:\n",
        "\n",
        "* spend\n",
        "* budget\n",
        "* configured thresholds\n",
        "\n",
        "Thatâ€™s how grown-up systems evaluate success.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ Thresholds Live Outside the Logic (Critical Design Choice)\n",
        "\n",
        "Notice what this module *does not* do:\n",
        "\n",
        "* It does not hard-code what â€œgoodâ€ means\n",
        "* It does not assume universal targets\n",
        "* It does not bake business judgment into math\n",
        "\n",
        "Instead, it accepts:\n",
        "\n",
        "```python\n",
        "thresholds: Dict[str, float]\n",
        "```\n",
        "\n",
        "This keeps:\n",
        "\n",
        "* **policy** in config\n",
        "* **evidence** in state\n",
        "* **judgment logic** here\n",
        "\n",
        "That separation is one of the strongest architectural signals in your entire system.\n",
        "\n",
        "It means:\n",
        "\n",
        "* the same logic works for different companies\n",
        "* the same agent works under different strategies\n",
        "* executives can change expectations without code changes\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Campaign Analysis Produces *Objects*, Not Opinions\n",
        "\n",
        "The output of `analyze_campaign` is a **structured judgment object**:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"total_assets\": 4,\n",
        "  \"active_experiments\": 1,\n",
        "  \"total_spend\": 4200.0,\n",
        "  \"roi_ratio\": 2.23,\n",
        "  \"overall_performance\": \"exceeding_expectations\"\n",
        "}\n",
        "```\n",
        "\n",
        "This matters because:\n",
        "\n",
        "* downstream nodes can reason over it\n",
        "* reporting can summarize it\n",
        "* decisions can reference it\n",
        "* audits can inspect it\n",
        "\n",
        "You are not producing text.\n",
        "You are producing **decision-grade artifacts**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ No Hidden Intelligence, No Narrative Leakage\n",
        "\n",
        "This module does **zero storytelling**.\n",
        "\n",
        "No:\n",
        "\n",
        "* â€œThis campaign looks strongâ€\n",
        "* â€œWe believe performance is improvingâ€\n",
        "* â€œThis suggests momentumâ€\n",
        "\n",
        "Instead, it says:\n",
        "\n",
        "* here are the numbers\n",
        "* here are the rules\n",
        "* here is the classification\n",
        "\n",
        "This is *foundational* to trust.\n",
        "\n",
        "Narrative belongs later â€” after evidence is locked.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  The Deeper Pattern (This Is the Real Value)\n",
        "\n",
        "What youâ€™ve built here is a **judgment engine**, not an analytics helper.\n",
        "\n",
        "It follows a very specific philosophy:\n",
        "\n",
        "> **Judgment = Evidence Ã— Rules Ã— Context**\n",
        "\n",
        "Where:\n",
        "\n",
        "* evidence = metrics + ledger\n",
        "* rules = thresholds from config\n",
        "* context = campaign budget & intent\n",
        "\n",
        "Thatâ€™s exactly how executives think â€” just usually without code.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Module Will Age Well\n",
        "\n",
        "This code will still be valuable when:\n",
        "\n",
        "* metrics change\n",
        "* channels evolve\n",
        "* LLMs improve\n",
        "* dashboards are replaced\n",
        "\n",
        "Because it encodes *how to decide*, not *how to display*.\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Summary You Should Keep\n",
        "\n",
        "If you ever want to describe this part of the system:\n",
        "\n",
        "> **â€œCampaign performance is evaluated through explicit, configurable rules applied to verifiable evidence â€” not inferred narratives.â€**\n",
        "\n",
        "Thatâ€™s the value.\n",
        "\n"
      ],
      "metadata": {
        "id": "yt8OsurXxAQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh6QD_0kvIc9"
      },
      "outputs": [],
      "source": [
        "\"\"\"Campaign Analysis Utilities\n",
        "\n",
        "Analyze campaign performance, status, and overall health.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "\n",
        "\n",
        "def count_campaign_assets(\n",
        "    campaign_id: str,\n",
        "    assets: List[Dict[str, Any]]\n",
        ") -> int:\n",
        "    \"\"\"Count total assets for a campaign\"\"\"\n",
        "    return len([asset for asset in assets if asset.get(\"campaign_id\") == campaign_id])\n",
        "\n",
        "\n",
        "def count_campaign_experiments(\n",
        "    campaign_id: str,\n",
        "    experiments: List[Dict[str, Any]],\n",
        "    status: Optional[str] = None\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Count experiments for a campaign, optionally filtered by status.\n",
        "\n",
        "    Args:\n",
        "        campaign_id: Campaign ID to count experiments for\n",
        "        experiments: List of experiment dictionaries\n",
        "        status: Optional status filter (\"running\", \"completed\", etc.)\n",
        "\n",
        "    Returns:\n",
        "        Count of experiments matching criteria\n",
        "    \"\"\"\n",
        "    filtered = [\n",
        "        exp for exp in experiments\n",
        "        if exp.get(\"campaign_id\") == campaign_id\n",
        "    ]\n",
        "\n",
        "    if status:\n",
        "        filtered = [exp for exp in filtered if exp.get(\"status\") == status]\n",
        "\n",
        "    return len(filtered)\n",
        "\n",
        "\n",
        "def calculate_campaign_spend(\n",
        "    campaign_id: str,\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    assets: List[Dict[str, Any]],\n",
        "    roi_ledger: List[Dict[str, Any]]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculate total spend for a campaign.\n",
        "\n",
        "    Includes:\n",
        "    - Media spend from performance metrics (cost field)\n",
        "    - Media spend from ROI ledger (media_spend field)\n",
        "\n",
        "    Args:\n",
        "        campaign_id: Campaign ID\n",
        "        metrics: List of performance metrics\n",
        "        assets: List of creative assets\n",
        "        roi_ledger: List of ROI ledger entries\n",
        "\n",
        "    Returns:\n",
        "        Total spend for the campaign\n",
        "    \"\"\"\n",
        "    # Get asset IDs for this campaign\n",
        "    campaign_asset_ids = {\n",
        "        asset[\"asset_id\"] for asset in assets\n",
        "        if asset.get(\"campaign_id\") == campaign_id\n",
        "    }\n",
        "\n",
        "    # Sum costs from metrics\n",
        "    metrics_spend = sum(\n",
        "        metric.get(\"cost\", 0.0) for metric in metrics\n",
        "        if metric.get(\"asset_id\") in campaign_asset_ids\n",
        "    )\n",
        "\n",
        "    # Get media spend from ROI ledger\n",
        "    ledger_entry = next(\n",
        "        (entry for entry in roi_ledger if entry.get(\"campaign_id\") == campaign_id),\n",
        "        None\n",
        "    )\n",
        "    ledger_spend = ledger_entry.get(\"media_spend\", 0.0) if ledger_entry else 0.0\n",
        "\n",
        "    # Use the maximum of the two (they might overlap, but ledger is more authoritative)\n",
        "    # For MVP, we'll use ledger if available, otherwise metrics\n",
        "    if ledger_spend > 0:\n",
        "        return ledger_spend\n",
        "    return metrics_spend\n",
        "\n",
        "\n",
        "def calculate_campaign_revenue_proxy(\n",
        "    campaign_id: str,\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    assets: List[Dict[str, Any]]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculate total revenue proxy for a campaign.\n",
        "\n",
        "    Args:\n",
        "        campaign_id: Campaign ID\n",
        "        metrics: List of performance metrics\n",
        "        assets: List of creative assets\n",
        "\n",
        "    Returns:\n",
        "        Total revenue proxy for the campaign\n",
        "    \"\"\"\n",
        "    # Get asset IDs for this campaign\n",
        "    campaign_asset_ids = {\n",
        "        asset[\"asset_id\"] for asset in assets\n",
        "        if asset.get(\"campaign_id\") == campaign_id\n",
        "    }\n",
        "\n",
        "    # Sum revenue_proxy from metrics\n",
        "    return sum(\n",
        "        metric.get(\"revenue_proxy\", 0.0) for metric in metrics\n",
        "        if metric.get(\"asset_id\") in campaign_asset_ids\n",
        "    )\n",
        "\n",
        "\n",
        "def determine_performance_status(\n",
        "    revenue_proxy: float,\n",
        "    spend: float,\n",
        "    budget: float,\n",
        "    thresholds: Dict[str, float]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Determine overall performance status for a campaign.\n",
        "\n",
        "    Args:\n",
        "        revenue_proxy: Total revenue proxy\n",
        "        spend: Total spend\n",
        "        budget: Campaign budget\n",
        "        thresholds: Performance thresholds from config\n",
        "            {\n",
        "                \"exceeding_expectations\": 1.2,  # > 120% of target\n",
        "                \"meeting_expectations\": 0.8,    # 80-120% of target\n",
        "                \"below_expectations\": 0.0       # < 80% of target\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        Performance status: \"exceeding_expectations\", \"meeting_expectations\", or \"below_expectations\"\n",
        "    \"\"\"\n",
        "    if spend == 0:\n",
        "        return \"below_expectations\"\n",
        "\n",
        "    # Calculate ROI ratio (revenue / spend)\n",
        "    roi_ratio = revenue_proxy / spend\n",
        "\n",
        "    # Calculate budget efficiency (revenue / budget)\n",
        "    if budget > 0:\n",
        "        budget_efficiency = revenue_proxy / budget\n",
        "    else:\n",
        "        budget_efficiency = roi_ratio\n",
        "\n",
        "    # Use budget efficiency as primary metric\n",
        "    if budget_efficiency >= thresholds.get(\"exceeding_expectations\", 1.2):\n",
        "        return \"exceeding_expectations\"\n",
        "    elif budget_efficiency >= thresholds.get(\"meeting_expectations\", 0.8):\n",
        "        return \"meeting_expectations\"\n",
        "    else:\n",
        "        return \"below_expectations\"\n",
        "\n",
        "\n",
        "def analyze_campaign(\n",
        "    campaign: Dict[str, Any],\n",
        "    assets: List[Dict[str, Any]],\n",
        "    experiments: List[Dict[str, Any]],\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    roi_ledger: List[Dict[str, Any]],\n",
        "    thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyze a single campaign.\n",
        "\n",
        "    Args:\n",
        "        campaign: Campaign dictionary\n",
        "        assets: List of all creative assets\n",
        "        experiments: List of all experiments\n",
        "        metrics: List of all performance metrics\n",
        "        roi_ledger: List of ROI ledger entries\n",
        "        thresholds: Performance thresholds from config\n",
        "\n",
        "    Returns:\n",
        "        Campaign analysis dictionary\n",
        "    \"\"\"\n",
        "    campaign_id = campaign[\"campaign_id\"]\n",
        "\n",
        "    # Count assets\n",
        "    total_assets = count_campaign_assets(campaign_id, assets)\n",
        "\n",
        "    # Count experiments\n",
        "    active_experiments = count_campaign_experiments(campaign_id, experiments, status=\"running\")\n",
        "    completed_experiments = count_campaign_experiments(campaign_id, experiments, status=\"completed\")\n",
        "\n",
        "    # Calculate spend and revenue\n",
        "    total_spend = calculate_campaign_spend(campaign_id, metrics, assets, roi_ledger)\n",
        "    total_revenue_proxy = calculate_campaign_revenue_proxy(campaign_id, metrics, assets)\n",
        "\n",
        "    # Determine performance status\n",
        "    budget = campaign.get(\"budget\", 0.0)\n",
        "    overall_performance = determine_performance_status(\n",
        "        total_revenue_proxy,\n",
        "        total_spend,\n",
        "        budget,\n",
        "        thresholds\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"campaign_id\": campaign_id,\n",
        "        \"campaign_name\": campaign.get(\"name\", \"\"),\n",
        "        \"status\": campaign.get(\"status\", \"unknown\"),\n",
        "        \"objective\": campaign.get(\"objective\", \"\"),\n",
        "        \"primary_kpi\": campaign.get(\"primary_kpi\", \"\"),\n",
        "        \"total_assets\": total_assets,\n",
        "        \"active_experiments\": active_experiments,\n",
        "        \"completed_experiments\": completed_experiments,\n",
        "        \"total_spend\": round(total_spend, 2),\n",
        "        \"total_revenue_proxy\": round(total_revenue_proxy, 2),\n",
        "        \"budget\": budget,\n",
        "        \"budget_utilization\": round((total_spend / budget * 100) if budget > 0 else 0, 2),\n",
        "        \"roi_ratio\": round((total_revenue_proxy / total_spend) if total_spend > 0 else 0, 2),\n",
        "        \"overall_performance\": overall_performance\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_all_campaigns(\n",
        "    campaigns: List[Dict[str, Any]],\n",
        "    assets: List[Dict[str, Any]],\n",
        "    experiments: List[Dict[str, Any]],\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    roi_ledger: List[Dict[str, Any]],\n",
        "    thresholds: Dict[str, float]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyze all campaigns.\n",
        "\n",
        "    Args:\n",
        "        campaigns: List of campaign dictionaries\n",
        "        assets: List of all creative assets\n",
        "        experiments: List of all experiments\n",
        "        metrics: List of all performance metrics\n",
        "        roi_ledger: List of ROI ledger entries\n",
        "        thresholds: Performance thresholds from config\n",
        "\n",
        "    Returns:\n",
        "        List of campaign analysis dictionaries\n",
        "    \"\"\"\n",
        "    analyses = []\n",
        "    for campaign in campaigns:\n",
        "        analysis = analyze_campaign(\n",
        "            campaign,\n",
        "            assets,\n",
        "            experiments,\n",
        "            metrics,\n",
        "            roi_ledger,\n",
        "            thresholds\n",
        "        )\n",
        "        analyses.append(analysis)\n",
        "\n",
        "    return analyses\n"
      ]
    }
  ]
}