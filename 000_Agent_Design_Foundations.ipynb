{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNZjoLzXnk9h2CeViuoFHpE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/000_Agent_Design_Foundations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– What is an AI Agent?\n",
        "\n",
        "An **AI agent** is a system that can:\n",
        "\n",
        "> **Perceive its environment, make decisions, and take actions to achieve a goal â€” often with some level of autonomy.**\n",
        "\n",
        "It can range from something very simple (like a chatbot that answers your questions) to something very advanced (like a personal assistant that books appointments, sends emails, or even writes code for you).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  The Core Components of an AI Agent\n",
        "\n",
        "1. **Perception** â€“ Gathers input (text, voice, data, etc.)\n",
        "2. **Reasoning** â€“ Decides what to do based on the input\n",
        "3. **Action** â€“ Responds or performs tasks\n",
        "4. **Memory** (optional but valuable) â€“ Remembers past interactions\n",
        "5. **Tool Use** (advanced) â€“ Uses APIs, calculators, web search, or file systems\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ Why Are AI Agents Valuable?\n",
        "\n",
        "AI agents **save time, scale expertise, and automate tasks** that would otherwise require human attention.\n",
        "\n",
        "### They're valuable because they can:\n",
        "- Work 24/7\n",
        "- Handle repetitive tasks with no fatigue\n",
        "- Provide consistent answers\n",
        "- Scale customer support, marketing, scheduling, etc.\n",
        "- Make data-driven decisions instantly\n",
        "- Interface with other systems (APIs, documents, databases)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¥ Real-World Use Cases\n",
        "\n",
        "| Use Case                    | Example                                                                 |\n",
        "|----------------------------|-------------------------------------------------------------------------|\n",
        "| ðŸ’¬ Chatbots                | Answer customer questions (e.g., banking, e-commerce)                   |\n",
        "| ðŸ“… Virtual Assistants      | Schedule meetings, send emails (e.g., x.ai, Siri, Google Assistant)     |\n",
        "| ðŸ“Š Data Agents             | Analyze spreadsheets, create reports, generate insights                 |\n",
        "| ðŸ•¸ï¸ Web Agents             | Search the web, extract data, summarize articles                        |\n",
        "| ðŸ§ª Research Agents         | Read papers, summarize findings, write citations                        |\n",
        "| ðŸ› ï¸ Tool-Using Agents      | Use calculators, access APIs, write or edit code                        |\n",
        "| ðŸ“¦ RPA (Robotic Process Automation) | Handle invoice processing, HR onboarding, finance ops          |\n",
        "| ðŸŽ® Game Agents             | Play or assist in video games (e.g., bots that learn by playing)         |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ¤ AI Agent vs Chatbot vs Assistant\n",
        "\n",
        "| Term         | What it Means                                                                 |\n",
        "|--------------|--------------------------------------------------------------------------------|\n",
        "| **Chatbot**  | Conversational tool, usually rules-based or LLM-based                         |\n",
        "| **Assistant**| Smarter chatbot â€” more memory, maybe tools or goals                           |\n",
        "| **Agent**    | More autonomous, goal-oriented, tool-using, adaptable                         |\n",
        "\n",
        "A chatbot is **a type of agent**, but an agent can do **much more** than just chat.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Example of an AI Agent in Action\n",
        "\n",
        "Letâ€™s say you build a **\"Marketing Assistant\"** agent. It could:\n",
        "1. Accept a product description\n",
        "2. Search for trending keywords\n",
        "3. Generate a blog post\n",
        "4. Create social media posts\n",
        "5. Schedule them via API\n",
        "6. Email a report to your team\n",
        "\n",
        "Thatâ€™s a full workflow automated by an agent â€” not just a conversation.\n",
        "\n",
        "\n",
        "## Agents are systems that independently accomplish tasks on your behalf.\n",
        "\n",
        "An **agent** is a system that performs multi-step tasks on your behalf using an LLM. Unlike traditional automation or simple chatbots, agents can:\n",
        "- **Independently manage workflows**\n",
        "- **Make decisions**, detect when a task is complete, and handle errors or unexpected conditions\n",
        "- **Use tools dynamically** to gather data and take action\n",
        "- Operate within **guardrails** to ensure safe and predictable behavior\n",
        "\n",
        "> ðŸ§  **Key Idea**: Agents donâ€™t just assistâ€”they act *on your behalf* with autonomy.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ When Should You Build an Agent?\n",
        "Agents shine in workflows where traditional automation struggles:\n",
        "1. **Complex Decision-Making**: Context-aware tasks like refund approvals.\n",
        "2. **Hard-to-Maintain Rules**: Situations with complicated, brittle rule systems (e.g., security reviews).\n",
        "3. **Unstructured Data**: Tasks involving language understanding, document analysis, or conversational input (e.g., insurance claims).\n",
        "\n",
        "Before building, ask: *Is the workflow too ambiguous, dynamic, or context-rich for rule-based automation?*\n",
        "\n",
        "> âœ… If yes â€” an agent may be the better solution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWOP--J1GTkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ðŸ§  Agent Design Foundations\n",
        "\n",
        "In its most essential form, an **AI agent** consists of three core components:\n",
        "\n",
        "### ðŸ”¹ 1. Model (The Agentâ€™s Brain)\n",
        "- This is the LLM (Large Language Model) that powers the agentâ€™s **reasoning and decision-making**.\n",
        "- It interprets user input, selects actions, and can even guide tool usage.\n",
        "- In our case, weâ€™re using Hugging Faceâ€™s `flan-t5-base` model for its balance of size, speed, and instruction-following ability.\n",
        "\n",
        "### ðŸ”¹ 2. Tools (The Agentâ€™s Hands)\n",
        "- Tools are **functions or external APIs** the agent can call to take real-world action.\n",
        "- Examples in our notebook:\n",
        "  - `get_weather()`\n",
        "  - `save_note()`\n",
        "  - `search_faq()`\n",
        "- In production systems, tools could connect to databases, send emails, generate reports, or control devices.\n",
        "\n",
        "### ðŸ”¹ 3. Instructions (The Agentâ€™s Personality and Guardrails)\n",
        "- These are **explicit prompts or guidelines** that define how the agent should behave.\n",
        "- They help steer the modelâ€™s responses, ensuring consistency, safety, and task alignment.\n",
        "- Example: We instructed the model to *only use tools* for weather, notes, and FAQ â€” and to *politely decline off-topic questions*.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Model Selection Principles\n",
        "\n",
        "Choosing the right model is key to building a smart, fast, and cost-effective agent. The PDF recommends the following approach:\n",
        "\n",
        "### âœ… Best Practices for Model Selection\n",
        "\n",
        "1. **Start with the most capable model** available to build your prototype.\n",
        "   - This helps you measure the *ideal behavior* without performance issues.\n",
        "2. **Evaluate your agentâ€™s performance** (accuracy, reliability, tool selection).\n",
        "3. **Replace with smaller or cheaper models** only if they maintain acceptable performance.\n",
        "   - Example: A small model might work fine for tool selection, but fail at nuanced decision-making.\n",
        "\n",
        "### âš–ï¸ Tradeoffs to Consider\n",
        "\n",
        "| Task Type | Model Recommendation |\n",
        "|-----------|----------------------|\n",
        "| Simple classification, intent detection | Small, fast models (`flan-t5-small`, `distilbert`) |\n",
        "| General instruction-following | Mid-size models (`flan-t5-base`, `mistral`) |\n",
        "| Complex reasoning, judgment, summarization | Large models (`mistral-7b`, `llama2-13b`, GPT-4) |\n",
        "\n",
        "> **Key Idea**: Don't over-engineer early. Start strong, then optimize later.\n",
        "\n"
      ],
      "metadata": {
        "id": "cBFCWUBVKsLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s walk through this first block *not just as code*, but as your **first real lesson in agent thinking**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Agent Lab 1 â€“ Setup & Core Concepts\n",
        "\n",
        "This setup lays the foundation for how agents work. Letâ€™s unpack each piece from a learning perspective:\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 1. `transformers` Pipeline\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "model_name = \"google/flan-t5-base\"\n",
        "llm = pipeline(\"text2text-generation\", model=model_name)\n",
        "```\n",
        "\n",
        "> **ðŸ§  Key Concept: The Model = Your Agentâ€™s Brain**\n",
        "\n",
        "- You're loading a small Hugging Face model that can follow instructions.\n",
        "- In agent design, this is the **reasoning engine** â€” it reads input and decides what to do.\n",
        "- Weâ€™re using a *pipeline* to simplify everything: tokenization, model inference, and decoding are wrapped into one step.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 2. Defining Tools\n",
        "\n",
        "```python\n",
        "def get_weather(city=\"Gainesville\"):\n",
        "    return f\"The weather in {city} is sunny with a high of 82Â°F.\"\n",
        "\n",
        "def save_note(note):\n",
        "    return f\"Note saved: {note}\"\n",
        "```\n",
        "\n",
        "> **ðŸ§  Key Concept: Tools = How Your Agent Takes Action**\n",
        "\n",
        "- Tools are like the agentâ€™s â€œhandsâ€ â€” they interact with the world.\n",
        "- In real agents, these could be APIs, databases, or browser actions.\n",
        "- Here, we mock tools for simplicity so you can focus on **tool selection** and logic.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 3. Tool Registry\n",
        "\n",
        "```python\n",
        "tools = {\n",
        "    \"get_weather\": get_weather,\n",
        "    \"save_note\": save_note\n",
        "}\n",
        "```\n",
        "\n",
        "> **ðŸ§  Key Concept: A Tool Registry = Your Agentâ€™s Toolbox**\n",
        "\n",
        "- This lets the agent look up and call tools by name.\n",
        "- Later, youâ€™ll use this to simulate decision-making: the agent will choose a tool like a command from a menu.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Learning Takeaways So Far\n",
        "\n",
        "| Concept | Meaning | How It Maps to Agents |\n",
        "|--------|---------|------------------------|\n",
        "| **LLM Model** | Reasoning engine | Makes decisions and interprets user input |\n",
        "| **Tool** | Action function | Executes steps on behalf of user |\n",
        "| **Tool Registry** | Dictionary of tools | Lets the agent find and call tools dynamically |\n"
      ],
      "metadata": {
        "id": "JW63Yz61OLaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7afRY2TNkDb",
        "outputId": "30a6a6f4-82b9-4f2b-876a-906148214a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’¡ Why the Pipeline Is So Powerful\n",
        "\n",
        "The `pipeline()` function from Hugging Face abstracts away **all the gritty, low-level details** of how transformers actually work under the hood:\n",
        "\n",
        "| Step | Without Pipeline | With `pipeline()` |\n",
        "|------|------------------|-------------------|\n",
        "| **Tokenization** | You'd manually load a tokenizer, tokenize input | âœ… Done for you |\n",
        "| **Model Inference** | Youâ€™d load a model, pass tokenized input, manage tensors | âœ… Done for you |\n",
        "| **Decoding** | Youâ€™d convert output IDs back into readable text | âœ… Done for you |\n",
        "| **Task-Specific Behavior** | Youâ€™d need custom logic (e.g., for text2text, QA, etc.) | âœ… Handled by pipeline type |\n",
        "\n"
      ],
      "metadata": {
        "id": "GNFkgUA5O7RU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ðŸ¤– Chatbot vs ðŸ§  Agent\n",
        "\n",
        "| Aspect | **Chatbot** | **Agent** |\n",
        "|--------|-------------|-----------|\n",
        "| ðŸ—£ï¸ **Goal** | Answer a question or have a conversation | Complete a *task* on your behalf |\n",
        "| ðŸ§  **Thinking** | Responds to input, usually single-turn or narrow | Maintains state, evaluates, makes decisions |\n",
        "| ðŸ› ï¸ **Tools** | Usually none â€” answers from memory (model weights) | Has tools and APIs it can **use** |\n",
        "| ðŸ” **Action** | Mostly text replies | Takes **actions** like retrieving data, saving info, sending messages |\n",
        "| âš™ï¸ **Autonomy** | You drive the flow | The **agent drives the workflow** â€” you just provide the goal |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  You Said It Perfectly:\n",
        "> *â€œAn agent has tools at its disposal to take in information and take action based on that information.â€*\n",
        "\n",
        "Exactly â€” itâ€™s a **reasoning engine with hands**.\n",
        "\n",
        "- **Input**: â€œCan you save this note?â€\n",
        "- **Agent thinks**: Is this a save_note task? Whatâ€™s the note content?\n",
        "- **Agent acts**: Calls `save_note(note=\"Remember to review the guide\")`\n",
        "- **Agent responds**: â€œNote saved: Remember to review the guideâ€\n",
        "\n",
        "And it can repeat this process across multiple steps, adapting as it goes.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš€ Thatâ€™s What Makes Agents So Powerful:\n",
        "Theyâ€™re not just â€œtalkersâ€ â€” theyâ€™re **doers**.\n",
        "\n",
        "This is what makes them suited for:\n",
        "- Customer support automation\n",
        "- Internal tools and ops agents\n",
        "- Personal assistants\n",
        "- Data analyzers\n",
        "- Code committers\n",
        "- Forecasting bots (ðŸ˜‰ like yours!)\n",
        "\n"
      ],
      "metadata": {
        "id": "xNdLaEVfQP9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§  Agent Lesson: **Model Selection = Picking the Right Brain for the Job**\n",
        "\n",
        "### ðŸ” Why Model Selection Matters\n",
        "\n",
        "Every agent has a **goal** â€” and the model is its **thinking engine**. But not all brains are built the same.\n",
        "\n",
        "| Goal | Best Model Type | Why |\n",
        "|------|------------------|-----|\n",
        "| Simple lookups / intent detection | Small model (e.g., `distilbert`, `flan-t5-small`) | Fast, cheap, low latency |\n",
        "| Answering questions or following instructions | Medium model (e.g., `flan-t5-base`, `mistral-7b`) | Good balance of performance and cost |\n",
        "| Complex reasoning, judgment, summarization | Larger model (e.g., `mistral-7b-instruct`, `llama2-13b`) | Stronger reasoning skills |\n",
        "| Multimodal tasks (e.g., vision + text) | Specialized model (e.g., `llava`, `gemini`, `gpt-4o`) | Different inputs or formats |\n",
        "\n",
        "> âœ… Lesson: **Not all agent steps need a big model.** You might use:\n",
        "- A small model for tool selection\n",
        "- A medium model for planning\n",
        "- A large model only when needed for complex judgment\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“š You Also Noted:\n",
        "> *\"That would require a deep knowledge of available models, right?\"*\n",
        "\n",
        "**YES â€” and that's part of becoming a great agent builder.** But hereâ€™s the good news:\n",
        "\n",
        "- You donâ€™t have to memorize everything.\n",
        "- Hugging Face model hub is like a catalog â€” search by task (e.g., summarization, classification).\n",
        "- Your job is to **experiment + evaluate**: start with a strong model, then try cheaper/smaller ones.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ› ï¸ In Practice\n",
        "Hereâ€™s what agent builders usually do:\n",
        "1. Prototype with a strong general model (like `flan-t5-base` or `gpt-4`)\n",
        "2. Measure performance\n",
        "3. Optimize: replace with smaller models where possible (to cut cost, increase speed)\n",
        "\n",
        "> Think of it like hiring employees â€” you donâ€™t need a rocket scientist to answer phone calls, but you might want one to design the rocket.\n",
        "\n"
      ],
      "metadata": {
        "id": "_QVXBcQ8RVHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "f6a76c5f17954d7e934f9f4eb659ca5c",
            "7c5729f40fbe4b50b429f4b05eab37f5",
            "25cf75c8279541bbb95c49f303d4a6aa",
            "35fb056417be40879b447ed05bf8e232",
            "353fc283c96248588d0cdac57b737e00",
            "97d54c7d62f148969a6d5b75332c912d",
            "a80139c1e28144699fc103f86237c1a0",
            "6bc4888bc28a440aaa2ec99e71eee5fd",
            "c38276fbb15147a2ba4c1db63cd63622",
            "a7fb9d1a3c2e405cba6069c0a03d9d4c",
            "8a733083371e4c0b9e86fdd034d5c93b",
            "2922d8487ef84af782f9f65997dd39ef",
            "efb7435058d940c892d25ab72d8f10d1",
            "8bf472f935034acfb7c52ad88770b27b",
            "21d8a65c3ccd47eb919cac49c97ed95f",
            "a0ce6adf0cb241859f17a02d8080dadf",
            "a4fdd03e189e4baf99d8e5b45ece2e9a",
            "cb5bdbaa990c465db191e36f43af7b34",
            "d3a021d5d6734c21a90569ee953f9f5d",
            "8b98a8f8391a404894de01edebc4e934",
            "c4dd2a65eda04160a62fe642500b7bf4",
            "f02819a5e36446f0b76c39a30c929e6a",
            "b4f55ddfc3eb48cdb6b5942aab282816",
            "48ff633e1fdb4ef385b1dbd34afe7a94",
            "710814ba65dd4708877ced7e001bfcae",
            "aa93ee549990421fab4990cabb5f78d3",
            "4a4ae6ecd8594d098d40d5ead7158a23",
            "7e6207cda6a04795a18717c852f2128f",
            "1c91eb57cf9e4793bcfccee58c6717f2",
            "a4dc9b35db2849ef8d4d64b4089f0a4a",
            "0a828980bd6f4f429e480db3ab5ba62c",
            "2f53705a0d8a4098b4c5bce796410964",
            "fda850e6bc6f473b8770173feb23c215",
            "ccce5abaf3144831a585e339b6426211",
            "f75515f675c24cc4915911ec0741153f",
            "e6fe67198d0a4db7af3b9a32a575c31f",
            "d9a6438fda3148a29599d3fda57f9bab",
            "11bfc29d26c0438994a20b39c8d10d7a",
            "1d76e21cc2624007bc0d98836d4e2196",
            "69fcb5f7088947c4bbf23cab3b8a0629",
            "941990680cdb4d81af7540210070a036",
            "f8df06c1cd3c4f838b14dc1c294af19c",
            "dc31491486c34d5595a9b11cef2a238d",
            "52221372852648069b3e96d3a12b30a1",
            "f71a11c640ed436a81e70833e8cd3777",
            "9b55705677894be295add43b2124f6de",
            "f8962b7ed37c4f5e93664728908007d5",
            "bff36ab2e9a24512871bdb6210eb9d9e",
            "16d7d7c3666448c3acdf73bfe8297fc5",
            "6e02163f11314cdfbb941edb4d5b4c77",
            "fdf64223690c4f4b8d6c4252f9f252e5",
            "bf964b33fd324ee0a1c8f17bfab801b9",
            "b0dc527afcba4041affd4c6b248688f4",
            "9f1aaa26638d416294c1302e45b67fd7",
            "eaa49e7d5046416991363ce1127ab8b3",
            "77e9ab9e8e7f4201b10fadc201861a48",
            "c7f3c5c7877b44a887acd021dd3d009e",
            "13bebd13b52148b7ac62c65f45dec456",
            "d4f298c61fd94b278b9ce01546837b02",
            "e406dbc90ed04fa49c3f49a9fdb1e502",
            "e1236f45c1b14e34a6191cba30a1db34",
            "fbbb0125e938454fbd531e91c27fc73d",
            "3aa563263cc94e84b8d391fba5124418",
            "40af492aeeb04a5eafbcc45bd3ac4738",
            "6723959501a748ee90924eb121a4885f",
            "aa196895726741d8b9d92c188950703a",
            "ac5bef1a294344d1a0452d78ef0db2b4",
            "dc9d8b4a89404faa83493ffea0c781ef",
            "dc3fd70df7744546b41b690abd4e67ec",
            "a34b2a6ec08c4a168aec868773a3e192",
            "5292e51a2f0e483795d81281895673b5",
            "d5b72810e3024825b2ec95519add7363",
            "9022531afb1c4793b53e3161443d5e3a",
            "81e1bfed0da54d469be34f4d7a32e3af",
            "4e6367a3441d45378194aaa33a06525d",
            "4c529af1989f46beb20a1f52008a30c8",
            "338855ba57a14c2c9c86e7e7c76664a6"
          ]
        },
        "id": "XLQ2wabpKGzM",
        "outputId": "ba6664ab-dcec-4ddc-fcb6-ed3564c8faec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6a76c5f17954d7e934f9f4eb659ca5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2922d8487ef84af782f9f65997dd39ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4f55ddfc3eb48cdb6b5942aab282816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccce5abaf3144831a585e339b6426211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f71a11c640ed436a81e70833e8cd3777"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77e9ab9e8e7f4201b10fadc201861a48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5bef1a294344d1a0452d78ef0db2b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a small instruction-following model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "llm = pipeline(\"text2text-generation\", model=model_name)\n",
        "\n",
        "# Simple tools\n",
        "def get_weather(city=\"Gainesville\"):\n",
        "    return f\"The weather in {city} is sunny with a high of 82Â°F.\"\n",
        "\n",
        "def save_note(note):\n",
        "    return f\"Note saved: {note}\"\n",
        "\n",
        "# Tool dictionary\n",
        "tools = {\n",
        "    \"get_weather\": get_weather,\n",
        "    \"save_note\": save_note\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª Agent Lab 2 â€“ Run Loop\n",
        "\n",
        "Letâ€™s break it down clearly and make sure youâ€™re learning the *right lessons*, not just what the code does.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ” Agent Run Loop â€” Line-by-Line, Lesson-by-Lesson\n",
        "\n",
        "Hereâ€™s the loop again in chunks â€” Iâ€™ll walk you through both **how it works** and **what you should take away as a future agent builder**.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Step 1: Ask the model to choose a tool\n",
        "\n",
        "```python\n",
        "tool_prompt = f\"\"\"\n",
        "You are a helpful assistant. Choose the right tool for this request:\n",
        "\n",
        "Available tools:\n",
        "- get_weather\n",
        "- save_note\n",
        "\n",
        "Instructions:\n",
        "- Return ONLY the name of the tool to use.\n",
        "- If the request is to save something, use 'save_note'.\n",
        "- If the request is about weather, use 'get_weather'.\n",
        "\n",
        "User request: {user_input}\n",
        "Tool:\n",
        "\"\"\"\n",
        "tool_response = llm(tool_prompt, max_new_tokens=10)[0]['generated_text'].strip()\n",
        "```\n",
        "\n",
        "> ðŸ§  **Key Lesson: Prompt engineering powers decision-making.**\n",
        "\n",
        "- Youâ€™re giving the model a **clear menu of choices**.\n",
        "- You're being specific: *\"Return ONLY the name of the tool.\"*\n",
        "- Youâ€™re *delegating responsibility* to the model â€” and itâ€™s doing lightweight reasoning.\n",
        "\n",
        "**As an agent builder, this is your job:**  \n",
        "âž¡ï¸ Design prompts that help the model **interpret intent** and **choose the right action**.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Step 2: Execute the tool\n",
        "\n",
        "```python\n",
        "if tool_response == \"get_weather\":\n",
        "    result = tools[\"get_weather\"]()\n",
        "elif tool_response == \"save_note\":\n",
        "    note_content = user_input.replace(\"save\", \"\").strip()\n",
        "    result = tools[\"save_note\"](note_content)\n",
        "else:\n",
        "    result = f\"Sorry, I don't know how to handle: {tool_response}\"\n",
        "```\n",
        "\n",
        "> ðŸ§  **Key Lesson: Tools make the agent *do something* beyond just chatting.**\n",
        "\n",
        "- This is the agentâ€™s â€œaction stepâ€ â€” the model says â€œuse this tool,â€ and you let it call that tool.\n",
        "- You're **routing control dynamically** based on LLM output â€” that's the core of agent behavior.\n",
        "- Right now weâ€™re doing simple string replacement â€” later weâ€™ll do full parsing or use models to extract inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ Step 3: Return the result\n",
        "\n",
        "```python\n",
        "return result\n",
        "```\n",
        "\n",
        "> ðŸ§  **Key Lesson: Agents *complete* tasks, not just answer questions.**\n",
        "\n",
        "This isnâ€™t a back-and-forth chat â€” this is â€œyou asked â†’ I thought â†’ I acted â†’ hereâ€™s the result.â€\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ What You Should Be Learning\n",
        "\n",
        "Here are the **top takeaways** from this first agent:\n",
        "\n",
        "| ðŸ§  Concept | ðŸ’¬ What It Means | ðŸš€ Why It Matters |\n",
        "|-----------|------------------|-------------------|\n",
        "| **Prompt = agentâ€™s instruction** | Your prompt is the agentâ€™s \"brain script\" | Agents act how you tell them |\n",
        "| **LLM = decision maker** | Model decides which tool to use | LLM â‰  answer machine â€” itâ€™s a planner |\n",
        "| **Tools = execution layer** | Real work gets done here | Agents donâ€™t just talk â€” they *do* things |\n",
        "| **Loop = workflow engine** | Think â†’ Act â†’ Respond | The basis of any single- or multi-agent system |\n",
        "| **Simple routing = powerful logic** | If model says \"get_weather\", call that tool | Dynamic behavior from static code |\n",
        "\n"
      ],
      "metadata": {
        "id": "xI6EgW_-RhSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_run(user_input):\n",
        "    # Step 1: Ask the model what tool to use\n",
        "    tool_prompt = f\"\"\"\n",
        "    You are a helpful assistant. Choose the right tool for this request:\n",
        "\n",
        "    Available tools:\n",
        "    - get_weather\n",
        "    - save_note\n",
        "\n",
        "    Instructions:\n",
        "    - Return ONLY the name of the tool to use.\n",
        "    - If the request is to save something, use 'save_note'.\n",
        "    - If the request is about weather, use 'get_weather'.\n",
        "\n",
        "    User request: {user_input}\n",
        "    Tool:\n",
        "    \"\"\"\n",
        "    tool_response = llm(tool_prompt, max_new_tokens=10)[0]['generated_text'].strip()\n",
        "\n",
        "    # Step 2: Use the tool (very basic input handling for now)\n",
        "    if tool_response == \"get_weather\":\n",
        "        result = tools[\"get_weather\"]()\n",
        "    elif tool_response == \"save_note\":\n",
        "        # Just use the whole input as the note for now\n",
        "        note_content = user_input.replace(\"save\", \"\").strip()\n",
        "        result = tools[\"save_note\"](note_content)\n",
        "    else:\n",
        "        result = f\"Sorry, I don't know how to handle: {tool_response}\"\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "U9LkTISaRiB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Weather\n",
        "print(agent_run(\"What's the weather today?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7GvpfqgR8c6",
        "outputId": "e5992c87-91a1-4709-cc62-6610f4db5efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Gainesville is sunny with a high of 82Â°F.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Save a note\n",
        "print(agent_run(\"Can you save 'pick up milk'?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjyZtlUJR_0A",
        "outputId": "52397796-eedf-478f-8af2-34f88a2b1a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note saved: Can you  'pick up milk'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ðŸ› ï¸ Agent Upgrade Path\n",
        "\n",
        "### âœ… Step 1: Add a New Tool â€” `search_faq`\n",
        "We'll simulate a FAQ search tool that \"retrieves\" an answer from a predefined list.\n",
        "\n",
        "### â­ Coming Next:\n",
        "2. Handle edge cases (e.g., unrecognized tool, unclear input)  \n",
        "3. Let the user choose a tool manually if auto-selection fails  \n",
        "4. Add memory so the agent can handle multi-turn chats  \n",
        "5. Build guardrails and instructions into your agent (e.g., stop bad inputs, enforce safety)"
      ],
      "metadata": {
        "id": "QH4r5fhmVKWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”§ Step 1: Add a New Tool (`search_faq`)"
      ],
      "metadata": {
        "id": "BCAwhls-Vdj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FAQ database (mocked)\n",
        "faq_db = {\n",
        "    \"return policy\": \"You can return any item within 30 days with a receipt.\",\n",
        "    \"shipping time\": \"Standard shipping takes 5-7 business days.\",\n",
        "    \"support hours\": \"We offer support Monday to Friday, 9am to 5pm EST.\"\n",
        "}\n",
        "\n",
        "# New tool\n",
        "def search_faq(query):\n",
        "    for key in faq_db:\n",
        "        if key in query.lower():\n",
        "            return faq_db[key]\n",
        "    return \"Sorry, I couldn't find an answer to that question.\"\n",
        "\n",
        "# Add it to the tool registry\n",
        "tools[\"search_faq\"] = search_faq"
      ],
      "metadata": {
        "id": "eEK8NDeOVT0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Update the prompt used in `agent_run()"
      ],
      "metadata": {
        "id": "qwN54FBjVp3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_run(user_input):\n",
        "    # ðŸ§  Prompt to ask the LLM which tool to use, now including 'search_faq'\n",
        "    tool_prompt = f\"\"\"\n",
        "    You are a helpful assistant. Choose the right tool for this request:\n",
        "\n",
        "    Available tools:\n",
        "    - get_weather\n",
        "    - save_note\n",
        "    - search_faq  # ðŸ†• New FAQ tool added\n",
        "\n",
        "    Instructions:\n",
        "    - If the request is to save something, use 'save_note'.\n",
        "    - If the request is about weather, use 'get_weather'.\n",
        "    - If the request is asking a general FAQ (like return policy, shipping, support), use 'search_faq'.\n",
        "\n",
        "    User request: {user_input}\n",
        "    Tool:\n",
        "    \"\"\"\n",
        "\n",
        "    # ðŸ§  Ask the LLM to decide which tool to use based on user input\n",
        "    tool_response = llm(tool_prompt, max_new_tokens=10)[0]['generated_text'].strip()\n",
        "\n",
        "    # ðŸ›  Route the decision to the correct tool function\n",
        "    if tool_response == \"get_weather\":\n",
        "        result = tools[\"get_weather\"]()\n",
        "\n",
        "    elif tool_response == \"save_note\":\n",
        "        # âœï¸ Extract the note from the input and call save_note\n",
        "        note_content = user_input.replace(\"save\", \"\").strip()\n",
        "        result = tools[\"save_note\"](note_content)\n",
        "\n",
        "    elif tool_response == \"search_faq\":\n",
        "        # ðŸ” NEW: Route to the FAQ tool using user input as a search query\n",
        "        result = tools[\"search_faq\"](user_input)\n",
        "\n",
        "    else:\n",
        "        # ðŸš« Unknown tool or unrecognized response\n",
        "        result = f\"Sorry, I don't know how to handle: {tool_response}\"\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "4Ic981v7VTwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent_run(\"Whatâ€™s your return policy?\"),'\\n')\n",
        "print(agent_run(\"How long does shipping take?\"),'\\n')\n",
        "print(agent_run(\"What's the weather today?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7038n-SVTp8",
        "outputId": "ef975b31-9afd-4092-b02f-ddeb757667a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can return any item within 30 days with a receipt. \n",
            "\n",
            "Sorry, I couldn't find an answer to that question. \n",
            "\n",
            "The weather in Gainesville is sunny with a high of 82Â°F.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘ That's a win! You now have a working agent that can reason about different requests and call the right tool â€” even gracefully handle unanswerable FAQs. Letâ€™s level it up.\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Agent Lab 3 - Add Edge-Case Handling\n",
        "\n",
        "### ðŸ§  Why This Matters:\n",
        "Agents donâ€™t live in perfect worlds. Real users:\n",
        "- Type typos\n",
        "- Ask weird or unrelated things\n",
        "- Confuse the model\n",
        "- Trigger tool names that donâ€™t exist\n",
        "\n",
        "You need to **protect your agent from making mistakes** by handling:\n",
        "1. Unrecognized tool names\n",
        "2. Empty or unclear model responses\n",
        "3. Completely off-topic questions\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ› ï¸ Letâ€™s Add Some Basic Guarding\n",
        "\n",
        "\n",
        "## ðŸ§  What Youâ€™re Learning\n",
        "\n",
        "| ðŸ’¡ Lesson | Why It Matters |\n",
        "|----------|----------------|\n",
        "| Validate LLM output | Models arenâ€™t always predictable â€” verify before acting |\n",
        "| Normalize input | Always `strip()` and `lower()` before matching |\n",
        "| Fallbacks = good UX | Clear errors prevent confusion and make agents feel smarter |\n",
        "| Agents need protection | Just like apps need try/except â€” agents need guardrails |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WLzewML3YSH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_run(user_input):\n",
        "    # Prompt with available tools and instructions\n",
        "    tool_prompt = f\"\"\"\n",
        "    You are a helpful assistant. Choose the right tool for this request:\n",
        "\n",
        "    Available tools:\n",
        "    - get_weather\n",
        "    - save_note\n",
        "    - search_faq\n",
        "\n",
        "    Instructions:\n",
        "    - If the request is to save something, use 'save_note'.\n",
        "    - If the request is about weather, use 'get_weather'.\n",
        "    - If the request is asking a general FAQ (like return policy, shipping, support), use 'search_faq'.\n",
        "\n",
        "    User request: {user_input}\n",
        "    Tool:\n",
        "    \"\"\"\n",
        "    tool_response_raw = llm(tool_prompt, max_new_tokens=10)[0]['generated_text'].strip()\n",
        "\n",
        "    # Clean and normalize the response\n",
        "    tool_response = tool_response_raw.lower().strip()\n",
        "\n",
        "    # ðŸš§ EDGE CASE #1: Empty model response\n",
        "    if not tool_response:\n",
        "        return \"Hmm, I couldn't understand your request. Could you rephrase it?\"\n",
        "\n",
        "    # ðŸš§ EDGE CASE #2: Tool not in registry\n",
        "    if tool_response not in tools:\n",
        "        return f\"Sorry, I donâ€™t recognize the tool: '{tool_response}'. Please try again with a clearer request.\"\n",
        "\n",
        "    # Main tool logic\n",
        "    if tool_response == \"get_weather\":\n",
        "        result = tools[\"get_weather\"]()\n",
        "\n",
        "    elif tool_response == \"save_note\":\n",
        "        note_content = user_input.replace(\"save\", \"\").strip()\n",
        "        result = tools[\"save_note\"](note_content)\n",
        "\n",
        "    elif tool_response == \"search_faq\":\n",
        "        result = tools[\"search_faq\"](user_input)\n",
        "\n",
        "    else:\n",
        "        result = \"Unexpected error. Something went wrong.\"\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "JKCaRQ6fYrfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent_run(\"Tell me about your elephant policy\"))  # Should trigger fallback\n",
        "print(agent_run(\"\"))  # Empty input\n",
        "print(agent_run(\"save this for later\"))  # Should work\n",
        "print(agent_run(\"What's the status of my package\"))  # Uncovered edge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZPcWibVTnm",
        "outputId": "7dfcd852-dae1-4c3e-c04d-c28a55533a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I couldn't find an answer to that question.\n",
            "Sorry, I couldn't find an answer to that question.\n",
            "Note saved: this for later\n",
            "Sorry, I couldn't find an answer to that question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youâ€™ve just touched on one of the **central design tensions in building AI agents**:  \n",
        "\n",
        "> ðŸ§  **Where should logic be hardcoded vs. delegated to the LLM?**\n",
        "\n",
        "This is **the art of agent design** â€” knowing when to let the model reason freely, and when to box it in with clear instructions or rules.\n",
        "\n",
        "Letâ€™s explore this in depth.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ¤” Hardcoded Logic vs. LLM Reasoning\n",
        "\n",
        "| ðŸŽ¯ Purpose | ðŸ§± Hardcoded Logic | ðŸ¤– LLM Reasoning |\n",
        "|------------|------------------|------------------|\n",
        "| Safety & predictability | âœ… Always preferred | âŒ Can hallucinate |\n",
        "| Common edge cases | âœ… Efficient | âŒ Overkill |\n",
        "| Flexible understanding | âŒ Brittle / rigid | âœ… Great at interpreting nuance |\n",
        "| User-specific nuance | âŒ Needs code for every case | âœ… Adapts from prompt context |\n",
        "| Open-ended tasks | âŒ Impossible to enumerate | âœ… Natural fit for LLM |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§­ Rule of Thumb:\n",
        "> âœ¨ **Use hardcoding for structure, guardrails, and system safety. Let the LLM handle fuzzy human reasoning, edge-case interpretation, and natural language understanding.**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ›¡ï¸ Guardrails: Youâ€™re *Exactly* Right\n",
        "\n",
        "Instead of hardcoding rejections, you can guide the LLM with instructions like:\n",
        "\n",
        "> â€œIf the user asks questions beyond the scope of this agent, politely remind them that this assistant is focused on {subject} and steer them back.â€\n",
        "\n",
        "This lives in your **prompt**, and works beautifully in most cases â€” especially when the topic is defined (e.g., customer service, weather agent, product FAQ, forecasting bot, etc.).\n",
        "\n",
        "### âœ… Example Prompt Addition\n",
        "\n",
        "Add this to the top of your tool-picking prompt:\n",
        "\n",
        "```text\n",
        "IMPORTANT: This assistant only handles weather, notes, and FAQs.\n",
        "If a user asks something outside that scope, politely respond:\n",
        "\"I'm here to help with weather updates, notes, or common questions. Could you ask something related to that?\"\n",
        "```\n",
        "\n",
        "Now the LLM might return:\n",
        "> `\"I'm here to help with FAQs, weather, or saving notes â€” can I help with one of those?\"`\n",
        "\n",
        "Instead of:\n",
        "> `\"Answering your question about quantum physics...\"`\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  So to answer your question directly:\n",
        "Yes â€” we **can and should** use high-quality system-level instructions like the one you suggested. Itâ€™s not â€œhardcoding,â€ itâ€™s **soft-guiding** the model toward safe and relevant behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "6woe9zGTeaYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_run(user_input):\n",
        "    # Prompt with available tools and instructions\n",
        "    tool_prompt = f\"\"\"\n",
        "    You are a helpful assistant that can only handle the following tools:\n",
        "\n",
        "    Available tools:\n",
        "    - get_weather: For answering questions about the weather.\n",
        "    - save_note: For saving user notes or reminders.\n",
        "    - search_faq: For answering common questions about return policy, shipping, or support hours.\n",
        "\n",
        "    IMPORTANT RULES:\n",
        "    - If the user request is unrelated to these tools, politely respond with:\n",
        "      \"I'm here to help with weather updates, saving notes, or answering common customer questions. Could you ask something related to that?\"\n",
        "    - Otherwise, return ONLY the name of the tool you will use: get_weather, save_note, or search_faq.\n",
        "    - DO NOT return full sentences or descriptions â€” just the tool name.\n",
        "\n",
        "    User request: {user_input}\n",
        "    Your response:\n",
        "    \"\"\"\n",
        "\n",
        "    tool_response = llm(tool_prompt, max_new_tokens=50)[0]['generated_text'].strip()\n",
        "\n",
        "    # EDGE CASE: Empty or unclear response\n",
        "    # if not tool_response:\n",
        "    #     return \"Hmm, I couldn't understand your request. Could you rephrase it?\"\n",
        "\n",
        "    # Normalize and extract tool name if embedded in a sentence\n",
        "    tool_response_cleaned = tool_response.lower().strip()\n",
        "\n",
        "    # Try to extract a known tool name from the model's response\n",
        "    matched_tool = next((name for name in tools if name in tool_response_cleaned), None)\n",
        "\n",
        "    if not matched_tool:\n",
        "        return tool_response  # Let the model respond naturally if no tool was matched\n",
        "\n",
        "    # Handle polite redirect messages directly\n",
        "    if matched_tool == \"get_weather\":\n",
        "        return tools[\"get_weather\"]()\n",
        "\n",
        "    elif matched_tool == \"save_note\":\n",
        "        note_content = user_input.replace(\"save\", \"\").strip()\n",
        "        return tools[\"save_note\"](note_content)\n",
        "\n",
        "    elif matched_tool == \"search_faq\":\n",
        "        return tools[\"search_faq\"](user_input)\n",
        "\n",
        "    else:\n",
        "        return \"Unexpected error. Something went wrong.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "sy62H5qUe36W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ðŸ”§ 1. **The Problem: The LLM wasn't returning clean tool names**\n",
        "\n",
        "You told it:  \n",
        "> â€œReturn ONLY the tool nameâ€  \n",
        "But it replied:\n",
        "> `\"get_weather: For answering questions about the weather.\"`\n",
        "\n",
        "**Why?**  \n",
        "LLMs like `flan-t5-base` were trained to be helpful and verbose. They often **ignore strict formatting** â€” especially small models â€” even when your prompt begs them not to.\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… 2. **The Fix: We added post-processing logic to catch tool names**\n",
        "\n",
        "```python\n",
        "# Extract tool name from model output (even if it includes extra text)\n",
        "matched_tool = next((name for name in tools if name in tool_response.lower()), None)\n",
        "```\n",
        "\n",
        "This line:\n",
        "- Loops through your tool names: `\"get_weather\"`, `\"save_note\"`, `\"search_faq\"`\n",
        "- Checks if any of them appear inside the model's output string\n",
        "- Selects the **first match it finds**\n",
        "\n",
        "This allows us to **handle fuzzy LLM output gracefully**, even when it's verbose.\n",
        "\n",
        "#### Example:\n",
        "If the model says:\n",
        "```text\n",
        "\"get_weather: For answering questions about the weather.\"\n",
        "```\n",
        "Your logic still says:\n",
        "```python\n",
        "â†’ matched_tool = \"get_weather\"\n",
        "â†’ run tools[\"get_weather\"]()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Why This Matters\n",
        "\n",
        "This is one of the most important agent design patterns:\n",
        "\n",
        "> **LLMs donâ€™t return clean values â€” you interpret their intent.**\n",
        "\n",
        "This is exactly what real agents do in production:\n",
        "- Use patterns to match tool names\n",
        "- Post-process output\n",
        "- Fall back to defaults if nothing matches\n",
        "- Sometimes log the full output for debugging\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’¬ Bonus Fix: Polite redirect\n",
        "\n",
        "When **no valid tool is matched**, you let the model speak freely:\n",
        "\n",
        "```python\n",
        "if not matched_tool:\n",
        "    return tool_response  # Let it respond naturally if no tool matched\n",
        "```\n",
        "\n",
        "This lets the model **return your fallback message**, like:\n",
        "> â€œI'm here to help with weather updates, saving notes, or answering common customer questions.â€\n",
        "\n",
        "You didnâ€™t hardcode that â€” the **model decided to say it**, thanks to your prompt. That's soft guardrails in action.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Bottom Line\n",
        "\n",
        "| What You Did | What You Learned |\n",
        "|--------------|------------------|\n",
        "| Matched fuzzy model output | LLMs = not APIs, theyâ€™re probabilistic |\n",
        "| Soft-guarded off-topic questions | Prompt + polite fallback = graceful UX |\n",
        "| Combined logic + language | Agents live at the boundary of code and conversation |\n",
        "\n"
      ],
      "metadata": {
        "id": "G5wU_jstwiJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… In-scope:\n",
        "print(agent_run(\"What's the weather today?\"))\n",
        "print(agent_run(\"Can you save 'pick up groceries'?\"))\n",
        "print(agent_run(\"Whatâ€™s your return policy?\"))\n",
        "# âŒ Out-of-scope:\n",
        "print(agent_run(\"Can you write me a Python function?\"))\n",
        "print(agent_run(\"Whatâ€™s the meaning of life?\"))\n",
        "print(agent_run(\"Should I break up with my boyfriend?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK6_fAg_fHjs",
        "outputId": "15ce9b08-0726-4dbd-98fe-971263b20edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Gainesville is sunny with a high of 82Â°F.\n",
            "Note saved: Can you  'pick up groceries'?\n",
            "You can return any item within 30 days with a receipt.\n",
            "If the user request is unrelated to these tools, politely respond with: \"I'm here to help with weather updates, saving notes, or answering common customer questions. Could you ask something related to that?\"\n",
            "The weather in Gainesville is sunny with a high of 82Â°F.\n",
            "Sorry, I couldn't find an answer to that question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G1rB_9yfJ4I",
        "outputId": "3d519c0b-57f1-4a8a-b7e2-24a7ea789097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_weather: For answering questions about the weather.\n",
            "get_weather: For answering questions about the weather.\n",
            "get_weather: For answering questions about the weather.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Widgets"
      ],
      "metadata": {
        "id": "npGKwWCEMTcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/AI AGENTS/000_Agent_Design_Foundations.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"âœ… Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"âœ… Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"âœ… Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f9n-poHfOZK",
        "outputId": "7b058a62-cc70-404e-895f-55e3caf5df87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    }
  ]
}