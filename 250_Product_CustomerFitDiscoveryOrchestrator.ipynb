{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJzlgHri4jno4p0ZXqal5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/250_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 0: Planning - Product-Customer Fit Discovery Orchestrator\n",
        "\n",
        "**Date:** 2025-12-04  \n",
        "**Status:** In Progress  \n",
        "**Purpose:** Complete planning before coding begins\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 1. Deep Data Analysis\n",
        "\n",
        "### Data Structure Analysis\n",
        "\n",
        "#### **customers.csv**\n",
        "- **200 customers** (C001-C200)\n",
        "- **Fields:**\n",
        "  - `Customer_ID`: Unique identifier (string)\n",
        "  - `Age_Group`: Categorical (18-24, 35-44, 45-54, 55+)\n",
        "  - `Location_Tier`: Categorical (Tier 1 High, Tier 2 Medium, Tier 3 Low)\n",
        "  - `Acquisition_Channel`: Categorical (Email, Referral, Search, Social)\n",
        "- **Coverage:** 183/200 customers have transactions (17 inactive)\n",
        "- **No null values** ‚úì\n",
        "\n",
        "#### **transactions.csv**\n",
        "- **1,815 transactions** (T0000-T0191)\n",
        "- **Fields:**\n",
        "  - `Transaction_ID`: Unique identifier\n",
        "  - `Customer_ID`: Foreign key to customers.csv\n",
        "  - `Product_ID`: Foreign key to product_catalog.csv\n",
        "  - `Transaction_Date`: Date range 2025-01-01 to 2026-08-24 (~20 months)\n",
        "  - `Usage_Metric`: Numeric (10.61 to 99.98, mean 82.15)\n",
        "- **Distribution:**\n",
        "  - Highly skewed: Median 1 transaction, Max 185 transactions\n",
        "  - Mean 9.92 transactions per customer\n",
        "- **Product Usage:**\n",
        "  - P01: 890 transactions (49%)\n",
        "  - P05: 747 transactions (41%)\n",
        "  - Others: 10-20 transactions each\n",
        "  - P20: 0 transactions (unused product)\n",
        "- **No null values** ‚úì\n",
        "\n",
        "#### **product_catalog.csv**\n",
        "- **20 products** (P01-P20)\n",
        "- **Fields:**\n",
        "  - `Product_ID`: Unique identifier\n",
        "  - `Product_Type`: Categorical (Hardware, Software, Service)\n",
        "  - `Feature_Set`: **Comma-separated string** (e.g., \"B, A\", \"A, B, C\") ‚ö†Ô∏è NEEDS PARSING\n",
        "  - `Monetization_Model`: Categorical (One-Time Purchase, Freemium, Subscription)\n",
        "- **Feature Sets:** A, B, C, D (some products have multiple)\n",
        "- **P20 unused** - decision needed: include or exclude\n",
        "\n",
        "### Data Quality Issues Identified\n",
        "\n",
        "1. **Feature_Set Format:** Comma-separated strings need parsing into lists\n",
        "2. **P20 Unused Product:** No transactions - decide inclusion strategy\n",
        "3. **Transaction Skew:** Heavy concentration in P01/P05 may affect pattern detection\n",
        "4. **Sparse Products:** Most products have <20 transactions - may need special handling\n",
        "\n",
        "### Data Preprocessing Requirements\n",
        "\n",
        "1. **Parse Feature_Set:** Convert \"B, A\" ‚Üí [\"B\", \"A\"]\n",
        "2. **Handle P20:** Decision: Include but flag as \"unused\" for analysis\n",
        "3. **Create Derived Features:**\n",
        "   - Customer engagement score (based on transaction frequency)\n",
        "   - Product popularity score\n",
        "   - Usage intensity tiers (high/medium/low based on Usage_Metric)\n",
        "4. **Normalize Data:**\n",
        "   - One-hot encode categoricals (Age_Group, Location_Tier, etc.)\n",
        "   - Normalize Usage_Metric for clustering\n",
        "5. **Build Graph Structures:**\n",
        "   - Customer-Product bipartite graph\n",
        "   - Product co-occurrence graph\n",
        "   - Customer similarity graph\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 2. Decision Rule Analysis\n",
        "\n",
        "### Core Discovery Rules\n",
        "\n",
        "#### **Clustering Agent Rules:**\n",
        "1. **Customer Segmentation:**\n",
        "   - Cluster by: Demographics (Age, Location) + Behavior (Usage patterns, Product mix)\n",
        "   - Algorithm: K-means or DBSCAN (start with K-means, MVP)\n",
        "   - Number of clusters: Determine via elbow method (start with 3-5)\n",
        "   - Output: Customer segments with characteristics\n",
        "\n",
        "2. **Product Clustering:**\n",
        "   - Cluster by: Feature sets, Monetization model, Usage patterns\n",
        "   - Purpose: Identify natural product bundles\n",
        "   - Algorithm: K-means on feature vectors\n",
        "\n",
        "#### **Pattern Mining Agent Rules:**\n",
        "1. **Association Rules:**\n",
        "   - Find frequent product combinations\n",
        "   - Minimum support: 5% (adjust based on data)\n",
        "   - Minimum confidence: 30%\n",
        "   - Output: Rules like \"P01 ‚Üí P05\" (customers with P01 often have P05)\n",
        "\n",
        "2. **Sequential Patterns:**\n",
        "   - Find purchase sequences (if temporal data supports)\n",
        "   - Minimum sequence length: 2\n",
        "   - Output: Common purchase paths\n",
        "\n",
        "#### **Graph Motif Agent Rules:**\n",
        "1. **Motif Detection:**\n",
        "   - Find recurring sub-graph patterns\n",
        "   - Focus on: 3-node motifs (triangles, chains)\n",
        "   - Significance threshold: Z-score > 2.0\n",
        "   - Output: Significant relationship patterns\n",
        "\n",
        "2. **Centrality Analysis:**\n",
        "   - Identify hub products (high degree centrality)\n",
        "   - Identify bridge customers (high betweenness)\n",
        "   - Output: Key nodes in network\n",
        "\n",
        "#### **Synthesis Agent Rules:**\n",
        "1. **Opportunity Scoring:**\n",
        "   - Combine insights from all agents\n",
        "   - Score by: Business value, Market gap size, Implementation feasibility\n",
        "   - Output: Ranked opportunities\n",
        "\n",
        "2. **Insight Validation:**\n",
        "   - Cross-validate findings across agents\n",
        "   - Flag high-confidence insights\n",
        "   - Output: Validated strategic recommendations\n",
        "\n",
        "### Rule Dependencies\n",
        "\n",
        "```\n",
        "Data Preprocessing\n",
        "    ‚Üì\n",
        "Clustering Agent (independent)\n",
        "    ‚Üì\n",
        "Pattern Mining Agent (can use clustering results)\n",
        "    ‚Üì\n",
        "Graph Motif Agent (can use pattern mining results)\n",
        "    ‚Üì\n",
        "Synthesis Agent (combines all results)\n",
        "```\n",
        "\n",
        "**Decision:** Sequential execution (simpler for MVP), can parallelize later\n",
        "\n",
        "---\n",
        "\n",
        "## üìê 3. State Schema Design\n",
        "\n",
        "### Complete State Schema\n",
        "\n",
        "See `config.py` for full `ProductCustomerFitState` definition.\n",
        "\n",
        "**Key Sections:**\n",
        "1. **Input Fields:** Data file paths, analysis parameters\n",
        "2. **Goal & Planning:** Fixed goal, execution plan\n",
        "3. **Data Ingestion:** Raw and preprocessed data\n",
        "4. **Clustering Results:** Customer and product segments\n",
        "5. **Pattern Mining Results:** Association rules, sequences\n",
        "6. **Graph Analysis Results:** Motifs, centrality metrics\n",
        "7. **Synthesized Insights:** Combined opportunities\n",
        "8. **Output:** Final report and file paths\n",
        "9. **Metadata:** Errors, processing time\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è 4. Architecture Planning\n",
        "\n",
        "### Node Structure (One Responsibility Each)\n",
        "\n",
        "1. **goal_node:** Define discovery objective\n",
        "2. **planning_node:** Create execution plan\n",
        "3. **data_ingestion_node:** Load raw CSV files\n",
        "4. **data_preprocessing_node:** Parse, normalize, build graphs\n",
        "5. **clustering_agent_node:** Run customer/product clustering\n",
        "6. **pattern_mining_agent_node:** Find association rules\n",
        "7. **graph_motif_agent_node:** Detect network patterns\n",
        "8. **synthesis_agent_node:** Combine insights\n",
        "9. **report_generation_node:** Generate final report\n",
        "\n",
        "### Utility Structure (Reusable Business Logic)\n",
        "\n",
        "#### **tools/data_preprocessing.py**\n",
        "- `load_customers_csv()` ‚Üí Load customers.csv\n",
        "- `load_transactions_csv()` ‚Üí Load transactions.csv\n",
        "- `load_product_catalog_csv()` ‚Üí Load product_catalog.csv\n",
        "- `parse_feature_set()` ‚Üí Parse comma-separated features\n",
        "- `normalize_usage_metrics()` ‚Üí Normalize for clustering\n",
        "- `build_customer_product_graph()` ‚Üí Create NetworkX graph\n",
        "- `create_derived_features()` ‚Üí Engagement scores, etc.\n",
        "\n",
        "#### **tools/clustering.py**\n",
        "- `cluster_customers()` ‚Üí K-means on customer features\n",
        "- `cluster_products()` ‚Üí K-means on product features\n",
        "- `analyze_cluster_characteristics()` ‚Üí Describe segments\n",
        "- `find_underserved_segments()` ‚Üí Identify gaps\n",
        "\n",
        "#### **tools/pattern_mining.py**\n",
        "- `find_association_rules()` ‚Üí Apriori algorithm\n",
        "- `find_sequential_patterns()` ‚Üí Sequential pattern mining\n",
        "- `score_pattern_significance()` ‚Üí Statistical significance\n",
        "\n",
        "#### **tools/graph_analysis.py**\n",
        "- `detect_motifs()` ‚Üí Find recurring sub-graphs\n",
        "- `calculate_centrality()` ‚Üí Degree, betweenness, etc.\n",
        "- `find_relationship_patterns()` ‚Üí Significant connections\n",
        "\n",
        "#### **tools/synthesis.py**\n",
        "- `combine_insights()` ‚Üí Merge all agent results\n",
        "- `score_opportunities()` ‚Üí Business value scoring\n",
        "- `validate_insights()` ‚Üí Cross-agent validation\n",
        "- `rank_opportunities()` ‚Üí Final ranking\n",
        "\n",
        "#### **tools/report_generation.py**\n",
        "- `generate_discovery_report()` ‚Üí Markdown report\n",
        "- `save_report()` ‚Üí File I/O\n",
        "\n",
        "### Error Handling Strategy\n",
        "\n",
        "1. **Data Validation:** Check file existence, schema validation\n",
        "2. **Algorithm Failures:** Graceful degradation (e.g., if clustering fails, continue with other agents)\n",
        "3. **Empty Results:** Handle gracefully (e.g., \"No significant patterns found\")\n",
        "4. **Error Accumulation:** Collect errors in state, continue processing\n",
        "\n",
        "### Testing Strategy\n",
        "\n",
        "1. **Unit Tests:** Test each utility independently\n",
        "2. **Integration Tests:** Test nodes with real data\n",
        "3. **End-to-End Test:** Full workflow with sample data\n",
        "4. **Edge Cases:** Empty data, single customer, single product\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Deliverables Checklist\n",
        "\n",
        "### Phase 0: Planning\n",
        "- [x] Deep data analysis complete\n",
        "- [x] Decision rule analysis complete\n",
        "- [ ] **State schema design complete** ‚Üê IN PROGRESS\n",
        "- [ ] Architecture planning complete\n",
        "- [ ] Error handling strategy defined\n",
        "- [ ] Testing strategy defined\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. **Complete State Schema** in `config.py`\n",
        "2. **Create Config Class** in `config.py`\n",
        "3. **Begin Phase 1:** Goal & Planning Nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "gmdCrAUP5EXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product-Customer Fit Discovery Orchestrator Agent"
      ],
      "metadata": {
        "id": "nRvUuXlo5jL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm3jeqWE1SwY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Product-Customer Fit Discovery Orchestrator Agent\n",
        "# ============================================================================\n",
        "\n",
        "class ProductCustomerFitState(TypedDict, total=False):\n",
        "    \"\"\"State for Product-Customer Fit Discovery Orchestrator Agent\"\"\"\n",
        "\n",
        "    # Input fields\n",
        "    data_dir: Optional[str]                 # Directory containing data files (default: \"data/\")\n",
        "    customers_file: Optional[str]            # Path to customers.csv (default: \"data/customers.csv\")\n",
        "    transactions_file: Optional[str]         # Path to transactions.csv (default: \"data/transactions.csv\")\n",
        "    products_file: Optional[str]            # Path to product_catalog.csv (default: \"data/product_catalog.csv\")\n",
        "\n",
        "    # Analysis Configuration\n",
        "    include_unused_products: bool           # Whether to include products with no transactions (default: True)\n",
        "    clustering_algorithm: str               # \"kmeans\" or \"dbscan\" (default: \"kmeans\")\n",
        "    num_customer_clusters: Optional[int]     # Number of customer clusters (None = auto-determine)\n",
        "    num_product_clusters: Optional[int]     # Number of product clusters (None = auto-determine)\n",
        "    min_support: float                      # Minimum support for association rules (default: 0.05)\n",
        "    min_confidence: float                   # Minimum confidence for association rules (default: 0.30)\n",
        "    motif_significance_threshold: float     # Z-score threshold for motif significance (default: 2.0)\n",
        "\n",
        "    # Goal & Planning fields (MVP: Fixed goal, template-based plan)\n",
        "    goal: Dict[str, Any]                    # Goal definition (from goal_node)\n",
        "    plan: List[Dict[str, Any]]              # Execution plan (from planning_node)\n",
        "\n",
        "    # Data Ingestion\n",
        "    raw_customers: List[Dict[str, Any]]      # Raw customer data from CSV\n",
        "    raw_transactions: List[Dict[str, Any]]  # Raw transaction data from CSV\n",
        "    raw_products: List[Dict[str, Any]]      # Raw product data from CSV\n",
        "\n",
        "    # Data Preprocessing\n",
        "    preprocessed_data: Dict[str, Any]       # Preprocessed and normalized data\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"customers\": List[Dict[str, Any]],  # Customers with parsed features\n",
        "    #   \"transactions\": List[Dict[str, Any]],  # Transactions with normalized metrics\n",
        "    #   \"products\": List[Dict[str, Any]],  # Products with parsed feature sets\n",
        "    #   \"customer_product_graph\": Any,  # NetworkX graph object\n",
        "    #   \"product_cooccurrence_graph\": Any,  # NetworkX graph object\n",
        "    #   \"customer_similarity_graph\": Any,  # NetworkX graph object\n",
        "    #   \"feature_matrix\": Any,  # NumPy array for clustering\n",
        "    #   \"data_quality_report\": Dict[str, Any]\n",
        "    # }\n",
        "\n",
        "    # Clustering Agent Results\n",
        "    customer_clusters: List[Dict[str, Any]]  # Customer segmentation results\n",
        "    # Structure per cluster:\n",
        "    # {\n",
        "    #   \"cluster_id\": int,\n",
        "    #   \"cluster_label\": str,  # e.g., \"High-Value Tech Enthusiasts\"\n",
        "    #   \"customer_ids\": List[str],\n",
        "    #   \"size\": int,\n",
        "    #   \"characteristics\": {\n",
        "    #     \"avg_age_group\": str,\n",
        "    #     \"common_location_tiers\": List[str],\n",
        "    #     \"common_acquisition_channels\": List[str],\n",
        "    #     \"avg_usage_metric\": float,\n",
        "    #     \"top_products\": List[str],\n",
        "    #     \"product_diversity\": float\n",
        "    #   },\n",
        "    #   \"underserved_products\": List[str],  # Products this segment doesn't use\n",
        "    #   \"business_value\": float  # Estimated value of this segment\n",
        "    # }\n",
        "\n",
        "    product_clusters: List[Dict[str, Any]]   # Product bundling results\n",
        "    # Structure per cluster:\n",
        "    # {\n",
        "    #   \"cluster_id\": int,\n",
        "    #   \"cluster_label\": str,  # e.g., \"Enterprise Software Suite\"\n",
        "    #   \"product_ids\": List[str],\n",
        "    #   \"size\": int,\n",
        "    #   \"characteristics\": {\n",
        "    #     \"common_features\": List[str],\n",
        "    #     \"monetization_models\": List[str],\n",
        "    #     \"product_types\": List[str],\n",
        "    #     \"avg_usage_metric\": float\n",
        "    #   },\n",
        "    #   \"bundle_potential\": float  # Likelihood these products are bundled\n",
        "    # }\n",
        "\n",
        "    clustering_summary: Dict[str, Any]      # Clustering analysis summary\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"num_customer_clusters\": int,\n",
        "    #   \"num_product_clusters\": int,\n",
        "    #   \"cluster_quality_metrics\": Dict[str, float],  # Silhouette score, etc.\n",
        "    #   \"underserved_segments\": List[str],  # Segments with unmet needs\n",
        "    #   \"natural_bundles\": List[str]  # Product bundles identified\n",
        "    # }\n",
        "\n",
        "    # Pattern Mining Agent Results\n",
        "    association_rules: List[Dict[str, Any]]  # Product association rules\n",
        "    # Structure per rule:\n",
        "    # {\n",
        "    #   \"antecedent\": List[str],  # Products in \"if\" part (e.g., [\"P01\"])\n",
        "    #   \"consequent\": List[str],   # Products in \"then\" part (e.g., [\"P05\"])\n",
        "    #   \"support\": float,          # Frequency of rule (0-1)\n",
        "    #   \"confidence\": float,      # Probability of consequent given antecedent (0-1)\n",
        "    #   \"lift\": float,            # Strength of association (>1 = positive)\n",
        "    #   \"business_value\": float,  # Estimated revenue impact\n",
        "    #   \"rule_type\": str  # \"cross_sell\", \"upsell\", \"bundle\"\n",
        "    # }\n",
        "\n",
        "    sequential_patterns: List[Dict[str, Any]]  # Purchase sequence patterns\n",
        "    # Structure per pattern:\n",
        "    # {\n",
        "    #   \"sequence\": List[str],  # Ordered product IDs (e.g., [\"P01\", \"P05\", \"P12\"])\n",
        "    #   \"frequency\": int,       # How often this sequence occurs\n",
        "    #   \"avg_time_between\": float,  # Average days between steps\n",
        "    #   \"customer_count\": int,  # Number of customers following this path\n",
        "    #   \"completion_rate\": float,  # % who complete full sequence\n",
        "    #   \"value_path\": float    # Average revenue of customers on this path\n",
        "    # }\n",
        "\n",
        "    pattern_mining_summary: Dict[str, Any]   # Pattern mining analysis summary\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"total_rules\": int,\n",
        "    #   \"high_confidence_rules\": int,\n",
        "    #   \"total_sequences\": int,\n",
        "    #   \"most_common_sequence\": List[str],\n",
        "    #   \"top_cross_sell_opportunities\": List[str],\n",
        "    #   \"top_bundle_opportunities\": List[str]\n",
        "    # }\n",
        "\n",
        "    # Graph Motif Agent Results\n",
        "    graph_motifs: List[Dict[str, Any]]      # Significant network motifs\n",
        "    # Structure per motif:\n",
        "    # {\n",
        "    #   \"motif_type\": str,  # \"triangle\", \"chain\", \"star\", etc.\n",
        "    #   \"nodes\": List[str],  # Customer/Product IDs in motif\n",
        "    #   \"frequency\": int,    # How often this motif appears\n",
        "    #   \"expected_frequency\": float,  # Expected in random graph\n",
        "    #   \"z_score\": float,   # Statistical significance\n",
        "    #   \"significance\": str,  # \"high\", \"medium\", \"low\"\n",
        "    #   \"business_insight\": str  # What this pattern means\n",
        "    # }\n",
        "\n",
        "    centrality_metrics: Dict[str, Any]      # Network centrality analysis\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"hub_products\": List[Dict[str, Any]],  # Products with high degree centrality\n",
        "    #   # [{\"product_id\": \"P01\", \"centrality_score\": 0.85, \"role\": \"hub\"}]\n",
        "    #   \"bridge_customers\": List[Dict[str, Any]],  # Customers with high betweenness\n",
        "    #   # [{\"customer_id\": \"C025\", \"centrality_score\": 0.72, \"role\": \"bridge\"}]\n",
        "    #   \"influencer_products\": List[Dict[str, Any]],  # Products that drive others\n",
        "    #   \"isolated_products\": List[str]  # Products with low connectivity\n",
        "    # }\n",
        "\n",
        "    graph_analysis_summary: Dict[str, Any]   # Graph analysis summary\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"total_nodes\": int,\n",
        "    #   \"total_edges\": int,\n",
        "    #   \"graph_density\": float,\n",
        "    #   \"num_motifs_found\": int,\n",
        "    #   \"significant_motifs\": int,\n",
        "    #   \"network_clusters\": int,  # Community detection\n",
        "    #   \"key_insights\": List[str]\n",
        "    # }\n",
        "\n",
        "    # Synthesis Agent Results\n",
        "    synthesized_insights: List[Dict[str, Any]]  # Combined insights from all agents\n",
        "    # Structure per insight:\n",
        "    # {\n",
        "    #   \"insight_id\": str,\n",
        "    #   \"insight_type\": str,  # \"product_gap\", \"customer_segment\", \"bundle_opportunity\", \"market_gap\"\n",
        "    #   \"title\": str,  # e.g., \"Untapped Market: Young Professionals in Tier 2\"\n",
        "    #   \"description\": str,  # Detailed description\n",
        "    #   \"confidence\": float,  # 0-1, based on cross-agent validation\n",
        "    #   \"business_value\": float,  # Estimated revenue impact\n",
        "    #   \"evidence\": {\n",
        "    #     \"from_clustering\": List[str],  # Supporting evidence from clustering\n",
        "    #     \"from_patterns\": List[str],    # Supporting evidence from pattern mining\n",
        "    #     \"from_graph\": List[str]         # Supporting evidence from graph analysis\n",
        "    #   },\n",
        "    #   \"recommended_actions\": List[str],  # Business actions to take\n",
        "    #   \"implementation_feasibility\": str  # \"high\", \"medium\", \"low\"\n",
        "    # }\n",
        "\n",
        "    opportunity_ranking: List[Dict[str, Any]]  # Ranked opportunities\n",
        "    # Same structure as synthesized_insights, but sorted by business_value * confidence\n",
        "\n",
        "    top_opportunities: List[Dict[str, Any]]     # Top N opportunities (configurable)\n",
        "\n",
        "    synthesis_summary: Dict[str, Any]          # Synthesis analysis summary\n",
        "    # Structure:\n",
        "    # {\n",
        "    #   \"total_insights\": int,\n",
        "    #   \"high_confidence_insights\": int,\n",
        "    #   \"total_potential_value\": float,\n",
        "    #   \"insights_by_type\": Dict[str, int],\n",
        "    #   \"cross_validated_insights\": int,\n",
        "    #   \"top_opportunity_types\": List[str]\n",
        "    # }\n",
        "\n",
        "    # Output\n",
        "    discovery_report: str                      # Final markdown report\n",
        "    report_file_path: Optional[str]           # Path to saved report file\n",
        "\n",
        "    # Metadata\n",
        "    errors: List[str]                         # Any errors encountered\n",
        "    processing_time: Optional[float]          # Time taken to process\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProductCustomerFitConfig:\n",
        "    \"\"\"Configuration for Product-Customer Fit Discovery Orchestrator Agent\"\"\"\n",
        "    llm_model: str = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
        "    temperature: float = 0.3\n",
        "    reports_dir: str = \"output/product_customer_fit_reports\"  # Where to save reports\n",
        "\n",
        "    # Data Configuration\n",
        "    data_dir: str = \"data\"\n",
        "    customers_file: str = \"data/customers.csv\"\n",
        "    transactions_file: str = \"data/transactions.csv\"\n",
        "    products_file: str = \"data/product_catalog.csv\"\n",
        "    include_unused_products: bool = True  # Include products with no transactions\n",
        "\n",
        "    # Clustering Configuration\n",
        "    clustering_algorithm: str = \"kmeans\"  # \"kmeans\" or \"dbscan\"\n",
        "    num_customer_clusters: Optional[int] = None  # None = auto-determine via elbow method\n",
        "    num_product_clusters: Optional[int] = None   # None = auto-determine\n",
        "    max_clusters: int = 10  # Maximum clusters to consider\n",
        "    min_cluster_size: int = 5  # Minimum customers/products per cluster\n",
        "\n",
        "    # Pattern Mining Configuration\n",
        "    min_support: float = 0.05  # Minimum support for association rules (5%)\n",
        "    min_confidence: float = 0.30  # Minimum confidence for association rules (30%)\n",
        "    max_rule_length: int = 3  # Maximum items in association rule\n",
        "    min_sequence_length: int = 2  # Minimum length for sequential patterns\n",
        "\n",
        "    # Graph Analysis Configuration\n",
        "    motif_significance_threshold: float = 2.0  # Z-score threshold for significance\n",
        "    min_motif_frequency: int = 3  # Minimum occurrences to consider\n",
        "    centrality_top_n: int = 10  # Top N products/customers by centrality\n",
        "\n",
        "    # Synthesis Configuration\n",
        "    top_n_opportunities: int = 10  # Number of top opportunities to highlight\n",
        "    min_confidence_threshold: float = 0.6  # Minimum confidence for top opportunities\n",
        "    cross_validation_required: bool = True  # Require evidence from multiple agents\n",
        "\n",
        "    # LLM Enhancement (Phase 8 - Optional)\n",
        "    enable_llm_insights: bool = False  # Enable LLM-enhanced insight descriptions\n",
        "    llm_insight_max_opportunities: int = 5  # Max opportunities to enhance (cost control)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product-Customer Fit Discovery Orchestrator - Build Progress\n",
        "\n",
        "**Started:** 2025-12-04  \n",
        "**Status:** Phase 0 Complete ‚Üí Starting Phase 1\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Phase 0: Planning - COMPLETE\n",
        "\n",
        "### Completed Tasks\n",
        "\n",
        "1. **Deep Data Analysis** ‚úì\n",
        "   - Analyzed all 3 CSV files (customers, transactions, products)\n",
        "   - Identified data quality issues (Feature_Set parsing, P20 unused product)\n",
        "   - Documented data distribution and skew patterns\n",
        "   - Defined preprocessing requirements\n",
        "\n",
        "2. **Decision Rule Analysis** ‚úì\n",
        "   - Mapped clustering agent rules (customer/product segmentation)\n",
        "   - Defined pattern mining rules (association rules, sequences)\n",
        "   - Specified graph motif detection rules\n",
        "   - Created synthesis agent scoring rules\n",
        "   - Documented rule dependencies (sequential execution for MVP)\n",
        "\n",
        "3. **State Schema Design** ‚úì\n",
        "   - Created complete `ProductCustomerFitState` TypedDict\n",
        "   - Designed progressive state enrichment pattern\n",
        "   - Documented all field structures with examples\n",
        "   - Added comprehensive configuration class `ProductCustomerFitConfig`\n",
        "\n",
        "4. **Architecture Planning** ‚úì\n",
        "   - Planned 9-node workflow structure\n",
        "   - Designed utility modules (data_preprocessing, clustering, pattern_mining, graph_analysis, synthesis, report_generation)\n",
        "   - Defined error handling strategy\n",
        "   - Created testing strategy\n",
        "\n",
        "### Key Decisions Made\n",
        "\n",
        "- **Sequential Agent Execution:** Start with sequential (simpler for MVP), can parallelize later\n",
        "- **Include P20:** Include unused product but flag for analysis\n",
        "- **Clustering Algorithm:** Start with K-means (simpler), can add DBSCAN later\n",
        "- **Graph Library:** Use NetworkX (simpler, no external dependencies for MVP)\n",
        "- **MVP First:** Rule-based analysis first, LLM enhancement in Phase 8\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Phase 1: Foundation - IN PROGRESS\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Build Goal Node** (simplest, no dependencies)\n",
        "   - Define discovery objective\n",
        "   - Set analysis parameters\n",
        "   - Test independently\n",
        "\n",
        "2. **Build Planning Node**\n",
        "   - Create execution plan\n",
        "   - Map workflow steps\n",
        "   - Test with goal node\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Architecture Overview\n",
        "\n",
        "### Workflow Structure\n",
        "\n",
        "```\n",
        "Goal ‚Üí Planning ‚Üí Data Ingestion ‚Üí Data Preprocessing ‚Üí\n",
        "Clustering Agent ‚Üí Pattern Mining Agent ‚Üí Graph Motif Agent ‚Üí\n",
        "Synthesis Agent ‚Üí Report Generation\n",
        "```\n",
        "\n",
        "### State Enrichment Flow\n",
        "\n",
        "```\n",
        "Initial State (input paths)\n",
        "    ‚Üì\n",
        "Goal & Planning (objective, plan)\n",
        "    ‚Üì\n",
        "Raw Data (CSV files loaded)\n",
        "    ‚Üì\n",
        "Preprocessed Data (parsed, normalized, graphs built)\n",
        "    ‚Üì\n",
        "Clustering Results (customer/product segments)\n",
        "    ‚Üì\n",
        "Pattern Mining Results (association rules, sequences)\n",
        "    ‚Üì\n",
        "Graph Analysis Results (motifs, centrality)\n",
        "    ‚Üì\n",
        "Synthesized Insights (combined opportunities)\n",
        "    ‚Üì\n",
        "Final Report (markdown output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìÅ File Structure\n",
        "\n",
        "```\n",
        "agents/\n",
        "  product_customer_fit/\n",
        "    nodes.py              # All workflow nodes\n",
        "    orchestrator.py       # LangGraph workflow definition\n",
        "\n",
        "tools/\n",
        "  data_preprocessing.py  # CSV loading, parsing, normalization\n",
        "  clustering.py          # K-means clustering utilities\n",
        "  pattern_mining.py      # Association rules, sequences\n",
        "  graph_analysis.py      # NetworkX graph operations\n",
        "  synthesis.py           # Insight combination, scoring\n",
        "  report_generation.py   # Markdown report creation\n",
        "\n",
        "tests/\n",
        "  test_data_preprocessing.py\n",
        "  test_clustering.py\n",
        "  test_pattern_mining.py\n",
        "  test_graph_analysis.py\n",
        "  test_synthesis.py\n",
        "  test_nodes.py\n",
        "  test_orchestrator.py\n",
        "\n",
        "config.py                # State schema & config (‚úì COMPLETE)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Success Criteria\n",
        "\n",
        "### MVP (Phase 1-7)\n",
        "- [ ] All nodes working end-to-end\n",
        "- [ ] Produces valid discovery report\n",
        "- [ ] Identifies at least 3 customer segments\n",
        "- [ ] Finds at least 5 association rules\n",
        "- [ ] Detects at least 2 significant graph motifs\n",
        "- [ ] Synthesizes at least 3 business opportunities\n",
        "- [ ] All tests passing\n",
        "\n",
        "### Enhanced (Phase 8)\n",
        "- [ ] LLM-enhanced insight descriptions\n",
        "- [ ] Improved opportunity ranking\n",
        "- [ ] Natural language explanations\n",
        "\n",
        "---\n",
        "\n",
        "*Last Updated: Phase 0 Complete*\n",
        "\n"
      ],
      "metadata": {
        "id": "6yl_YuxV5zFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleanup Summary\n",
        "\n",
        "**Date:** 2025-12-04  \n",
        "**Purpose:** Simplify data preprocessing to focus on agent architecture\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Changes Made\n",
        "\n",
        "### 1. **product_catalog.csv** - Feature_Set Normalization\n",
        "- **Before:** Inconsistent ordering (e.g., \"B, A\", \"D, C, A\")\n",
        "- **After:** Alphabetically sorted for consistency (e.g., \"A, B\", \"A, C, D\")\n",
        "- **Impact:** Simpler parsing - can use `sorted(features.split(\", \"))` consistently\n",
        "\n",
        "### 2. **transactions.csv** - Added P20 Transactions\n",
        "- **Before:** P20 had 0 transactions (completely unused)\n",
        "- **After:** Added 9 P20 transactions across different customers and dates\n",
        "- **Impact:** P20 is now usable for analysis, no special edge case handling needed\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Updated Data Statistics\n",
        "\n",
        "- **Total Transactions:** 1,824 (was 1,815, added 9 for P20)\n",
        "- **Products with Transactions:** 20/20 (100% coverage)\n",
        "- **Feature_Set Format:** Consistent alphabetical ordering\n",
        "- **Data Quality:** All foreign keys valid, no nulls\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Simplified Preprocessing\n",
        "\n",
        "With cleaned data, preprocessing utilities can be much simpler:\n",
        "\n",
        "```python\n",
        "# Simple Feature_Set parsing (no edge cases needed)\n",
        "def parse_feature_set(feature_string: str) -> List[str]:\n",
        "    \"\"\"Parse comma-separated feature set\"\"\"\n",
        "    return sorted([f.strip() for f in feature_string.split(\",\")])\n",
        "\n",
        "# No special handling for unused products\n",
        "# All products have transactions\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Benefits\n",
        "\n",
        "1. **Focus on Architecture:** Can focus on orchestrator patterns, not data cleaning\n",
        "2. **Simpler Utilities:** Preprocessing code is straightforward\n",
        "3. **Better Learning:** Understand multi-agent coordination without data complexity\n",
        "4. **Faster Development:** Less time debugging data issues\n",
        "\n",
        "---\n",
        "\n",
        "*Data is now ready for agent development!*\n",
        "\n"
      ],
      "metadata": {
        "id": "FacJhxdv6mnb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fZ7iwlF53jf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}