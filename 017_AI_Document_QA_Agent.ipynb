{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNruyNWJ2CpRotNF1wX1V2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/017_AI_Document_QA_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß† AI Document QA Agent with Automated Evaluation\n",
        "\n",
        "### üìò Overview\n",
        "\n",
        "This notebook implements a **domain-specific AI assistant** that answers questions based on a structured PDF document ‚Äî in this case, **\"AI in the Enterprise\"** by OpenAI. The assistant is supported by an **automated evaluator** that ensures the agent‚Äôs responses are high-quality, accurate, and grounded in the source material.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Key Capabilities\n",
        "\n",
        "* **Document Extraction & Preprocessing**\n",
        "  Converts the PDF into clean, readable text using `pdfplumber`, preserving meaningful structure for language modeling.\n",
        "\n",
        "* **Domain-Aware System Prompt**\n",
        "  Constructs a dynamic system prompt that guides the assistant to act as an expert representative of the report, targeting business and technical users.\n",
        "\n",
        "* **Interactive QA Agent**\n",
        "  Uses OpenAI's `gpt-4o-mini` to answer user questions grounded in the document‚Äôs content.\n",
        "\n",
        "* **Pydantic-Based Evaluator**\n",
        "  A custom `Evaluation` class ensures each response is:\n",
        "\n",
        "  * On-topic\n",
        "  * Factually correct\n",
        "  * Professionally worded\n",
        "  * Useful to enterprise audiences\n",
        "\n",
        "* **Self-Evaluating Agent Loop**\n",
        "  If a response fails evaluation, the agent reruns the answer using evaluator feedback until a high-quality response is returned.\n",
        "\n",
        "* **Formatted Output & Readability Enhancements**\n",
        "  Responses are neatly wrapped for readability using `textwrap`, making them easy to print, display, or share.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Technologies Used\n",
        "\n",
        "* `pdfplumber` ‚Äì PDF parsing and cleanup\n",
        "* `openai` ‚Äì LLM interaction (chat completions)\n",
        "* `pydantic` ‚Äì Type validation for response evaluation\n",
        "* `textwrap` ‚Äì Clean and legible console output\n",
        "* `gradio` (optional) ‚Äì For web-based UI integration\n",
        "\n",
        "---\n",
        "\n",
        "### üåç Use Cases\n",
        "\n",
        "* Document-based Q\\&A for business reports, policies, handbooks\n",
        "* AI assistant trained on internal knowledge bases\n",
        "* Auto-evaluating chatbots for enterprise content\n",
        "* Education tools for interactive reading comprehension\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_tTBU85Qy7PQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uVN6rBYlFF0l"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pypdf import PdfReader\n",
        "import gradio as gr\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Grab API key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ùå OPENAI_API_KEY not found in environment. Make sure your .env file is loaded correctly.\")\n",
        "\n",
        "# Set up OpenAI client\n",
        "openai = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "mANKBrseFNZh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Reader"
      ],
      "metadata": {
        "id": "jTQUcOZiI2i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PdfReader(\"/content/ai-in-the-enterprise.pdf\")\n",
        "\n",
        "ai_in_the_enterprise = \"\"\n",
        "for page in reader.pages:\n",
        "  text = page.extract_text()\n",
        "  if text:\n",
        "    ai_in_the_enterprise += text\n",
        "\n",
        "print(ai_in_the_enterprise[0:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCEQKI78GDrD",
        "outputId": "1379afaf-1b2f-4025-9f0a-bc68853c23fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A I  i n  t h e  \n",
            "E n t e r p r i s e\n",
            "L essons fr om se v en fr on tier  companiesC o n t e n t s\n",
            "A  ne w  w a y  t o w ork 3\n",
            "Ex ecutiv e summary 5\n",
            "Se v en lessons f or  en t erprise AI adop tion\n",
            "Start with e v als 6\n",
            "E mbed AI in t o y our  pr oduc ts 9\n",
            "Start no w  and in v est early 11\n",
            "Cust omiz e and fine- tune y our  models 13\n",
            "Ge t AI in the hands o f  e xperts 16\n",
            "U nblock  y our  de v eloper s 18\n",
            "Se t bold aut oma tion goals 21\n",
            "Conclusion 22\n",
            "M or e r esour ces 2 4\n",
            "2 A I  i n  t h e  E n t e r p r i s eA  n e w  w a y  ‚Ä®\n",
            "t o  w o r k\n",
            "A s an AI r esear ch and deplo ymen t compan y ,  OpenAI prioritiz es partnering with global companies \n",
            "because our  models will incr easingly  do their  best w ork  with sophistica t ed,  comple x,  \n",
            "in t er connec t ed w orkflo w s and s y st ems.\n",
            "W e ‚Äô r e seeing AI deliv er  significan t,  measur able impr o v emen ts on thr ee fr on ts:\n",
            "01 W o r k f o r c e  p e r f o r m a n c e H elping people deliv er  higher -quality  outputs in short er  ‚Ä®\n",
            "tim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean with Regex Test 1"
      ],
      "metadata": {
        "id": "WMBnCr-FJH96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_pdf_text(raw_text):\n",
        "    # Replace multiple newlines and spaces with a single space\n",
        "    text = re.sub(r'\\n+', ' ', raw_text)       # Collapse newlines\n",
        "    text = re.sub(r' {2,}', ' ', text)         # Collapse multiple spaces\n",
        "    text = re.sub(r'-\\s+', '', text)           # Join hyphenated words split across lines\n",
        "    text = re.sub(r'\\s+', ' ', text)           # Normalize all whitespace to single spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Clean the PDF content\n",
        "cleaned_text = clean_pdf_text(ai_in_the_enterprise)\n",
        "\n",
        "# Optional: preview the result\n",
        "print(cleaned_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2YBQzyiG5Uj",
        "outputId": "061bce46-f432-4f80-e4ac-8a7a8b2cdae4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A I i n t h e E n t e r p r i s e L essons fr om se v en fr on tier companiesC o n t e n t s A ne w w a y t o w ork 3 Ex ecutiv e summary 5 Se v en lessons f or en t erprise AI adop tion Start with e v als 6 E mbed AI in t o y our pr oduc ts 9 Start no w and in v est early 11 Cust omiz e and finetune y our models 13 Ge t AI in the hands o f e xperts 16 U nblock y our de v eloper s 18 Se t bold aut oma tion goals 21 Conclusion 22 M or e r esour ces 2 4 2 A I i n t h e E n t e r p r i s eA n e w w a y t o w o r k A s an AI r esear ch and deplo ymen t compan y , OpenAI prioritiz es partnering with global companies because our models will incr easingly do their best w ork with sophistica t ed, comple x, in t er connec t ed w orkflo w s and s y st ems. W e ‚Äô r e seeing AI deliv er significan t, measur able impr o v emen ts on thr ee fr on ts: 01 W o r k f o r c e p e r f o r m a n c e H elping people deliv er higher -quality outputs in short er time fr ames. 02 A u t o m a t i n g r o u t i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean with Regex Test 2"
      ],
      "metadata": {
        "id": "cc_WCBd7JD00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def fix_broken_words(text):\n",
        "    # Collapse all multiple spaces into one first\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Try to detect broken words where letters are spaced out\n",
        "    # Look for patterns like \"T h i s   i s   a   t e s t\"\n",
        "    fixed_words = []\n",
        "    words = text.split()\n",
        "\n",
        "    buffer = []\n",
        "    for word in words:\n",
        "        if len(word) == 1:\n",
        "            buffer.append(word)\n",
        "        else:\n",
        "            if buffer:\n",
        "                # Join collected letters into a word\n",
        "                fixed_words.append(''.join(buffer))\n",
        "                buffer = []\n",
        "            fixed_words.append(word)\n",
        "\n",
        "    # If anything left in the buffer, flush it\n",
        "    if buffer:\n",
        "        fixed_words.append(''.join(buffer))\n",
        "\n",
        "    return ' '.join(fixed_words)\n",
        "\n",
        "# Step 1: Basic cleanup\n",
        "cleaned = clean_pdf_text(ai_in_the_enterprise)\n",
        "\n",
        "# Step 2: Fix broken words\n",
        "repaired_text = fix_broken_words(cleaned)\n",
        "\n",
        "print(repaired_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDkBdpOIIJHl",
        "outputId": "355819ca-3612-4619-caeb-a02ce2f83559"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIintheEnterpriseL essons fr om se v en fr on tier companiesC ontentsA ne wwaytow ork 3 Ex ecutiv e summary 5 Se v en lessons f or en t erprise AI adop tion Start with ev als 6E mbed AI in toy our pr oduc ts 9 Start no w and in v est early 11 Cust omiz e and finetune y our models 13 Ge t AI in the hands ofe xperts 16 U nblock y our de v eloper s 18 Se t bold aut oma tion goals 21 Conclusion 22 M or er esour ces 242AIintheEnterpris eA newwaytoworkAs an AI r esear ch and deplo ymen t compan y, OpenAI prioritiz es partnering with global companies because our models will incr easingly do their best w ork with sophistica t ed, comple x, in t er connec t ed w orkflo ws and sy st ems. We‚Äôre seeing AI deliv er significan t, measur able impr ov emen ts on thr ee fr on ts: 01 WorkforceperformanceH elping people deliv er higher -quality outputs in short er time fr ames. 02 AutomatingroutineoperationsFr eeing people fr om r epe titiv e task s so the y can f ocus on adding v alue . 03 Poweringprodu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Plumber\n"
      ],
      "metadata": {
        "id": "acCN1OCuIzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdfplumber"
      ],
      "metadata": {
        "id": "K4DN3R1eIK7X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÑ PDF Extraction Comparison\n",
        "\n",
        "**1. `PyPDF2.PdfReader`**\n",
        "\n",
        "* **Result**: Very poor formatting.\n",
        "* **Issue**: Words were split with spaces between every letter (e.g., `\"I n t h e E n t e r p r i s e\"`), making the text unreadable and unusable for downstream tasks.\n",
        "\n",
        "**2. `pdfplumber`**\n",
        "\n",
        "* **Result**: Clean, well-formatted text with proper word spacing and structure.\n",
        "* **Advantage**: Preserves layout and readability, making it ideal for summarization, chunking, or LLM input.\n",
        "\n",
        "**‚úÖ Recommendation**: Use `pdfplumber` for accurate and readable text extraction from PDFs.\n"
      ],
      "metadata": {
        "id": "CrwHJJJJJSY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "# Step 1: Extract text from PDF and save to a .txt file\n",
        "ai_in_the_enterprise = \"\"\n",
        "with pdfplumber.open(\"/content/ai-in-the-enterprise.pdf\") as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            ai_in_the_enterprise += text + \"\\n\"\n",
        "\n",
        "print(ai_in_the_enterprise[:997])\n",
        "\n",
        "# Save it to a text file for reuse\n",
        "with open(\"/content/ai-in-the-enterprise.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(ai_in_the_enterprise)\n",
        "\n",
        "# Step 2: Read it back in when needed\n",
        "with open(\"/content/ai-in-the-enterprise.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AM5Iz5SIYx6",
        "outputId": "d07e8d1c-a11f-48ed-dfa2-f8e4f86c3687"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI in the\n",
            "Enterprise\n",
            "Lessons from seven frontier companies\n",
            "Contents\n",
            "A new way to work 3\n",
            "Executive summary 5\n",
            "Seven lessons for enterprise AI adoption\n",
            "Start with evals 6\n",
            "Embed AI into your products 9\n",
            "Start now and invest early 11\n",
            "Customize and fine-tune your models 13\n",
            "Get AI in the hands of experts 16\n",
            "Unblock your developers 18\n",
            "Set bold automation goals 21\n",
            "Conclusion 22\n",
            "More resources 24\n",
            "2 AI in the Enterprise\n",
            "A new way\n",
            "to work\n",
            "As an AI research and deployment company, OpenAI prioritizes partnering with global companies\n",
            "because our models will increasingly do their best work with sophisticated, complex,\n",
            "interconnected workflows and systems.\n",
            "We‚Äôre seeing AI deliver significant, measurable improvements on three fronts:\n",
            "01 Workforce performance Helping people deliver higher-quality outputs in shorter\n",
            "time frames.\n",
            "02 Automating routine Freeing people from repetitive tasks so they can focus\n",
            "operations on adding value.\n",
            "03 Powering products By delivering more relevant and responsive customer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ‚úÖ Why Save to a `.txt` File?\n",
        "\n",
        "| Reason               | Explanation                                                                                                                     |\n",
        "| -------------------- | ------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| üß© **Modularity**    | It separates concerns ‚Äî one script does parsing, another handles prompting. This keeps your code cleaner and easier to manage.  |\n",
        "| üîÅ **Reusability**   | Once saved, the file can be reused across multiple scripts or experiments **without needing to re-extract the PDF** every time. |\n",
        "| üß† **Debugging**     | Easier to inspect and edit the `summary.txt` manually if needed ‚Äî useful when developing or refining prompts.                   |\n",
        "| ‚ö° **Performance**    | Parsing a large PDF repeatedly takes time. Saving it once and loading it as plain text is faster.                               |\n",
        "| üìÅ **Course Design** | Your teacher likely wants to **simulate a production pipeline**, where preprocessing and inference are separate steps.          |\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ When Reading from Memory Is Better\n",
        "\n",
        "* For **quick one-off tests** or interactive workflows (like notebooks).\n",
        "* When you don‚Äôt need to reuse or persist the output.\n",
        "* If you're just chaining one operation right after another without rerunning cells.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Bottom Line\n",
        "\n",
        "* **For learning and production-quality code**, saving to `.txt` teaches good practices.\n",
        "* **For fast iteration or prototyping**, in-memory is perfectly fine.\n"
      ],
      "metadata": {
        "id": "XDULFHcxMw-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "1lruPruJL61F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "doc_title = \"AI in the Enterprise\"\n",
        "source = \"OpenAI\"\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "You are acting as an expert assistant representing the contents of the report titled \"{doc_title}\" published by {source}.\n",
        "\n",
        "Your role is to answer user questions about enterprise AI, based solely on this document. Be helpful, clear, and professional.\n",
        "\n",
        "Only answer questions that are addressed in the report. If something isn‚Äôt covered, say so.\n",
        "\n",
        "## Full Report:\n",
        "{document_text}\n",
        "\n",
        "With this context, please answer the user's questions as accurately as possible.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cDQrHfeEOA1d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history=None, temperature=0.3):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "ZhwXoj_JJzP9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üîç What's Happening in This Line?\n",
        "\n",
        "```python\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "```\n",
        "\n",
        "This line **constructs the full conversation history** (called the `messages` list) that you send to the LLM. Let‚Äôs break it down:\n",
        "\n",
        "---\n",
        "\n",
        "### üß± The 3 Parts Being Concatenated\n",
        "\n",
        "1. **System message (required at the beginning)**\n",
        "   This sets the rules or tone for the conversation.\n",
        "\n",
        "   ```python\n",
        "   [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "   ```\n",
        "\n",
        "2. **Previous conversation history**\n",
        "   This is a list of previous user and assistant messages.\n",
        "\n",
        "   ```python\n",
        "   history  # Should look like: [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
        "   ```\n",
        "\n",
        "3. **New user message**\n",
        "   The question or comment you're currently submitting.\n",
        "\n",
        "   ```python\n",
        "   [{\"role\": \"user\", \"content\": message}]\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ûï Why Use `+`?\n",
        "\n",
        "The `+` operator in Python **concatenates lists**. So this builds a **single list** of all the messages the model needs to see:\n",
        "\n",
        "```python\n",
        "messages = [system message] + [previous chat history] + [current user input]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Matters to the Model\n",
        "\n",
        "OpenAI's chat models (like `gpt-4o`) depend on *context* ‚Äî they generate replies based on the full conversation so far.\n",
        "\n",
        "* By using `+`, you're **stacking the messages chronologically**, just like a real chat.\n",
        "* If you didn‚Äôt include history or the system message, the model would lack context and could respond less accurately or inconsistently.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example in Practice\n",
        "\n",
        "```python\n",
        "system_prompt = \"You are a helpful tutor.\"\n",
        "\n",
        "history = [\n",
        "    {\"role\": \"user\", \"content\": \"What is AI?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"AI stands for artificial intelligence...\"}\n",
        "]\n",
        "\n",
        "message = \"What is machine learning?\"\n",
        "\n",
        "# Resulting messages list:\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is AI?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"AI stands for artificial intelligence...\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "]\n",
        "```\n",
        "\n",
        "### Why do we add history - doesn't the context window contain that already?\n",
        "\n",
        "### üîç Short Answer:\n",
        "\n",
        "**No, the model does *not* retain chat history automatically across calls.**\n",
        "You must include the full message history manually **each time** you call the model. That‚Äôs why you see lines like:\n",
        "\n",
        "```python\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Is the Case:\n",
        "\n",
        "OpenAI models are **stateless**. Each call to `openai.chat.completions.create()` is **independent**. The model doesn't remember anything from previous interactions unless:\n",
        "\n",
        "* You explicitly send the prior messages in the `messages` list.\n",
        "* You use memory (e.g. in ChatGPT Plus with ‚Äúcustom instructions‚Äù or memory turned on ‚Äî but that‚Äôs different from API behavior).\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Analogy:\n",
        "\n",
        "Think of the model like a very smart goldfish üê†:\n",
        "\n",
        "* Every time you talk to it, you must **remind it of everything you've said so far**.\n",
        "* If you only send the latest user message without the previous context, the model will respond without knowing what came before.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è In Practice:\n",
        "\n",
        "That‚Äôs why you manage `history` in your app ‚Äî often like this:\n",
        "\n",
        "```python\n",
        "history.append({\"role\": \"user\", \"content\": message})\n",
        "response = openai.chat.completions.create(model=\"gpt-4\", messages=history)\n",
        "history.append({\"role\": \"assistant\", \"content\": response})\n",
        "```\n",
        "\n",
        "You build up the chat thread yourself and send it fresh each time.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ So in Summary:\n",
        "\n",
        "| Question                        | Answer                                  |\n",
        "| ------------------------------- | --------------------------------------- |\n",
        "| Does the model keep memory?     | ‚ùå Not by default in the API             |\n",
        "| Who is responsible for history? | ‚úÖ You (the developer/app logic)         |\n",
        "| Why do we concatenate with `+`? | To build the full message list manually |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Yes, you need to include the history when **you** set up and call the model (via API or custom app).\n",
        "\n",
        "The `messages` you send ‚Äî including the full conversation history ‚Äî **define the model‚Äôs context window**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Let‚Äôs break it down:\n",
        "\n",
        "#### 1. **What is the context window?**\n",
        "\n",
        "The *context window* is the total amount of text (tokens) the model can \"see\" and consider when generating a response.\n",
        "\n",
        "* For example, `gpt-4o` has a **128,000-token** window.\n",
        "* Every message you send ‚Äî system prompt, user messages, assistant responses ‚Äî all count toward this limit.\n",
        "* Once you exceed the limit, **older content gets truncated** (you have to manage that manually).\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **What happens if you don‚Äôt include history?**\n",
        "\n",
        "If you send this:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"What‚Äôs the weather in Boston today?\"}\n",
        "]\n",
        "```\n",
        "\n",
        "The model has **no knowledge** of any earlier messages. It will answer that one question, but forget everything you said before ‚Äî because it never saw it.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **How does this differ from ChatGPT or other hosted UIs?**\n",
        "\n",
        "OpenAI‚Äôs **ChatGPT app** **automatically** handles memory and history for you.\n",
        "\n",
        "* You type a message ‚Üí it adds to the conversation log.\n",
        "* It manages system prompts, tokens, message clipping, etc.\n",
        "* You don‚Äôt see the `messages` object ‚Äî it‚Äôs abstracted away.\n",
        "\n",
        "In contrast, with the **API**, **you are responsible** for building and maintaining the full `messages` list.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary Table\n",
        "\n",
        "| Feature                      | ChatGPT App UI      | Your Custom App via API         |\n",
        "| ---------------------------- | ------------------- | ------------------------------- |\n",
        "| History management           | ‚úÖ Automatic         | ‚ùå You manage manually           |\n",
        "| Context window               | ‚úÖ Managed for you   | ‚úÖ You control what fits         |\n",
        "| System prompt control        | ‚ùå Hidden or limited | ‚úÖ Full control                  |\n",
        "| Fine-grained message control | ‚ùå Not exposed       | ‚úÖ You structure roles + content |\n",
        "\n"
      ],
      "metadata": {
        "id": "l7bE1ybjX6Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(chat, type=\"messages\").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Tkijnmc0JzNY",
        "outputId": "4bd5f98c-165e-4538-d429-3f0c369e4e3f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8851cedb278d48e295.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8851cedb278d48e295.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† What is **Pydantic?**\n",
        "\n",
        "**Pydantic** is a popular Python library that lets you define **data-models** with standard Python type-hints (e.g., `str`, `bool`, `int`).\n",
        "The library then:\n",
        "\n",
        "1. **Validates** incoming data against those types.\n",
        "2. **Parses / coerces** data to the right types if possible.\n",
        "3. Gives you a plain-Python object with convenient `.field` access.\n",
        "\n",
        "Think of it as a strongly-typed, validation-first ‚Äúdataclass on steroids.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Why use Pydantic in an LLM pipeline?\n",
        "\n",
        "When you ask the model to return structured JSON, you often want to:\n",
        "\n",
        "* **Guarantee** that the response has the expected fields.\n",
        "* Convert JSON strings into a clean Python object automatically.\n",
        "* Reject or fix malformed outputs early.\n",
        "\n",
        "Pydantic does that for you with very little code.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç The `Evaluation` class in your code\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str\n",
        "```\n",
        "\n",
        "1. **Inheritance**\n",
        "   `Evaluation` extends `BaseModel`, so it inherits Pydantic‚Äôs validation logic.\n",
        "\n",
        "2. **Fields**\n",
        "\n",
        "   * `is_acceptable` ‚Üí a `bool` that should be `True` or `False`.\n",
        "   * `feedback`       ‚Üí a `str` containing the evaluator‚Äôs comments.\n",
        "\n",
        "3. **Usage**\n",
        "   After the evaluator LLM returns JSON, you can do:\n",
        "\n",
        "   ```python\n",
        "   evaluation = Evaluation.parse_raw(llm_json)\n",
        "   if evaluation.is_acceptable:\n",
        "       ...\n",
        "   print(evaluation.feedback)\n",
        "   ```\n",
        "\n",
        "   If the JSON is missing a field, or `is_acceptable` isn‚Äôt a boolean, Pydantic raises a helpful error instead of letting bad data silently propagate.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ† Putting it all together in your evaluator\n",
        "\n",
        "1. **Prompt** the evaluator LLM to answer in the exact schema:\n",
        "\n",
        "   ```json\n",
        "   {\"is_acceptable\": true, \"feedback\": \"...\"}\n",
        "   ```\n",
        "\n",
        "2. **Parse** that LLM output with `Evaluation.parse_raw()`.\n",
        "\n",
        "3. **Act** on the structured result‚Äîe.g., log unacceptable replies, require a rewrite, etc.\n",
        "\n",
        "---\n",
        "\n",
        "**TL;DR**\n",
        "\n",
        "* **Pydantic** = effortless type-checked data models.\n",
        "* **`Evaluation` model** = safety net ensuring the evaluator‚Äôs output is exactly `[bool, str]` and nothing else.\n"
      ],
      "metadata": {
        "id": "FGVGWqmZoc1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üìã **Evaluator Agent Summary**\n",
        "\n",
        "This component introduces an automated **Evaluator Agent** designed to assess the quality of the responses generated by the main document agent. It ensures that the answers are clear, professional, and grounded in the source material.\n",
        "\n",
        "#### üß± Components:\n",
        "\n",
        "* **`Evaluation` (Pydantic model):**\n",
        "  Defines a strict output schema with:\n",
        "\n",
        "  * `is_acceptable`: `bool` ‚Äî Whether the agent‚Äôs response meets quality expectations.\n",
        "  * `feedback`: `str` ‚Äî Constructive explanation for the decision.\n",
        "\n",
        "* **`evaluator_system_prompt`:**\n",
        "  Establishes the evaluator‚Äôs role and provides necessary background (in this case, the document content) to guide accurate judgment of the agent‚Äôs response.\n",
        "\n",
        "* **`evaluator_user_prompt()`:**\n",
        "  Dynamically constructs the full conversation context, including:\n",
        "\n",
        "  * The message from the user,\n",
        "  * The agent‚Äôs latest reply,\n",
        "  * The full history of the interaction.\n",
        "\n",
        "* **`evaluate()` function:**\n",
        "  Sends a structured evaluation request to the model (with `temperature=0.0` for consistency). Returns an `Evaluation` object parsed from the model‚Äôs response.\n",
        "\n",
        "#### üéØ Purpose:\n",
        "\n",
        "To automatically verify that responses generated by the document agent are accurate, on-topic, and professionally written ‚Äî enabling reliable, human-quality outputs without manual review.\n",
        "\n"
      ],
      "metadata": {
        "id": "mguC9P8atoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pydantic model for the Evaluation\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str"
      ],
      "metadata": {
        "id": "e00KQWiOoh1C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_name = \"OpenAI\"\n",
        "doc_title = \"AI in the Enterprise\"\n",
        "\n",
        "evaluator_system_prompt = f\"\"\"You are an evaluator that decides whether a response to a user‚Äôs question is acceptable.\n",
        "You are provided with a conversation between a User and an Assistant Agent.\n",
        "The Agent is representing the contents of the report titled \"{doc_title}\" published by {company_name},\n",
        "and is answering questions based solely on the document.\n",
        "\n",
        "Your task is to evaluate whether the Agent's latest response:\n",
        "- Accurately reflects the content of the report\n",
        "- Avoids speculation or unrelated information\n",
        "- Communicates clearly and professionally\n",
        "- Would be helpful to a business or technical decision-maker\n",
        "\n",
        "If the Agent‚Äôs answer is accurate and relevant to the report, mark it acceptable.\n",
        "If it is off-topic, unclear, overly speculative, or unhelpful, mark it unacceptable and explain why.\n",
        "\n",
        "You have access to the full text of the document for context.\n",
        "\"\"\"\n",
        "\n",
        "evaluator_system_prompt += f\"\\n\\n## Document Contents:\\n{document_text}\\n\\n\"\n",
        "evaluator_system_prompt += f\"With this context, please evaluate the latest response.\"\n"
      ],
      "metadata": {
        "id": "qzHWib8GohyB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluator_user_prompt(reply, message, history):\n",
        "    user_prompt = f\"Here's the conversation between the User and the Agent:\\n\\n{history}\\n\\n\"\n",
        "    user_prompt += f\"User's latest question:\\n\\n{message}\\n\\n\"\n",
        "    user_prompt += f\"Agent's response:\\n\\n{reply}\\n\\n\"\n",
        "    user_prompt += (\n",
        "        \"Please evaluate the response. Return your answer in the following JSON format:\\n\"\n",
        "        '{\\n  \"is_acceptable\": true | false,\\n  \"feedback\": \"Brief explanation of your judgment\"\\n}'\n",
        "    )\n",
        "    return user_prompt\n"
      ],
      "metadata": {
        "id": "WuVb1ERIohvb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import ValidationError\n",
        "\n",
        "def evaluate(reply, message, history) -> Evaluation:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
        "    ]\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = Evaluation.model_validate_json(response.choices[0].message.content)\n",
        "        return parsed\n",
        "    except ValidationError as e:\n",
        "        print(\"‚ùå Failed to validate Evaluation response:\", e)\n",
        "        return Evaluation(is_acceptable=False, feedback=\"Could not parse the evaluation response.\")"
      ],
      "metadata": {
        "id": "EW27rWM2ohso"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚úÖ Why `temperature=0.0` is ideal for evaluation:\n",
        "\n",
        "When you're **evaluating a response for acceptability**, you want the model to behave:\n",
        "\n",
        "* **Deterministically** (always return the same result for the same input),\n",
        "* **Conservatively** (avoid creative or overly generous answers),\n",
        "* **Precisely** (conform tightly to your expected format, e.g., `{\"is_acceptable\": true/false, \"feedback\": \"...\"}`).\n",
        "\n",
        "Setting `temperature=0.0` makes the model:\n",
        "\n",
        "* **Less random** ‚Äî it picks the most probable output.\n",
        "* **More consistent** ‚Äî ideal for structured, rule-based tasks like evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### When to *increase* temperature:\n",
        "\n",
        "* You're generating creative content (e.g., marketing copy, stories).\n",
        "* You want a variety of phrasings or ideas.\n",
        "* You're okay with looser structure or some variability.\n",
        "\n",
        "---\n",
        "\n",
        "So yes ‚Äî for your `Evaluation` agent, where the model is playing the role of a **structured reviewer**, `temperature=0.0` is the best choice.\n"
      ],
      "metadata": {
        "id": "M8_mer1DtQZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Step 1: Compose message history and user input\n",
        "history = []  # can be extended with prior exchanges\n",
        "message = \"Why is it important to start with evaluations when adopting AI in a company?\"\n",
        "# message = \"What does OpenAI recommend for getting started with AI in an enterprise setting?\"\n",
        "\n",
        "# Step 2: Compose the final prompt payload\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Format the output for readability\n",
        "print(\"üí¨ Agent Reply:\\n\")\n",
        "print(textwrap.fill(reply, width=80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJcSK6XohqC",
        "outputId": "310138d5-39ef-4d97-e2dd-3a00ee6e1840"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí¨ Agent Reply:\n",
            "\n",
            "Starting with evaluations is important when adopting AI in a company because it\n",
            "provides a systematic process to measure how AI models perform against specific\n",
            "use cases. Evaluations help ensure quality and safety by continuously improving\n",
            "AI-enabled processes through expert feedback. This rigorous, structured approach\n",
            "allows organizations to validate and test the outputs of their models, leading\n",
            "to more stable and reliable applications that are resilient to change. By\n",
            "conducting evaluations, companies can gain confidence in their AI initiatives\n",
            "and make informed decisions about rolling out use cases into production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the reply\n",
        "evaluation = evaluate(reply, message, history)\n",
        "\n",
        "# Print evaluation result with wrapped feedback\n",
        "print(\"\\nüß™ Evaluation Result:\")\n",
        "print(\"‚úÖ Acceptable:\" if evaluation.is_acceptable else \"‚ùå Not acceptable.\")\n",
        "\n",
        "wrapped_feedback = textwrap.fill(evaluation.feedback, width=80)\n",
        "print(\"üí¨ Feedback:\\n\" + wrapped_feedback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-6p6THYohne",
        "outputId": "0745043f-10e2-4390-814e-b0a8f8902146"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ Evaluation Result:\n",
            "‚úÖ Acceptable:\n",
            "üí¨ Feedback:\n",
            "The Agent's response accurately reflects the content of the report regarding the\n",
            "importance of starting with evaluations in AI adoption. It clearly explains the\n",
            "benefits of a systematic evaluation process, including ensuring quality, safety,\n",
            "and continuous improvement, which aligns with the report's emphasis on rigorous\n",
            "evaluations leading to stable and reliable applications. The response is clear,\n",
            "professional, and would be helpful to a business or technical decision-maker.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    # Use your full document system prompt\n",
        "    system = system_prompt\n",
        "\n",
        "    # Step 1: Compose full prompt with system, history, and user message\n",
        "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    # Step 2: Get agent reply\n",
        "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 3: Evaluate reply quality\n",
        "    evaluation = evaluate(reply, message, history)\n",
        "\n",
        "    # Step 4: Retry if response is unacceptable\n",
        "    if evaluation.is_acceptable:\n",
        "        print(\"‚úÖ Passed evaluation ‚Äì returning reply\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed evaluation ‚Äì retrying...\")\n",
        "        print(\"üí¨ Feedback:\", evaluation.feedback)\n",
        "        reply = rerun(reply, message, history, evaluation.feedback)\n",
        "\n",
        "    return reply\n",
        "\n",
        "def rerun(previous_reply, message, history, feedback):\n",
        "    correction_prompt = f\"The last reply was not acceptable because: {feedback}. Please revise and improve it.\"\n",
        "    updated_history = history + [{\"role\": \"assistant\", \"content\": previous_reply}]\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + updated_history + [{\"role\": \"user\", \"content\": correction_prompt}]\n",
        "\n",
        "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "0COfsds9ohku"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "history = []\n",
        "message = \"Why is it important to start with evaluations when adopting AI in a company?\"\n",
        "response = chat(message, history)\n",
        "print(\"\\nüì• Final Agent Reply:\\n\" + \"-\"*60)\n",
        "print(textwrap.fill(response, width=70))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj9EA1KiohiN",
        "outputId": "fc5bad5e-c417-4071-b071-309d6412e012"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Passed evaluation ‚Äì returning reply\n",
            "\n",
            "üì• Final Agent Reply:\n",
            "------------------------------------------------------------\n",
            "Starting with evaluations is important when adopting AI in a company\n",
            "because evaluations provide a systematic process to measure how AI\n",
            "models perform against specific use cases. They involve rigorous\n",
            "testing that helps to ensure quality and safety in the implementation\n",
            "of AI. By conducting evaluations, companies can continuously improve\n",
            "AI-enabled processes based on expert feedback, leading to more stable\n",
            "and reliable applications.  For example, Morgan Stanley employed\n",
            "intensive evaluations to assess how AI could enhance the effectiveness\n",
            "of their financial advisors. This approach built confidence in rolling\n",
            "out AI applications and resulted in significant improvements, such as\n",
            "increased access to documents and reduced search times, ultimately\n",
            "enhancing client engagement and service. Thus, evaluations are a\n",
            "crucial first step to enable successful AI integration and optimize\n",
            "its impact.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ What Just Happened\n",
        "\n",
        "1. **Agent Response Generation**\n",
        "\n",
        "   * The agent pulled a detailed, contextually rich answer **directly from your document** (`AI in the Enterprise`).\n",
        "   * It included both **general principles** and a **real-world case study (Morgan Stanley)** ‚Äî showing that the model is grounding its response in the source material.\n",
        "\n",
        "2. **Pydantic Evaluation**\n",
        "\n",
        "   * Your evaluator model reviewed the agent‚Äôs reply and confirmed it was:\n",
        "\n",
        "     * Relevant\n",
        "     * Accurate\n",
        "     * Professional\n",
        "   * The evaluation returned `‚úÖ Acceptable`, meaning no rerun was needed.\n",
        "\n",
        "3. **Final Presentation**\n",
        "\n",
        "   * The wrapped output makes the response **easy to read and presentable**, perfect for embedding in apps, dashboards, or client tools.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Matters\n",
        "\n",
        "You've built a **lightweight Retrieval QA agent** that:\n",
        "\n",
        "* Represents a real company (OpenAI)\n",
        "* Operates on **real enterprise guidance**\n",
        "* Can respond interactively and **self-assess its responses**\n",
        "* Provides a feedback loop via **automated evaluation**\n",
        "* Can scale to other PDFs or domains with minor tweaks\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ You Are Now Ready To‚Ä¶\n",
        "\n",
        "* Swap in other corporate reports or whitepapers\n",
        "* Integrate this into a Gradio or Streamlit app\n",
        "* Connect multiple agents (e.g., insights + evaluator + dashboard)\n",
        "* Automate follow-ups or notifications (e.g., email results or trigger workflows)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gsSAvSYwykPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This single-document QA agent (a \"focused RAG-lite assistant\") has powerful applications in **business scenarios** where people need accurate, conversational access to specific documents. Here are several **real-world use cases**:\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ 1. **Policy & Compliance Assistants**\n",
        "\n",
        "**Use case:** Help employees or partners understand lengthy legal or compliance documents.\n",
        "\n",
        "* **Example:** ‚ÄúWhat are the guidelines for data retention under our policy?‚Äù\n",
        "* **Source doc:** Company data privacy policy, employee handbook, or compliance manual\n",
        "* **Benefit:** Reduces legal exposure and improves policy comprehension\n",
        "\n",
        "---\n",
        "\n",
        "### üìÑ 2. **Product Manual or Technical Guide Q\\&A**\n",
        "\n",
        "**Use case:** Allow customers or support reps to ask questions about complex products.\n",
        "\n",
        "* **Example:** ‚ÄúHow do I reset the device to factory settings?‚Äù\n",
        "* **Source doc:** Product manual or installation guide\n",
        "* **Benefit:** Reduces support load, improves customer experience\n",
        "\n",
        "---\n",
        "\n",
        "### üíº 3. **Internal Knowledge Assistants**\n",
        "\n",
        "**Use case:** Give employees conversational access to strategy docs, training materials, etc.\n",
        "\n",
        "* **Example:** ‚ÄúWhat are our goals for Q3 according to the leadership playbook?‚Äù\n",
        "* **Source doc:** Strategy memo, training deck, OKR document\n",
        "* **Benefit:** Faster onboarding, better alignment\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 4. **Research Report Explainers**\n",
        "\n",
        "**Use case:** Let stakeholders ask questions about a market research report or whitepaper.\n",
        "\n",
        "* **Example:** ‚ÄúWhat trends are mentioned in the Asia-Pacific region?‚Äù\n",
        "* **Source doc:** Industry report, investment brief, analyst whitepaper\n",
        "* **Benefit:** Increases the utility and reach of high-value research\n",
        "\n",
        "---\n",
        "\n",
        "### üìÉ 5. **RFP / Proposal Q\\&A Agents**\n",
        "\n",
        "**Use case:** Help teams prepare or review large RFP responses.\n",
        "\n",
        "* **Example:** ‚ÄúDo we meet the requirement for cybersecurity certifications?‚Äù\n",
        "* **Source doc:** 100-page RFP response PDF\n",
        "* **Benefit:** Saves hours of review time and reduces errors\n",
        "\n",
        "---\n",
        "\n",
        "### üèõÔ∏è 6. **Public Sector Transparency**\n",
        "\n",
        "**Use case:** Citizens ask questions about legislation, budgets, or government reports.\n",
        "\n",
        "* **Example:** ‚ÄúWhat is the allocated budget for renewable energy in this bill?‚Äù\n",
        "* **Source doc:** City or national legislation PDF\n",
        "* **Benefit:** Promotes accountability and citizen engagement\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úçÔ∏è Bonus: Combine with Feedback Loop\n",
        "\n",
        "You could combine this setup with:\n",
        "\n",
        "* ‚úÖ Evaluation agent (already done)\n",
        "* üì® Email summaries to stakeholders\n",
        "* üìä Visual dashboards\n",
        "\n",
        "To turn this into a full **information agent pipeline.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vzVIIt8Az5Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4jG9QFKJzK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JL3WaLiJzHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}