{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsS4TWoK5unDVRsj7uWrdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/005_Pipelines_Models_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🤖 Hugging Face Pipelines: What They Do\n",
        "\n",
        "A **pipeline** in Hugging Face is a pre-wired wrapper that:\n",
        "1. Tokenizes your input\n",
        "2. Runs inference on a model\n",
        "3. Decodes the output into readable text\n",
        "\n",
        "Each pipeline is matched to a **task type** — and you choose the pipeline **based on what you want the model to do**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Summary Table: Pipelines, Tasks, and Models\n",
        "\n",
        "| **Pipeline** | **What It Does** | **Example Use** | **Good Models** |\n",
        "|--------------|------------------|------------------|------------------|\n",
        "| `text-generation` | Predicts next tokens in freeform text | Autocomplete, writing assistants | `gpt2`, `falcon-rw-1b`, `DialoGPT` |\n",
        "| `text2text-generation` | Translates one text into another (task-following) | Instructions → output, translation, summarization | `flan-t5-base`, `t5-small`, `bart-base` |\n",
        "| `text-classification` | Labels input text with a category | Sentiment, spam detection | `bert-base-uncased`, `distilbert-base-uncased` |\n",
        "| `question-answering` | Answers a question given a context | \"What is his name?\" → from a paragraph | `bert-large-uncased-whole-word-masking-finetuned-squad` |\n",
        "| `summarization` | Condenses input text to a shorter version | News summaries, TL;DR | `bart-large-cnn`, `t5-base` |\n",
        "| `translation` | Converts text from one language to another | English → French | `t5`, `mbart-large-50`, `opus-mt-en-fr` |\n",
        "| `conversational` | Handles back-and-forth dialogue with memory | Chatbots with context | `DialoGPT`, `Blenderbot` |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Quick Cheat Sheet\n",
        "\n",
        "### 🧠 `text-generation`\n",
        "- **Goal**: Complete a sentence, story, or thought\n",
        "- **Think**: \"Once upon a time...\"\n",
        "- **Style**: Open-ended, creative\n",
        "- **Model style**: GPT-style\n",
        "- ✅ Use when you don’t need strict structure\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `text2text-generation`\n",
        "- **Goal**: Follow instructions and output something specific\n",
        "- **Think**: “Summarize this article” or “Translate this to French”\n",
        "- **Style**: Precise, controllable\n",
        "- **Model style**: T5-style (encoder-decoder)\n",
        "- ✅ Best for AI agents, tools, workflows\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `text-classification`\n",
        "- **Goal**: Predict a label (positive/negative, topic, intent)\n",
        "- **Style**: Single answer from fixed set\n",
        "- ✅ Use when the output should be one of a few choices\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `question-answering`\n",
        "- **Goal**: Find answer **in a given paragraph**\n",
        "- **Style**: Not generative — it **extracts** answers from provided context\n",
        "- ✅ Use in retrieval-augmented generation or open-book QA\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `summarization`\n",
        "- **Goal**: Shorten text while preserving meaning\n",
        "- ✅ Use for reports, articles, transcripts\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `translation`\n",
        "- **Goal**: Translate between languages\n",
        "- ✅ Specialized for multilingual support\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `conversational`\n",
        "- **Goal**: Maintain turn-by-turn memory across chats\n",
        "- ✅ Use when you need memory in an assistant or chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Which Pipeline for Agents?\n",
        "\n",
        "| Agent Behavior | Best Pipeline | Why |\n",
        "|----------------|---------------|-----|\n",
        "| Follow instructions (tool use, info extraction) | `text2text-generation` | Clear prompt + clean output |\n",
        "| Freeform conversation or idea expansion | `text-generation` | Open-ended creativity |\n",
        "| Routing or intent detection | `text-classification` | You get structured labels |\n",
        "| Summarize large blocks of text | `summarization` | Less creative, more compression |\n",
        "| FAQ-style with a document | `question-answering` | Great with context chunks |\n",
        "\n"
      ],
      "metadata": {
        "id": "8AYbYkhMAiSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers huggingface_hub"
      ],
      "metadata": {
        "id": "3INc2XE_BfDs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔮 `text-generation` Pipeline (GPT-style models)\n",
        "\n",
        "### ✅ What It Does:\n",
        "This type of model is trained to **predict the next word** given what came before. It’s not trying to follow instructions — it’s just continuing the flow of text.\n",
        "\n",
        "Think of it as:\n",
        "> 🧠 \"Complete this sentence like a human would.\"\n",
        "\n",
        "Each of these will return something reasonable — but maybe a little unpredictable or unstructured.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xIZy6qO7Btg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFBAys7DAhvG",
        "outputId": "914d4d80-fff0-4f65-d3ef-41412c9b8a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1️⃣ Completion:\n",
            " The future of AI agents is murky, but a very real possibility,\" said Lawrence Berkeley National Laboratory professor and co-author Richard Moore, who holds a Ph.D. in computational\n",
            "2️⃣ Story:\n",
            " Once upon a time in a world powered by AI, it's difficult to avoid. And if they can't catch us in sight through AI, they are too.\n",
            "\n",
            "When you make a mistake,\n",
            "3️⃣ List:\n",
            " Three reasons to use AI agents are:\n",
            "\n",
            "Autoworking and AI agents must be configured based on the algorithm and are very efficient in the short-term.\n",
            "\n",
            "Autow\n"
          ]
        }
      ],
      "source": [
        "# supress warning\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a small text-generation model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# 1. Autocomplete a thought\n",
        "prompt = \"The future of AI agents is\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"1️⃣ Completion:\\n\", output)\n",
        "\n",
        "# 2. Start a story\n",
        "prompt = \"Once upon a time in a world powered by AI,\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"2️⃣ Story:\\n\", output)\n",
        "\n",
        "# 3. Give a list of ideas\n",
        "prompt = \"Three reasons to use AI agents are:\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"3️⃣ List:\\n\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔥 Now Let's Test Its Limits\n",
        "\n",
        "### 😬 Where `text-generation` *Fails*\n",
        "\n",
        "#### ❌ 1. Follow Instructions (It won't!)\n",
        "\n",
        "```python\n",
        "prompt = \"Translate this sentence into French: I love machine learning.\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(output)\n",
        "```\n",
        "\n",
        "→ You’ll probably get something like:\n",
        "\n",
        "> Translate this sentence into French: I love machine learning. It’s a fascinating field that…\n",
        "\n",
        "🙅 It **does not translate** — it continues the thought.\n",
        "\n",
        "---\n",
        "\n",
        "#### ❌ 2. Extract Information or Answer Questions Cleanly\n",
        "\n",
        "```python\n",
        "prompt = \"What is the capital of France?\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(output)\n",
        "```\n",
        "\n",
        "→ It might say something **correct**, but it could also say:\n",
        "\n",
        "> What is the capital of France? The capital of France has long been known for its...\n",
        "\n",
        "🙅 It wanders. No guarantee of a clean answer like `\"Paris\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Why This Happens\n",
        "\n",
        "- GPT-style models are **trained to generate flowing language**, not follow specific instructions.\n",
        "- There's no native structure to the response.\n",
        "- It doesn’t “understand” your prompt as a task.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Where `text2text-generation` Shines\n",
        "\n",
        "Now you’ll see:\n",
        "- Instruction-following\n",
        "- Clean format\n",
        "- Task-specific outputs\n",
        "\n",
        "For example:\n",
        "\n",
        "| Task | GPT2 (text-gen) | FLAN-T5 (text2text-gen) |\n",
        "|------|------------------|--------------------------|\n",
        "| Translate to French | Wanders off | `\"J'aime l'apprentissage automatique\"` |\n",
        "| Answer a direct question | Talks in circles | `\"Paris\"` |\n",
        "| Rephrase this sentence | Just continues | Rephrases as asked |\n"
      ],
      "metadata": {
        "id": "MxShpGfnD691"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Translation attempt\n",
        "prompt1 = \"Translate this sentence into French: I love machine learning.\"\n",
        "output1 = generator(prompt1, max_new_tokens=30)[0][\"generated_text\"]\n",
        "\n",
        "# Example 2: Question answering attempt\n",
        "prompt2 = \"What is the capital of France?\"\n",
        "output2 = generator(prompt2, max_new_tokens=30)[0][\"generated_text\"]\n",
        "\n",
        "# Pretty print\n",
        "print(\"❌ Example 1: Translation with text-generation\")\n",
        "print(\"📝 Prompt:\\n\", prompt1)\n",
        "print(\"🤖 Output:\\n\", output1)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"❌ Example 2: Question answering with text-generation\")\n",
        "print(\"📝 Prompt:\\n\", prompt2)\n",
        "print(\"🤖 Output:\\n\", output2)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MvcCW14D-5d",
        "outputId": "2eb06eba-efdc-4086-9b11-a6feb0d55736"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Example 1: Translation with text-generation\n",
            "📝 Prompt:\n",
            " Translate this sentence into French: I love machine learning.\n",
            "🤖 Output:\n",
            " Translate this sentence into French: I love machine learning.\n",
            "\n",
            "I love machine learning.\n",
            "\n",
            "Because Machine Learning is a powerful tool, it's always worth looking at those examples of it in a broader\n",
            "\n",
            "============================================================\n",
            "\n",
            "❌ Example 2: Question answering with text-generation\n",
            "📝 Prompt:\n",
            " What is the capital of France?\n",
            "🤖 Output:\n",
            " What is the capital of France?\n",
            "\n",
            "Let me say that I've seen that capital in France from the French side, where they were a real hard-working country, not a\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📚 Understanding `text-generation` vs. `text2text-generation`\n",
        "\n",
        "### 🔮 `text-generation`\n",
        "- **Model Type**: GPT-style (causal language models)\n",
        "- **Pipeline**: `text-generation`\n",
        "- **Purpose**: Freeform text continuation\n",
        "- **Strengths**:\n",
        "  - Great for creative writing, storytelling, or idea expansion\n",
        "  - Can generate flowing, human-like text\n",
        "- **Limitations**:\n",
        "  - Does *not* follow instructions reliably\n",
        "  - Unstructured output — hard to parse or control\n",
        "  - Poor at task-specific prompts like “translate this” or “summarize this”\n",
        "\n",
        "> **Examples of good use**:  \n",
        "> `\"Once upon a time...\"` → continues the story  \n",
        "> `\"Write a fantasy scene about dragons...\"`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `text2text-generation`\n",
        "- **Model Type**: T5, FLAN, BART (encoder-decoder)\n",
        "- **Pipeline**: `text2text-generation`\n",
        "- **Purpose**: Instruction following — turns input text into output text\n",
        "- **Strengths**:\n",
        "  - Designed for structured tasks like translation, summarization, rephrasing, Q&A\n",
        "  - More reliable and precise than `text-generation`\n",
        "  - Better suited for use in AI agents and task automation\n",
        "- **Limitations**:\n",
        "  - Not great for longform creative writing\n",
        "  - Tends to give short, controlled responses (not ideal for open-ended generation)\n",
        "\n",
        "> **Examples of good use**:  \n",
        "> `\"Translate this into French: I love AI\"`  \n",
        "> `\"Summarize this paragraph...\"`  \n",
        "> `\"What is the capital of France?\"` → `\"Paris\"`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Summary\n",
        "| Feature                 | `text-generation`        | `text2text-generation`        |\n",
        "|------------------------|--------------------------|-------------------------------|\n",
        "| Style                  | Open-ended               | Instruction-following         |\n",
        "| Structure              | Loose, unpredictable     | Clean and controlled           |\n",
        "| Best for               | Storytelling, creative    | Translation, summarization     |\n",
        "| Output Format          | Natural continuation     | Task-specific response         |\n",
        "| Example Model          | `gpt2`                   | `flan-t5-base`, `t5-small`     |\n",
        "| Common Mistake         | Tries to “think aloud”   | Tries to be brief and literal  |\n",
        "\n"
      ],
      "metadata": {
        "id": "6xSl8prpGTQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an instruction-following model\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# --- Successful Prompts ---\n",
        "prompts = [\n",
        "    (\"Translate this sentence into French: I love machine learning.\", \"🌍 Translation\"),\n",
        "    (\"What is the capital of France?\", \"📍 Direct Question Answering\"),\n",
        "    (\"Summarize: Artificial intelligence is a branch of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence.\", \"🧾 Summarization\"),\n",
        "    (\"Rephrase: I am not sure if I can help you.\", \"🗣️ Rephrasing\")\n",
        "]\n",
        "\n",
        "print(\"✅ Successful Examples Using `text2text-generation`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in prompts:\n",
        "    output = generator(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
        "    print(f\"{label}\\n📝 Prompt:\\n{prompt}\\n🤖 Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuw_otftFycc",
        "outputId": "3b9d0c28-fbd4-458e-8df6-708c51709c14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successful Examples Using `text2text-generation`\n",
            "============================================================\n",
            "\n",
            "🌍 Translation\n",
            "📝 Prompt:\n",
            "Translate this sentence into French: I love machine learning.\n",
            "🤖 Output:\n",
            "J'aime l'apprentissage de la machine.\n",
            "============================================================\n",
            "\n",
            "📍 Direct Question Answering\n",
            "📝 Prompt:\n",
            "What is the capital of France?\n",
            "🤖 Output:\n",
            "london\n",
            "============================================================\n",
            "\n",
            "🧾 Summarization\n",
            "📝 Prompt:\n",
            "Summarize: Artificial intelligence is a branch of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence.\n",
            "🤖 Output:\n",
            "Understand the science behind artificial intelligence.\n",
            "============================================================\n",
            "\n",
            "🗣️ Rephrasing\n",
            "📝 Prompt:\n",
            "Rephrase: I am not sure if I can help you.\n",
            "🤖 Output:\n",
            "I am not sure if I can help you.\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts text2text may struggle with ---\n",
        "fail_prompts = [\n",
        "    (\"Continue the story: Once upon a time in a world of intelligent machines,\", \"📖 Freeform Storytelling\"),\n",
        "    (\"Write a poem about space exploration.\", \"🧑‍🚀 Creative Writing\")\n",
        "]\n",
        "\n",
        "print(\"❌ Limitations of `text2text-generation`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in fail_prompts:\n",
        "    output = generator(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
        "    print(f\"{label}\\n📝 Prompt:\\n{prompt}\\n🤖 Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKnGjvg9FyZU",
        "outputId": "5aaa4dba-45d2-48a4-ed89-65de807a1a4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Limitations of `text2text-generation`\n",
            "============================================================\n",
            "\n",
            "📖 Freeform Storytelling\n",
            "📝 Prompt:\n",
            "Continue the story: Once upon a time in a world of intelligent machines,\n",
            "🤖 Output:\n",
            "a robot was able to make a robot.\n",
            "============================================================\n",
            "\n",
            "🧑‍🚀 Creative Writing\n",
            "📝 Prompt:\n",
            "Write a poem about space exploration.\n",
            "🤖 Output:\n",
            "i was a kid and i was a teenager and i was a kid and i was a kid and i was a kid and i was a kid and i was a kid and i was a kid and i was \n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏷️ `text-classification` Pipeline\n",
        "\n",
        "## 🧠 What It Does:\n",
        "This model doesn't generate text — instead, it **labels** text with a category.  \n",
        "You give it something to read, and it tells you what “kind” of thing it is.\n",
        "\n",
        "- **Purpose**: Predict a category or label from text\n",
        "- **Pipeline**: `text-classification`\n",
        "- **Best For**: Sentiment analysis, spam detection, topic labeling\n",
        "- **Example Models**: `bert-base-uncased`, `distilbert-base-uncased`\n",
        "- **Strengths**:\n",
        "  - Fast and highly accurate for fixed-label tasks\n",
        "  - Works out of the box for binary classification (e.g., positive/negative)\n",
        "- **Limitations**:\n",
        "  - Cannot generate output\n",
        "  - Useless for translation, summarization, or reasoning\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Use Cases\n",
        "- Sentiment analysis (positive/negative)\n",
        "- Spam detection\n",
        "- Topic classification\n",
        "- Intent detection\n"
      ],
      "metadata": {
        "id": "DEq3iJ2KHCe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a simple classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Success cases\n",
        "prompts = [\n",
        "    (\"I absolutely love this new AI assistant!\", \"😊 Positive Sentiment\"),\n",
        "    (\"This is the worst update I’ve ever seen.\", \"😠 Negative Sentiment\"),\n",
        "]\n",
        "\n",
        "print(\"✅ Successful Examples Using `text-classification`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in prompts:\n",
        "    output = classifier(prompt)[0]\n",
        "    print(f\"{label}\\n📝 Prompt:\\n{prompt}\\n🏷️ Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3UztvedFyWf",
        "outputId": "2315ecd2-58d6-4f4d-a35d-836f02e3228d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successful Examples Using `text-classification`\n",
            "============================================================\n",
            "\n",
            "😊 Positive Sentiment\n",
            "📝 Prompt:\n",
            "I absolutely love this new AI assistant!\n",
            "🏷️ Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9998743534088135}\n",
            "============================================================\n",
            "\n",
            "😠 Negative Sentiment\n",
            "📝 Prompt:\n",
            "This is the worst update I’ve ever seen.\n",
            "🏷️ Output:\n",
            "{'label': 'NEGATIVE', 'score': 0.9997844099998474}\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fail cases: tasks that require actual output generation\n",
        "fail_prompts = [\n",
        "    (\"Translate this sentence into Spanish: I love data science.\", \"🌍 Translation\"),\n",
        "    (\"Summarize: Artificial intelligence is a field focused on building intelligent machines...\", \"🧾 Summarization\")\n",
        "]\n",
        "\n",
        "print(\"❌ Limitations of `text-classification`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in fail_prompts:\n",
        "    output = classifier(prompt)[0]\n",
        "    print(f\"{label}\\n📝 Prompt:\\n{prompt}\\n🏷️ Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA69InQIFyNG",
        "outputId": "297b02dc-7d0d-4d7c-9143-88c9d7513a2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Limitations of `text-classification`\n",
            "============================================================\n",
            "\n",
            "🌍 Translation\n",
            "📝 Prompt:\n",
            "Translate this sentence into Spanish: I love data science.\n",
            "🏷️ Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9994205236434937}\n",
            "============================================================\n",
            "\n",
            "🧾 Summarization\n",
            "📝 Prompt:\n",
            "Summarize: Artificial intelligence is a field focused on building intelligent machines...\n",
            "🏷️ Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9885655045509338}\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ❓ `question-answering` Pipeline\n",
        "\n",
        "## 🧠 What It Does:\n",
        "Unlike chatbots or text generation, this pipeline **answers a question using a provided context**.  \n",
        "It doesn’t guess — it **extracts** the answer from a given passage.\n",
        "\n",
        "> 🧠 Think of it as:  \n",
        "> “Read this paragraph and find the answer *inside it*.”\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Good For:\n",
        "- Closed-book question answering\n",
        "- FAQ bots with known documents\n",
        "- Extractive QA in RAG systems (Retrieval-Augmented Generation)\n",
        "\n",
        "### ❓ `question-answering`\n",
        "- **Purpose**: Extract an answer from a provided context paragraph\n",
        "- **Pipeline**: `question-answering`\n",
        "- **Best For**: FAQs, document-based QA, extractive RAG systems\n",
        "- **Example Models**: `bert-large-uncased-whole-word-masking-finetuned-squad`, `distilbert-base-uncased-distilled-squad`\n",
        "- **Strengths**:\n",
        "  - Great for “open-book” style QA\n",
        "  - Fast and accurate when context is included\n",
        "- **Limitations**:\n",
        "  - Fails without a context\n",
        "  - Cannot answer general or creative questions"
      ],
      "metadata": {
        "id": "kMXUvR9wIStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained QA model\n",
        "qa = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Provide context and ask a question\n",
        "examples = [\n",
        "    (\n",
        "        \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\",\n",
        "        \"Who was Marie Curie?\",\n",
        "        \"🧬 Biography Fact\"\n",
        "    ),\n",
        "    (\n",
        "        \"The capital of France is Paris. It is known for its culture, art, and landmarks like the Eiffel Tower.\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"🗺️ Simple Fact Extraction\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"✅ Successful Examples Using `question-answering`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for context, question, label in examples:\n",
        "    output = qa(question=question, context=context)\n",
        "    print(f\"{label}\\n❓ Question: {question}\\n📚 Context: {context}\\n✅ Answer: {output['answer']}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOchQK_BIacC",
        "outputId": "beb2875b-5242-4857-b0a9-986fa3940224"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successful Examples Using `question-answering`\n",
            "============================================================\n",
            "\n",
            "🧬 Biography Fact\n",
            "❓ Question: Who was Marie Curie?\n",
            "📚 Context: Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\n",
            "✅ Answer: a physicist and chemist\n",
            "============================================================\n",
            "\n",
            "🗺️ Simple Fact Extraction\n",
            "❓ Question: What is the capital of France?\n",
            "📚 Context: The capital of France is Paris. It is known for its culture, art, and landmarks like the Eiffel Tower.\n",
            "✅ Answer: Paris\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fail_examples = [\n",
        "    (\"What is the meaning of life?\", \"\", \"🌌 Philosophical Question\"),\n",
        "    (\"Who is the current president of the United States?\", \"\", \"📰 Real-time Fact\"),\n",
        "]\n",
        "\n",
        "print(\"❌ Limitations of `question-answering`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for question, context, label in fail_examples:\n",
        "    try:\n",
        "        output = qa(question=question, context=context)\n",
        "        print(f\"{label}\\n❓ Question: {question}\\n📚 Context: [EMPTY]\\n❌ Output: {output['answer']}\\n\" + \"=\"*60 + \"\\n\")\n",
        "    except ValueError:\n",
        "        print(f\"{label}\\n❓ Question: {question}\\n📚 Context: [EMPTY]\\n⚠️ Skipped: Question-answering requires context.\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb3A1xFmKquc",
        "outputId": "4d5dff6c-7d69-4fd7-a5e2-cde092bf3791"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Limitations of `question-answering`\n",
            "============================================================\n",
            "\n",
            "🌌 Philosophical Question\n",
            "❓ Question: What is the meaning of life?\n",
            "📚 Context: [EMPTY]\n",
            "⚠️ Skipped: Question-answering requires context.\n",
            "============================================================\n",
            "\n",
            "📰 Real-time Fact\n",
            "❓ Question: Who is the current president of the United States?\n",
            "📚 Context: [EMPTY]\n",
            "⚠️ Skipped: Question-answering requires context.\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ⚠️ The `question-answering` pipeline cannot work without a context paragraph.\n",
        "> If you provide an empty string or leave it out, the model will raise an error:\n",
        "> `ValueError: context cannot be empty`\n",
        ">\n",
        "> This reinforces that QA models are extractive — they don’t guess, they **retrieve** answers from known input.\n"
      ],
      "metadata": {
        "id": "3lUKeVuxMtF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔬 Model Naming: `distilbert-base-uncased-…`\n",
        "\n",
        "You're seeing:\n",
        "\n",
        "- `distilbert-base-uncased-finetuned-sst-2-english` → used in **text-classification**\n",
        "- `distilbert-base-uncased-distilled-squad` → used in **question-answering**\n",
        "\n",
        "So what’s going on here?\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 First: What is DistilBERT?\n",
        "\n",
        "**DistilBERT** is a smaller, faster version of **BERT**.  \n",
        "It was created using a technique called **knowledge distillation**, which \"compresses\" a large model into a lighter one with minimal loss in accuracy.\n",
        "\n",
        "| Feature        | BERT                      | DistilBERT                   |\n",
        "|----------------|---------------------------|------------------------------|\n",
        "| Size           | ~110M+ parameters         | ~66M parameters              |\n",
        "| Speed          | Slower                    | 60% faster inference         |\n",
        "| Accuracy       | Slightly higher           | Slightly reduced (~97%)      |\n",
        "| Use Case       | Best when performance > speed | Best when speed matters   |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Second: Decoding the Model Names\n",
        "\n",
        "### 1️⃣ `distilbert-base-uncased-finetuned-sst-2-english`\n",
        "\n",
        "- `distilbert-base`: the **base DistilBERT model**\n",
        "- `uncased`: lowercase-only vocabulary (doesn’t distinguish between `Apple` and `apple`)\n",
        "- `finetuned-sst-2-english`: fine-tuned on the **SST-2 dataset** for **sentiment classification**\n",
        "\n",
        "✅ Ideal for **text-classification**  \n",
        "→ Returns \"POSITIVE\" or \"NEGATIVE\"\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ `distilbert-base-uncased-distilled-squad`\n",
        "\n",
        "- Same base model: `distilbert-base-uncased`\n",
        "- Fine-tuned on the **SQuAD dataset** (Stanford Question Answering Dataset)\n",
        "- Task: Answer questions from a given paragraph\n",
        "\n",
        "✅ Ideal for **question-answering**  \n",
        "→ Returns answers extracted from context\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 TL;DR\n",
        "\n",
        "| Model Name | Task | Dataset | Pipeline |\n",
        "|------------|------|---------|----------|\n",
        "| `distilbert-base-uncased-finetuned-sst-2-english` | Sentiment classification | SST-2 | `text-classification` |\n",
        "| `distilbert-base-uncased-distilled-squad` | QA (extractive) | SQuAD | `question-answering` |\n",
        "\n",
        "They share the same **core model architecture**, but are **fine-tuned for different tasks**.\n",
        "\n"
      ],
      "metadata": {
        "id": "SSF5rSqwJQyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧾 `summarization` Pipeline\n",
        "\n",
        "## 🧠 What It Does:\n",
        "This pipeline takes a **long input** and returns a **shorter version** that preserves the main ideas.\n",
        "\n",
        "> Think of it like an AI-powered TL;DR engine.\n",
        "\n",
        "It’s great for:\n",
        "- Articles\n",
        "- Reports\n",
        "- Emails\n",
        "- Transcripts\n",
        "\n",
        "---\n",
        "\n",
        "### 🧾 `summarization`\n",
        "- **Purpose**: Condense long text into a shorter summary\n",
        "- **Pipeline**: `summarization`\n",
        "- **Best For**: News articles, documentation, reports, lecture notes\n",
        "- **Example Models**: `t5-small`, `facebook/bart-large-cnn`\n",
        "- **Strengths**:\n",
        "  - Extracts core ideas from longer input\n",
        "  - Works well for compressing verbose content\n",
        "- **Limitations**:\n",
        "  - Can’t answer questions or follow instructions\n",
        "  - May hallucinate or miss nuance in very short or vague prompts\n",
        "---\n",
        "\n",
        "## ✅ Good Models\n",
        "- `facebook/bart-large-cnn`\n",
        "- `t5-small`, `t5-base`\n",
        "\n",
        "We’ll use `t5-small` for speed.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRaCDUrALGD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a small summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "# Good examples\n",
        "texts = [\n",
        "    (\n",
        "        \"Artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence, such as understanding language, recognizing patterns, and making decisions.\",\n",
        "        \"🤖 Summary of AI Definition\"\n",
        "    ),\n",
        "    (\n",
        "        \"In 1969, the first humans landed on the Moon as part of NASA’s Apollo 11 mission. Neil Armstrong became the first person to walk on the Moon, followed by Buzz Aldrin. This historic event marked a major milestone in space exploration.\",\n",
        "        \"🌕 Summary of Moon Landing\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"✅ Successful Examples Using `summarization`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for text, label in texts:\n",
        "    output = summarizer(text, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
        "    print(f\"{label}\\n📚 Input:\\n{text}\\n🧾 Summary:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBHTtznUIdd0",
        "outputId": "e3cae309-140e-4de0-ab69-ad0acfa7e4bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successful Examples Using `summarization`\n",
            "============================================================\n",
            "\n",
            "🤖 Summary of AI Definition\n",
            "📚 Input:\n",
            "Artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence, such as understanding language, recognizing patterns, and making decisions.\n",
            "🧾 Summary:\n",
            "artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence .\n",
            "============================================================\n",
            "\n",
            "🌕 Summary of Moon Landing\n",
            "📚 Input:\n",
            "In 1969, the first humans landed on the Moon as part of NASA’s Apollo 11 mission. Neil Armstrong became the first person to walk on the Moon, followed by Buzz Aldrin. This historic event marked a major milestone in space exploration.\n",
            "🧾 Summary:\n",
            "in 1969, the first humans landed on the moon as part of NASA's Apollo 11 mission . the event marked a major milestone in space exploration .\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fail cases — wrong tool for the job\n",
        "bad_prompts = [\n",
        "    (\"What is the capital of Canada?\", \"📍 Direct Q&A\"),\n",
        "    (\"Translate: I love space exploration.\", \"🌍 Translation Task\")\n",
        "]\n",
        "\n",
        "print(\"❌ Limitations of `summarization`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in bad_prompts:\n",
        "    output = summarizer(prompt, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
        "    print(f\"{label}\\n📝 Prompt:\\n{prompt}\\n🧾 Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYTs7HwtLjoY",
        "outputId": "049bf631-576f-4825-d485-43bb9af02b30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Limitations of `summarization`\n",
            "============================================================\n",
            "\n",
            "📍 Direct Q&A\n",
            "📝 Prompt:\n",
            "What is the capital of Canada?\n",
            "🧾 Output:\n",
            "what is the capital of Canada? if you live in a city in the u.s., it is a capital of the capital.\n",
            "============================================================\n",
            "\n",
            "🌍 Translation Task\n",
            "📝 Prompt:\n",
            "Translate: I love space exploration.\n",
            "🧾 Output:\n",
            "translation: I love space exploration . a new translation of the translation .\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 💬 `conversational` Pipeline\n",
        "\n",
        "### 🧠 What It Does:\n",
        "This pipeline is designed for **chat-like interactions**, where the model can **remember previous messages** and continue a dialogue.\n",
        "\n",
        "> It simulates a **conversation history** — you give it a list of messages and it replies accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "## 🗨️ Good For:\n",
        "- Chatbots\n",
        "- Virtual assistants\n",
        "- Multi-turn conversations\n",
        "- Social dialog modeling (like talking to a character)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Powered by models like:\n",
        "- `microsoft/DialoGPT-small`\n",
        "- `facebook/blenderbot-400M-distill`\n",
        "\n",
        "These models are **fine-tuned for dialogue**, not just text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "i715fkPdNfJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# !pip uninstall -y transformers\n",
        "# !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVK0EysbNvb6",
        "outputId": "b36d08b5-973b-47e4-fdb6-b3f3043b3388"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely — let’s break it down clearly and concisely:\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 `conversational` Pipeline (vs. `text-generation`)\n",
        "\n",
        "### 🎯 Purpose\n",
        "The `conversational` pipeline is built specifically to simulate **back-and-forth human conversations**, whereas `text-generation` is designed to **continue text from a prompt**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Key Differences\n",
        "\n",
        "| Feature                  | `conversational`                               | `text-generation`                        |\n",
        "|--------------------------|-----------------------------------------------|------------------------------------------|\n",
        "| **Goal**                 | Chat with memory                              | Generate open-ended text                 |\n",
        "| **Input Format**         | Dialogue turns (via `Conversation` object)    | One continuous prompt                    |\n",
        "| **Context Awareness**    | Maintains multi-turn memory                   | No built-in memory                       |\n",
        "| **Good For**             | Chatbots, virtual assistants, characters      | Storytelling, brainstorming, free text   |\n",
        "| **Typical Models**       | `DialoGPT`, `Blenderbot`                      | `GPT2`, `Falcon`, `LLaMA`                |\n",
        "| **Response Style**       | Conversational, short, casual                 | Flowing, longform, creative              |\n",
        "| **Trained On**           | Dialogue datasets like Reddit conversations   | Broad text (books, web, Wikipedia, etc.) |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Example Prompts\n",
        "\n",
        "| Input                           | `conversational` Response         | `text-generation` Response                        |\n",
        "|----------------------------------|------------------------------------|----------------------------------------------------|\n",
        "| “Hi there!”                     | “Hey! How can I help?”            | “Hi there! I’m writing to let you know about...”   |\n",
        "| “Tell me about AI agents.”     | “They’re used for automating tasks.” | “Tell me about AI agents. They are complex systems...” |\n",
        "| “How’s your day going?”        | “Pretty good! How about you?”     | “How’s your day going? The weather outside is...”  |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Summary\n",
        "\n",
        "- Use **`text-generation`** when you want **open-ended, creative writing**.\n",
        "- Use **`conversational`** when you want **chat-style, multi-turn dialogue** with a more casual, interactive tone.\n",
        "\n"
      ],
      "metadata": {
        "id": "y9TYUXK4RciB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# Load the conversational model\n",
        "chatbot = pipeline(\"conversational\", model=\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Create multi-turn conversations\n",
        "print(\"✅ Successful Examples Using `conversational`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "conv1 = Conversation(\"Hi there!\")\n",
        "chatbot(conv1)\n",
        "print(\"🗨️ 1st message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "conv1.add_user_input(\"How are you today?\")\n",
        "chatbot(conv1)\n",
        "print(\"🗨️ 2nd message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "conv1.add_user_input(\"What do you think about AI agents?\")\n",
        "chatbot(conv1)\n",
        "print(\"🗨️ 3rd message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "p7ieR9PoQgW7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fail_prompts = [\n",
        "    \"Translate this into Spanish: I love machine learning.\",\n",
        "    \"Summarize: Artificial intelligence is a field that focuses on...\",\n",
        "    \"What is the capital of France?\"\n",
        "]\n",
        "\n",
        "print(\"❌ Limitations of `conversational`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt in fail_prompts:\n",
        "    conv = Conversation(prompt)\n",
        "    chatbot(conv)\n",
        "    print(f\"📝 Prompt:\\n{prompt}\\n🤖 Output:\\n{conv.messages[-1]['content']}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "_0Nd5s66LnfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean widgets"
      ],
      "metadata": {
        "id": "lLcjxDRkSPWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/AI AGENTS/005_Pipelines_Models_Comparison.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"✅ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"✅ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"✅ Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDeC5ezqLnaR",
        "outputId": "2733bec1-c2e3-4483-d92f-f06e050f4592"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sC7E21uULnXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8VEfhUYLnUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}