{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsS4TWoK5unDVRsj7uWrdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/005_Pipelines_Models_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ü§ñ Hugging Face Pipelines: What They Do\n",
        "\n",
        "A **pipeline** in Hugging Face is a pre-wired wrapper that:\n",
        "1. Tokenizes your input\n",
        "2. Runs inference on a model\n",
        "3. Decodes the output into readable text\n",
        "\n",
        "Each pipeline is matched to a **task type** ‚Äî and you choose the pipeline **based on what you want the model to do**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Summary Table: Pipelines, Tasks, and Models\n",
        "\n",
        "| **Pipeline** | **What It Does** | **Example Use** | **Good Models** |\n",
        "|--------------|------------------|------------------|------------------|\n",
        "| `text-generation` | Predicts next tokens in freeform text | Autocomplete, writing assistants | `gpt2`, `falcon-rw-1b`, `DialoGPT` |\n",
        "| `text2text-generation` | Translates one text into another (task-following) | Instructions ‚Üí output, translation, summarization | `flan-t5-base`, `t5-small`, `bart-base` |\n",
        "| `text-classification` | Labels input text with a category | Sentiment, spam detection | `bert-base-uncased`, `distilbert-base-uncased` |\n",
        "| `question-answering` | Answers a question given a context | \"What is his name?\" ‚Üí from a paragraph | `bert-large-uncased-whole-word-masking-finetuned-squad` |\n",
        "| `summarization` | Condenses input text to a shorter version | News summaries, TL;DR | `bart-large-cnn`, `t5-base` |\n",
        "| `translation` | Converts text from one language to another | English ‚Üí French | `t5`, `mbart-large-50`, `opus-mt-en-fr` |\n",
        "| `conversational` | Handles back-and-forth dialogue with memory | Chatbots with context | `DialoGPT`, `Blenderbot` |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Quick Cheat Sheet\n",
        "\n",
        "### üß† `text-generation`\n",
        "- **Goal**: Complete a sentence, story, or thought\n",
        "- **Think**: \"Once upon a time...\"\n",
        "- **Style**: Open-ended, creative\n",
        "- **Model style**: GPT-style\n",
        "- ‚úÖ Use when you don‚Äôt need strict structure\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `text2text-generation`\n",
        "- **Goal**: Follow instructions and output something specific\n",
        "- **Think**: ‚ÄúSummarize this article‚Äù or ‚ÄúTranslate this to French‚Äù\n",
        "- **Style**: Precise, controllable\n",
        "- **Model style**: T5-style (encoder-decoder)\n",
        "- ‚úÖ Best for AI agents, tools, workflows\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `text-classification`\n",
        "- **Goal**: Predict a label (positive/negative, topic, intent)\n",
        "- **Style**: Single answer from fixed set\n",
        "- ‚úÖ Use when the output should be one of a few choices\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `question-answering`\n",
        "- **Goal**: Find answer **in a given paragraph**\n",
        "- **Style**: Not generative ‚Äî it **extracts** answers from provided context\n",
        "- ‚úÖ Use in retrieval-augmented generation or open-book QA\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `summarization`\n",
        "- **Goal**: Shorten text while preserving meaning\n",
        "- ‚úÖ Use for reports, articles, transcripts\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `translation`\n",
        "- **Goal**: Translate between languages\n",
        "- ‚úÖ Specialized for multilingual support\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `conversational`\n",
        "- **Goal**: Maintain turn-by-turn memory across chats\n",
        "- ‚úÖ Use when you need memory in an assistant or chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Which Pipeline for Agents?\n",
        "\n",
        "| Agent Behavior | Best Pipeline | Why |\n",
        "|----------------|---------------|-----|\n",
        "| Follow instructions (tool use, info extraction) | `text2text-generation` | Clear prompt + clean output |\n",
        "| Freeform conversation or idea expansion | `text-generation` | Open-ended creativity |\n",
        "| Routing or intent detection | `text-classification` | You get structured labels |\n",
        "| Summarize large blocks of text | `summarization` | Less creative, more compression |\n",
        "| FAQ-style with a document | `question-answering` | Great with context chunks |\n",
        "\n"
      ],
      "metadata": {
        "id": "8AYbYkhMAiSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers huggingface_hub"
      ],
      "metadata": {
        "id": "3INc2XE_BfDs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÆ `text-generation` Pipeline (GPT-style models)\n",
        "\n",
        "### ‚úÖ What It Does:\n",
        "This type of model is trained to **predict the next word** given what came before. It‚Äôs not trying to follow instructions ‚Äî it‚Äôs just continuing the flow of text.\n",
        "\n",
        "Think of it as:\n",
        "> üß† \"Complete this sentence like a human would.\"\n",
        "\n",
        "Each of these will return something reasonable ‚Äî but maybe a little unpredictable or unstructured.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xIZy6qO7Btg9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFBAys7DAhvG",
        "outputId": "914d4d80-fff0-4f65-d3ef-41412c9b8a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1Ô∏è‚É£ Completion:\n",
            " The future of AI agents is murky, but a very real possibility,\" said Lawrence Berkeley National Laboratory professor and co-author Richard Moore, who holds a Ph.D. in computational\n",
            "2Ô∏è‚É£ Story:\n",
            " Once upon a time in a world powered by AI, it's difficult to avoid. And if they can't catch us in sight through AI, they are too.\n",
            "\n",
            "When you make a mistake,\n",
            "3Ô∏è‚É£ List:\n",
            " Three reasons to use AI agents are:\n",
            "\n",
            "Autoworking and AI agents must be configured based on the algorithm and are very efficient in the short-term.\n",
            "\n",
            "Autow\n"
          ]
        }
      ],
      "source": [
        "# supress warning\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a small text-generation model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# 1. Autocomplete a thought\n",
        "prompt = \"The future of AI agents is\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"1Ô∏è‚É£ Completion:\\n\", output)\n",
        "\n",
        "# 2. Start a story\n",
        "prompt = \"Once upon a time in a world powered by AI,\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"2Ô∏è‚É£ Story:\\n\", output)\n",
        "\n",
        "# 3. Give a list of ideas\n",
        "prompt = \"Three reasons to use AI agents are:\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(\"3Ô∏è‚É£ List:\\n\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî• Now Let's Test Its Limits\n",
        "\n",
        "### üò¨ Where `text-generation` *Fails*\n",
        "\n",
        "#### ‚ùå 1. Follow Instructions (It won't!)\n",
        "\n",
        "```python\n",
        "prompt = \"Translate this sentence into French: I love machine learning.\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(output)\n",
        "```\n",
        "\n",
        "‚Üí You‚Äôll probably get something like:\n",
        "\n",
        "> Translate this sentence into French: I love machine learning. It‚Äôs a fascinating field that‚Ä¶\n",
        "\n",
        "üôÖ It **does not translate** ‚Äî it continues the thought.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚ùå 2. Extract Information or Answer Questions Cleanly\n",
        "\n",
        "```python\n",
        "prompt = \"What is the capital of France?\"\n",
        "output = generator(prompt, max_new_tokens=30)[0][\"generated_text\"]\n",
        "print(output)\n",
        "```\n",
        "\n",
        "‚Üí It might say something **correct**, but it could also say:\n",
        "\n",
        "> What is the capital of France? The capital of France has long been known for its...\n",
        "\n",
        "üôÖ It wanders. No guarantee of a clean answer like `\"Paris\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why This Happens\n",
        "\n",
        "- GPT-style models are **trained to generate flowing language**, not follow specific instructions.\n",
        "- There's no native structure to the response.\n",
        "- It doesn‚Äôt ‚Äúunderstand‚Äù your prompt as a task.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Where `text2text-generation` Shines\n",
        "\n",
        "Now you‚Äôll see:\n",
        "- Instruction-following\n",
        "- Clean format\n",
        "- Task-specific outputs\n",
        "\n",
        "For example:\n",
        "\n",
        "| Task | GPT2 (text-gen) | FLAN-T5 (text2text-gen) |\n",
        "|------|------------------|--------------------------|\n",
        "| Translate to French | Wanders off | `\"J'aime l'apprentissage automatique\"` |\n",
        "| Answer a direct question | Talks in circles | `\"Paris\"` |\n",
        "| Rephrase this sentence | Just continues | Rephrases as asked |\n"
      ],
      "metadata": {
        "id": "MxShpGfnD691"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Translation attempt\n",
        "prompt1 = \"Translate this sentence into French: I love machine learning.\"\n",
        "output1 = generator(prompt1, max_new_tokens=30)[0][\"generated_text\"]\n",
        "\n",
        "# Example 2: Question answering attempt\n",
        "prompt2 = \"What is the capital of France?\"\n",
        "output2 = generator(prompt2, max_new_tokens=30)[0][\"generated_text\"]\n",
        "\n",
        "# Pretty print\n",
        "print(\"‚ùå Example 1: Translation with text-generation\")\n",
        "print(\"üìù Prompt:\\n\", prompt1)\n",
        "print(\"ü§ñ Output:\\n\", output1)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"‚ùå Example 2: Question answering with text-generation\")\n",
        "print(\"üìù Prompt:\\n\", prompt2)\n",
        "print(\"ü§ñ Output:\\n\", output2)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MvcCW14D-5d",
        "outputId": "2eb06eba-efdc-4086-9b11-a6feb0d55736"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Example 1: Translation with text-generation\n",
            "üìù Prompt:\n",
            " Translate this sentence into French: I love machine learning.\n",
            "ü§ñ Output:\n",
            " Translate this sentence into French: I love machine learning.\n",
            "\n",
            "I love machine learning.\n",
            "\n",
            "Because Machine Learning is a powerful tool, it's always worth looking at those examples of it in a broader\n",
            "\n",
            "============================================================\n",
            "\n",
            "‚ùå Example 2: Question answering with text-generation\n",
            "üìù Prompt:\n",
            " What is the capital of France?\n",
            "ü§ñ Output:\n",
            " What is the capital of France?\n",
            "\n",
            "Let me say that I've seen that capital in France from the French side, where they were a real hard-working country, not a\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üìö Understanding `text-generation` vs. `text2text-generation`\n",
        "\n",
        "### üîÆ `text-generation`\n",
        "- **Model Type**: GPT-style (causal language models)\n",
        "- **Pipeline**: `text-generation`\n",
        "- **Purpose**: Freeform text continuation\n",
        "- **Strengths**:\n",
        "  - Great for creative writing, storytelling, or idea expansion\n",
        "  - Can generate flowing, human-like text\n",
        "- **Limitations**:\n",
        "  - Does *not* follow instructions reliably\n",
        "  - Unstructured output ‚Äî hard to parse or control\n",
        "  - Poor at task-specific prompts like ‚Äútranslate this‚Äù or ‚Äúsummarize this‚Äù\n",
        "\n",
        "> **Examples of good use**:  \n",
        "> `\"Once upon a time...\"` ‚Üí continues the story  \n",
        "> `\"Write a fantasy scene about dragons...\"`\n",
        "\n",
        "---\n",
        "\n",
        "### üß† `text2text-generation`\n",
        "- **Model Type**: T5, FLAN, BART (encoder-decoder)\n",
        "- **Pipeline**: `text2text-generation`\n",
        "- **Purpose**: Instruction following ‚Äî turns input text into output text\n",
        "- **Strengths**:\n",
        "  - Designed for structured tasks like translation, summarization, rephrasing, Q&A\n",
        "  - More reliable and precise than `text-generation`\n",
        "  - Better suited for use in AI agents and task automation\n",
        "- **Limitations**:\n",
        "  - Not great for longform creative writing\n",
        "  - Tends to give short, controlled responses (not ideal for open-ended generation)\n",
        "\n",
        "> **Examples of good use**:  \n",
        "> `\"Translate this into French: I love AI\"`  \n",
        "> `\"Summarize this paragraph...\"`  \n",
        "> `\"What is the capital of France?\"` ‚Üí `\"Paris\"`\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Summary\n",
        "| Feature                 | `text-generation`        | `text2text-generation`        |\n",
        "|------------------------|--------------------------|-------------------------------|\n",
        "| Style                  | Open-ended               | Instruction-following         |\n",
        "| Structure              | Loose, unpredictable     | Clean and controlled           |\n",
        "| Best for               | Storytelling, creative    | Translation, summarization     |\n",
        "| Output Format          | Natural continuation     | Task-specific response         |\n",
        "| Example Model          | `gpt2`                   | `flan-t5-base`, `t5-small`     |\n",
        "| Common Mistake         | Tries to ‚Äúthink aloud‚Äù   | Tries to be brief and literal  |\n",
        "\n"
      ],
      "metadata": {
        "id": "6xSl8prpGTQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an instruction-following model\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# --- Successful Prompts ---\n",
        "prompts = [\n",
        "    (\"Translate this sentence into French: I love machine learning.\", \"üåç Translation\"),\n",
        "    (\"What is the capital of France?\", \"üìç Direct Question Answering\"),\n",
        "    (\"Summarize: Artificial intelligence is a branch of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence.\", \"üßæ Summarization\"),\n",
        "    (\"Rephrase: I am not sure if I can help you.\", \"üó£Ô∏è Rephrasing\")\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Successful Examples Using `text2text-generation`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in prompts:\n",
        "    output = generator(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
        "    print(f\"{label}\\nüìù Prompt:\\n{prompt}\\nü§ñ Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuw_otftFycc",
        "outputId": "3b9d0c28-fbd4-458e-8df6-708c51709c14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successful Examples Using `text2text-generation`\n",
            "============================================================\n",
            "\n",
            "üåç Translation\n",
            "üìù Prompt:\n",
            "Translate this sentence into French: I love machine learning.\n",
            "ü§ñ Output:\n",
            "J'aime l'apprentissage de la machine.\n",
            "============================================================\n",
            "\n",
            "üìç Direct Question Answering\n",
            "üìù Prompt:\n",
            "What is the capital of France?\n",
            "ü§ñ Output:\n",
            "london\n",
            "============================================================\n",
            "\n",
            "üßæ Summarization\n",
            "üìù Prompt:\n",
            "Summarize: Artificial intelligence is a branch of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence.\n",
            "ü§ñ Output:\n",
            "Understand the science behind artificial intelligence.\n",
            "============================================================\n",
            "\n",
            "üó£Ô∏è Rephrasing\n",
            "üìù Prompt:\n",
            "Rephrase: I am not sure if I can help you.\n",
            "ü§ñ Output:\n",
            "I am not sure if I can help you.\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompts text2text may struggle with ---\n",
        "fail_prompts = [\n",
        "    (\"Continue the story: Once upon a time in a world of intelligent machines,\", \"üìñ Freeform Storytelling\"),\n",
        "    (\"Write a poem about space exploration.\", \"üßë‚ÄçüöÄ Creative Writing\")\n",
        "]\n",
        "\n",
        "print(\"‚ùå Limitations of `text2text-generation`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in fail_prompts:\n",
        "    output = generator(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
        "    print(f\"{label}\\nüìù Prompt:\\n{prompt}\\nü§ñ Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKnGjvg9FyZU",
        "outputId": "5aaa4dba-45d2-48a4-ed89-65de807a1a4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Limitations of `text2text-generation`\n",
            "============================================================\n",
            "\n",
            "üìñ Freeform Storytelling\n",
            "üìù Prompt:\n",
            "Continue the story: Once upon a time in a world of intelligent machines,\n",
            "ü§ñ Output:\n",
            "a robot was able to make a robot.\n",
            "============================================================\n",
            "\n",
            "üßë‚ÄçüöÄ Creative Writing\n",
            "üìù Prompt:\n",
            "Write a poem about space exploration.\n",
            "ü§ñ Output:\n",
            "i was a kid and i was a teenager and i was a kid and i was a kid and i was a kid and i was a kid and i was a kid and i was a kid and i was \n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üè∑Ô∏è `text-classification` Pipeline\n",
        "\n",
        "## üß† What It Does:\n",
        "This model doesn't generate text ‚Äî instead, it **labels** text with a category.  \n",
        "You give it something to read, and it tells you what ‚Äúkind‚Äù of thing it is.\n",
        "\n",
        "- **Purpose**: Predict a category or label from text\n",
        "- **Pipeline**: `text-classification`\n",
        "- **Best For**: Sentiment analysis, spam detection, topic labeling\n",
        "- **Example Models**: `bert-base-uncased`, `distilbert-base-uncased`\n",
        "- **Strengths**:\n",
        "  - Fast and highly accurate for fixed-label tasks\n",
        "  - Works out of the box for binary classification (e.g., positive/negative)\n",
        "- **Limitations**:\n",
        "  - Cannot generate output\n",
        "  - Useless for translation, summarization, or reasoning\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Use Cases\n",
        "- Sentiment analysis (positive/negative)\n",
        "- Spam detection\n",
        "- Topic classification\n",
        "- Intent detection\n"
      ],
      "metadata": {
        "id": "DEq3iJ2KHCe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a simple classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Success cases\n",
        "prompts = [\n",
        "    (\"I absolutely love this new AI assistant!\", \"üòä Positive Sentiment\"),\n",
        "    (\"This is the worst update I‚Äôve ever seen.\", \"üò† Negative Sentiment\"),\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Successful Examples Using `text-classification`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in prompts:\n",
        "    output = classifier(prompt)[0]\n",
        "    print(f\"{label}\\nüìù Prompt:\\n{prompt}\\nüè∑Ô∏è Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3UztvedFyWf",
        "outputId": "2315ecd2-58d6-4f4d-a35d-836f02e3228d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successful Examples Using `text-classification`\n",
            "============================================================\n",
            "\n",
            "üòä Positive Sentiment\n",
            "üìù Prompt:\n",
            "I absolutely love this new AI assistant!\n",
            "üè∑Ô∏è Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9998743534088135}\n",
            "============================================================\n",
            "\n",
            "üò† Negative Sentiment\n",
            "üìù Prompt:\n",
            "This is the worst update I‚Äôve ever seen.\n",
            "üè∑Ô∏è Output:\n",
            "{'label': 'NEGATIVE', 'score': 0.9997844099998474}\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fail cases: tasks that require actual output generation\n",
        "fail_prompts = [\n",
        "    (\"Translate this sentence into Spanish: I love data science.\", \"üåç Translation\"),\n",
        "    (\"Summarize: Artificial intelligence is a field focused on building intelligent machines...\", \"üßæ Summarization\")\n",
        "]\n",
        "\n",
        "print(\"‚ùå Limitations of `text-classification`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in fail_prompts:\n",
        "    output = classifier(prompt)[0]\n",
        "    print(f\"{label}\\nüìù Prompt:\\n{prompt}\\nüè∑Ô∏è Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA69InQIFyNG",
        "outputId": "297b02dc-7d0d-4d7c-9143-88c9d7513a2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Limitations of `text-classification`\n",
            "============================================================\n",
            "\n",
            "üåç Translation\n",
            "üìù Prompt:\n",
            "Translate this sentence into Spanish: I love data science.\n",
            "üè∑Ô∏è Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9994205236434937}\n",
            "============================================================\n",
            "\n",
            "üßæ Summarization\n",
            "üìù Prompt:\n",
            "Summarize: Artificial intelligence is a field focused on building intelligent machines...\n",
            "üè∑Ô∏è Output:\n",
            "{'label': 'POSITIVE', 'score': 0.9885655045509338}\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚ùì `question-answering` Pipeline\n",
        "\n",
        "## üß† What It Does:\n",
        "Unlike chatbots or text generation, this pipeline **answers a question using a provided context**.  \n",
        "It doesn‚Äôt guess ‚Äî it **extracts** the answer from a given passage.\n",
        "\n",
        "> üß† Think of it as:  \n",
        "> ‚ÄúRead this paragraph and find the answer *inside it*.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Good For:\n",
        "- Closed-book question answering\n",
        "- FAQ bots with known documents\n",
        "- Extractive QA in RAG systems (Retrieval-Augmented Generation)\n",
        "\n",
        "### ‚ùì `question-answering`\n",
        "- **Purpose**: Extract an answer from a provided context paragraph\n",
        "- **Pipeline**: `question-answering`\n",
        "- **Best For**: FAQs, document-based QA, extractive RAG systems\n",
        "- **Example Models**: `bert-large-uncased-whole-word-masking-finetuned-squad`, `distilbert-base-uncased-distilled-squad`\n",
        "- **Strengths**:\n",
        "  - Great for ‚Äúopen-book‚Äù style QA\n",
        "  - Fast and accurate when context is included\n",
        "- **Limitations**:\n",
        "  - Fails without a context\n",
        "  - Cannot answer general or creative questions"
      ],
      "metadata": {
        "id": "kMXUvR9wIStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained QA model\n",
        "qa = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Provide context and ask a question\n",
        "examples = [\n",
        "    (\n",
        "        \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\",\n",
        "        \"Who was Marie Curie?\",\n",
        "        \"üß¨ Biography Fact\"\n",
        "    ),\n",
        "    (\n",
        "        \"The capital of France is Paris. It is known for its culture, art, and landmarks like the Eiffel Tower.\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"üó∫Ô∏è Simple Fact Extraction\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Successful Examples Using `question-answering`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for context, question, label in examples:\n",
        "    output = qa(question=question, context=context)\n",
        "    print(f\"{label}\\n‚ùì Question: {question}\\nüìö Context: {context}\\n‚úÖ Answer: {output['answer']}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOchQK_BIacC",
        "outputId": "beb2875b-5242-4857-b0a9-986fa3940224"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successful Examples Using `question-answering`\n",
            "============================================================\n",
            "\n",
            "üß¨ Biography Fact\n",
            "‚ùì Question: Who was Marie Curie?\n",
            "üìö Context: Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\n",
            "‚úÖ Answer: a physicist and chemist\n",
            "============================================================\n",
            "\n",
            "üó∫Ô∏è Simple Fact Extraction\n",
            "‚ùì Question: What is the capital of France?\n",
            "üìö Context: The capital of France is Paris. It is known for its culture, art, and landmarks like the Eiffel Tower.\n",
            "‚úÖ Answer: Paris\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fail_examples = [\n",
        "    (\"What is the meaning of life?\", \"\", \"üåå Philosophical Question\"),\n",
        "    (\"Who is the current president of the United States?\", \"\", \"üì∞ Real-time Fact\"),\n",
        "]\n",
        "\n",
        "print(\"‚ùå Limitations of `question-answering`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for question, context, label in fail_examples:\n",
        "    try:\n",
        "        output = qa(question=question, context=context)\n",
        "        print(f\"{label}\\n‚ùì Question: {question}\\nüìö Context: [EMPTY]\\n‚ùå Output: {output['answer']}\\n\" + \"=\"*60 + \"\\n\")\n",
        "    except ValueError:\n",
        "        print(f\"{label}\\n‚ùì Question: {question}\\nüìö Context: [EMPTY]\\n‚ö†Ô∏è Skipped: Question-answering requires context.\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb3A1xFmKquc",
        "outputId": "4d5dff6c-7d69-4fd7-a5e2-cde092bf3791"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Limitations of `question-answering`\n",
            "============================================================\n",
            "\n",
            "üåå Philosophical Question\n",
            "‚ùì Question: What is the meaning of life?\n",
            "üìö Context: [EMPTY]\n",
            "‚ö†Ô∏è Skipped: Question-answering requires context.\n",
            "============================================================\n",
            "\n",
            "üì∞ Real-time Fact\n",
            "‚ùì Question: Who is the current president of the United States?\n",
            "üìö Context: [EMPTY]\n",
            "‚ö†Ô∏è Skipped: Question-answering requires context.\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ö†Ô∏è The `question-answering` pipeline cannot work without a context paragraph.\n",
        "> If you provide an empty string or leave it out, the model will raise an error:\n",
        "> `ValueError: context cannot be empty`\n",
        ">\n",
        "> This reinforces that QA models are extractive ‚Äî they don‚Äôt guess, they **retrieve** answers from known input.\n"
      ],
      "metadata": {
        "id": "3lUKeVuxMtF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¨ Model Naming: `distilbert-base-uncased-‚Ä¶`\n",
        "\n",
        "You're seeing:\n",
        "\n",
        "- `distilbert-base-uncased-finetuned-sst-2-english` ‚Üí used in **text-classification**\n",
        "- `distilbert-base-uncased-distilled-squad` ‚Üí used in **question-answering**\n",
        "\n",
        "So what‚Äôs going on here?\n",
        "\n",
        "---\n",
        "\n",
        "## üß† First: What is DistilBERT?\n",
        "\n",
        "**DistilBERT** is a smaller, faster version of **BERT**.  \n",
        "It was created using a technique called **knowledge distillation**, which \"compresses\" a large model into a lighter one with minimal loss in accuracy.\n",
        "\n",
        "| Feature        | BERT                      | DistilBERT                   |\n",
        "|----------------|---------------------------|------------------------------|\n",
        "| Size           | ~110M+ parameters         | ~66M parameters              |\n",
        "| Speed          | Slower                    | 60% faster inference         |\n",
        "| Accuracy       | Slightly higher           | Slightly reduced (~97%)      |\n",
        "| Use Case       | Best when performance > speed | Best when speed matters   |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Second: Decoding the Model Names\n",
        "\n",
        "### 1Ô∏è‚É£ `distilbert-base-uncased-finetuned-sst-2-english`\n",
        "\n",
        "- `distilbert-base`: the **base DistilBERT model**\n",
        "- `uncased`: lowercase-only vocabulary (doesn‚Äôt distinguish between `Apple` and `apple`)\n",
        "- `finetuned-sst-2-english`: fine-tuned on the **SST-2 dataset** for **sentiment classification**\n",
        "\n",
        "‚úÖ Ideal for **text-classification**  \n",
        "‚Üí Returns \"POSITIVE\" or \"NEGATIVE\"\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ `distilbert-base-uncased-distilled-squad`\n",
        "\n",
        "- Same base model: `distilbert-base-uncased`\n",
        "- Fine-tuned on the **SQuAD dataset** (Stanford Question Answering Dataset)\n",
        "- Task: Answer questions from a given paragraph\n",
        "\n",
        "‚úÖ Ideal for **question-answering**  \n",
        "‚Üí Returns answers extracted from context\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ TL;DR\n",
        "\n",
        "| Model Name | Task | Dataset | Pipeline |\n",
        "|------------|------|---------|----------|\n",
        "| `distilbert-base-uncased-finetuned-sst-2-english` | Sentiment classification | SST-2 | `text-classification` |\n",
        "| `distilbert-base-uncased-distilled-squad` | QA (extractive) | SQuAD | `question-answering` |\n",
        "\n",
        "They share the same **core model architecture**, but are **fine-tuned for different tasks**.\n",
        "\n"
      ],
      "metadata": {
        "id": "SSF5rSqwJQyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üßæ `summarization` Pipeline\n",
        "\n",
        "## üß† What It Does:\n",
        "This pipeline takes a **long input** and returns a **shorter version** that preserves the main ideas.\n",
        "\n",
        "> Think of it like an AI-powered TL;DR engine.\n",
        "\n",
        "It‚Äôs great for:\n",
        "- Articles\n",
        "- Reports\n",
        "- Emails\n",
        "- Transcripts\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ `summarization`\n",
        "- **Purpose**: Condense long text into a shorter summary\n",
        "- **Pipeline**: `summarization`\n",
        "- **Best For**: News articles, documentation, reports, lecture notes\n",
        "- **Example Models**: `t5-small`, `facebook/bart-large-cnn`\n",
        "- **Strengths**:\n",
        "  - Extracts core ideas from longer input\n",
        "  - Works well for compressing verbose content\n",
        "- **Limitations**:\n",
        "  - Can‚Äôt answer questions or follow instructions\n",
        "  - May hallucinate or miss nuance in very short or vague prompts\n",
        "---\n",
        "\n",
        "## ‚úÖ Good Models\n",
        "- `facebook/bart-large-cnn`\n",
        "- `t5-small`, `t5-base`\n",
        "\n",
        "We‚Äôll use `t5-small` for speed.\n",
        "\n"
      ],
      "metadata": {
        "id": "eRaCDUrALGD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a small summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "# Good examples\n",
        "texts = [\n",
        "    (\n",
        "        \"Artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence, such as understanding language, recognizing patterns, and making decisions.\",\n",
        "        \"ü§ñ Summary of AI Definition\"\n",
        "    ),\n",
        "    (\n",
        "        \"In 1969, the first humans landed on the Moon as part of NASA‚Äôs Apollo 11 mission. Neil Armstrong became the first person to walk on the Moon, followed by Buzz Aldrin. This historic event marked a major milestone in space exploration.\",\n",
        "        \"üåï Summary of Moon Landing\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Successful Examples Using `summarization`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for text, label in texts:\n",
        "    output = summarizer(text, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
        "    print(f\"{label}\\nüìö Input:\\n{text}\\nüßæ Summary:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBHTtznUIdd0",
        "outputId": "e3cae309-140e-4de0-ab69-ad0acfa7e4bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successful Examples Using `summarization`\n",
            "============================================================\n",
            "\n",
            "ü§ñ Summary of AI Definition\n",
            "üìö Input:\n",
            "Artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence, such as understanding language, recognizing patterns, and making decisions.\n",
            "üßæ Summary:\n",
            "artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence .\n",
            "============================================================\n",
            "\n",
            "üåï Summary of Moon Landing\n",
            "üìö Input:\n",
            "In 1969, the first humans landed on the Moon as part of NASA‚Äôs Apollo 11 mission. Neil Armstrong became the first person to walk on the Moon, followed by Buzz Aldrin. This historic event marked a major milestone in space exploration.\n",
            "üßæ Summary:\n",
            "in 1969, the first humans landed on the moon as part of NASA's Apollo 11 mission . the event marked a major milestone in space exploration .\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fail cases ‚Äî wrong tool for the job\n",
        "bad_prompts = [\n",
        "    (\"What is the capital of Canada?\", \"üìç Direct Q&A\"),\n",
        "    (\"Translate: I love space exploration.\", \"üåç Translation Task\")\n",
        "]\n",
        "\n",
        "print(\"‚ùå Limitations of `summarization`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt, label in bad_prompts:\n",
        "    output = summarizer(prompt, max_length=40, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
        "    print(f\"{label}\\nüìù Prompt:\\n{prompt}\\nüßæ Output:\\n{output}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYTs7HwtLjoY",
        "outputId": "049bf631-576f-4825-d485-43bb9af02b30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Limitations of `summarization`\n",
            "============================================================\n",
            "\n",
            "üìç Direct Q&A\n",
            "üìù Prompt:\n",
            "What is the capital of Canada?\n",
            "üßæ Output:\n",
            "what is the capital of Canada? if you live in a city in the u.s., it is a capital of the capital.\n",
            "============================================================\n",
            "\n",
            "üåç Translation Task\n",
            "üìù Prompt:\n",
            "Translate: I love space exploration.\n",
            "üßæ Output:\n",
            "translation: I love space exploration . a new translation of the translation .\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üí¨ `conversational` Pipeline\n",
        "\n",
        "### üß† What It Does:\n",
        "This pipeline is designed for **chat-like interactions**, where the model can **remember previous messages** and continue a dialogue.\n",
        "\n",
        "> It simulates a **conversation history** ‚Äî you give it a list of messages and it replies accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "## üó®Ô∏è Good For:\n",
        "- Chatbots\n",
        "- Virtual assistants\n",
        "- Multi-turn conversations\n",
        "- Social dialog modeling (like talking to a character)\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Powered by models like:\n",
        "- `microsoft/DialoGPT-small`\n",
        "- `facebook/blenderbot-400M-distill`\n",
        "\n",
        "These models are **fine-tuned for dialogue**, not just text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "i715fkPdNfJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "# !pip uninstall -y transformers\n",
        "# !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVK0EysbNvb6",
        "outputId": "b36d08b5-973b-47e4-fdb6-b3f3043b3388"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely ‚Äî let‚Äôs break it down clearly and concisely:\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ `conversational` Pipeline (vs. `text-generation`)\n",
        "\n",
        "### üéØ Purpose\n",
        "The `conversational` pipeline is built specifically to simulate **back-and-forth human conversations**, whereas `text-generation` is designed to **continue text from a prompt**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Differences\n",
        "\n",
        "| Feature                  | `conversational`                               | `text-generation`                        |\n",
        "|--------------------------|-----------------------------------------------|------------------------------------------|\n",
        "| **Goal**                 | Chat with memory                              | Generate open-ended text                 |\n",
        "| **Input Format**         | Dialogue turns (via `Conversation` object)    | One continuous prompt                    |\n",
        "| **Context Awareness**    | Maintains multi-turn memory                   | No built-in memory                       |\n",
        "| **Good For**             | Chatbots, virtual assistants, characters      | Storytelling, brainstorming, free text   |\n",
        "| **Typical Models**       | `DialoGPT`, `Blenderbot`                      | `GPT2`, `Falcon`, `LLaMA`                |\n",
        "| **Response Style**       | Conversational, short, casual                 | Flowing, longform, creative              |\n",
        "| **Trained On**           | Dialogue datasets like Reddit conversations   | Broad text (books, web, Wikipedia, etc.) |\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example Prompts\n",
        "\n",
        "| Input                           | `conversational` Response         | `text-generation` Response                        |\n",
        "|----------------------------------|------------------------------------|----------------------------------------------------|\n",
        "| ‚ÄúHi there!‚Äù                     | ‚ÄúHey! How can I help?‚Äù            | ‚ÄúHi there! I‚Äôm writing to let you know about...‚Äù   |\n",
        "| ‚ÄúTell me about AI agents.‚Äù     | ‚ÄúThey‚Äôre used for automating tasks.‚Äù | ‚ÄúTell me about AI agents. They are complex systems...‚Äù |\n",
        "| ‚ÄúHow‚Äôs your day going?‚Äù        | ‚ÄúPretty good! How about you?‚Äù     | ‚ÄúHow‚Äôs your day going? The weather outside is...‚Äù  |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Summary\n",
        "\n",
        "- Use **`text-generation`** when you want **open-ended, creative writing**.\n",
        "- Use **`conversational`** when you want **chat-style, multi-turn dialogue** with a more casual, interactive tone.\n",
        "\n"
      ],
      "metadata": {
        "id": "y9TYUXK4RciB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# Load the conversational model\n",
        "chatbot = pipeline(\"conversational\", model=\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Create multi-turn conversations\n",
        "print(\"‚úÖ Successful Examples Using `conversational`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "conv1 = Conversation(\"Hi there!\")\n",
        "chatbot(conv1)\n",
        "print(\"üó®Ô∏è 1st message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "conv1.add_user_input(\"How are you today?\")\n",
        "chatbot(conv1)\n",
        "print(\"üó®Ô∏è 2nd message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "conv1.add_user_input(\"What do you think about AI agents?\")\n",
        "chatbot(conv1)\n",
        "print(\"üó®Ô∏è 3rd message:\", conv1.messages[-1]['content'])\n",
        "\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "p7ieR9PoQgW7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fail_prompts = [\n",
        "    \"Translate this into Spanish: I love machine learning.\",\n",
        "    \"Summarize: Artificial intelligence is a field that focuses on...\",\n",
        "    \"What is the capital of France?\"\n",
        "]\n",
        "\n",
        "print(\"‚ùå Limitations of `conversational`\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "for prompt in fail_prompts:\n",
        "    conv = Conversation(prompt)\n",
        "    chatbot(conv)\n",
        "    print(f\"üìù Prompt:\\n{prompt}\\nü§ñ Output:\\n{conv.messages[-1]['content']}\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "_0Nd5s66LnfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean widgets"
      ],
      "metadata": {
        "id": "lLcjxDRkSPWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/AI AGENTS/005_Pipelines_Models_Comparison.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"‚úÖ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"‚úÖ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDeC5ezqLnaR",
        "outputId": "2733bec1-c2e3-4483-d92f-f06e050f4592"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sC7E21uULnXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8VEfhUYLnUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}