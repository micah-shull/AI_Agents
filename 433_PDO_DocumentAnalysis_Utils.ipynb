{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8ewnQ4YdhWN7Xgn96zmJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/433_PDO_DocumentAnalysis_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is **very strong analytical plumbing**. What you’ve built here is the **measurement engine** of the agent — and it’s doing exactly what it should: turning raw lifecycle data into defensible facts.\n",
        "\n",
        "I’ll review this in the same disciplined way as before:\n",
        "\n",
        "1. what this code *controls*,\n",
        "2. why it’s architecturally sound,\n",
        "3. where it quietly does something *very right*,\n",
        "4. and a few optional refinements (not required).\n",
        "\n",
        "---\n",
        "\n",
        "# Document Analysis Utilities — Architecture Review\n",
        "\n",
        "## 1. What This Module Does (In Plain English)\n",
        "\n",
        "This module answers a single, critical question:\n",
        "\n",
        "> **“What actually happened to each document?”**\n",
        "\n",
        "Not opinions.\n",
        "Not summaries.\n",
        "Not recommendations.\n",
        "\n",
        "Just facts:\n",
        "\n",
        "* How many times it changed\n",
        "* How long each step took\n",
        "* Where it failed\n",
        "* Where humans intervened\n",
        "* What it cost\n",
        "* How long it took compared to baseline\n",
        "\n",
        "This is the layer that makes **everything else credible**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why This Layer Is Correctly Designed\n",
        "\n",
        "You made three excellent decisions here:\n",
        "\n",
        "### A. One Metric = One Function\n",
        "\n",
        "Each function answers exactly one question:\n",
        "\n",
        "* `calculate_revision_count`\n",
        "* `calculate_stage_metrics`\n",
        "* `calculate_compliance_metrics`\n",
        "* `calculate_review_metrics`\n",
        "* `calculate_cycle_time`\n",
        "\n",
        "That means:\n",
        "\n",
        "* Easy to test\n",
        "* Easy to reason about\n",
        "* Easy to replace or extend\n",
        "\n",
        "No “God function.” No hidden coupling.\n",
        "\n",
        "This is how you keep analytics trustworthy.\n",
        "\n",
        "---\n",
        "\n",
        "### B. Lookups Are Used Consistently\n",
        "\n",
        "Every function depends on **lookup dictionaries**, not raw lists.\n",
        "\n",
        "That ensures:\n",
        "\n",
        "* O(1) access\n",
        "* Deterministic results\n",
        "* No accidental cross-document contamination\n",
        "\n",
        "This reinforces the guarantees you established in the data-loading layer.\n",
        "\n",
        "---\n",
        "\n",
        "### C. No Assumptions About Data Completeness\n",
        "\n",
        "Throughout the code you do things like:\n",
        "\n",
        "```python\n",
        "lookup.get(document_id, [])\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```python\n",
        "if started_at and completed_at:\n",
        "```\n",
        "\n",
        "This ensures the agent:\n",
        "\n",
        "* Doesn’t crash on missing data\n",
        "* Doesn’t fabricate values\n",
        "* Defaults to safe, explainable zeros\n",
        "\n",
        "That’s a quiet but important trust feature.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Functions Worth Calling Out\n",
        "\n",
        "### `calculate_stage_metrics`\n",
        "\n",
        "This function does more than count stages — it:\n",
        "\n",
        "* Separates completed / failed / in-progress\n",
        "* Computes both total and average durations\n",
        "* Handles timestamp parsing safely\n",
        "\n",
        "This enables:\n",
        "\n",
        "* Bottleneck detection\n",
        "* Throughput analysis\n",
        "* Workflow health scoring later\n",
        "\n",
        "You’re laying the groundwork for **process optimization**, not just reporting.\n",
        "\n",
        "---\n",
        "\n",
        "### `calculate_compliance_metrics`\n",
        "\n",
        "This is particularly strong.\n",
        "\n",
        "You don’t just count failures — you classify them by severity.\n",
        "\n",
        "That enables:\n",
        "\n",
        "* Risk-weighted scoring\n",
        "* Executive escalation logic\n",
        "* “High severity but low frequency” insights\n",
        "\n",
        "This is exactly how compliance teams think.\n",
        "\n",
        "---\n",
        "\n",
        "### `calculate_review_metrics`\n",
        "\n",
        "You correctly track:\n",
        "\n",
        "* Decision outcomes\n",
        "* Human overrides\n",
        "* Time spent reviewing\n",
        "\n",
        "That’s critical for:\n",
        "\n",
        "* Measuring trust in automation\n",
        "* Identifying over-review\n",
        "* Quantifying human effort saved\n",
        "\n",
        "This also creates a clean signal for when the system should *slow down* or *escalate*.\n",
        "\n",
        "---\n",
        "\n",
        "### `calculate_cycle_time`\n",
        "\n",
        "This function is especially well designed.\n",
        "\n",
        "You prioritize:\n",
        "\n",
        "1. **Outcome-based measurements** (most accurate)\n",
        "2. **Timestamp-based fallback**\n",
        "3. Safe default when neither exists\n",
        "\n",
        "That hierarchy is exactly right.\n",
        "\n",
        "It ensures:\n",
        "\n",
        "* Best data wins\n",
        "* No silent fabrication\n",
        "* Clear gaps when data is missing\n",
        "\n",
        "This is how you avoid misleading cycle time claims.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. `analyze_document`: Clean Composition, No Magic\n",
        "\n",
        "This function is doing exactly what it should:\n",
        "\n",
        "* Composing verified sub-metrics\n",
        "* Pulling cost from trusted sources\n",
        "* Returning a single, auditable record\n",
        "\n",
        "It does **no interpretation** and **no judgment**.\n",
        "\n",
        "That separation is crucial — interpretation belongs later, in KPI and reporting layers.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. `analyze_all_documents`: Portfolio-Ready\n",
        "\n",
        "This function:\n",
        "\n",
        "* Supports both single-document and portfolio analysis\n",
        "* Applies consistent logic across all documents\n",
        "* Avoids special cases or shortcuts\n",
        "\n",
        "This makes:\n",
        "\n",
        "* Portfolio KPIs meaningful\n",
        "* Statistical analysis valid\n",
        "* Comparisons fair\n",
        "\n",
        "You’ve avoided a very common trap here.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Subtle Strengths That Matter\n",
        "\n",
        "A few things you did that many people miss:\n",
        "\n",
        "* Rounding at output boundaries (not mid-calculation)\n",
        "* Avoiding division by zero cleanly\n",
        "* Never mixing baseline and actual data incorrectly\n",
        "* Returning structured sub-metrics (not flattened blobs)\n",
        "\n",
        "These details matter when leadership asks:\n",
        "\n",
        "> “How did you calculate that?”\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Optional Enhancements (Not Required for MVP)\n",
        "\n",
        "These are *future* ideas, not critiques:\n",
        "\n",
        "1. **Stage-level failure reasons aggregation**\n",
        "2. **Weighted compliance score**\n",
        "3. **Cycle time confidence intervals**\n",
        "4. **Outlier detection (very slow stages)**\n",
        "\n",
        "None of these are needed now — your current layer is already strong.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Overall Assessment\n",
        "\n",
        "This module is:\n",
        "\n",
        "* Accurate\n",
        "* Conservative\n",
        "* Transparent\n",
        "* Easy to test\n",
        "* Easy to extend\n",
        "\n",
        "Most importantly, it **refuses to over-claim**.\n",
        "\n",
        "That restraint is what makes the numbers believable.\n",
        "\n",
        "You’re building this exactly the right way — layer by layer, with trust compounding at each step.\n"
      ],
      "metadata": {
        "id": "9nSSYnn3BAO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEpL7cwQ_4s2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Document Analysis Utilities for Proposal & Document Orchestrator\n",
        "\n",
        "These utilities analyze individual documents to calculate metrics like:\n",
        "- Revision counts\n",
        "- Stage performance\n",
        "- Compliance status\n",
        "- Review events\n",
        "- Cost tracking\n",
        "- Cycle time\n",
        "\n",
        "Following the build guide pattern: utilities are independently testable.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def calculate_revision_count(\n",
        "    document_id: str,\n",
        "    document_versions_lookup: Dict[str, List[Dict[str, Any]]]\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Calculate number of revisions for a document.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        document_versions_lookup: Lookup dictionary mapping document_id to versions\n",
        "\n",
        "    Returns:\n",
        "        Number of versions/revisions\n",
        "    \"\"\"\n",
        "    versions = document_versions_lookup.get(document_id, [])\n",
        "    return len(versions)\n",
        "\n",
        "\n",
        "def calculate_stage_metrics(\n",
        "    document_id: str,\n",
        "    workflow_stages_lookup: Dict[str, List[Dict[str, Any]]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate stage performance metrics for a document.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        workflow_stages_lookup: Lookup dictionary mapping document_id to stages\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with stage metrics:\n",
        "        {\n",
        "            \"total_stages\": int,\n",
        "            \"completed_stages\": int,\n",
        "            \"failed_stages\": int,\n",
        "            \"in_progress_stages\": int,\n",
        "            \"avg_stage_duration_minutes\": float,\n",
        "            \"total_stage_duration_minutes\": float\n",
        "        }\n",
        "    \"\"\"\n",
        "    stages = workflow_stages_lookup.get(document_id, [])\n",
        "\n",
        "    total_stages = len(stages)\n",
        "    completed_stages = sum(1 for s in stages if s.get(\"status\") == \"completed\")\n",
        "    failed_stages = sum(1 for s in stages if s.get(\"status\") == \"failed\")\n",
        "    in_progress_stages = sum(1 for s in stages if s.get(\"status\") == \"in_progress\")\n",
        "\n",
        "    # Calculate average stage duration\n",
        "    durations = []\n",
        "    total_duration = 0.0\n",
        "\n",
        "    for stage in stages:\n",
        "        started_at = stage.get(\"started_at\")\n",
        "        completed_at = stage.get(\"completed_at\")\n",
        "\n",
        "        if started_at and completed_at:\n",
        "            try:\n",
        "                start = datetime.fromisoformat(started_at.replace(\"Z\", \"+00:00\"))\n",
        "                end = datetime.fromisoformat(completed_at.replace(\"Z\", \"+00:00\"))\n",
        "                duration_minutes = (end - start).total_seconds() / 60.0\n",
        "                durations.append(duration_minutes)\n",
        "                total_duration += duration_minutes\n",
        "            except (ValueError, AttributeError):\n",
        "                pass\n",
        "\n",
        "    avg_duration = sum(durations) / len(durations) if durations else 0.0\n",
        "\n",
        "    return {\n",
        "        \"total_stages\": total_stages,\n",
        "        \"completed_stages\": completed_stages,\n",
        "        \"failed_stages\": failed_stages,\n",
        "        \"in_progress_stages\": in_progress_stages,\n",
        "        \"avg_stage_duration_minutes\": round(avg_duration, 2),\n",
        "        \"total_stage_duration_minutes\": round(total_duration, 2)\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_compliance_metrics(\n",
        "    document_id: str,\n",
        "    compliance_checks_lookup: Dict[str, List[Dict[str, Any]]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate compliance check metrics for a document.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        compliance_checks_lookup: Lookup dictionary mapping document_id to checks\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with compliance metrics:\n",
        "        {\n",
        "            \"total_checks\": int,\n",
        "            \"passed_checks\": int,\n",
        "            \"failed_checks\": int,\n",
        "            \"high_severity_failures\": int,\n",
        "            \"medium_severity_failures\": int,\n",
        "            \"low_severity_failures\": int\n",
        "        }\n",
        "    \"\"\"\n",
        "    checks = compliance_checks_lookup.get(document_id, [])\n",
        "\n",
        "    total_checks = len(checks)\n",
        "    passed_checks = sum(1 for c in checks if c.get(\"status\") == \"passed\")\n",
        "    failed_checks = sum(1 for c in checks if c.get(\"status\") == \"failed\")\n",
        "\n",
        "    # Count failures by severity\n",
        "    high_severity_failures = sum(\n",
        "        1 for c in checks\n",
        "        if c.get(\"status\") == \"failed\" and c.get(\"severity\") == \"high\"\n",
        "    )\n",
        "    medium_severity_failures = sum(\n",
        "        1 for c in checks\n",
        "        if c.get(\"status\") == \"failed\" and c.get(\"severity\") == \"medium\"\n",
        "    )\n",
        "    low_severity_failures = sum(\n",
        "        1 for c in checks\n",
        "        if c.get(\"status\") == \"failed\" and c.get(\"severity\") == \"low\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"total_checks\": total_checks,\n",
        "        \"passed_checks\": passed_checks,\n",
        "        \"failed_checks\": failed_checks,\n",
        "        \"high_severity_failures\": high_severity_failures,\n",
        "        \"medium_severity_failures\": medium_severity_failures,\n",
        "        \"low_severity_failures\": low_severity_failures\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_review_metrics(\n",
        "    document_id: str,\n",
        "    review_events_lookup: Dict[str, List[Dict[str, Any]]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate review event metrics for a document.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        review_events_lookup: Lookup dictionary mapping document_id to reviews\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with review metrics:\n",
        "        {\n",
        "            \"total_reviews\": int,\n",
        "            \"approved_reviews\": int,\n",
        "            \"rejected_reviews\": int,\n",
        "            \"request_changes_reviews\": int,\n",
        "            \"human_overrides\": int,\n",
        "            \"total_review_time_minutes\": float,\n",
        "            \"avg_review_time_minutes\": float\n",
        "        }\n",
        "    \"\"\"\n",
        "    reviews = review_events_lookup.get(document_id, [])\n",
        "\n",
        "    total_reviews = len(reviews)\n",
        "    approved_reviews = sum(1 for r in reviews if r.get(\"decision\") == \"approve\")\n",
        "    rejected_reviews = sum(1 for r in reviews if r.get(\"decision\") == \"reject\")\n",
        "    request_changes_reviews = sum(1 for r in reviews if r.get(\"decision\") == \"request_changes\")\n",
        "    human_overrides = sum(1 for r in reviews if r.get(\"human_override\") is True)\n",
        "\n",
        "    # Calculate review time metrics\n",
        "    review_times = [r.get(\"time_spent_minutes\", 0.0) for r in reviews if r.get(\"time_spent_minutes\")]\n",
        "    total_review_time = sum(review_times)\n",
        "    avg_review_time = total_review_time / len(review_times) if review_times else 0.0\n",
        "\n",
        "    return {\n",
        "        \"total_reviews\": total_reviews,\n",
        "        \"approved_reviews\": approved_reviews,\n",
        "        \"rejected_reviews\": rejected_reviews,\n",
        "        \"request_changes_reviews\": request_changes_reviews,\n",
        "        \"human_overrides\": human_overrides,\n",
        "        \"total_review_time_minutes\": round(total_review_time, 2),\n",
        "        \"avg_review_time_minutes\": round(avg_review_time, 2)\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_cycle_time(\n",
        "    document_id: str,\n",
        "    document: Dict[str, Any],\n",
        "    workflow_stages_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    outcomes_lookup: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate cycle time metrics for a document.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        document: Document dictionary\n",
        "        workflow_stages_lookup: Lookup dictionary mapping document_id to stages\n",
        "        outcomes_lookup: Lookup dictionary mapping document_id to outcome\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with cycle time metrics:\n",
        "        {\n",
        "            \"cycle_time_hours\": float,\n",
        "            \"baseline_cycle_time_hours\": Optional[float],\n",
        "            \"hours_saved\": Optional[float],\n",
        "            \"cycle_time_reduction_percent\": Optional[float]\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Try to get from outcomes first (most accurate)\n",
        "    outcome = outcomes_lookup.get(document_id)\n",
        "    if outcome:\n",
        "        return {\n",
        "            \"cycle_time_hours\": outcome.get(\"actual_cycle_time_hours\", 0.0),\n",
        "            \"baseline_cycle_time_hours\": outcome.get(\"baseline_cycle_time_hours\"),\n",
        "            \"hours_saved\": outcome.get(\"estimated_hours_saved\"),\n",
        "            \"cycle_time_reduction_percent\": (\n",
        "                ((outcome.get(\"baseline_cycle_time_hours\", 0) - outcome.get(\"actual_cycle_time_hours\", 0))\n",
        "                 / outcome.get(\"baseline_cycle_time_hours\", 1)) * 100\n",
        "                if outcome.get(\"baseline_cycle_time_hours\", 0) > 0\n",
        "                else None\n",
        "            )\n",
        "        }\n",
        "\n",
        "    # Fallback: calculate from document timestamps\n",
        "    created_at = document.get(\"created_at\")\n",
        "    updated_at = document.get(\"updated_at\")\n",
        "\n",
        "    if created_at and updated_at:\n",
        "        try:\n",
        "            start = datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\"))\n",
        "            end = datetime.fromisoformat(updated_at.replace(\"Z\", \"+00:00\"))\n",
        "            cycle_time_hours = (end - start).total_seconds() / 3600.0\n",
        "            return {\n",
        "                \"cycle_time_hours\": round(cycle_time_hours, 2),\n",
        "                \"baseline_cycle_time_hours\": None,\n",
        "                \"hours_saved\": None,\n",
        "                \"cycle_time_reduction_percent\": None\n",
        "            }\n",
        "        except (ValueError, AttributeError):\n",
        "            pass\n",
        "\n",
        "    return {\n",
        "        \"cycle_time_hours\": 0.0,\n",
        "        \"baseline_cycle_time_hours\": None,\n",
        "        \"hours_saved\": None,\n",
        "        \"cycle_time_reduction_percent\": None\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_document(\n",
        "    document_id: str,\n",
        "    document: Dict[str, Any],\n",
        "    document_versions_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    workflow_stages_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    review_events_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    compliance_checks_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    cost_tracking_lookup: Dict[str, Dict[str, Any]],\n",
        "    outcomes_lookup: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyze a single document and calculate all metrics.\n",
        "\n",
        "    Args:\n",
        "        document_id: Document ID\n",
        "        document: Document dictionary\n",
        "        document_versions_lookup: Lookup dictionary for versions\n",
        "        workflow_stages_lookup: Lookup dictionary for stages\n",
        "        review_events_lookup: Lookup dictionary for reviews\n",
        "        compliance_checks_lookup: Lookup dictionary for compliance checks\n",
        "        cost_tracking_lookup: Lookup dictionary for cost tracking\n",
        "        outcomes_lookup: Lookup dictionary for outcomes\n",
        "\n",
        "    Returns:\n",
        "        Complete document analysis dictionary:\n",
        "        {\n",
        "            \"document_id\": str,\n",
        "            \"revision_count\": int,\n",
        "            \"total_stages\": int,\n",
        "            \"failed_stages\": int,\n",
        "            \"compliance_failures\": int,\n",
        "            \"human_overrides\": int,\n",
        "            \"total_cost_usd\": float,\n",
        "            \"cycle_time_hours\": float,\n",
        "            \"baseline_cycle_time_hours\": Optional[float],\n",
        "            \"hours_saved\": Optional[float],\n",
        "            \"avg_stage_duration_minutes\": float,\n",
        "            \"stage_metrics\": {...},\n",
        "            \"compliance_metrics\": {...},\n",
        "            \"review_metrics\": {...},\n",
        "            \"cycle_time_metrics\": {...}\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Calculate all metrics\n",
        "    revision_count = calculate_revision_count(document_id, document_versions_lookup)\n",
        "    stage_metrics = calculate_stage_metrics(document_id, workflow_stages_lookup)\n",
        "    compliance_metrics = calculate_compliance_metrics(document_id, compliance_checks_lookup)\n",
        "    review_metrics = calculate_review_metrics(document_id, review_events_lookup)\n",
        "    cycle_time_metrics = calculate_cycle_time(\n",
        "        document_id, document, workflow_stages_lookup, outcomes_lookup\n",
        "    )\n",
        "\n",
        "    # Get cost\n",
        "    cost_entry = cost_tracking_lookup.get(document_id, {})\n",
        "    total_cost_usd = cost_entry.get(\"total_cost_usd\", 0.0)\n",
        "\n",
        "    # Build analysis\n",
        "    analysis = {\n",
        "        \"document_id\": document_id,\n",
        "        \"revision_count\": revision_count,\n",
        "        \"total_stages\": stage_metrics[\"total_stages\"],\n",
        "        \"failed_stages\": stage_metrics[\"failed_stages\"],\n",
        "        \"compliance_failures\": compliance_metrics[\"failed_checks\"],\n",
        "        \"human_overrides\": review_metrics[\"human_overrides\"],\n",
        "        \"total_cost_usd\": round(total_cost_usd, 2),\n",
        "        \"cycle_time_hours\": cycle_time_metrics[\"cycle_time_hours\"],\n",
        "        \"baseline_cycle_time_hours\": cycle_time_metrics.get(\"baseline_cycle_time_hours\"),\n",
        "        \"hours_saved\": cycle_time_metrics.get(\"hours_saved\"),\n",
        "        \"avg_stage_duration_minutes\": stage_metrics[\"avg_stage_duration_minutes\"],\n",
        "        \"stage_metrics\": stage_metrics,\n",
        "        \"compliance_metrics\": compliance_metrics,\n",
        "        \"review_metrics\": review_metrics,\n",
        "        \"cycle_time_metrics\": cycle_time_metrics\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "\n",
        "def analyze_all_documents(\n",
        "    documents: List[Dict[str, Any]],\n",
        "    document_versions_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    workflow_stages_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    review_events_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    compliance_checks_lookup: Dict[str, List[Dict[str, Any]]],\n",
        "    cost_tracking_lookup: Dict[str, Dict[str, Any]],\n",
        "    outcomes_lookup: Dict[str, Dict[str, Any]],\n",
        "    filter_document_id: Optional[str] = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyze all documents (or a single document if filter_document_id is provided).\n",
        "\n",
        "    Args:\n",
        "        documents: List of all documents\n",
        "        document_versions_lookup: Lookup dictionary for versions\n",
        "        workflow_stages_lookup: Lookup dictionary for stages\n",
        "        review_events_lookup: Lookup dictionary for reviews\n",
        "        compliance_checks_lookup: Lookup dictionary for compliance checks\n",
        "        cost_tracking_lookup: Lookup dictionary for cost tracking\n",
        "        outcomes_lookup: Lookup dictionary for outcomes\n",
        "        filter_document_id: Optional document ID to filter to single document\n",
        "\n",
        "    Returns:\n",
        "        List of document analysis dictionaries\n",
        "    \"\"\"\n",
        "    # Filter documents if needed\n",
        "    docs_to_analyze = documents\n",
        "    if filter_document_id:\n",
        "        docs_to_analyze = [d for d in documents if d.get(\"document_id\") == filter_document_id]\n",
        "\n",
        "    # Analyze each document\n",
        "    analyses = []\n",
        "    for document in docs_to_analyze:\n",
        "        document_id = document.get(\"document_id\")\n",
        "        if document_id:\n",
        "            analysis = analyze_document(\n",
        "                document_id,\n",
        "                document,\n",
        "                document_versions_lookup,\n",
        "                workflow_stages_lookup,\n",
        "                review_events_lookup,\n",
        "                compliance_checks_lookup,\n",
        "                cost_tracking_lookup,\n",
        "                outcomes_lookup\n",
        "            )\n",
        "            analyses.append(analysis)\n",
        "\n",
        "    return analyses\n"
      ]
    }
  ]
}