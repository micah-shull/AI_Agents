{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7Xg5c1xvHs1FmR6rIFunD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/016_quasi_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ü§ñ Why This Is a **Quasi-Agent**\n",
        "\n",
        "A **quasi-agent** behaves *like* an agent in some ways ‚Äî it performs a task through a sequence of LLM calls ‚Äî but lacks the **core traits of a true autonomous agent**:\n",
        "\n",
        "| Trait                          | Quasi-Agent (Your Project)                            | Full Agent                                                     |\n",
        "| ------------------------------ | ----------------------------------------------------- | -------------------------------------------------------------- |\n",
        "| ‚úÖ Multi-step logic             | Yes, follows a 3-step code generation process         | Yes                                                            |\n",
        "| ‚ùå Decision-making              | No ‚Äî fixed prompts and steps                          | Yes ‚Äî can decide what to do next                               |\n",
        "| ‚ùå Memory or context management | Minimal ‚Äî just passes previous output                 | Yes ‚Äî tracks goals, tasks, and failures                        |\n",
        "| ‚ùå Reactivity                   | No ‚Äî doesn‚Äôt adapt if LLM output is bad or incomplete | Yes ‚Äî can retry, validate, branch                              |\n",
        "| ‚ùå Tools / environment actions  | No ‚Äî doesn‚Äôt use APIs, run code, or call functions    | Yes ‚Äî can take actions (e.g., call APIs, run code, browse web) |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Concrete Example\n",
        "\n",
        "Let‚Äôs say the LLM generates bad test code. A **quasi-agent** will still save it and move on. A **full agent** might:\n",
        "\n",
        "* Notice the code has syntax errors\n",
        "* Rerun the generation step with a clarification\n",
        "* Try an alternate method or model\n",
        "* Ask the user to clarify\n",
        "* Retry until success\n",
        "\n",
        "Your current project doesn‚Äôt have that loop ‚Äî it just chains static prompts.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† So What Makes a **Full Agent**?\n",
        "\n",
        "A real agent often includes:\n",
        "\n",
        "1. **A planning loop** (e.g., ReAct, CoT, or Task Decomposition)\n",
        "2. **State management** (memory, goals, scratchpad)\n",
        "3. **Tool use** (calls to Python, web search, APIs)\n",
        "4. **Autonomy** (runs until task is done or fails)\n",
        "5. **Error handling and retries**\n",
        "\n",
        "Popular agent frameworks like:\n",
        "\n",
        "* **AutoGPT**, **BabyAGI**\n",
        "* **LangChain Agents**\n",
        "* **OpenAI Function Calling + Planning**\n",
        "\n",
        "‚Ä¶combine LLM reasoning with memory, tools, and feedback loops.\n",
        "\n"
      ],
      "metadata": {
        "id": "uqvzjGzA3mm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G78BLMIyqls8",
        "outputId": "6bfad917-3137-477b-d5ed-99f303bd8180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate huggingface_hub litellm python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß† Using Code LLaMA for Python Code Generation with Transformers\n",
        "\n",
        "This notebook builds a quasi-agent that generates Python functions, adds documentation, and creates unit tests based on user input. It uses the Hugging Face `transformers` library and the `CodeLlama-7b-Instruct` model ‚Äî a powerful, open-source large language model optimized for code generation.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Key Libraries and Their Roles\n",
        "\n",
        "\n",
        "| Component                     | Description                                                                                                                                                                                      |\n",
        "| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
        "| `AutoModelForCausalLM`        | Loads a **causal (left-to-right) language model** for text/code generation. Perfect for tasks where the model generates tokens one after another, like writing code or autocompleting text.      |\n",
        "| `AutoTokenizer`               | Converts text into tokens (IDs) the model can understand, and converts output tokens back into readable text. Ensures compatibility with the specific model's vocabulary and tokenization rules. |\n",
        "| `pipeline(\"text-generation\")` | A high-level abstraction that combines tokenization, model inference, and decoding into one simple call ‚Äî allowing us to generate code with minimal overhead.                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why Causal Language Modeling?\n",
        "\n",
        "**Causal Language Modeling** (CLM) trains a model to predict the next token based only on previous tokens ‚Äî like writing left to right. This makes it ideal for tasks like:\n",
        "\n",
        "* Generating code from prompts\n",
        "* Writing documentation progressively\n",
        "* Autocompleting functions\n",
        "\n",
        "In contrast to models like BERT (which look in both directions), CLMs are autoregressive and **better suited for generative tasks**, especially where **sequence order matters** (like in Python syntax).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why This Setup?\n",
        "\n",
        "This setup is chosen because it meets several key goals for the assignment:\n",
        "\n",
        "* ‚úÖ **Open-source**: No API keys or external service needed; runs locally in Colab using Hugging Face models.\n",
        "* ‚úÖ **Instruction-tuned model**: `CodeLlama-7b-Instruct` understands user instructions well and produces high-quality, structured Python code.\n",
        "* ‚úÖ **Aligned with real-world tools**: The `transformers` library is industry-standard for LLMs, and learning it provides long-term value.\n",
        "* ‚úÖ **Simplified workflow**: Using the `pipeline` interface streamlines tokenization, inference, and decoding, making the code more readable and maintainable.\n",
        "\n",
        "---\n",
        "\n",
        "By combining a causal language model with this toolkit, we're able to build a responsive, context-aware code generation system ‚Äî perfect for automating multi-step Python development tasks like function creation, documentation, and testing.\n"
      ],
      "metadata": {
        "id": "SMG1mLQ400U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load CodeLlama model\n",
        "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def ask_llama(prompt, max_tokens=512):\n",
        "    result = pipe(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "    return result\n",
        "\n",
        "# Step 1: Ask the user and generate a basic function\n",
        "user_input = input(\"What function do you want to create? \")\n",
        "step1_prompt = f\"\"\"### Instruction:\n",
        "Write a Python function that does the following:\n",
        "{user_input}\n",
        "Only provide code output.\n",
        "\n",
        "### Response:\\n\"\"\"\n",
        "basic_function = ask_llama(step1_prompt)\n",
        "print(\"Generated Code:\\n\", basic_function)\n",
        "\n",
        "# Step 2: Add documentation\n",
        "step2_prompt = f\"\"\"### Instruction:\n",
        "Add full documentation to the following Python function. Include:\n",
        "- Function description\n",
        "- Parameter descriptions\n",
        "- Return value description\n",
        "- Example usage\n",
        "- Edge cases\n",
        "\n",
        "### Code:\n",
        "{basic_function}\n",
        "\n",
        "### Response:\\n\"\"\"\n",
        "documented_function = ask_llama(step2_prompt)\n",
        "print(\"Documented Code:\\n\", documented_function)\n",
        "\n",
        "# Step 3: Generate unit tests\n",
        "step3_prompt = f\"\"\"### Instruction:\n",
        "Write Python `unittest` test cases for the following function. Cover:\n",
        "- Basic functionality\n",
        "- Edge cases\n",
        "- Error cases\n",
        "- Various input scenarios\n",
        "\n",
        "### Code:\n",
        "{documented_function}\n",
        "\n",
        "### Response:\\n\"\"\"\n",
        "test_code = ask_llama(step3_prompt)\n",
        "print(\"Test Cases:\\n\", test_code)\n",
        "\n",
        "# Step 4: Save to a file\n",
        "with open(\"final_function.py\", \"w\") as f:\n",
        "    f.write(documented_function)\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(test_code)\n"
      ],
      "metadata": {
        "id": "4hhl-kzTvArp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35FtRrd1yoX9",
        "outputId": "a33c3a62-90ec-46bc-a474-daa90a3cc111"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚ùå Why the First Approach (Code LLaMA on CPU) Failed\n",
        "\n",
        "The first attempt used the **CodeLLaMA-7B-Instruct** model from Hugging Face, which is a powerful but **large open-source model** (\\~13 billion parameters with weights). This approach failed primarily due to:\n",
        "\n",
        "* **No GPU support**: Running large models like CodeLLaMA on CPU is **extremely slow and memory-intensive**, often leading to crashes or indefinite execution times.\n",
        "* **Colab Free tier limitations**: Without access to GPU, even downloading and loading the model into memory can take 10‚Äì20+ minutes or fail outright.\n",
        "* **High RAM & VRAM requirements**: CodeLLaMA needs \\~12‚Äì16 GB of VRAM to run smoothly, which a CPU-only environment can‚Äôt provide.\n",
        "\n",
        "> ‚ö†Ô∏è Large transformer models are designed with parallel GPU computation in mind. CPUs are simply too slow and memory-limited for efficient inference.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Using OpenAI Worked\n",
        "\n",
        "Switching to **OpenAI's GPT models** (via their API) resolved these issues:\n",
        "\n",
        "* **Model inference runs on OpenAI's high-performance GPUs**, not your local machine ‚Äî removing hardware limitations.\n",
        "* **Fast and reliable**: GPT-3.5 and GPT-4 are highly optimized, returning responses within seconds.\n",
        "* **No model loading required**: You don‚Äôt need to download, cache, or manage large models.\n",
        "* **Production-grade quality**: The outputs are consistently clean, formatted, and instruction-following.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Conclusion\n",
        "\n",
        "| Approach               | Outcome                             | Notes                           |\n",
        "| ---------------------- | ----------------------------------- | ------------------------------- |\n",
        "| CodeLLaMA (local, CPU) | ‚ùå Failed (too slow, resource-heavy) | Needs GPU for practical use     |\n",
        "| OpenAI (via API)       | ‚úÖ Success (fast, reliable)          | Ideal for CPU-only environments |\n",
        "\n",
        "**Using OpenAI is the better choice when:**\n",
        "\n",
        "* You're on a CPU-only setup\n",
        "* You want quick results without infrastructure setup\n",
        "* You need production-quality output\n",
        "\n"
      ],
      "metadata": {
        "id": "KONIEBXW2k14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "import litellm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load token from .env.\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "def call_openai(prompt):\n",
        "    response = litellm.completion(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "# Step 1: Ask the user and generate a basic function ==========\n",
        "user_input = input(\"What function do you want to create? \")\n",
        "prompt1 = f\"Write a Python function for: {user_input}\"\n",
        "basic_function = call_openai(prompt1)\n",
        "print(\"Generated Code:\\n\", basic_function)\n",
        "\n",
        "\n",
        "# Step 2: Ask for documentation =================\n",
        "prompt2 = f\"\"\"Add full documentation to this Python function including:\n",
        "- Description\n",
        "- Parameters\n",
        "- Return value\n",
        "- Example usage\n",
        "- Edge cases\n",
        "\n",
        "Here is the code:\n",
        "{basic_function}\n",
        "\"\"\"\n",
        "documented_function = call_openai(prompt2)\n",
        "print(\"Documented Code:\\n\", documented_function)\n",
        "\n",
        "\n",
        "# Step 3: Generate unit tests ===============\n",
        "prompt3 = f\"\"\"Write unittests for the following Python function. Cover:\n",
        "- Basic functionality\n",
        "- Edge cases\n",
        "- Error handling\n",
        "\n",
        "Here is the code:\n",
        "{documented_function}\n",
        "\"\"\"\n",
        "test_code = call_openai(prompt3)\n",
        "print(\"Test Cases:\\n\", test_code)\n",
        "\n",
        "\n",
        "# Step 4: Save everything ==============\n",
        "with open(\"final_function.py\", \"w\") as f:\n",
        "    f.write(documented_function)\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(test_code)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md9AXan5rdlO",
        "outputId": "71ab3180-c4de-48fe-bbb6-aef42a3cf2b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What function do you want to create? can you write a function to calculate the square root of a number?\n",
            "Generated Code:\n",
            " Certainly! Here is a Python function to calculate the square root of a number:\n",
            "\n",
            "```python\n",
            "def square_root(num):\n",
            "    return num ** 0.5\n",
            "\n",
            "# Test the function\n",
            "number = 25\n",
            "result = square_root(number)\n",
            "print(f\"The square root of {number} is {result}\")\n",
            "```\n",
            "\n",
            "You can use this `square_root` function by passing a number as an argument to get its square root.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='Certainl...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documented Code:\n",
            " \"\"\"\n",
            "Description:\n",
            "This function takes a number as input and calculates its square root using the exponentiation operator (**).\n",
            "\n",
            "Parameters:\n",
            "- num: A numerical value for which the square root needs to be calculated.\n",
            "\n",
            "Return value:\n",
            "The function returns the square root of the input number as a float.\n",
            "\n",
            "Example usage:\n",
            "number = 25\n",
            "result = square_root(number)\n",
            "print(f\"The square root of {number} is {result}\")\n",
            "\n",
            "This will output:\n",
            "The square root of 25 is 5.0\n",
            "\n",
            "Edge cases:\n",
            "- Passing a negative number will result in a ValueError as the square root of a negative number is not a real number.\n",
            "- If the input is a decimal number, the function will still calculate the square root, but the result may not be exact due to floating-point precision.\n",
            "\"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='\"\"\"\\nDes...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Cases:\n",
            " import unittest\n",
            "\n",
            "def square_root(num):\n",
            "    if num < 0:\n",
            "        raise ValueError(\"Cannot calculate square root of a negative number\")\n",
            "    return num ** 0.5\n",
            "\n",
            "class TestSquareRootFunction(unittest.TestCase):\n",
            "\n",
            "    def test_basic_functionality(self):\n",
            "        self.assertEqual(square_root(25), 5.0)\n",
            "        self.assertEqual(square_root(16), 4.0)\n",
            "        self.assertEqual(square_root(0), 0.0)\n",
            "\n",
            "    def test_edge_cases(self):\n",
            "        with self.assertRaises(ValueError):\n",
            "            square_root(-1)\n",
            "        self.assertAlmostEqual(square_root(2), 1.41421356237, places=10)\n",
            "        self.assertAlmostEqual(square_root(10.5), 3.2403703492, places=10)\n",
            "\n",
            "    def test_error_handling(self):\n",
            "        with self.assertRaises(TypeError):\n",
            "            square_root(\"abc\")\n",
            "        with self.assertRaises(TypeError):\n",
            "            square_root([25])\n",
            "        with self.assertRaises(TypeError):\n",
            "            square_root(None)\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='import u...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ask_openai(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = litellm.completion(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# Step 1: Ask the user what function to create\n",
        "user_input = \"Create a function to sort a list of dictionaries by a specific key.\"\n",
        "\n",
        "print(f\"\\nüßë User request: {user_input}\")\n",
        "prompt1 = f\"Write a Python function for this task: {user_input}\"\n",
        "basic_function = ask_openai(prompt1)\n",
        "print(\"\\nüõ†Ô∏è Generated Function:\\n\", basic_function)\n",
        "\n",
        "# Step 2: Add documentation\n",
        "prompt2 = f\"\"\"Add full documentation to the following Python function. Include:\n",
        "- Description\n",
        "- Parameters\n",
        "- Return value\n",
        "- Example usage\n",
        "- Edge cases\n",
        "\n",
        "Function:\n",
        "{basic_function}\n",
        "\"\"\"\n",
        "documented_function = ask_openai(prompt2)\n",
        "print(\"\\nüìò Documented Code:\\n\", documented_function)\n",
        "\n",
        "# Step 3: Add unit tests\n",
        "prompt3 = f\"\"\"Write unit tests for this function using Python's unittest framework.\n",
        "Cover:\n",
        "- Sorting by string keys\n",
        "- Sorting by numeric keys\n",
        "- Missing key edge cases\n",
        "- Error handling\n",
        "\n",
        "Function:\n",
        "{documented_function}\n",
        "\"\"\"\n",
        "test_code = ask_openai(prompt3)\n",
        "print(\"\\n‚úÖ Unit Tests:\\n\", test_code)\n",
        "\n",
        "# Step 4: Save to file\n",
        "with open(\"final_function.py\", \"w\") as f:\n",
        "    f.write(documented_function)\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(test_code)\n",
        "\n",
        "print(\"\\nüíæ Saved to final_function.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0_yEFOPuy6E",
        "outputId": "6a168e5c-f75b-470d-ebbe-e9f52cf90647"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßë User request: Create a function to sort a list of dictionaries by a specific key.\n",
            "\n",
            "üõ†Ô∏è Generated Function:\n",
            " Here is a Python function that sorts a list of dictionaries by a specific key:\n",
            "\n",
            "```python\n",
            "def sort_dict_list(dict_list, key):\n",
            "    return sorted(dict_list, key=lambda x: x[key])\n",
            "\n",
            "# Example usage\n",
            "list_of_dicts = [{'name': 'Alice', 'age': 25},\n",
            "                 {'name': 'Bob', 'age': 30},\n",
            "                 {'name': 'Charlie', 'age': 20}]\n",
            "\n",
            "sorted_list = sort_dict_list(list_of_dicts, 'age')\n",
            "print(sorted_list)\n",
            "```\n",
            "\n",
            "This function takes a list of dictionaries `dict_list` and a key `key` to sort the list of dictionaries by. The `sorted()` function is then used with a lambda function as the `key` to extract the specific key from each dictionary for sorting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"Here is ...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò Documented Code:\n",
            " Description:\n",
            "This function sorts a list of dictionaries by a specific key.\n",
            "\n",
            "Parameters:\n",
            "- dict_list: a list of dictionaries to be sorted\n",
            "- key: the key by which the list of dictionaries should be sorted\n",
            "\n",
            "Return value:\n",
            "The function returns the sorted list of dictionaries.\n",
            "\n",
            "Example usage:\n",
            "```\n",
            "list_of_dicts = [{'name': 'Alice', 'age': 25},\n",
            "                 {'name': 'Bob', 'age': 30},\n",
            "                 {'name': 'Charlie', 'age': 20}]\n",
            "\n",
            "sorted_list = sort_dict_list(list_of_dicts, 'age')\n",
            "print(sorted_list)\n",
            "```\n",
            "\n",
            "Edge cases:\n",
            "- If `dict_list` is empty, the function will return an empty list since there is nothing to sort.\n",
            "- If `key` does not exist in any of the dictionaries in `dict_list`, the function may raise a KeyError.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"Descript...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Unit Tests:\n",
            " Here are the unit tests for the provided function using Python's unittest framework:\n",
            "\n",
            "```python\n",
            "import unittest\n",
            "from sort_dict_list import sort_dict_list\n",
            "\n",
            "class TestSortDictList(unittest.TestCase):\n",
            "\n",
            "    def test_sort_by_string_keys(self):\n",
            "        dict_list = [{'name': 'Alice', 'age': 25},\n",
            "                     {'name': 'Bob', 'age': 30},\n",
            "                     {'name': 'Charlie', 'age': 20}]\n",
            "        sorted_list = sort_dict_list(dict_list, 'name')\n",
            "        self.assertEqual(sorted_list, [{'name': 'Alice', 'age': 25},\n",
            "                                       {'name': 'Bob', 'age': 30},\n",
            "                                       {'name': 'Charlie', 'age': 20}])\n",
            "\n",
            "    def test_sort_by_numeric_keys(self):\n",
            "        dict_list = [{'name': 'Alice', 'age': 25},\n",
            "                     {'name': 'Bob', 'age': 30},\n",
            "                     {'name': 'Charlie', 'age': 20}]\n",
            "        sorted_list = sort_dict_list(dict_list, 'age')\n",
            "        self.assertEqual(sorted_list, [{'name': 'Charlie', 'age': 20},\n",
            "                                       {'name': 'Alice', 'age': 25},\n",
            "                                       {'name': 'Bob', 'age': 30}])\n",
            "\n",
            "    def test_missing_key_edge_case(self):\n",
            "        dict_list = [{'name': 'Alice', 'age': 25},\n",
            "                     {'name': 'Bob', 'age': 30},\n",
            "                     {'name': 'Charlie'}]\n",
            "        with self.assertRaises(KeyError):\n",
            "            sort_dict_list(dict_list, 'age')\n",
            "\n",
            "    def test_empty_dict_list(self):\n",
            "        dict_list = []\n",
            "        sorted_list = sort_dict_list(dict_list, 'age')\n",
            "        self.assertEqual(sorted_list, [])\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "```\n",
            "\n",
            "Make sure to replace `'sort_dict_list'` with the actual name of the file and function being tested.\n",
            "\n",
            "üíæ Saved to final_function.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"Here are...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ What This Output Demonstrates\n",
        "\n",
        "### üß† Quasi-Agent Functionality in Action\n",
        "\n",
        "| Step                | What Happened                                   | Success                         |\n",
        "| ------------------- | ----------------------------------------------- | ------------------------------- |\n",
        "| **User Prompt**     | You asked for a sorting function                | ‚úÖ Interpreted correctly         |\n",
        "| **Code Generation** | The agent created a clean, Pythonic function    | ‚úÖ Uses `sorted()` with a lambda |\n",
        "| **Documentation**   | Added real doc-style formatting with edge cases | ‚úÖ Includes examples & caveats   |\n",
        "| **Test Suite**      | Generated `unittest` tests for 4 cases          | ‚úÖ Covers normal + edge cases    |\n",
        "| **Output Handling** | Saved to file, clear terminal output            | ‚úÖ ‚úÖ ‚úÖ                           |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è About the Warnings Again\n",
        "\n",
        "These LiteLLM + Pydantic warnings are **cosmetic**. You can safely ignore or suppress them, as they don‚Äôt impact output:\n",
        "\n",
        "```python\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° What You‚Äôve Learned (and Practiced)\n",
        "\n",
        "* How to run a **multi-step LLM workflow**\n",
        "* How to **preserve context** between steps\n",
        "* How to **manage output** from generation to testing\n",
        "* Why it‚Äôs a **quasi-agent** (not adaptive, but sequential and helpful)\n",
        "* How to **use OpenAI's hosted power** to bypass local resource limits\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Where You Can Go From Here\n",
        "\n",
        "### üß≠ Extend This Quasi-Agent\n",
        "\n",
        "* Add **error detection** (e.g., parse code with `ast`, retry on syntax errors)\n",
        "* Add **user refinement loop**: \"Do you want to change the function?\"\n",
        "* Add support for **multiple function types** in a menu\n",
        "\n",
        "### ü§ñ Turn Into a Real Agent\n",
        "\n",
        "* Add tool use: execute the generated code and return the result\n",
        "* Track memory, goals, and retry logic (e.g., with LangChain Agents or ReAct loop)\n",
        "* Dynamically modify its own behavior based on output or errors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzH2OR_162Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ What Are Unit Tests?\n",
        "\n",
        "**Unit tests** are small, automated tests that verify that a **single \"unit\" of code** (usually a function or method) works as expected.\n",
        "\n",
        "> A unit test answers: *‚ÄúGiven this input, does the function return the correct output?‚Äù*\n",
        "\n",
        "They're like a safety net ‚Äî they make sure your function still works even if other parts of your code change later.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example\n",
        "\n",
        "Say you have this function:\n",
        "\n",
        "```python\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "```\n",
        "\n",
        "A unit test for it might look like:\n",
        "\n",
        "```python\n",
        "import unittest\n",
        "\n",
        "class TestAddFunction(unittest.TestCase):\n",
        "    def test_add_positive_numbers(self):\n",
        "        self.assertEqual(add(2, 3), 5)\n",
        "\n",
        "    def test_add_negative_numbers(self):\n",
        "        self.assertEqual(add(-1, -1), -2)\n",
        "```\n",
        "\n",
        "These tests run automatically and alert you if something breaks ‚Äî much better than manually checking results.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† How to Design Good Unit Tests\n",
        "\n",
        "Designing tests *is* a bit of an art, and you're right ‚Äî it requires thinking like both a coder and a skeptic. Here's a solid checklist:\n",
        "\n",
        "### 1. **Test Expected Behavior (Happy Path)**\n",
        "\n",
        "Test that the function works when given normal, valid input.\n",
        "\n",
        "> ‚úÖ *Does it work for common use cases?*\n",
        "\n",
        "### 2. **Test Edge Cases**\n",
        "\n",
        "Test extreme or unusual values.\n",
        "\n",
        "> ‚úÖ *What happens with 0, empty lists, or very large numbers?*\n",
        "\n",
        "### 3. **Test Error Handling**\n",
        "\n",
        "Deliberately feed it bad inputs ‚Äî wrong types, missing keys, etc.\n",
        "\n",
        "> ‚úÖ *Does it raise the right exceptions?*\n",
        "\n",
        "### 4. **Test Performance Assumptions (Optional)**\n",
        "\n",
        "Test with large inputs or time-sensitive code.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç For Your Example (Sort Dictionary List)\n",
        "\n",
        "You were testing:\n",
        "\n",
        "| Case            | Example                     | Why it matters                              |\n",
        "| --------------- | --------------------------- | ------------------------------------------- |\n",
        "| ‚úÖ Basic sorting | Sort by `'age'` or `'name'` | Proves the core logic works                 |\n",
        "| ‚úÖ Edge case     | Empty list                  | Should return empty list, not crash         |\n",
        "| ‚úÖ Missing key   | One dict has no `'age'`     | Should raise a `KeyError`                   |\n",
        "| ‚úÖ Wrong type    | Input isn't a list          | Should raise `TypeError` or fail gracefully |\n",
        "\n",
        "This shows you understand what could go wrong ‚Äî and **build confidence** that your function behaves well.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Tools Used\n",
        "\n",
        "In Python, the standard tool is:\n",
        "\n",
        "```python\n",
        "import unittest\n",
        "```\n",
        "\n",
        "But many developers also use:\n",
        "\n",
        "* `pytest` (more flexible, readable syntax)\n",
        "* `doctest` (tests in docstrings)\n",
        "* Custom test runners (for CI/CD)\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Why Unit Testing Is Powerful\n",
        "\n",
        "* Catches bugs early\n",
        "* Documents what your function *should* do\n",
        "* Makes refactoring safe\n",
        "* Builds trust in your code\n",
        "\n"
      ],
      "metadata": {
        "id": "wn1317gd8hKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† What is `LiteLLM`?\n",
        "\n",
        "`LiteLLM` is a lightweight Python wrapper that gives you a **unified interface** to talk to different large language models (LLMs), including:\n",
        "\n",
        "* ‚úÖ OpenAI (`gpt-3.5-turbo`, `gpt-4`)\n",
        "* ‚úÖ Anthropic (`Claude`)\n",
        "* ‚úÖ Hugging Face Inference models\n",
        "* ‚úÖ Azure OpenAI\n",
        "* ‚úÖ Google Gemini (via proxy)\n",
        "* ‚úÖ Local models (optionally)\n",
        "\n",
        "> Basically, it lets you **write one prompt call**, and switch LLMs behind the scenes without rewriting your whole app.\n",
        "\n",
        "---\n",
        "\n",
        "### üìû What is `completion`?\n",
        "\n",
        "The function `completion(...)` is **LiteLLM's version of a chat-style API call**. It‚Äôs similar to `openai.ChatCompletion.create(...)` but:\n",
        "\n",
        "* Works across providers\n",
        "* Has a more consistent response structure\n",
        "* Is easier to swap models or tune behaviors\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example\n",
        "\n",
        "```python\n",
        "from litellm import completion\n",
        "\n",
        "response = completion(\n",
        "    model=\"openai/gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "# Output: \"The capital of France is Paris.\"\n",
        "```\n",
        "\n",
        "It returns an object with `.choices[0].message.content`, just like OpenAI's API ‚Äî so it‚Äôs mostly a drop-in replacement.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why Use LiteLLM?\n",
        "\n",
        "| Benefit           | Description                                                      |\n",
        "| ----------------- | ---------------------------------------------------------------- |\n",
        "| üåê Multi-provider | Talk to OpenAI, Anthropic, Hugging Face, etc. with the same code |\n",
        "| üîÅ Model swapping | Change from GPT-4 to Claude or Mistral in one line               |\n",
        "| üß™ Simplified API | Easier to test, consistent structure                             |\n",
        "| üí∞ Cost tracking  | Optional built-in logging + pricing                              |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ LiteLLM is perfect for **prototyping** when:\n",
        "\n",
        "* You know **what kind of output** you want (e.g., generate code, summarize text, write SQL).\n",
        "* You want to **experiment quickly** across different LLMs (OpenAI, Claude, Mistral, Hugging Face).\n",
        "* You care about **performance, cost, or latency** and want to compare models easily.\n",
        "* You‚Äôre building a **lean script or tool** and don‚Äôt want to pull in a full agent framework like LangChain.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example Use Case\n",
        "\n",
        "Let‚Äôs say you want to see which model writes better docstrings:\n",
        "\n",
        "```python\n",
        "from litellm import completion\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Write a Python function to reverse a string.\"}]\n",
        "\n",
        "# Try GPT-4\n",
        "response = completion(model=\"openai/gpt-4\", messages=messages)\n",
        "print(\"GPT-4:\", response.choices[0].message.content)\n",
        "\n",
        "# Try Claude 3\n",
        "response = completion(model=\"anthropic/claude-3-opus-20240229\", messages=messages)\n",
        "print(\"Claude 3:\", response.choices[0].message.content)\n",
        "\n",
        "# Try Mistral\n",
        "response = completion(model=\"mistralai/mistral-7b-instruct\", messages=messages)\n",
        "print(\"Mistral:\", response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "> With just one line (`model=\"...\"`), you swap providers ‚Äî **no need to rewrite your whole prompt structure**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Once You Decide‚Ä¶\n",
        "\n",
        "Once you find the best model for your use case:\n",
        "\n",
        "* You can hardcode it\n",
        "* Or migrate to a larger framework (like LangChain) if you need memory, agents, retrieval, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† TL;DR\n",
        "\n",
        "> **LiteLLM = Rapid A/B testing + model swapping with minimal setup.**\n",
        "\n",
        "Perfect for:\n",
        "\n",
        "* Hackathons\n",
        "* Prototyping\n",
        "* Internal tools\n",
        "* Developer testing\n",
        "* Cost comparisons\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R7xHe2n79zvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "import sys\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "   \"\"\"Call LLM to get response\"\"\"\n",
        "   response = completion(\n",
        "      model=\"openai/gpt-4\",\n",
        "      messages=messages, # Pass the full conversation (acts as context)\n",
        "      max_tokens=1024    # Limit the response length to avoid overflow\n",
        "   )\n",
        "   return response.choices[0].message.content # # Return only the generated text portion (not metadata or tokens)\n",
        "\n",
        "def extract_code_block(response: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the first code block from an LLM response.\n",
        "\n",
        "    This function assumes the LLM output is in markdown-style format using triple backticks (```).\n",
        "    It pulls out the code portion ‚Äî and removes any optional language tag like 'python'.\n",
        "\n",
        "    Parameters:\n",
        "    - response (str): The raw text output from the LLM.\n",
        "\n",
        "    Returns:\n",
        "    - str: The extracted code block, or the original string if no code block is found.\n",
        "    \"\"\"\n",
        "\n",
        "    # If there's no triple backtick in the response, just return the full response\n",
        "    if '```' not in response:\n",
        "        return response\n",
        "\n",
        "    # Split the response by backticks and grab the first code block after the opening ```\n",
        "    code_block = response.split('```')[1].strip()\n",
        "\n",
        "    # If the code block starts with \"python\", remove it (it's a formatting tag, not actual code)\n",
        "    if code_block.startswith(\"python\"):\n",
        "        code_block = code_block[6:]  # Removes the \"python\" prefix\n",
        "\n",
        "    return code_block\n",
        "\n",
        "\n",
        "def develop_custom_function():\n",
        "   # Get user input for function description\n",
        "   print(\"\\nWhat kind of function would you like to create?\")\n",
        "   print(\"Example: 'A function that calculates the factorial of a number'\")\n",
        "   print(\"Your description: \", end='')\n",
        "   function_description = input().strip()\n",
        "\n",
        "   # Initialize conversation with system prompt\n",
        "   messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a Python expert helping to develop a function.\"}\n",
        "   ]\n",
        "\n",
        "   # First prompt - Basic function\n",
        "   messages.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": f\"Write a Python function that {function_description}. Output the function in a ```python code block```.\"\n",
        "   })\n",
        "   initial_function = generate_response(messages)\n",
        "\n",
        "   # Parse the response to get the function code\n",
        "   initial_function = extract_code_block(initial_function)\n",
        "\n",
        "   print(\"\\n=== Initial Function ===\")\n",
        "   print(initial_function)\n",
        "\n",
        "   # Add assistant's response to conversation\n",
        "   # Notice that I am purposely causing it to forget its commentary and just see the code so that\n",
        "   # it appears that is always outputting just code.\n",
        "   messages.append({\"role\": \"assistant\", \"content\": \"\\`\\`\\`python\\n\\n\"+initial_function+\"\\n\\n\\`\\`\\`\"})\n",
        "\n",
        "   # Second prompt - Add documentation\n",
        "   messages.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Add comprehensive documentation to this function, including description, parameters, \"\n",
        "                 \"return value, examples, and edge cases. Output the function in a ```python code block```.\"\n",
        "   })\n",
        "   documented_function = generate_response(messages)\n",
        "   documented_function = extract_code_block(documented_function)\n",
        "   print(\"\\n=== Documented Function ===\")\n",
        "   print(documented_function)\n",
        "\n",
        "   # Add documentation response to conversation\n",
        "   messages.append({\"role\": \"assistant\", \"content\": \"\\`\\`\\`python\\n\\n\"+documented_function+\"\\n\\n\\`\\`\\`\"})\n",
        "\n",
        "   # Third prompt - Add test cases\n",
        "   messages.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Add unittest test cases for this function, including tests for basic functionality, \"\n",
        "                 \"edge cases, error cases, and various input scenarios. Output the code in a \\`\\`\\`python code block\\`\\`\\`.\"\n",
        "   })\n",
        "   test_cases = generate_response(messages)\n",
        "   # We will likely run into random problems here depending on if it outputs JUST the test cases or the\n",
        "   # test cases AND the code. This is the type of issue we will learn to work through with agents in the course.\n",
        "   test_cases = extract_code_block(test_cases)\n",
        "   print(\"\\n=== Test Cases ===\")\n",
        "   print(test_cases)\n",
        "\n",
        "   # Generate filename from function description\n",
        "   filename = function_description.lower()\n",
        "   filename = ''.join(c for c in filename if c.isalnum() or c.isspace())\n",
        "   filename = filename.replace(' ', '_')[:30] + '.py'\n",
        "\n",
        "   # Save final version\n",
        "   with open(filename, 'w') as f:\n",
        "      f.write(documented_function + '\\n\\n' + test_cases)\n",
        "\n",
        "   return documented_function, test_cases, filename\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "   function_code, tests, filename = develop_custom_function()\n",
        "   print(f\"\\nFinal code has been saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anvXLf8A4K2C",
        "outputId": "fa657c30-6edf-412b-f8d3-c93a97706db2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What kind of function would you like to create?\n",
            "Example: 'A function that calculates the factorial of a number'\n",
            "Your description: Create a function to return the fibonnaci sequence\n",
            "\n",
            "=== Initial Function ===\n",
            "\n",
            "def fibonacci(n):\n",
            "    fib_seq = []\n",
            "    a, b = 0, 1\n",
            "    while a < n:\n",
            "        fib_seq.append(a)\n",
            "        a, b = b, a+b\n",
            "    return fib_seq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content=\"Sure, he...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Documented Function ===\n",
            "\n",
            "def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Function to generate a sequential list of Fibonacci numbers up to a specified value.\n",
            "\n",
            "    Parameters:\n",
            "    n (int): A positive integer. The function generates the Fibonacci sequence up to this number.\n",
            "\n",
            "    Returns:\n",
            "    fib_seq (list): A list of integers where each item is a Fibonacci number, generated up to (but not exceeding) \n",
            "    the input parameter n.\n",
            "\n",
            "    Example:\n",
            "      - If the function is called with n=10, the output will be [0, 1, 1, 2, 3, 5, 8], \n",
            "      as these are the Fibonacci numbers up to 10.\n",
            "      - Calling the function with n=1 will output [0, 1, 1].\n",
            "\n",
            "    Edge Cases:\n",
            "      - Calling the function with n less than or equal to 0 will result in an empty list \n",
            "      because the Fibonacci sequence starts with 0 and thus cannot generate numbers up to \n",
            "      a non-positive number.\n",
            "    \"\"\"\n",
            "    fib_seq = []\n",
            "    a, b = 0, 1\n",
            "    while a < n:\n",
            "        fib_seq.append(a)\n",
            "        a, b = b, a+b\n",
            "    return fib_seq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='```pytho...: None}, annotations=[]), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Cases ===\n",
            "\n",
            "import unittest\n",
            "\n",
            "class TestFibonacci(unittest.TestCase):\n",
            "    def test_functionality(self):\n",
            "        self.assertEqual(fibonacci(10), [0, 1, 1, 2, 3, 5, 8])\n",
            "        self.assertEqual(fibonacci(6), [0, 1, 1, 2, 3, 5])\n",
            "        \n",
            "    def test_edge_cases(self):\n",
            "        self.assertEqual(fibonacci(0), [])\n",
            "        self.assertEqual(fibonacci(1), [0,1,1])\n",
            "        \n",
            "    def test_error_cases(self):\n",
            "        with self.assertRaises(TypeError):\n",
            "            fibonacci(\"10\")\n",
            "            \n",
            "    def test_various_inputs(self):\n",
            "        self.assertEqual(fibonacci(50), [0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
            "        self.assertEqual(fibonacci(100), [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89])\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "\n",
            "Final code has been saved to create_a_function_to_return_th.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct** is one of the most important ideas in modern agent design. It's not React (the JavaScript library), but something completely different in the context of AI agents.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is ReAct?\n",
        "\n",
        "**ReAct** stands for:\n",
        "\n",
        "> **Reasoning + Acting**\n",
        "\n",
        "It's a prompting and architecture strategy that enables language models to:\n",
        "\n",
        "1. **Think step-by-step** (Reason)\n",
        "2. **Take actions** using tools (Act)\n",
        "3. **Reflect and continue** based on the results (React)\n",
        "\n",
        "It was introduced in a 2022 research paper from Google and Princeton:\n",
        "üìÑ *ReAct: Synergizing Reasoning and Acting in Language Models*\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Why Is It Important?\n",
        "\n",
        "Most LLMs can only generate *text*. But agents often need to:\n",
        "\n",
        "* Query a database\n",
        "* Call an API\n",
        "* Run a calculator\n",
        "* Read a file\n",
        "* Retry something if it fails\n",
        "\n",
        "**ReAct gives the LLM a structure to \"decide\" when to take actions**, and how to reason before or after.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ ReAct Workflow (Simplified)\n",
        "\n",
        "Here‚Äôs what it looks like under the hood:\n",
        "\n",
        "```\n",
        "User: What's the population of France + Germany combined?\n",
        "\n",
        "Agent:\n",
        "Thought: I need to look up the populations of both countries.\n",
        "Action: search(\"population of France\")\n",
        "\n",
        "Observation: France has 67 million people.\n",
        "\n",
        "Thought: Now I‚Äôll get Germany's population.\n",
        "Action: search(\"population of Germany\")\n",
        "\n",
        "Observation: Germany has 83 million people.\n",
        "\n",
        "Thought: 67 + 83 = 150\n",
        "Answer: The combined population is 150 million.\n",
        "```\n",
        "\n",
        "Here, the LLM isn‚Äôt just answering directly ‚Äî it‚Äôs using **\"Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí Answer\"** as a loop.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ Where is ReAct used?\n",
        "\n",
        "| Platform            | ReAct Support                                |\n",
        "| ------------------- | -------------------------------------------- |\n",
        "| üîó LangChain        | ‚úÖ Yes (built-in agent framework)             |\n",
        "| üîÅ AutoGPT          | ‚úÖ Sort of ‚Äî more goal/task-based             |\n",
        "| üß† OpenAI Functions | ‚úÖ Possible, with structured prompts          |\n",
        "| üß™ Custom agents    | ‚úÖ You can build your own ReAct loop manually |\n",
        "\n",
        "---\n",
        "\n",
        "### üîß How Do You Use It?\n",
        "\n",
        "In LangChain, you can build ReAct agents like this:\n",
        "\n",
        "```python\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Define tools the agent can use\n",
        "tools = [\n",
        "    Tool(name=\"Calculator\", func=my_calculator, description=\"Useful for math\"),\n",
        "    Tool(name=\"Search\", func=google_search, description=\"Useful for finding info\"),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(tools, OpenAI(), agent=\"zero-shot-react-description\")\n",
        "\n",
        "agent.run(\"How many more people live in Japan than in Canada?\")\n",
        "```\n",
        "\n",
        "The model will:\n",
        "\n",
        "* Decide when to use tools\n",
        "* Reflect on tool outputs\n",
        "* Continue reasoning until it‚Äôs ready to answer\n",
        "\n",
        "---\n",
        "\n",
        "### üß† TL;DR\n",
        "\n",
        "> **ReAct** is a powerful prompting pattern that helps LLMs reason and act in cycles ‚Äî enabling true multi-step agents.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCkKLcg__PTl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mP5T_K-t_SqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}