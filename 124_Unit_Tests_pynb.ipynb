{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtkCmG4UNd/QOh/VtLEo+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/124_Unit_Tests_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unit tests** are one of the most valuable tools in your developer toolkit, especially when you're building agents or any modular system.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ What Are Unit Tests?\n",
        "\n",
        "**Unit tests** are small, focused tests that check whether **a single unit of code** (usually a function or method) works as expected.\n",
        "\n",
        "* ‚úÖ **Test what should work**\n",
        "* ‚ùå **Catch what might break**\n",
        "\n",
        "A **unit** = the smallest testable part of your code (e.g. `parse_tool_response()`).\n",
        "\n",
        "---\n",
        "\n",
        "## üí° What Do Unit Tests Do?\n",
        "\n",
        "They verify that:\n",
        "\n",
        "* Your code **produces correct results** for valid inputs\n",
        "* Your code **raises the right errors** for invalid inputs\n",
        "* Your code behaves consistently as it evolves\n",
        "\n",
        "In short: they act as a **safety net** so you can refactor, optimize, or add features without breaking things.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Why Are Unit Tests Important?\n",
        "\n",
        "| Reason            | Explanation                                                                      |\n",
        "| ----------------- | -------------------------------------------------------------------------------- |\n",
        "| **Confidence**    | Know when your code breaks ‚Äî instantly.                                          |\n",
        "| **Speed**         | Catch bugs early, before full runs or deployment.                                |\n",
        "| **Documentation** | Tests show others (and your future self) what your function is *supposed* to do. |\n",
        "| **Maintenance**   | Easier to upgrade or refactor without breaking working code.                     |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß How Do Unit Tests Work?\n",
        "\n",
        "In Python, the most common tool is [`pytest`](https://docs.pytest.org/en/stable/).\n",
        "\n",
        "\n",
        "\n",
        "## üß† Where Unit Tests Fit in Agent Development\n",
        "\n",
        "| Layer      | Unit Test What?                                   |\n",
        "| ---------- | ------------------------------------------------- |\n",
        "| Tool logic | Parsing responses, validating schema              |\n",
        "| Wrappers   | Retry logic, error boundaries                     |\n",
        "| Planning   | Individual planner functions, parsing thoughts    |\n",
        "| End-to-end | (later) integration tests to simulate entire runs |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "* Unit tests **check correctness** of individual functions\n",
        "* They **fail fast**, letting you fix small bugs early\n",
        "* Tests help make your code **modular, testable, maintainable**\n",
        "* You‚Äôll need them **especially** as your agent grows in complexity\n",
        "\n",
        "---\n",
        "\n",
        "Would you like to:\n",
        "\n",
        "* ‚úÖ Write your first test suite for `parse_tool_response()`?\n",
        "* ‚öôÔ∏è Learn how to set up `pytest` in Colab?\n",
        "* üì¶ Learn about testing tools for LLMs/Agents specifically?\n"
      ],
      "metadata": {
        "id": "xjtxopyNu9hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pytest.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ4ahRMmw2Oc",
        "outputId": "6c45a8ae-13b6-4296-c3b4-1409520c1b78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ‚úÖ Define a Simple Function"
      ],
      "metadata": {
        "id": "FpTEgXJ9v_we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this to a .py file\n",
        "%%writefile my_utils.py\n",
        "\n",
        "def add(x, y):\n",
        "    if not isinstance(x, (int, float)) or not isinstance(y, (int, float)):\n",
        "        raise TypeError(\"x and y must be numbers\")\n",
        "    return x + y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSV8jnolvd1l",
        "outputId": "0617fcf7-7bdb-4d5f-aa3a-9eabb8d32ff9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. üß™ Write Test Cases Using pytest"
      ],
      "metadata": {
        "id": "IcB7cPgfwExD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your tests to a separate test file\n",
        "%%writefile test_my_utils.py\n",
        "\n",
        "import pytest\n",
        "from my_utils import add\n",
        "\n",
        "def test_add_positive():\n",
        "    assert add(2, 3) == 5\n",
        "\n",
        "def test_add_floats():\n",
        "    assert add(1.5, 2.5) == 4.0\n",
        "\n",
        "def test_add_negative():\n",
        "    assert add(-1, -2) == -3\n",
        "\n",
        "def test_add_type_error():\n",
        "    with pytest.raises(TypeError):\n",
        "        add(\"a\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRlntzbAvdzO",
        "outputId": "614d851c-08ba-4653-8138-4038a85b4995"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_my_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ‚ñ∂Ô∏è Run Your Tests"
      ],
      "metadata": {
        "id": "NQMmZp31wLLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_my_utils.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhAW6cx5vqoA",
        "outputId": "38563d3a-fca3-4d8f-f8d1-bd12ec036010"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-4.10.0, langsmith-0.4.23, typeguard-4.4.4\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_my_utils.py::test_add_positive \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 25%]\u001b[0m\n",
            "test_my_utils.py::test_add_floats \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 50%]\u001b[0m\n",
            "test_my_utils.py::test_add_negative \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 75%]\u001b[0m\n",
            "test_my_utils.py::test_add_type_error \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful! üéâ You just ran your first suite of **unit tests** using `pytest`, and all tests passed ‚Äî that‚Äôs a huge step in writing **reliable**, **defensive**, and **robust** code.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Let‚Äôs Reflect: What Just Happened?\n",
        "\n",
        "You‚Äôve now practiced:\n",
        "\n",
        "### ‚úÖ Creating a function with validation\n",
        "\n",
        "```python\n",
        "def add(x, y):\n",
        "    # Type checking!\n",
        "    if not isinstance(x, (int, float)) or not isinstance(y, (int, float)):\n",
        "        raise TypeError(\"x and y must be numbers\")\n",
        "    return x + y\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Writing focused tests\n",
        "\n",
        "Each test checked a specific behavior or failure mode:\n",
        "\n",
        "| Test Name             | What It Checks              | Outcome |\n",
        "| --------------------- | --------------------------- | ------- |\n",
        "| `test_add_positive`   | Simple positive numbers     | ‚úÖ Pass  |\n",
        "| `test_add_floats`     | Handling of float values    | ‚úÖ Pass  |\n",
        "| `test_add_negative`   | Negative inputs             | ‚úÖ Pass  |\n",
        "| `test_add_type_error` | Error raised for wrong type | ‚úÖ Pass  |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Seeing Immediate Feedback\n",
        "\n",
        "```bash\n",
        "test_my_utils.py::test_add_type_error PASSED\n",
        "```\n",
        "\n",
        "This shows that **your code behaves exactly as expected** ‚Äî or lets you fix it fast if it doesn‚Äôt.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6vcg6sLbwwfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùå Example: Function with a Bug\n",
        "\n",
        "Let‚Äôs write a function that mistakenly divides instead of adds:"
      ],
      "metadata": {
        "id": "RLs6y3rLxTEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(x, y):\n",
        "    return x / y  # ‚ùå Bug: should be x + y\n",
        "\n",
        "# Now, write a test expecting correct addition:\n",
        "\n",
        "def test_addition_should_work():\n",
        "    assert add(2, 3) == 5\n",
        "\n",
        "!pytest test_buggy_utils.py -v\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFmsmbSxwzBx",
        "outputId": "aa17b5f4-d007-41eb-f7ff-0f26da529418"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-4.10.0, langsmith-0.4.23, typeguard-4.4.4\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 0 items                                                              \u001b[0m\n",
            "\n",
            "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =============================\u001b[0m\n",
            "\u001b[31mERROR: file or directory not found: test_buggy_utils.py\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**writing and running tests in Colab (or Jupyter) requires some structure and order**. Here's a quick guide to help you smoothly **set up and run unit tests with `pytest`** in a notebook environment like Colab:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Unit Testing in Google Colab: Setup Guide\n",
        "\n",
        "### üß± 1. **Write Modules and Test Files in Separate Cells**\n",
        "\n",
        "Always use `%%writefile` and run **each file definition in its own cell**:\n",
        "\n",
        "* Module (e.g., `buggy_utils.py`)\n",
        "* Test script (e.g., `test_buggy_utils.py`)\n",
        "\n",
        "‚úÖ Why?\n",
        "\n",
        "* `%%writefile` only works when it's the **first line in a cell**.\n",
        "* Files don't get written unless you **run the cell**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 2. **Follow This Pattern**\n",
        "\n",
        "#### ‚úèÔ∏è Code file\n",
        "\n",
        "```python\n",
        "%%writefile my_utils.py\n",
        "\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "```\n",
        "\n",
        "#### ‚úÖ Test file\n",
        "\n",
        "```python\n",
        "%%writefile test_my_utils.py\n",
        "\n",
        "from my_utils import add\n",
        "\n",
        "def test_add_positive():\n",
        "    assert add(2, 3) == 5\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ñ∂Ô∏è 3. **Run Tests with `pytest`**\n",
        "\n",
        "Now run this in a **new cell**:\n",
        "\n",
        "```python\n",
        "!pytest test_my_utils.py -v\n",
        "```\n",
        "\n",
        "It will show you:\n",
        "\n",
        "* Which test ran\n",
        "* Whether it **passed or failed**\n",
        "* Detailed **traceback info** for failures\n",
        "\n",
        "---\n",
        "\n",
        "### üìå 4. **Avoid Common Pitfalls**\n",
        "\n",
        "| Mistake                                       | Fix                                                             |\n",
        "| --------------------------------------------- | --------------------------------------------------------------- |\n",
        "| Writing multiple files in one cell            | Use **one `%%writefile` per cell**                              |\n",
        "| Forgetting to run a cell after writing a file | **Always execute** file-writing cells before using them         |\n",
        "| Typos in filenames                            | Keep test file names like `test_*.py` so `pytest` picks them up |\n",
        "| Not using `assert`                            | `pytest` works by evaluating `assert` expressions               |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 5. **Test File Structure Tips**\n",
        "\n",
        "Test files should:\n",
        "\n",
        "* Start with `test_` in the filename (so `pytest` finds them)\n",
        "* Use simple function names like `test_add_positive`\n",
        "* Import the module you want to test\n",
        "* Use `assert` statements to check correctness\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Bonus: Re-run Tests Easily\n",
        "\n",
        "You can re-run the test file anytime after editing just the module:\n",
        "\n",
        "```python\n",
        "%%writefile my_utils.py\n",
        "# Fix your code here\n",
        "```\n",
        "\n",
        "Then:\n",
        "\n",
        "```python\n",
        "!pytest test_my_utils.py -v\n",
        "```\n",
        "\n",
        "No need to rewrite the test file unless you're adding more tests.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PfLXgl5PySRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. üîß Create the buggy_utils.py module\n",
        "%%writefile buggy_utils.py\n",
        "\n",
        "def add(x, y):\n",
        "    return x / y  # ‚ùå Bug: should be x + y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Yr-2V9x6Hq",
        "outputId": "5d65e293-55e9-45e1-8a69-75ce1df2b39f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting buggy_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. üß™ Create the test file\n",
        "%%writefile test_buggy_utils.py\n",
        "\n",
        "from buggy_utils import add\n",
        "\n",
        "def test_addition_should_work():\n",
        "    assert add(2, 3) == 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCZzDsx9zSjJ",
        "outputId": "ceca6b79-6363-4000-a91b-805619ed91ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_buggy_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Run Tests with pytest\n",
        "!pytest test_buggy_utils.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6efMbsZx6EK",
        "outputId": "17010282-295d-4d65-81a0-8ec1a045c0de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-4.10.0, langsmith-0.4.23, typeguard-4.4.4\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_buggy_utils.py::test_addition_should_work \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m__________________________ test_addition_should_work ___________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_addition_should_work\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m add(\u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m) == \u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       assert 0.6666666666666666 == 5\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where 0.6666666666666666 = add(2, 3)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_buggy_utils.py\u001b[0m:5: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_buggy_utils.py::\u001b[1mtest_addition_should_work\u001b[0m - assert 0.6666666666666666 == 5\n",
            "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.14s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**if you're using `pytest`**, you **must** save your functions (and test functions) to actual **Python files** on disk (like `.py` files), even in Colab or Jupyter. Here's why:\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Why `pytest` Requires Files\n",
        "\n",
        "### ‚úÖ `pytest` is a file-based test runner\n",
        "\n",
        "* It **scans Python files** (e.g., `test_*.py`) for test functions.\n",
        "* It doesn't \"see\" what's in your notebook memory unless it's saved to disk.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ What Needs to Be in Files?\n",
        "\n",
        "| Component           | File Needed? | Notes                                             |\n",
        "| ------------------- | ------------ | ------------------------------------------------- |\n",
        "| Your actual code    | ‚úÖ Yes        | Save to a module like `my_utils.py`               |\n",
        "| Your test functions | ‚úÖ Yes        | Save to a file like `test_my_utils.py`            |\n",
        "| Test runner         | ‚ùå No         | You can run `!pytest` directly in a notebook cell |\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Minimal Working Example\n",
        "\n",
        "#### ‚úÖ 1. Save your code to a file\n",
        "\n",
        "```python\n",
        "%%writefile my_utils.py\n",
        "\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "```\n",
        "\n",
        "#### ‚úÖ 2. Save your tests to another file\n",
        "\n",
        "```python\n",
        "%%writefile test_my_utils.py\n",
        "\n",
        "from my_utils import add\n",
        "\n",
        "def test_addition():\n",
        "    assert add(2, 3) == 5\n",
        "```\n",
        "\n",
        "#### ‚ñ∂Ô∏è 3. Run the tests\n",
        "\n",
        "```python\n",
        "!pytest test_my_utils.py -v\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tRqluCE30vI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a slightly more **realistic and complex example** that still teaches core testing skills but with a bit more logic.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Scenario: A Data Processing Utility\n",
        "\n",
        "We‚Äôll create a small utility function that:\n",
        "\n",
        "* Takes a list of numbers (as strings).\n",
        "* Tries to convert them to integers.\n",
        "* Skips invalid entries.\n",
        "* Returns the sum.\n",
        "\n",
        "This is a great opportunity to test:\n",
        "\n",
        "* Normal cases\n",
        "* Edge cases (empty list, all bad input)\n",
        "* Error handling\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What You're Practicing\n",
        "\n",
        "* ‚úÖ Writing real-world utility code\n",
        "* ‚úÖ Handling exceptions (`ValueError`)\n",
        "* ‚úÖ Writing clean, independent tests\n",
        "* ‚úÖ Using `pytest` to run and summarize test results\n",
        "* ‚úÖ Preparing functions that could easily go into Agents or Pipelines\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LFnTOCm13oBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 1. The Module Code (data_utils.py)\n",
        "%%writefile data_utils.py\n",
        "\n",
        "def sum_clean_numbers(number_strings):\n",
        "    \"\"\"\n",
        "    Convert a list of strings to integers and return the sum.\n",
        "    Skip values that cannot be converted.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    for s in number_strings:\n",
        "        try:\n",
        "            total += int(s)\n",
        "        except ValueError:\n",
        "            continue  # Skip invalid entries\n",
        "    return total\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMFpycd5x6BW",
        "outputId": "235e9ff0-0e06-479d-b8bf-ce5c18421e6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ 2. Test Code (test_data_utils.py)\n",
        "%%writefile test_data_utils.py\n",
        "\n",
        "from data_utils import sum_clean_numbers\n",
        "\n",
        "def test_all_valid_numbers():\n",
        "    assert sum_clean_numbers([\"1\", \"2\", \"3\"]) == 6\n",
        "\n",
        "def test_some_invalid_numbers():\n",
        "    assert sum_clean_numbers([\"10\", \"oops\", \"5\", \"NaN\"]) == 15\n",
        "\n",
        "def test_all_invalid():\n",
        "    assert sum_clean_numbers([\"oops\", \"NaN\", \"hello\"]) == 0\n",
        "\n",
        "def test_empty_list():\n",
        "    assert sum_clean_numbers([]) == 0\n",
        "\n",
        "def test_negative_and_positive():\n",
        "    assert sum_clean_numbers([\"-2\", \"5\", \"3\"]) == 6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu0XftI8x5-n",
        "outputId": "28ba3876-993f-481e-d1d3-c1bbc5f95b8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ñ∂Ô∏è 3. Run the Tests\n",
        "!pytest test_data_utils.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeM2Mau138LG",
        "outputId": "42cfbaa0-cd1b-4e89-9899-bb59fac468b3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-4.10.0, langsmith-0.4.23, typeguard-4.4.4\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_data_utils.py::test_all_valid_numbers \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 20%]\u001b[0m\n",
            "test_data_utils.py::test_some_invalid_numbers \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 40%]\u001b[0m\n",
            "test_data_utils.py::test_all_invalid \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 60%]\u001b[0m\n",
            "test_data_utils.py::test_empty_list \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 80%]\u001b[0m\n",
            "test_data_utils.py::test_negative_and_positive \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**writing 5 tests for a single function** can feel like a lot. But this is actually a strength of testing: **we‚Äôre breaking down possible inputs and edge cases** to make sure the function behaves correctly in *all* expected scenarios.\n",
        "\n",
        "Let‚Äôs walk through:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why Multiple Tests?\n",
        "\n",
        "Each test covers a **unique behavior or edge case**:\n",
        "\n",
        "* `test_all_valid_numbers`: normal use case\n",
        "* `test_some_invalid_numbers`: partial failure\n",
        "* `test_all_invalid`: error resilience\n",
        "* `test_empty_list`: edge case\n",
        "* `test_negative_and_positive`: input variety\n",
        "\n",
        "If you **combine all of these into one test**, you wouldn‚Äôt know *which case failed* if it breaks. Multiple tests = better debugging and confidence.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Best Practices for Writing Tests\n",
        "\n",
        "### 1. **One Assertion Per Test (ideally)**\n",
        "\n",
        "Each test should validate **one behavior** so it‚Äôs obvious what broke and why.\n",
        "\n",
        "```python\n",
        "def test_empty_list_returns_zero():\n",
        "    assert sum_clean_numbers([]) == 0\n",
        "```\n",
        "\n",
        "### 2. **Name Tests Clearly**\n",
        "\n",
        "Use readable test names:\n",
        "\n",
        "```python\n",
        "test_valid_input_returns_sum\n",
        "test_invalid_entries_are_ignored\n",
        "```\n",
        "\n",
        "### 3. **Start with Happy Path**\n",
        "\n",
        "Always test the most common, expected use case first (often called the *happy path*).\n",
        "\n",
        "### 4. **Then Add Edge Cases**\n",
        "\n",
        "Think:\n",
        "\n",
        "* Empty inputs\n",
        "* Wrong types\n",
        "* Huge inputs\n",
        "* Zero, None, or Null-like values\n",
        "\n",
        "### 5. **Avoid Logic in Tests**\n",
        "\n",
        "Tests should be simple and declarative. Avoid loops, conditionals, or complex calculations inside test code.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Tips and Tricks\n",
        "\n",
        "### ‚úÖ Use Parametrization (for less repetition)\n",
        "\n",
        "With `pytest.mark.parametrize`, you can test multiple inputs in one function:\n",
        "\n",
        "```python\n",
        "import pytest\n",
        "from data_utils import sum_clean_numbers\n",
        "\n",
        "@pytest.mark.parametrize(\"input_list, expected\", [\n",
        "    ([\"1\", \"2\", \"3\"], 6),\n",
        "    ([\"10\", \"oops\", \"5\", \"NaN\"], 15),\n",
        "    ([\"oops\", \"NaN\", \"hello\"], 0),\n",
        "    ([], 0),\n",
        "])\n",
        "def test_sum_clean_numbers(input_list, expected):\n",
        "    assert sum_clean_numbers(input_list) == expected\n",
        "```\n",
        "\n",
        "‚úÖ Saves time\n",
        "‚úÖ Easier to scale\n",
        "‚úÖ Still keeps test output clear\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Bonus: When Should You Write Tests?\n",
        "\n",
        "| Phase             | Strategy                                                          |\n",
        "| ----------------- | ----------------------------------------------------------------- |\n",
        "| ‚úÖ During dev      | Test as you go (Test-Driven Development is optional but powerful) |\n",
        "| ‚úÖ Before refactor | Ensure behavior doesn't change unexpectedly                       |\n",
        "| ‚úÖ After bug fix   | Write a test that would‚Äôve caught the bug                         |\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Summary\n",
        "\n",
        "| Principle                 | Why It Matters                     |\n",
        "| ------------------------- | ---------------------------------- |\n",
        "| Small, focused tests      | Better debug and traceability      |\n",
        "| Clear test names          | Easier for team and LLMs to reason |\n",
        "| Cover normal + edge cases | Prevent rare bugs from surfacing   |\n",
        "| Automate & run regularly  | Prevent regressions                |\n",
        "| Parametrize when possible | Cleaner and more scalable tests    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuhhxb244_CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ Original Function (for reference)\n",
        "def sum_clean_numbers(values):\n",
        "    total = 0\n",
        "    for val in values:\n",
        "        try:\n",
        "            total += int(val)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return total\n"
      ],
      "metadata": {
        "id": "XrPTfP-93_R-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Parametrized Test Version"
      ],
      "metadata": {
        "id": "5vXEvZaa5b1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Parametrized Test Version\n",
        "\n",
        "import pytest\n",
        "from data_utils import sum_clean_numbers\n",
        "\n",
        "@pytest.mark.parametrize(\"input_list, expected\", [\n",
        "    ([\"1\", \"2\", \"3\"], 6),                    # ‚úÖ all valid\n",
        "    ([\"10\", \"oops\", \"5\", \"NaN\"], 15),        # ‚ùå some invalid\n",
        "    ([\"oops\", \"NaN\", \"hello\"], 0),           # ‚ùå all invalid\n",
        "    ([], 0),                                 # ‚úÖ empty list\n",
        "    ([\"-3\", \"7\", \"0\", \"bad\"], 4),            # ‚úÖ mixed positive/negative\n",
        "])\n",
        "def test_sum_clean_numbers(input_list, expected):\n",
        "    assert sum_clean_numbers(input_list) == expected\n"
      ],
      "metadata": {
        "id": "iENtXvio5PT0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test_data_utils.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNEn9lGE5UZY",
        "outputId": "c1e9a412-1d9b-47d8-afd9-720ec931ea8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-4.10.0, langsmith-0.4.23, typeguard-4.4.4\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "test_data_utils.py::test_all_valid_numbers \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 20%]\u001b[0m\n",
            "test_data_utils.py::test_some_invalid_numbers \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 40%]\u001b[0m\n",
            "test_data_utils.py::test_all_invalid \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 60%]\u001b[0m\n",
            "test_data_utils.py::test_empty_list \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 80%]\u001b[0m\n",
            "test_data_utils.py::test_negative_and_positive \u001b[32mPASSED\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unit testing in the context of Agents** is a nuanced and evolving topic. Here‚Äôs a breakdown to guide your thinking:\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Unit Testing with Agents: Overview\n",
        "\n",
        "### üß™ What *Should* Be Tested?\n",
        "\n",
        "| ‚úÖ You **should** test:                                                 | ‚ùå You **shouldn't** test:                         |\n",
        "| ---------------------------------------------------------------------- | ------------------------------------------------- |\n",
        "| Core utilities & helper functions (e.g., parsing, validation, retries) | LLM outputs directly (too variable)               |\n",
        "| Tool behaviors (simulate how tools behave given inputs)                | Agent reasoning paths end-to-end (via unit tests) |\n",
        "| Error handling & fallbacks                                             | Randomness in prompts                             |\n",
        "| Prompt formatting functions                                            | Full LLM conversations                            |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What Is Worth Testing?\n",
        "\n",
        "### 1. **Functions that support the Agent**\n",
        "\n",
        "These are deterministic and easy to unit test:\n",
        "\n",
        "```python\n",
        "def format_prompt(user_input): ...\n",
        "def validate_json(data): ...\n",
        "def retry_if_fails(tool_call): ...\n",
        "```\n",
        "\n",
        "‚û°Ô∏è Test these like any normal Python function (you've already been doing this well!).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Tool wrappers**\n",
        "\n",
        "If your agent uses tools (APIs, DBs, scraping), test:\n",
        "\n",
        "* Input/output shapes\n",
        "* Error propagation\n",
        "* Timeout handling\n",
        "* Fallbacks\n",
        "\n",
        "Mock the tool if needed to avoid real API calls.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Custom error boundaries or retry logic**\n",
        "\n",
        "These are *critical* in agent pipelines and often break silently.\n",
        "Test:\n",
        "\n",
        "* Retryable vs non-retryable exceptions\n",
        "* Logging output\n",
        "* Agent can proceed when something fails\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå What‚Äôs Not Worth Unit Testing?\n",
        "\n",
        "### ‚ùå Prompt logic or LLM reasoning\n",
        "\n",
        "LLMs are probabilistic. You can‚Äôt write a unit test like:\n",
        "\n",
        "```python\n",
        "assert agent(\"summarize this\") == \"Here‚Äôs the summary\"\n",
        "```\n",
        "\n",
        "It will break randomly.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Alternative: Integration / Regression Testing\n",
        "\n",
        "While **unit tests** validate small pieces...\n",
        "\n",
        "üß© **Integration tests** run the full agent on *real-ish* input and check if it *completes successfully* (not exact outputs).\n",
        "\n",
        "You might check:\n",
        "\n",
        "* Did it call all tools correctly?\n",
        "* Was there a fallback?\n",
        "* Did it return a structured response?\n",
        "\n",
        "‚û°Ô∏è You **can snapshot outputs** (e.g., JSON), and compare formats over time.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary: Best Practices for Agents\n",
        "\n",
        "| Principle                 | Advice                              |\n",
        "| ------------------------- | ----------------------------------- |\n",
        "| Test small pieces         | Focus on utilities & tool wrappers  |\n",
        "| Avoid testing LLM outputs | Too unpredictable                   |\n",
        "| Use mocks                 | For API calls and tools             |\n",
        "| Use integration tests     | To catch regressions or flow issues |\n",
        "| Automate logs & traces    | Don‚Äôt rely on memory or guesses     |\n",
        "\n"
      ],
      "metadata": {
        "id": "Ut49ZbiQ6DJS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rF1Dj1rS5VDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}