{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4EGUrpj7DGywsSJ70pZV3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/163_Agentic_Patterns_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîé What are ‚Äúpatterns‚Äù in LangGraph?\n",
        "\n",
        "In this context, **patterns are reusable orchestration blueprints**.\n",
        "They aren‚Äôt code you must copy exactly, but **design idioms** ‚Äî ways of structuring agents, nodes, and edges to solve common categories of problems.\n",
        "\n",
        "Think of them like ‚Äúdesign patterns‚Äù in software engineering (Factory, Observer, etc.), but here they‚Äôre **agentic orchestration patterns**:\n",
        "\n",
        "* They show **how to break down tasks** into steps or branches.\n",
        "* They guide **how to connect nodes** (linear, branching, iterative).\n",
        "* They define **how to handle memory, reflection, or evaluation**.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Why are patterns useful?\n",
        "\n",
        "1. **Reusability**\n",
        "   You don‚Äôt have to reinvent orchestration for every new agent ‚Äî you can slot your domain-specific tools into a proven workflow.\n",
        "\n",
        "2. **Clarity**\n",
        "   Patterns give you mental models:\n",
        "\n",
        "   * Parallel Processing ‚Üí fan out and merge results.\n",
        "   * Reflection ‚Üí iterate until quality threshold.\n",
        "   * Tree of Thought ‚Üí branch and evaluate reasoning paths.\n",
        "\n",
        "3. **Scalability**\n",
        "   Complex systems are just compositions of simple patterns.\n",
        "   By learning patterns, you gain LEGO-like building blocks that can be combined.\n",
        "\n",
        "4. **Reliability**\n",
        "   Patterns encode best practices (like retry, review loops, consensus).\n",
        "   That means fewer dead ends, hallucinations, or brittle flows.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **So at a high level:**\n",
        "Patterns are **structured agent workflows** that balance exploration, feedback, and decision-making. They‚Äôre useful because they abstract away *how to orchestrate* so you can focus on *what your agent should do*.\n",
        "\n"
      ],
      "metadata": {
        "id": "pE8LBBH0jE0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **parallel processing pattern** is one of the most useful ones to learn in LangGraph because it introduces the idea of **fan-out and fan-in orchestration**. Let‚Äôs unpack what‚Äôs happening in your `parallel_processing.py` example.\n",
        "\n",
        "---\n",
        "\n",
        "## üîë What This Pattern Does\n",
        "\n",
        "* **Fan-out**: At the `START` node, the graph launches *multiple independent subtasks* in parallel:\n",
        "\n",
        "  * `fetch_trends`\n",
        "  * `analyze_competitors`\n",
        "  * `extract_sentiment`\n",
        "\n",
        "* **Independent execution**: Each of these nodes runs separately, queries the LLM, and writes back to different parts of the shared `MarketResearchState`.\n",
        "\n",
        "* **Fan-in (merge)**: All three outputs flow into the `summarize` node. This node reads the collected state (`trends`, `competitors`, `sentiment`) and produces a final `summary`.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Should Focus On\n",
        "\n",
        "1. **State Design**\n",
        "\n",
        "   * Notice how `MarketResearchState` defines fields for each subtask (`trends`, `competitors`, `sentiment`, and `summary`).\n",
        "   * This ensures the outputs don‚Äôt collide ‚Äî each node has a ‚Äúslot‚Äù in memory.\n",
        "\n",
        "2. **Parallel Edges**\n",
        "\n",
        "   * Multiple `graph_builder.add_edge(START, ...)` create branches that run independently.\n",
        "   * They all converge back into `summarize`.\n",
        "\n",
        "3. **Fan-in Dependency**\n",
        "\n",
        "   * The `summarize` node isn‚Äôt called until *all three feeder nodes have finished*.\n",
        "   * LangGraph handles that orchestration automatically ‚Äî you don‚Äôt need to manually synchronize.\n",
        "\n",
        "4. **Pattern Recognition**\n",
        "\n",
        "   * This is a reusable blueprint: any time you want to **gather multiple perspectives / analyses in parallel and then combine them**, you use this fan-out/fan-in shape.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Why It‚Äôs Useful\n",
        "\n",
        "* **Efficiency**: You can run multiple LLM calls in parallel rather than sequentially.\n",
        "* **Clarity**: The graph visually and structurally shows you‚Äôre collecting multiple ‚Äúingredients‚Äù before producing a final result.\n",
        "* **Extensibility**: You can easily add more branches (e.g. `regulatory_analysis`, `pricing_analysis`) without changing the orchestration logic.\n",
        "* **Reliability**: If one node fails, you can add error edges or fallbacks without disrupting the whole pattern.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå What to Learn From This Example\n",
        "\n",
        "* How to **define state fields** to keep parallel outputs organized.\n",
        "* How to **branch out from START** into multiple subtasks.\n",
        "* How LangGraph automatically **waits for all upstream nodes** before executing the downstream aggregator.\n",
        "* That this pattern is essentially a **map-reduce workflow**: map (independent tasks) ‚Üí reduce (summary node).\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Would you like me to draw this as an ASCII flowchart (fan-out/fan-in shape) so you can see the structure visually, the same way we did for your earlier orchestrator?\n"
      ],
      "metadata": {
        "id": "FKNlhLKB0OQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtpmUCKbiB-M"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Define state\n",
        "class MarketResearchState(TypedDict):\n",
        "    query: str\n",
        "    trends: str\n",
        "    competitors: str\n",
        "    sentiment: str\n",
        "    summary: str\n",
        "\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "\n",
        "def fetch_trends(state: MarketResearchState):\n",
        "    response = llm.invoke(f\"What are the latest market trends for {state['query']}?\")\n",
        "    return {\"trends\": response.content}\n",
        "\n",
        "\n",
        "def analyze_competitors(state: MarketResearchState):\n",
        "    response = llm.invoke(f\"List top competitors in {state['query']} market.\")\n",
        "    return {\"competitors\": response.content}\n",
        "\n",
        "\n",
        "def extract_sentiment(state: MarketResearchState):\n",
        "    response = llm.invoke(f\"What do customers feel about products in {state['query']} category?\")\n",
        "    return {\"sentiment\": response.content}\n",
        "\n",
        "\n",
        "def summarize(state: MarketResearchState):\n",
        "    summary_prompt = f\"\"\"\n",
        "    Product Research Summary:\n",
        "    - Trends: {state.get('trends')}\n",
        "    - Competitors: {state.get('competitors')}\n",
        "    - Customer Sentiment: {state.get('sentiment')}\n",
        "    Provide strategic insights for entering the {state['query']} market.\n",
        "    \"\"\"\n",
        "    response = llm.invoke(summary_prompt)\n",
        "    return {\"summary\": response.content}\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(MarketResearchState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"fetch_trends\", fetch_trends)\n",
        "graph_builder.add_node(\"analyze_competitors\", analyze_competitors)\n",
        "graph_builder.add_node(\"extract_sentiment\", extract_sentiment)\n",
        "graph_builder.add_node(\"summarize\", summarize)\n",
        "\n",
        "# TODO: Add edges for parallel execution\n",
        "graph_builder.add_edge(START,\"fetch_trends\")\n",
        "graph_builder.add_edge(START,\"analyze_competitors\")\n",
        "graph_builder.add_edge(START,\"extract_sentiment\")\n",
        "\n",
        "graph_builder.add_edge(\"fetch_trends\",\"summarize\")\n",
        "graph_builder.add_edge(\"analyze_competitors\",\"summarize\")\n",
        "graph_builder.add_edge(\"extract_sentiment\",\"summarize\")\n",
        "\n",
        "# Compile graph\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Run it\n",
        "inputs = {\"query\": \"Smart Water Bottle\"}\n",
        "result = graph.invoke(inputs)\n",
        "\n",
        "# Output\n",
        "print(\"\\n=== Final Market Summary ===\\n\")\n",
        "print(result[\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There‚Äôs actually **more than one reason** to use parallel processing in LangGraph.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö°Ô∏è 1. Speed (the obvious one)\n",
        "\n",
        "* Instead of doing:\n",
        "\n",
        "  ```\n",
        "  trends ‚Üí competitors ‚Üí sentiment ‚Üí summary\n",
        "  ```\n",
        "\n",
        "  (sequentially, where each step waits on the last),\n",
        "* You do:\n",
        "\n",
        "  ```\n",
        "  trends   ‚îê\n",
        "  competitors ‚îú‚îÄ‚îÄ> summary\n",
        "  sentiment ‚îò\n",
        "  ```\n",
        "\n",
        "  (in parallel, all three run at once).\n",
        "* That cuts latency roughly down to the longest branch instead of the sum of all branches.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 2. Independence of Subtasks\n",
        "\n",
        "* Many subtasks **don‚Äôt depend on each other** (e.g., fetching trends doesn‚Äôt need competitor analysis).\n",
        "* Running them sequentially would add artificial coupling.\n",
        "* Parallelization keeps logic *true to the problem structure*.\n",
        "\n",
        "---\n",
        "\n",
        "## üóÇÔ∏è 3. Separation of Concerns\n",
        "\n",
        "* Each branch writes to its own **state slot**.\n",
        "* That enforces **modularity**: your ‚Äútrends analysis‚Äù can evolve separately from ‚Äúcompetitor analysis.‚Äù\n",
        "* This makes maintenance and scaling easier.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ 4. Diversity of Perspectives\n",
        "\n",
        "* Parallel tasks are often **different perspectives** on the same data.\n",
        "* By fanning out, you encourage multiple ‚Äúangles‚Äù to be captured before aggregation.\n",
        "* This is especially useful in agentic patterns like **Tree of Thoughts** or **debate agents**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ 5. Natural Fit for Orchestration Tools\n",
        "\n",
        "* LangGraph automatically **waits for all inputs** before moving forward.\n",
        "* That makes fan-out/fan-in a clean way to ensure your final node has *complete* context.\n",
        "* It‚Äôs essentially a **MapReduce pattern**: map (parallel branches), then reduce (merge).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **So yes, speed is the headline advantage, but the *real power* comes from clarity, modularity, and the ability to collect multiple perspectives cleanly.**\n",
        "\n"
      ],
      "metadata": {
        "id": "6fEHqxeB1izn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because parallel processing is powerful, but **not always the right choice**. Here are the main times you *wouldn‚Äôt* want to use it:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå 1. When Tasks Depend on Each Other\n",
        "\n",
        "* If **Task B requires Task A‚Äôs output**, you can‚Äôt parallelize them.\n",
        "  Example: You must summarize a document *before* you can critique that summary.\n",
        "* Forcing parallelism here would either fail or produce nonsense.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå 2. When Order Matters\n",
        "\n",
        "* If tasks need to happen in a strict sequence (like steps in a math proof, or a multi-stage plan), parallelizing breaks the logical flow.\n",
        "* Think of it as: you can‚Äôt ‚Äúbake the cake‚Äù in parallel with ‚Äúmix the batter.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå 3. When Resources Are Constrained\n",
        "\n",
        "* Each branch is usually an LLM call ‚Üí more **tokens and cost**.\n",
        "* If you‚Äôre on a tight budget, parallelism multiplies expenses.\n",
        "* Sequential processing may be slower but cheaper.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå 4. When You Want a Single Coherent Narrative\n",
        "\n",
        "* Multiple branches = multiple perspectives. That‚Äôs great for research, but not if you want one **continuous, logically tight chain of thought**.\n",
        "* Example: writing a story chapter by chapter ‚Äî if each ‚Äúchapter agent‚Äù runs in parallel, they‚Äôll diverge.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå 5. When Concurrency Overhead Outweighs Gains\n",
        "\n",
        "* For very lightweight tasks (e.g., string formatting, metadata lookups), the parallel setup adds complexity with little benefit.\n",
        "* Parallel processing shines when subtasks are heavy or slow.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Rule of Thumb\n",
        "\n",
        "Use parallelism when subtasks are:\n",
        "\n",
        "* Independent,\n",
        "* Expensive enough to justify parallel calls,\n",
        "* And meant to be merged at the end.\n",
        "\n",
        "Avoid it when subtasks are **sequentially dependent, order-sensitive, or cost-sensitive**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yt38xG-X15JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîÑ What the Reflection Pattern Is\n",
        "\n",
        "It‚Äôs an **iterative loop** where the agent:\n",
        "\n",
        "1. **Generates** an initial solution (in your code: Python code for a given problem).\n",
        "2. **Reviews** that solution, producing feedback and a score.\n",
        "3. **Improves** the solution using the feedback.\n",
        "4. **Loops** back to review ‚Üí improve ‚Üí review, until:\n",
        "\n",
        "   * The score is ‚Äúgood enough‚Äù (‚â•9 in this case), or\n",
        "   * The max number of iterations is reached (3 here).\n",
        "\n",
        "Finally, it exits and returns the refined output.\n",
        "\n",
        "This is basically giving the agent a ‚Äúself-critique + self-improvement‚Äù cycle.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Should Learn Here\n",
        "\n",
        "### 1. **Self-Improvement Through Feedback**\n",
        "\n",
        "Instead of a single LLM pass, the agent improves results over multiple cycles.\n",
        "\n",
        "* First draft may be weak.\n",
        "* Review step identifies flaws.\n",
        "* Improvement step fixes them.\n",
        "* Each loop pushes quality upward.\n",
        "\n",
        "### 2. **Separation of Roles**\n",
        "\n",
        "* **Generator** ‚Üí ‚ÄúWrites code.‚Äù\n",
        "* **Reviewer** ‚Üí ‚ÄúCritiques code with a score.‚Äù\n",
        "* **Improver** ‚Üí ‚ÄúApplies changes.‚Äù\n",
        "  Each role has a clear persona (expert dev, senior reviewer, refiner).\n",
        "  This mirrors real-world workflows (writer ‚Üí editor ‚Üí rewriter).\n",
        "\n",
        "### 3. **Scoring & Stopping Criteria**\n",
        "\n",
        "The loop doesn‚Äôt run forever. It uses:\n",
        "\n",
        "* **Review score ‚â• 9** ‚Üí stop.\n",
        "* **Iteration ‚â• 3** ‚Üí stop.\n",
        "  This teaches you that reflection agents need **objective criteria** to avoid infinite loops.\n",
        "\n",
        "### 4. **LangGraph‚Äôs Power**\n",
        "\n",
        "Notice how clean the orchestration is:\n",
        "\n",
        "* Just three nodes (`generate_code`, `review_code`, `improve_code`).\n",
        "* `Command(goto=...)` decides whether to loop back or end.\n",
        "* State carries along the current code, feedback, score, and iteration count.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Why This Pattern Matters\n",
        "\n",
        "* **Quality improvement**: Better than one-shot answers.\n",
        "* **Reusability**: Works for code, essays, plans, images‚Ä¶ anything improvable.\n",
        "* **Transparency**: You can inspect every iteration‚Äôs output.\n",
        "* **Reliability**: Adds checkpoints (review + score) instead of blind generation.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **So the key takeaway here is:** Reflection agents let your LLM *simulate an editor/critic loop*, producing more polished outputs with clear stop conditions.\n"
      ],
      "metadata": {
        "id": "C0iJsbGIiIru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import END\n",
        "from langgraph.types import Command\n",
        "\n",
        "# Initialize OpenAI model\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "\n",
        "# Define Agent State\n",
        "class CodeState(TypedDict):\n",
        "    problem_statement: str\n",
        "    generated_code: str\n",
        "    review_feedback: str\n",
        "    refined_code: str\n",
        "    iteration: int\n",
        "    review_score: float\n",
        "\n",
        "\n",
        "# üü¢ Step 1: Generate Initial Code\n",
        "def generate_code(state: CodeState):\n",
        "    print(\"Generating Code\")\n",
        "    prompt = f\"\"\"\n",
        "    Write a clean, efficient, and well-commented Python solution for the following problem:\n",
        "\n",
        "    {state['problem_statement']}\n",
        "    \"\"\"\n",
        "    response = llm.invoke([SystemMessage(content=\"You are an expert Python developer.\"), HumanMessage(content=prompt)])\n",
        "\n",
        "    state[\"generated_code\"] = response.content\n",
        "    state[\"iteration\"] = 1\n",
        "    state[\"review_score\"] = 0\n",
        "    return Command(goto=\"review_code\", update=state)\n",
        "\n",
        "\n",
        "# üü¢ Step 2: Review the Generated Code with a Score\n",
        "def review_code(state: CodeState):\n",
        "    print(\"Reviewing Code\")\n",
        "    prompt = f\"\"\"\n",
        "    Review the following Python code for correctness, readability, efficiency, and best practices:\n",
        "\n",
        "    {state['generated_code']}\n",
        "\n",
        "    Provide a list of improvements and necessary changes.\n",
        "    Also, give a **review score** (1-10) for:\n",
        "    - Correctness\n",
        "    - Readability\n",
        "    - Efficiency\n",
        "    - Maintainability\n",
        "\n",
        "    Provide the average score out of 10 at the end.\n",
        "    The last line should contain just the final score as a final_score:score\n",
        "    \"\"\"\n",
        "    response = llm.invoke(\n",
        "        [SystemMessage(content=\"You are a senior software engineer reviewing code.\"), HumanMessage(content=prompt)]\n",
        "    )\n",
        "\n",
        "    state[\"review_feedback\"] = response.content\n",
        "\n",
        "    # Extract the review score from AI feedback (assuming last line is \"final_score: 8\")\n",
        "    try:\n",
        "        lines = response.content.split(\"\\n\")\n",
        "        last_line = lines[-1]\n",
        "        state[\"review_score\"] = float(last_line.split(\":\")[-1].strip())\n",
        "    except:\n",
        "        print(\"EXCEPT\")\n",
        "        state[\"review_score\"] = 5  # Default if parsing fails\n",
        "\n",
        "    return Command(goto=\"improve_code\", update=state)\n",
        "\n",
        "\n",
        "# üü¢ Step 3: Improve Code Based on Feedback\n",
        "def improve_code(state: CodeState):\n",
        "    print(\"Improving Code\")\n",
        "    print(\"Review Score \", state[\"review_score\"], \"Iteration \", state[\"iteration\"])\n",
        "    # TODO: Stop Iteration\n",
        "    if state[\"review_score\"] >= 9 or state[\"iteration\"] >= 3:\n",
        "        state[\"refined_code\"] = state[\"generated_code\"]\n",
        "        return Command(goto=END)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Here is the initial Python code:\n",
        "\n",
        "    {state['generated_code']}\n",
        "\n",
        "    And here is the review feedback:\n",
        "\n",
        "    {state['review_feedback']}\n",
        "\n",
        "    Apply the suggested improvements and rewrite the code with better efficiency, readability, and correctness.\n",
        "    \"\"\"\n",
        "    response = llm.invoke([SystemMessage(content=\"You are an AI code refiner.\"), HumanMessage(content=prompt)])\n",
        "\n",
        "    state[\"generated_code\"] = response.content\n",
        "    state[\"iteration\"] += 1\n",
        "    return Command(goto=\"review_code\", update=state)\n",
        "\n",
        "\n",
        "# üîµ Build the LangGraph Workflow\n",
        "workflow = StateGraph(CodeState)\n",
        "\n",
        "# Adding Nodes\n",
        "workflow.add_node(\"generate_code\", generate_code)\n",
        "workflow.add_node(\"review_code\", review_code)\n",
        "workflow.add_node(\"improve_code\", improve_code)\n",
        "\n",
        "# Define Execution Flow\n",
        "workflow.set_entry_point(\"generate_code\")\n",
        "\n",
        "# Compile Graph\n",
        "graph = workflow.compile()\n",
        "\n",
        "# üü¢ Run Example\n",
        "input_data = {\n",
        "    \"problem_statement\": \"Write a function to find the factorial of a number in Python.\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_data)\n",
        "\n",
        "print(\"üöÄ Final Code After Reflection:\\n\", result[\"generated_code\"])\n",
        "print(\"\\nüîç Final Review Feedback:\\n\", result[\"review_feedback\"])"
      ],
      "metadata": {
        "id": "LCPWYCvgiI9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ One of the biggest joys of working with **LangGraph**: once you learn the core building blocks, every new orchestrator is just a **plug-and-play swap of nodes**.\n",
        "\n",
        "The recipe always looks the same:\n",
        "\n",
        "1. **Define the State**\n",
        "\n",
        "   * A `TypedDict` (or Pydantic model) that holds whatever fields you need for memory, feedback, results, etc.\n",
        "   * This is the *contract* that your nodes read/write.\n",
        "\n",
        "2. **Write your Node functions**\n",
        "\n",
        "   * Each node is just a Python function: takes `state`, returns `{updates}`.\n",
        "   * Could be a tool call, an LLM prompt, a database query, whatever.\n",
        "   * If you want reflection, you add nodes like `generate`, `review`, `improve`.\n",
        "\n",
        "3. **Build the Graph**\n",
        "\n",
        "   * `builder = StateGraph(State)`\n",
        "   * `builder.add_node(\"generate\", generate_fn)`\n",
        "   * `builder.add_node(\"review\", review_fn)`\n",
        "   * `builder.add_node(\"improve\", improve_fn)`\n",
        "\n",
        "4. **Wire Edges**\n",
        "\n",
        "   * Normal flow: `builder.add_edge(\"generate\", \"review\")`\n",
        "   * Loops/conditions: `Command(goto=\"improve\")` or `END`.\n",
        "   * Error/fallbacks: add error edges if you want resilience.\n",
        "\n",
        "5. **Set entry point + compile**\n",
        "\n",
        "   * `builder.set_entry_point(\"generate\")`\n",
        "   * `graph = builder.compile()`\n",
        "\n",
        "6. **Run**\n",
        "\n",
        "   * `graph.invoke({\"prompt\": \"...\"})`\n",
        "\n",
        "---\n",
        "\n",
        "So whether you‚Äôre doing:\n",
        "\n",
        "* Parallel processing (fan-out/fan-in),\n",
        "* Reflection loops,\n",
        "* Tree-of-thought branching,\n",
        "* Or something custom‚Ä¶\n",
        "\n",
        "üëâ the orchestration scaffolding is always the same. Only your **nodes** (tools/logic) change.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Takeaway:** once you‚Äôve internalized `add_node ‚Üí add_edge ‚Üí set_entry_point ‚Üí compile`, you can build entirely new orchestrators by just swapping in different functions. Everything else is boilerplate that stays the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnXRo5HsiPHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üå≥ What the Tree of Thought (ToT) Pattern Is\n",
        "\n",
        "This workflow mimics a **branch-and-evaluate reasoning process**:\n",
        "\n",
        "1. **Generate branches** ‚Üí multiple candidate strategies (`expansion_options`).\n",
        "2. **Analyze each branch** ‚Üí evaluate them individually (`strategy_analysis`).\n",
        "3. **Select the best branch** ‚Üí consolidate analyses into a decision (`best_strategy`).\n",
        "\n",
        "It‚Äôs called ‚ÄúTree of Thought‚Äù because instead of one straight line of reasoning, the agent explores *multiple paths in parallel* before pruning down to one.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Should Learn Here\n",
        "\n",
        "### 1. **Exploration vs. Exploitation**\n",
        "\n",
        "* The agent doesn‚Äôt just pick the first idea.\n",
        "* It explores *several possible paths* (like branches of a tree).\n",
        "* Then it evaluates and prunes back to the strongest.\n",
        "  This is crucial for tasks where quality depends on considering alternatives.\n",
        "\n",
        "### 2. **Structured Evaluation**\n",
        "\n",
        "* Each branch gets a dedicated analysis node (`analyze_strategy`).\n",
        "* The analysis uses criteria (cost, risk, ROI).\n",
        "* This enforces **explicit evaluation dimensions** rather than fuzzy intuition.\n",
        "\n",
        "### 3. **Decision Consolidation**\n",
        "\n",
        "* A final node (`select_best_strategy`) makes the call.\n",
        "* This stage integrates prior evaluations into a ranked choice.\n",
        "* You‚Äôre seeing how LangGraph can implement **multi-agent debate or tournament-style reasoning**.\n",
        "\n",
        "### 4. **LangGraph Orchestration Advantage**\n",
        "\n",
        "* Even though this looks like a multi-step reasoning pipeline, it‚Äôs still just:\n",
        "\n",
        "  * Define state (with fields for options, analyses, and decision).\n",
        "  * Add nodes (generate ‚Üí analyze ‚Üí select).\n",
        "  * Wire edges.\n",
        "  * Compile & run.\n",
        "* Same scaffold, just a different reasoning *pattern*.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Why It Matters\n",
        "\n",
        "Tree of Thought is useful when:\n",
        "\n",
        "* **Open-ended brainstorming** is required (multiple possible answers).\n",
        "* **Trade-offs** need to be considered systematically.\n",
        "* **Decisions** must be justified (with structured analysis).\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Strategy selection (as in your demo).\n",
        "* Product design trade-offs.\n",
        "* Policy recommendations.\n",
        "* Even math/logic problems (different solution paths explored in parallel).\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Takeaway:** This pattern teaches you how to structure agents to *explore broadly, evaluate deeply, and converge intelligently*. It‚Äôs about going beyond single-pass answers to simulate deliberation.\n",
        "\n"
      ],
      "metadata": {
        "id": "hUB8aJ7C-qqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from typing import Dict, List\n",
        "\n",
        "# Initialize OpenAI model\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "\n",
        "# Define Agent State\n",
        "class StrategyState(Dict):\n",
        "    business_type: str\n",
        "    expansion_options: List[str]\n",
        "    strategy_analysis: Dict[str, str]\n",
        "    best_strategy: str\n",
        "\n",
        "\n",
        "# üü¢ Step 1: Generate Expansion Strategies\n",
        "def generate_expansion_options(state: StrategyState) -> StrategyState:\n",
        "    prompt = f\"\"\"\n",
        "    The company specializes in {state['business_type']}. Suggest three possible expansion strategies:\n",
        "\n",
        "    1. Entering a new geographical market.\n",
        "    2. Launching a new product line.\n",
        "    3. Partnering with an existing brand.\n",
        "\n",
        "    Provide a brief overview of each strategy.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke([SystemMessage(content=\"You are a business strategist.\"), HumanMessage(content=prompt)])\n",
        "\n",
        "    state[\"expansion_options\"] = response.content.split(\"\\n\")[:3]  # Extract first three options\n",
        "    return state\n",
        "\n",
        "\n",
        "# üü¢ Step 2: Analyze Each Strategy (Breaking Down Into ToT Paths)\n",
        "def analyze_strategy(state: StrategyState) -> StrategyState:\n",
        "    strategy_analysis = {}\n",
        "\n",
        "    for strategy in state[\"expansion_options\"]:\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following business expansion strategy:\n",
        "\n",
        "        {strategy}\n",
        "\n",
        "        Evaluate it based on:\n",
        "        - Cost implications\n",
        "        - Risk factors\n",
        "        - Potential return on investment (ROI)\n",
        "\n",
        "        Provide a structured breakdown.\n",
        "        \"\"\"\n",
        "        response = llm.invoke([SystemMessage(content=\"You are a business analyst.\"), HumanMessage(content=prompt)])\n",
        "        strategy_analysis[strategy] = response.content\n",
        "\n",
        "    state[\"strategy_analysis\"] = strategy_analysis\n",
        "    return state\n",
        "\n",
        "\n",
        "# üü¢ Step 3: Choose the Best Strategy (Final Decision)\n",
        "def select_best_strategy(state: StrategyState) -> StrategyState:\n",
        "    prompt = f\"\"\"\n",
        "    Given the following business expansion strategies and their analysis:\n",
        "\n",
        "    {state['strategy_analysis']}\n",
        "\n",
        "    Rank these strategies based on:\n",
        "    - Highest ROI\n",
        "    - Lowest risk\n",
        "    - Overall feasibility\n",
        "\n",
        "    Select the **best** strategy and explain why it is the optimal choice.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(\n",
        "        [SystemMessage(content=\"You are an expert business strategist.\"), HumanMessage(content=prompt)])\n",
        "\n",
        "    state[\"best_strategy\"] = response.content\n",
        "    return state\n",
        "\n",
        "\n",
        "# üîµ Build the LangGraph Workflow\n",
        "workflow = StateGraph(StrategyState)\n",
        "\n",
        "# Adding Nodes\n",
        "workflow.add_node(\"generate_expansion_options\", generate_expansion_options)\n",
        "workflow.add_node(\"analyze_strategy\", analyze_strategy)\n",
        "workflow.add_node(\"select_best_strategy\", select_best_strategy)\n",
        "\n",
        "# Define Execution Flow\n",
        "workflow.set_entry_point(\"generate_expansion_options\")\n",
        "workflow.add_edge(\"generate_expansion_options\", \"analyze_strategy\")\n",
        "workflow.add_edge(\"analyze_strategy\", \"select_best_strategy\")\n",
        "\n",
        "# Compile Graph\n",
        "graph = workflow.compile()\n",
        "\n",
        "# üü¢ Run Example\n",
        "input_data = {\n",
        "    \"business_type\": \"AI-based EdTech Startup\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_data)\n",
        "\n",
        "print(\"üöÄ AI-Generated Expansion Strategies:\\n\", result[\"expansion_options\"])\n",
        "print(\"\\nüîç Strategy Analysis:\\n\", result[\"strategy_analysis\"])\n",
        "print(\"\\nüèÜ Best Strategy Selected:\\n\", result[\"best_strategy\"])"
      ],
      "metadata": {
        "id": "xZ7j21zniPuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you‚Äôre seeing here is a **mix of progressive and ‚Äúparallel-like‚Äù steps**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Progressive flow\n",
        "\n",
        "The graph is wired strictly as a linear chain:\n",
        "\n",
        "```\n",
        "generate_expansion_options ‚Üí analyze_strategy ‚Üí select_best_strategy\n",
        "```\n",
        "\n",
        "That means you **can‚Äôt analyze until you‚Äôve generated**, and you **can‚Äôt select until you‚Äôve analyzed**. So orchestration-wise, it‚Äôs progressive (step-by-step).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Parallel work *inside a node*\n",
        "\n",
        "The **illusion of parallelism** happens in `analyze_strategy`.\n",
        "That node loops over *all* generated options:\n",
        "\n",
        "```python\n",
        "for strategy in state[\"expansion_options\"]:\n",
        "    response = llm.invoke(...)\n",
        "    strategy_analysis[strategy] = response.content\n",
        "```\n",
        "\n",
        "So while the graph shows it as a single node, conceptually it‚Äôs **fan-out and fan-in collapsed into one step**:\n",
        "\n",
        "* *Fan-out*: each option is analyzed independently.\n",
        "* *Fan-in*: results are merged back into `strategy_analysis`.\n",
        "\n",
        "If you wanted *true LangGraph parallelism* (like in your market research example), you‚Äôd break this into multiple nodes, one per strategy, and merge them. But here it‚Äôs handled by Python iteration inside one node.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why it‚Äôs useful\n",
        "\n",
        "This design pattern teaches you that:\n",
        "\n",
        "* Some patterns (like reflection or ToT) mix **progressive stages** (generate ‚Üí evaluate ‚Üí decide) with **internal branching** (looping or parallel analysis of multiple ideas).\n",
        "* You don‚Äôt always need to blow out the graph with explicit parallel edges if a loop works fine.\n",
        "* The trade-off: explicit graph parallelism gives you concurrency + orchestration transparency, while loops-in-a-node give you simplicity at the cost of less visibility.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **So the takeaway**:\n",
        "This ToT example is progressive at the orchestration level, but it **packs a parallel ‚Äúbranch exploration‚Äù inside one node**. That‚Äôs why it feels like a hybrid.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_eA7-Pa2_jtU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGYVCjit_nIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}