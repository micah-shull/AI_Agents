{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd2kqTzlfk0epq+4S2T26G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/096_the_Orchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Building a Simple Agent Framework (The Orchestrator Pattern)**\n",
        "\n",
        "In this lecture, you should focus on **how the full GAME framework comes together in a unified orchestrator loop** and why this design makes agents flexible, reusable, and easy to extend.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Orchestrator Is the Glue**\n",
        "\n",
        "* The `Agent` class acts as the **Orchestrator**, coordinating all GAME components:\n",
        "\n",
        "  * **Goals** (G) – What the agent is trying to achieve.\n",
        "  * **Actions** (A) – The tools it can use, stored in an `ActionRegistry`.\n",
        "  * **Memory** (M) – Context from prior interactions.\n",
        "  * **Environment** (E) – The “body” that actually executes actions.\n",
        "  * **AgentLanguage** – Formats prompts and parses responses for the LLM.\n",
        "  * **generate\\_response** – The LLM interface itself.\n",
        "* The orchestrator ensures these parts work in harmony without you rewriting the core loop for every new agent.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Six Steps in the Orchestrator Loop**\n",
        "\n",
        "1. **Construct Prompt** — Combines Goals, Memory, Actions, and Environment into a structured prompt via `AgentLanguage`.\n",
        "2. **Generate Response** — Uses `generate_response()` to call your chosen LLM.\n",
        "3. **Parse Response** — Maps the LLM’s output to an actionable `tool_name` + parameters.\n",
        "4. **Execute Action** — Delegates to the Environment, which calls the registered Action with the given arguments.\n",
        "5. **Update Memory** — Adds the agent’s decision and the execution result for future context.\n",
        "6. **Check Termination** — Ends if the selected action has `terminal=True`.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Benefits of the Orchestrator Design**\n",
        "\n",
        "* **Separation of Concerns** — Each component does one job; the orchestrator just coordinates.\n",
        "* **Reusability** — The same orchestrator loop can drive agents for coding, research, file management, or entirely different domains.\n",
        "* **Swappability** — Swap in different Goals, Actions, Memory strategies, or Environments without breaking the rest of the system.\n",
        "* **LLM-Agnostic** — Works with OpenAI, Anthropic, or local models by changing only the `generate_response` function.\n",
        "* **Low Maintenance** — Adding new tools means registering them; you never touch the orchestrator loop.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Key Abstractions to Notice**\n",
        "\n",
        "* **`AgentLanguage`** — The “translator” between your components and the LLM; controls prompt formatting & response parsing.\n",
        "* **`ActionRegistry`** — A labeled, structured “tool shed” where every action is documented and callable by name.\n",
        "* **`Environment`** — The body that handles operational details, freeing the orchestrator (brain) from implementation clutter.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The Orchestrated Flow**\n",
        "\n",
        "Mentally trace the data path:\n",
        "\n",
        "**User Input → Goals → Prompt → LLM → Parsed Action → Environment Execution → Result → Memory Update → Next Loop (or Terminate)**\n",
        "\n"
      ],
      "metadata": {
        "id": "fS7clJx2zhFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Callable\n",
        "\n",
        "# --- GAME Components (Simplified) ---\n",
        "\n",
        "class Goal:\n",
        "    def __init__(self, priority: int, name: str, description: str):\n",
        "        self.priority = priority\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "\n",
        "class Action:\n",
        "    def __init__(self, name: str, function: Callable, description: str, parameters: Dict, terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.parameters = parameters\n",
        "        self.terminal = terminal\n",
        "\n",
        "    def execute(self, **kwargs) -> Any:\n",
        "        return self.function(**kwargs)\n",
        "\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self._actions[action.name] = action\n",
        "\n",
        "    def get_action(self, name: str) -> Action:\n",
        "        return self._actions.get(name)\n",
        "\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []\n",
        "\n",
        "    def add_memory(self, memory: dict):\n",
        "        self.items.append(memory)\n",
        "\n",
        "    def get_memories(self, limit: int = None) -> List[dict]:\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e)}\n",
        "\n",
        "\n",
        "class AgentLanguage:\n",
        "    \"\"\"Mock AgentLanguage for demonstration purposes\"\"\"\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> str:\n",
        "        return f\"Goals: {[g.name for g in goals]} | Tools: {[a.name for a in actions]} | Memory: {memory.items}\"\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        # Simulated parsing: \"use <tool_name>\"\n",
        "        tool = response.split()[-1]\n",
        "        return {\"tool\": tool, \"args\": {}}\n",
        "\n",
        "# --- Orchestrator ---\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.generate_response = generate_response\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals, memory, actions):\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response):\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def and action_def.terminal\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 5):\n",
        "        memory = memory or Memory()\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": user_input})\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "            print(f\"Prompt to LLM: {prompt}\")\n",
        "\n",
        "            response = self.generate_response(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            action, invocation = self.get_action(response)\n",
        "            if not action:\n",
        "                print(\"Unknown action.\")\n",
        "                break\n",
        "\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            memory.add_memory({\"type\": \"assistant\", \"content\": response})\n",
        "            memory.add_memory({\"type\": \"system\", \"content\": str(result)})\n",
        "\n",
        "            if self.should_terminate(response):\n",
        "                print(\"Agent terminating.\")\n",
        "                break\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "def mock_generate_response(prompt):\n",
        "    # Always chooses the list_files tool for demo\n",
        "    return \"use list_files\"\n",
        "\n",
        "def list_files():\n",
        "    return [\"file1.txt\", \"file2.txt\"]\n",
        "\n",
        "# Setup components\n",
        "goals = [Goal(1, \"File Management\", \"Manage and read files.\")]\n",
        "registry = ActionRegistry()\n",
        "registry.register(Action(\"list_files\", list_files, \"List all files\", {\"type\": \"object\", \"properties\": {}}, terminal=False))\n",
        "environment = Environment()\n",
        "language = AgentLanguage()\n",
        "\n",
        "# Create and run agent\n",
        "agent = Agent(goals, language, registry, mock_generate_response, environment)\n",
        "agent.run(\"List all files in the directory\")\n"
      ],
      "metadata": {
        "id": "6gqLTCU-9-LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# What makes this a top-notch template\n",
        "\n",
        "* **Orchestrator pattern (brain)**: One small loop that just coordinates (prompt → decision → execute → memory → repeat). You never rewrite this.\n",
        "* **Separation of concerns (GAME)**:\n",
        "\n",
        "  * Goals = *what & how* (prioritized)\n",
        "  * Actions = *what you can do* (registry + schemas)\n",
        "  * Memory = *what happened* (uniform `{role, content}`)\n",
        "  * Environment = *how it’s executed* (uniform results)\n",
        "* **LLM-agnostic boundary**: `generate_response(prompt)` is a single swap point (mock, OpenAI, Anthropic, local).\n",
        "* **Uniform envelopes**: Environment always returns\n",
        "\n",
        "  * success → `{\"tool_executed\": True, \"result\": ...}`\n",
        "  * failure → `{\"tool_executed\": False, \"error\": \"...\", \"hint\": \"...\", \"retryable\": bool}`\n",
        "    This massively improves the model’s ability to recover.\n",
        "* **Schema-driven actions**: Each tool has a JSON-schema-like `parameters` and optional validation before execution.\n",
        "* **Error-aware memory updates**: Unknown tools, invalid args, and tool errors are all written back to memory—so the model can self-correct next turn.\n",
        "* **Termination & guardrails**: `terminal=True` on actions + `max_iterations`. Simple and reliable.\n",
        "\n",
        "# What to be sure to include in future agents\n",
        "\n",
        "* **Invariant interfaces (don’t break these):**\n",
        "\n",
        "  * `AgentLanguage.construct_prompt(...)` and `.parse_response(...)`\n",
        "  * `ActionRegistry.get_action(name)` and `Action.execute(**kwargs)`\n",
        "  * `Environment.execute_action(action, args)` → uniform envelope\n",
        "* **Consistent memory schema**: always `{role, content}`; pick one set of roles (user/assistant/tool) and stick to it.\n",
        "* **Deterministic vs. cognitive split**:\n",
        "\n",
        "  * Deterministic (Python): sorting, filtering, validation, truncation, safety checks\n",
        "  * Cognitive (LLM): planning, tradeoffs, summarization, choosing tools\n",
        "* **Just-in-time hints in errors**: return `hint`/`next_tool` in error payloads (e.g., “Call `list_txt_files` first”), not buried in the big system prompt.\n",
        "* **Observability hooks**: a place to log `on_step_start`, `on_action`, `on_result`, `on_terminate` (even if no-ops now).\n",
        "* **Deterministic prompt construction**: sort goals/actions, cap memory window, avoid noisy text; keep the prompt compact and stable.\n",
        "\n",
        "# Quick checklist before you “ship” an agent\n",
        "\n",
        "* [ ] Actions have clear names, good descriptions, and correct schemas.\n",
        "* [ ] Registry exports tools (if using function calling) and validates args.\n",
        "* [ ] Environment enforces safety (paths, sizes, timeouts) and returns uniform envelopes.\n",
        "* [ ] Memory uses consistent roles; context window is bounded (sliding window OK).\n",
        "* [ ] Orchestrator handles: unknown tool, invalid args, tool failure, termination.\n",
        "* [ ] `generate_response` is injectable (easy to mock in tests).\n",
        "\n",
        "# Nice future upgrades (when you need them)\n",
        "\n",
        "* Summarized/Vector recall memory strategies.\n",
        "* `return_schema` on actions for stricter post-validation.\n",
        "* `requires_approval`, `idempotent`, `timeout_s` in action `metadata`.\n",
        "* Function calling integration (so the LLM emits structured tool calls directly).\n",
        "\n"
      ],
      "metadata": {
        "id": "fUSaNzG4EzhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZCFLdPJuIUT"
      },
      "outputs": [],
      "source": [
        "# Orchestrator (Agent) — Updated Minimal Template\n",
        "# Includes:\n",
        "# - Consistent memory roles (user/assistant/tool)\n",
        "# - Unknown/invalid action handling recorded in memory\n",
        "# - Tool failure recovery (continue loop)\n",
        "# - Optional arg validation via registry schema\n",
        "# - Uniform environment result envelope\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ---------------- G: Goals ----------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "# --------------- A: Actions + Registry ---------------\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name = name\n",
        "        self.fn = fn\n",
        "        self.description = description\n",
        "        self.parameters = parameters  # JSON Schema-like dict\n",
        "        self.terminal = terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        required = schema.get(\"required\", [])\n",
        "        for key in required:\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        # (Extend with type checks or jsonschema as needed)\n",
        "        return True, \"ok\"\n",
        "\n",
        "# ---------------- M: Memory ----------------\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []  # each: {role, content}\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        # Expect keys: role, content\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: Optional[int]=None) -> List[Dict[str, Any]]:\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# ---------------- E: Environment ----------------\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e)}\n",
        "\n",
        "# ------------- AgentLanguage (prompt + parse) -------------\n",
        "class AgentLanguage:\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"goals\": [g.description for g in sorted(goals, key=lambda g: g.priority)],\n",
        "            \"tools\": [\n",
        "                {\"name\": a.name, \"description\": a.description, \"parameters\": a.parameters}\n",
        "                for a in actions\n",
        "            ],\n",
        "            \"memory\": memory.get_memories(8)\n",
        "        }\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Expect structured: {\"tool\": \"name\", \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "# ---------------- Orchestrator (Agent) ----------------\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.generate_response = generate_response  # Callable[prompt_dict] -> response_dict\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals, memory, actions):\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt):\n",
        "        return self.generate_response(full_prompt)\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)  # -> {tool, args}\n",
        "        action = self.actions.get_action(invocation.get(\"tool\"))\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response):\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return bool(action_def and action_def.terminal)\n",
        "\n",
        "    def update_memory(self, memory, response, result):\n",
        "        memory.add_memory({\"role\": \"assistant\", \"content\": response})\n",
        "        memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "\n",
        "    def run(self, user_input: str, memory: Optional[Memory]=None, max_iterations: int=5, verbose: bool=True) -> Memory:\n",
        "        memory = memory or Memory()\n",
        "        memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "            if verbose:\n",
        "                print(f\"Prompt → {prompt}\")\n",
        "\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            if verbose:\n",
        "                print(f\"Decision ← {response}\")\n",
        "\n",
        "            action, invocation = self.get_action(response)\n",
        "            if not action:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Unknown action: {invocation.get('tool')}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                if verbose:\n",
        "                    print(f\"Error ← {err}\")\n",
        "                break\n",
        "\n",
        "            # Optional: validate args before executing\n",
        "            ok, msg = self.actions.validate_args(action, invocation.get(\"args\", {}))\n",
        "            if not ok:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Invalid args: {msg}\", \"hint\": action.parameters}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                if verbose:\n",
        "                    print(f\"Arg Error ← {err}\")\n",
        "                continue\n",
        "\n",
        "            result = self.environment.execute_action(action, invocation.get(\"args\", {}))\n",
        "            if verbose:\n",
        "                print(f\"Result ← {result}\")\n",
        "\n",
        "            # Always feed back results to memory for self-correction\n",
        "            memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "\n",
        "            # If the tool failed, allow the model to recover next turn\n",
        "            if not result.get(\"tool_executed\", False):\n",
        "                memory.add_memory({\"role\": \"assistant\", \"content\": \"Received an error; selecting another action.\"})\n",
        "                continue\n",
        "\n",
        "            # Record decision too (optional if already added elsewhere)\n",
        "            memory.add_memory({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "            if self.should_terminate(response):\n",
        "                if verbose:\n",
        "                    print(\"Terminate signal: stopping loop.\")\n",
        "                break\n",
        "\n",
        "        return memory\n",
        "\n",
        "# ---------------- Example wiring ----------------\n",
        "# Toy action and mock LLM for quick sanity check\n",
        "\n",
        "def list_txt_files():\n",
        "    return [\"000.txt\", \"001.txt\"]\n",
        "\n",
        "registry = ActionRegistry()\n",
        "registry.register(Action(\n",
        "    name=\"list_txt_files\",\n",
        "    fn=list_txt_files,\n",
        "    description=\"Return .txt file names\",\n",
        "    parameters={\"type\":\"object\",\"properties\":{},\"required\":[]},\n",
        "))\n",
        "\n",
        "language = AgentLanguage()\n",
        "\n",
        "def mock_llm(prompt_dict):\n",
        "    # Always choose our single tool\n",
        "    return {\"tool\": \"list_txt_files\", \"args\": {}}\n",
        "\n",
        "env = Environment()\n",
        "goals = [Goal(1, \"list\", \"List available text files.\")]\n",
        "\n",
        "# Create agent and run one pass\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent(goals, language, registry, mock_llm, env)\n",
        "    _ = agent.run(\"Show me the files\", verbose=True)\n"
      ]
    }
  ]
}