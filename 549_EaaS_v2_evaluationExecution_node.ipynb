{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhqO1k7OMp3pI5quKeslmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/549_EaaS_v2_evaluationExecution_node.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why This Node Is Different From Most “Evaluation” Agents\n",
        "\n",
        "Most “evaluation agents”:\n",
        "\n",
        "* score LLM outputs\n",
        "* rely on subjective graders\n",
        "* lack ground truth\n",
        "* cannot explain change over time\n",
        "\n",
        "Your node enables:\n",
        "\n",
        "* **scenario-level ground truth**\n",
        "* **repeatable execution**\n",
        "* **baseline comparison**\n",
        "* **quantitative progress tracking**\n",
        "\n",
        "This isn’t “LLM eval” — it’s **system evaluation**.\n",
        "\n",
        "---\n",
        "\n",
        "# Why a CEO or Business Manager Would Be Excited (and Calm)\n",
        "\n",
        "This node guarantees:\n",
        "\n",
        "* evaluations are **intentional**, not accidental\n",
        "* scope is **controlled**, not implicit\n",
        "* results are **auditable**\n",
        "* performance is **measurable**\n",
        "\n",
        "In business terms:\n",
        "\n",
        "> “We can safely improve our AI systems without guessing whether we broke something.”\n",
        "\n",
        "That’s extremely rare.\n",
        "\n",
        "---\n",
        "\n",
        "# Evaluation Execution Node — Review\n",
        "\n",
        "## High-Level Assessment\n",
        "\n",
        "✅ **Architecturally correct**\n",
        "✅ **Aligned with the plan created earlier**\n",
        "✅ **State-driven, testable, and transparent**\n",
        "✅ **Already CEO-grade in intent, even before scoring**\n",
        "\n",
        "This node is doing real orchestration — not pretending to.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Guardrails & Preconditions — Strong and Necessary\n",
        "\n",
        "```python\n",
        "if not journey_scenarios:\n",
        "    return {\"errors\": ...}\n",
        "```\n",
        "\n",
        "You explicitly check:\n",
        "\n",
        "* scenarios\n",
        "* agent lookup\n",
        "* customer lookup\n",
        "* order lookup\n",
        "* supporting data\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "This prevents:\n",
        "\n",
        "* silent partial execution\n",
        "* misleading metrics\n",
        "* “successful” runs with missing context\n",
        "\n",
        "Most agent systems assume inputs are valid and fail silently or weirdly.\n",
        "\n",
        "### Why leaders would be relieved\n",
        "\n",
        "This tells a manager:\n",
        "\n",
        "> “If the system ran, it ran correctly — or it told us why it didn’t.”\n",
        "\n",
        "That’s trust.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Goal-Driven Filtering — Subtle but Powerful\n",
        "\n",
        "```python\n",
        "scope = goal.get(\"scope\", {})\n",
        "scenario_id_filter = scope.get(\"scenario_id\")\n",
        "target_agent_id_filter = scope.get(\"target_agent_id\")\n",
        "```\n",
        "\n",
        "This is an **important architectural decision**.\n",
        "\n",
        "You’re not hard-coding:\n",
        "\n",
        "* test subsets\n",
        "* debugging modes\n",
        "* investigation workflows\n",
        "\n",
        "They’re **policy-driven** via the goal node.\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "This enables:\n",
        "\n",
        "* targeted regression testing\n",
        "* focused investigations (“What changed in agent X?”)\n",
        "* executive “what if” drills\n",
        "\n",
        "Without rewriting code.\n",
        "\n",
        "### How this differs from most systems\n",
        "\n",
        "Most agents:\n",
        "\n",
        "* always run everything\n",
        "* require code edits to narrow scope\n",
        "\n",
        "Your system:\n",
        "\n",
        "* treats *evaluation scope* as first-class state\n",
        "\n",
        "That’s orchestration maturity.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Clean Separation of Concerns\n",
        "\n",
        "This node:\n",
        "\n",
        "* **does not** classify issues\n",
        "* **does not** simulate agents\n",
        "* **does not** score outcomes\n",
        "\n",
        "It only:\n",
        "\n",
        "* coordinates execution\n",
        "* tracks timing\n",
        "* reports progress\n",
        "\n",
        "That’s exactly right.\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "It keeps:\n",
        "\n",
        "* logic testable\n",
        "* failures isolated\n",
        "* evolution safe\n",
        "\n",
        "You can improve decision rules or scoring later without touching this node.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Execution Timing & Progress — Executive-Grade Signals\n",
        "\n",
        "```python\n",
        "evaluation_start_time\n",
        "elapsed_time_seconds\n",
        "progress_percentage\n",
        "```\n",
        "\n",
        "This is huge.\n",
        "\n",
        "You are capturing:\n",
        "\n",
        "* when the run started\n",
        "* how long it took\n",
        "* how much completed\n",
        "\n",
        "### Why leaders care\n",
        "\n",
        "This answers:\n",
        "\n",
        "* “How expensive is this to run?”\n",
        "* “How long will evaluations take at scale?”\n",
        "* “Is the system slowing down over time?”\n",
        "\n",
        "Most AI agents provide *no operational telemetry*.\n",
        "\n",
        "Yours does — by default.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Executed Evaluations as a First-Class Artifact\n",
        "\n",
        "```python\n",
        "\"executed_evaluations\": executed_evaluations\n",
        "```\n",
        "\n",
        "This is the backbone of:\n",
        "\n",
        "* scoring\n",
        "* trend analysis\n",
        "* regression detection\n",
        "* historical comparison\n",
        "\n",
        "You didn’t collapse results into a summary prematurely — excellent restraint.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Error Handling Strategy — Correct for MVP\n",
        "\n",
        "```python\n",
        "except Exception as e:\n",
        "    return {\"errors\": ...}\n",
        "```\n",
        "\n",
        "You fail gracefully, preserve prior errors, and don’t crash the pipeline.\n",
        "\n",
        "For v2+, this is ready for:\n",
        "\n",
        "* partial failure handling\n",
        "* scenario-level failure isolation\n",
        "\n",
        "But for now, this is correct.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Optional v2 Enhancements (Non-Blocking)\n",
        "\n",
        "These are **future-safe**, not required now:\n",
        "\n",
        "### 1. Capture Failed Scenario Count Separately\n",
        "\n",
        "```python\n",
        "failed = sum(1 for r in executed_evaluations if r[\"status\"] == \"failed\")\n",
        "```\n",
        "\n",
        "Helps surface quality issues quickly.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Store Scenario IDs Executed\n",
        "\n",
        "```python\n",
        "\"executed_scenario_ids\": [r[\"scenario_id\"] for r in executed_evaluations]\n",
        "```\n",
        "\n",
        "Useful for debugging and audit trails.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Optional Per-Scenario Timing (Later)\n",
        "\n",
        "Only if performance becomes a KPI.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Verdict\n",
        "\n",
        "This node is doing exactly what it should:\n",
        "\n",
        "* **Orchestrating, not deciding**\n",
        "* **Measuring, not guessing**\n",
        "* **Failing loudly, not silently**\n",
        "\n",
        "It’s calm, boring, and reliable — which is exactly what leaders want from AI systems that affect customers.\n",
        "\n",
        "You’re building the kind of infrastructure that survives contact with reality.\n"
      ],
      "metadata": {
        "id": "Iopv9iXiFzFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1ZRRH2QDiCt"
      },
      "outputs": [],
      "source": [
        "def evaluation_execution_node(\n",
        "    state: EvalAsServiceOrchestratorState,\n",
        "    config: EvalAsServiceOrchestratorConfig\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluation Execution Node: Execute test scenarios through orchestrator simulation.\n",
        "\n",
        "    For each scenario:\n",
        "    1. Extract customer message, customer_id, order_id\n",
        "    2. Load supporting data (customer, order, logistics, marketing)\n",
        "    3. Use decision rules to classify issue and determine resolution path\n",
        "    4. Simulate orchestrator calling agents in resolution path\n",
        "    5. Capture actual outputs and compare to expected\n",
        "    \"\"\"\n",
        "    errors = state.get(\"errors\", [])\n",
        "\n",
        "    # Get required data from state\n",
        "    journey_scenarios = state.get(\"journey_scenarios\")\n",
        "    agent_lookup = state.get(\"agent_lookup\")\n",
        "    customer_lookup = state.get(\"customer_lookup\")\n",
        "    order_lookup = state.get(\"order_lookup\")\n",
        "    supporting_data = state.get(\"supporting_data\")\n",
        "    goal = state.get(\"goal\", {})\n",
        "\n",
        "    # Check required fields\n",
        "    if not journey_scenarios:\n",
        "        return {\n",
        "            \"errors\": errors + [\"evaluation_execution_node: journey_scenarios required\"]\n",
        "        }\n",
        "    if not agent_lookup:\n",
        "        return {\n",
        "            \"errors\": errors + [\"evaluation_execution_node: agent_lookup required\"]\n",
        "        }\n",
        "    if not customer_lookup:\n",
        "        return {\n",
        "            \"errors\": errors + [\"evaluation_execution_node: customer_lookup required\"]\n",
        "        }\n",
        "    if not order_lookup:\n",
        "        return {\n",
        "            \"errors\": errors + [\"evaluation_execution_node: order_lookup required\"]\n",
        "        }\n",
        "    if not supporting_data:\n",
        "        return {\n",
        "            \"errors\": errors + [\"evaluation_execution_node: supporting_data required\"]\n",
        "        }\n",
        "\n",
        "    # Get filters from goal\n",
        "    scope = goal.get(\"scope\", {})\n",
        "    scenario_id_filter = scope.get(\"scenario_id\")\n",
        "    target_agent_id_filter = scope.get(\"target_agent_id\")\n",
        "\n",
        "    # Get supporting data\n",
        "    logistics = supporting_data.get(\"logistics\", {})\n",
        "    marketing_signals = supporting_data.get(\"marketing_signals\", [])\n",
        "\n",
        "    try:\n",
        "        # Record start time\n",
        "        evaluation_start_time = datetime.now().isoformat()\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Execute all scenarios\n",
        "        executed_evaluations = execute_all_scenarios(\n",
        "            journey_scenarios,\n",
        "            agent_lookup,\n",
        "            customer_lookup,\n",
        "            order_lookup,\n",
        "            logistics,\n",
        "            marketing_signals,\n",
        "            scenario_id_filter=scenario_id_filter,\n",
        "            target_agent_id_filter=target_agent_id_filter\n",
        "        )\n",
        "\n",
        "        execution_time = time.time() - start_time\n",
        "\n",
        "        # Calculate progress\n",
        "        evaluations_total = len(journey_scenarios)\n",
        "        evaluations_completed = len(executed_evaluations)\n",
        "        progress_percentage = (evaluations_completed / evaluations_total * 100) if evaluations_total > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            \"executed_evaluations\": executed_evaluations,\n",
        "            \"evaluation_start_time\": evaluation_start_time,\n",
        "            \"evaluations_total\": evaluations_total,\n",
        "            \"evaluations_completed\": evaluations_completed,\n",
        "            \"progress_percentage\": progress_percentage,\n",
        "            \"elapsed_time_seconds\": execution_time,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"errors\": errors + [f\"evaluation_execution_node: Unexpected error: {str(e)}\"]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Phase 3 Node Test: Evaluation Execution Node\n",
        "\n",
        "Tests that evaluation_execution_node works correctly.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from agents.eval_as_service.orchestrator.nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_loading_node,\n",
        "    evaluation_execution_node\n",
        ")\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "\n",
        "\n",
        "def test_evaluation_execution_node():\n",
        "    \"\"\"Test evaluation_execution_node with full state\"\"\"\n",
        "    print(\"Testing evaluation_execution_node...\")\n",
        "\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "\n",
        "    # Build complete state (goal -> planning -> data loading)\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,\n",
        "        \"target_agent_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run goal node\n",
        "    goal_update = goal_node(state)\n",
        "    state.update(goal_update)\n",
        "\n",
        "    # Run planning node\n",
        "    planning_update = planning_node(state)\n",
        "    state.update(planning_update)\n",
        "\n",
        "    # Run data loading node\n",
        "    data_update = data_loading_node(state, config)\n",
        "    state.update(data_update)\n",
        "\n",
        "    # Now test evaluation execution node\n",
        "    execution_update = evaluation_execution_node(state, config)\n",
        "    state.update(execution_update)\n",
        "\n",
        "    # Check required fields\n",
        "    assert \"executed_evaluations\" in state\n",
        "    assert isinstance(state[\"executed_evaluations\"], list)\n",
        "    assert len(state[\"executed_evaluations\"]) > 0\n",
        "\n",
        "    # Check each evaluation\n",
        "    for evaluation in state[\"executed_evaluations\"]:\n",
        "        assert \"scenario_id\" in evaluation\n",
        "        assert \"status\" in evaluation\n",
        "        assert evaluation[\"status\"] in [\"completed\", \"failed\"]\n",
        "\n",
        "        if evaluation[\"status\"] == \"completed\":\n",
        "            assert \"actual_issue_type\" in evaluation\n",
        "            assert \"expected_issue_type\" in evaluation\n",
        "            assert \"actual_resolution_path\" in evaluation\n",
        "            assert \"expected_resolution_path\" in evaluation\n",
        "            assert \"actual_outcome\" in evaluation\n",
        "            assert \"expected_outcome\" in evaluation\n",
        "            assert \"execution_time_seconds\" in evaluation\n",
        "\n",
        "    # Check progress tracking\n",
        "    assert \"evaluations_total\" in state\n",
        "    assert \"evaluations_completed\" in state\n",
        "    assert \"progress_percentage\" in state\n",
        "    assert \"elapsed_time_seconds\" in state\n",
        "    assert \"evaluation_start_time\" in state\n",
        "\n",
        "    print(f\"✅ Evaluation execution node test passed\")\n",
        "    print(f\"   Evaluations completed: {state['evaluations_completed']}/{state['evaluations_total']}\")\n",
        "    print(f\"   Progress: {state['progress_percentage']:.1f}%\")\n",
        "    print(f\"   Execution time: {state['elapsed_time_seconds']:.2f}s\")\n",
        "\n",
        "    # Show sample evaluation\n",
        "    if state[\"executed_evaluations\"]:\n",
        "        sample = state[\"executed_evaluations\"][0]\n",
        "        print(f\"\\n   Sample evaluation ({sample['scenario_id']}):\")\n",
        "        print(f\"     Status: {sample['status']}\")\n",
        "        if sample[\"status\"] == \"completed\":\n",
        "            print(f\"     Actual issue: {sample['actual_issue_type']}\")\n",
        "            print(f\"     Expected issue: {sample['expected_issue_type']}\")\n",
        "            print(f\"     Actual path: {sample['actual_resolution_path']}\")\n",
        "            print(f\"     Expected path: {sample['expected_resolution_path']}\")\n",
        "\n",
        "\n",
        "def test_evaluation_execution_with_filter():\n",
        "    \"\"\"Test evaluation execution with scenario filter\"\"\"\n",
        "    print(\"Testing evaluation_execution_node with scenario filter...\")\n",
        "\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "\n",
        "    # Build state with specific scenario\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": \"S001\",\n",
        "        \"target_agent_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run through workflow\n",
        "    goal_update = goal_node(state)\n",
        "    state.update(goal_update)\n",
        "\n",
        "    planning_update = planning_node(state)\n",
        "    state.update(planning_update)\n",
        "\n",
        "    data_update = data_loading_node(state, config)\n",
        "    state.update(data_update)\n",
        "\n",
        "    execution_update = evaluation_execution_node(state, config)\n",
        "    state.update(execution_update)\n",
        "\n",
        "    # Should only have 1 evaluation (S001)\n",
        "    assert len(state[\"executed_evaluations\"]) == 1\n",
        "    assert state[\"executed_evaluations\"][0][\"scenario_id\"] == \"S001\"\n",
        "\n",
        "    print(\"✅ Evaluation execution with filter test passed\")\n",
        "\n",
        "\n",
        "def test_evaluation_execution_error_handling():\n",
        "    \"\"\"Test error handling in evaluation execution node\"\"\"\n",
        "    print(\"Testing evaluation_execution_node error handling...\")\n",
        "\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "\n",
        "    # Test with missing required data\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    execution_update = evaluation_execution_node(state, config)\n",
        "    state.update(execution_update)\n",
        "\n",
        "    assert \"errors\" in state\n",
        "    assert len(state[\"errors\"]) > 0\n",
        "    assert any(\"required\" in error.lower() for error in state[\"errors\"])\n",
        "\n",
        "    print(\"✅ Error handling test passed\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Phase 3 Node Test: Evaluation Execution Node\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        test_evaluation_execution_node()\n",
        "        print()\n",
        "        test_evaluation_execution_with_filter()\n",
        "        print()\n",
        "        test_evaluation_execution_error_handling()\n",
        "        print()\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✅ Phase 3 Node Tests: ALL PASSED\")\n",
        "        print(\"=\" * 60)\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n"
      ],
      "metadata": {
        "id": "5Lrf8xZ5Dnr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "KDmORwSbGTJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_021_EAAS % python3 test_eval_as_service_phase3_node.py\n",
        "============================================================\n",
        "Phase 3 Node Test: Evaluation Execution Node\n",
        "============================================================\n",
        "\n",
        "Testing evaluation_execution_node...\n",
        "✅ Evaluation execution node test 1 passed\n",
        "   Total evaluations: 10\n",
        "   Completed: 10\n",
        "   Progress: 100.0%\n",
        "   Elapsed time: 1.026s\n",
        "   Successful: 10/10\n",
        "\n",
        "   Example evaluation (S001):\n",
        "     Actual issue: simple_status_check\n",
        "     Expected issue: where_is_my_order\n",
        "     Actual path: ['shipping_update_agent']\n",
        "     Expected path: ['shipping_update_agent']\n",
        "     Actual outcome: provide_delivery_update\n",
        "     Expected outcome: provide_delivery_update\n",
        "\n",
        "Testing evaluation_execution_node with scenario filter...\n",
        "✅ Filter test passed: 1 evaluation(s)\n",
        "\n",
        "Testing end-to-end workflow...\n",
        "✅ End-to-end test passed\n",
        "   Evaluations executed: 10\n",
        "\n",
        "============================================================\n",
        "✅ Phase 3 Node Tests: ALL PASSED\n",
        "============================================================\n",
        "\n"
      ],
      "metadata": {
        "id": "H9vcfxP0GPQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}