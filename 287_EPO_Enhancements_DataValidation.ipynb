{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPICuMHQwqdtiUZSSVVQS/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/287_EPO_Enhancements_DataValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentation Portfolio Orchestrator - Enhancement Roadmap\n",
        "\n",
        "**Current Status:** MVP Complete ‚úÖ  \n",
        "**Last Updated:** 2025-12-15\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Enhancement Philosophy\n",
        "\n",
        "Following **MVP-first approach**: Enhance incrementally, add value at each step, avoid complexity without clear benefit.\n",
        "\n",
        "**Priority Order:**\n",
        "1. **Reliability** - Make it production-ready\n",
        "2. **Business Value** - Add features that drive decisions\n",
        "3. **Intelligence** - Add LLM/smart features\n",
        "4. **Scale** - Handle more data, more complexity\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Recommended Next Steps (Prioritized)\n",
        "\n",
        "### **Phase 1: Production Readiness** ‚≠ê HIGH PRIORITY\n",
        "\n",
        "**Goal:** Make the agent robust and reliable for real-world use.\n",
        "\n",
        "#### 1.1 Statistical Significance Testing\n",
        "**Why:** Current confidence is rule-based. Real statistical tests make decisions trustworthy.\n",
        "\n",
        "**What to Add:**\n",
        "- P-value calculation (t-test, chi-square test)\n",
        "- Confidence intervals\n",
        "- Statistical power analysis\n",
        "- Sample size validation\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Decisions based on real statistics, not heuristics\n",
        "- ‚úÖ Reduces false positives/negatives\n",
        "- ‚úÖ Industry-standard approach\n",
        "\n",
        "**Effort:** Medium (requires statistics knowledge)\n",
        "**Files to Modify:**\n",
        "- `utilities/portfolio_analysis.py` - Add statistical test functions\n",
        "- `config.py` - Add statistical thresholds\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def calculate_statistical_significance(control_metrics, treatment_metrics):\n",
        "    \"\"\"Calculate p-value using appropriate statistical test\"\"\"\n",
        "    # t-test for continuous metrics\n",
        "    # chi-square for conversion rates\n",
        "    p_value = perform_statistical_test(...)\n",
        "    is_significant = p_value < 0.05\n",
        "    return {\"p_value\": p_value, \"is_significant\": is_significant}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.2 Enhanced Data Validation & Error Handling\n",
        "**Why:** Current MVP assumes clean data. Real data has edge cases.\n",
        "\n",
        "**What to Add:**\n",
        "- Validate data structure on load\n",
        "- Handle missing fields gracefully\n",
        "- Detect data quality issues\n",
        "- Better error messages with context\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Handles real-world messy data\n",
        "- ‚úÖ Clear error messages for debugging\n",
        "- ‚úÖ Prevents silent failures\n",
        "\n",
        "**Effort:** Low-Medium\n",
        "**Files to Modify:**\n",
        "- `utilities/data_loading.py` - Add validation\n",
        "- `nodes.py` - Better error handling\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def validate_experiment_data(portfolio_entry, definition, metrics):\n",
        "    \"\"\"Validate experiment data completeness and quality\"\"\"\n",
        "    errors = []\n",
        "    if not portfolio_entry.get(\"experiment_id\"):\n",
        "        errors.append(\"Missing experiment_id\")\n",
        "    if not definition.get(\"primary_metric\"):\n",
        "        errors.append(\"Missing primary_metric\")\n",
        "    # ... more validations\n",
        "    return errors\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 1.3 Cost & ROI Calculations\n",
        "**Why:** Business decisions need financial context. \"44% lift\" is great, but \"44% lift at $50K cost\" is actionable.\n",
        "\n",
        "**What to Add:**\n",
        "- Experiment cost tracking (infrastructure, time, resources)\n",
        "- ROI calculation (revenue impact vs cost)\n",
        "- Cost per experiment\n",
        "- Portfolio-level ROI summary\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Business-focused decisions\n",
        "- ‚úÖ Prioritize high-ROI experiments\n",
        "- ‚úÖ Justify scaling decisions with numbers\n",
        "\n",
        "**Effort:** Medium\n",
        "**Files to Add:**\n",
        "- `utilities/roi_calculation.py` - New utility\n",
        "- `data/experiment_costs.json` - New data file (or add to existing)\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def calculate_experiment_roi(experiment_id, analysis, cost_data):\n",
        "    \"\"\"Calculate ROI for an experiment\"\"\"\n",
        "    revenue_impact = calculate_revenue_impact(analysis)\n",
        "    total_cost = cost_data.get(\"infrastructure_cost\", 0) + cost_data.get(\"time_cost\", 0)\n",
        "    roi_percent = ((revenue_impact - total_cost) / total_cost) * 100\n",
        "    return {\"roi_percent\": roi_percent, \"revenue_impact\": revenue_impact, \"total_cost\": total_cost}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 2: Enhanced Intelligence** ‚≠ê MEDIUM PRIORITY\n",
        "\n",
        "**Goal:** Add smarter features that provide deeper insights.\n",
        "\n",
        "#### 2.1 LLM Enhancement Layer for Insights\n",
        "**Why:** Rule-based insights are good, but LLM can find patterns humans miss.\n",
        "\n",
        "**What to Add:**\n",
        "- LLM-generated portfolio insights\n",
        "- Personalized experiment recommendations\n",
        "- Natural language summaries\n",
        "- Pattern detection across experiments\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Deeper insights\n",
        "- ‚úÖ More natural language reports\n",
        "- ‚úÖ Pattern detection\n",
        "\n",
        "**Effort:** Medium (requires LLM integration)\n",
        "**Files to Add:**\n",
        "- `utilities/llm_insights.py` - New utility\n",
        "- `config.py` - Add LLM config\n",
        "\n",
        "**Important:** Add this AFTER Phase 1. LLM is enhancement, not foundation.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def generate_llm_insights(portfolio_summary, analyzed_experiments):\n",
        "    \"\"\"Generate insights using LLM\"\"\"\n",
        "    prompt = f\"Analyze this experiment portfolio: {portfolio_summary}...\"\n",
        "    # LLM call\n",
        "    insights = llm.generate(prompt)\n",
        "    return parse_insights(insights)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.2 Experiment Dependencies & Relationships\n",
        "**Why:** Experiments often build on each other. Track relationships.\n",
        "\n",
        "**What to Add:**\n",
        "- Experiment dependency tracking\n",
        "- \"Builds on\" relationships\n",
        "- \"Blocks\" relationships\n",
        "- Dependency-aware decision generation\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Understand experiment relationships\n",
        "- ‚úÖ Make decisions considering dependencies\n",
        "- ‚úÖ Identify critical path experiments\n",
        "\n",
        "**Effort:** Medium\n",
        "**Files to Add:**\n",
        "- `data/experiment_dependencies.json` - New data file\n",
        "- `utilities/dependency_analysis.py` - New utility\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "def analyze_experiment_dependencies(experiment_id, dependencies_lookup):\n",
        "    \"\"\"Analyze what experiments depend on this one\"\"\"\n",
        "    dependents = [exp_id for exp_id, deps in dependencies_lookup.items()\n",
        "                  if experiment_id in deps.get(\"depends_on\", [])]\n",
        "    return {\"dependents\": dependents, \"blocks\": len(dependents)}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.3 Historical Tracking & Trend Analysis\n",
        "**Why:** Track how experiments evolve over time.\n",
        "\n",
        "**What to Add:**\n",
        "- Experiment history tracking\n",
        "- Trend analysis (improving/declining)\n",
        "- Historical comparisons\n",
        "- Time-series analysis\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Understand experiment evolution\n",
        "- ‚úÖ Identify trends\n",
        "- ‚úÖ Historical context for decisions\n",
        "\n",
        "**Effort:** Medium-High\n",
        "**Files to Add:**\n",
        "- `data/experiment_history.json` - New data file\n",
        "- `utilities/trend_analysis.py` - New utility\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 3: Scale & Integration** ‚≠ê LOWER PRIORITY\n",
        "\n",
        "**Goal:** Handle more data, integrate with real systems.\n",
        "\n",
        "#### 3.1 Database Integration\n",
        "**Why:** JSON files don't scale. Real systems use databases.\n",
        "\n",
        "**What to Add:**\n",
        "- Replace JSON loading with database queries\n",
        "- Connection pooling\n",
        "- Query optimization\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Handles large portfolios\n",
        "- ‚úÖ Real-time data\n",
        "- ‚úÖ Production-ready\n",
        "\n",
        "**Effort:** High\n",
        "**Files to Modify:**\n",
        "- `utilities/data_loading.py` - Replace with DB queries\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.2 API Integration\n",
        "**Why:** Connect to real experiment platforms (Optimizely, LaunchDarkly, etc.)\n",
        "\n",
        "**What to Add:**\n",
        "- API clients for experiment platforms\n",
        "- Data synchronization\n",
        "- Real-time updates\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Integrates with real tools\n",
        "- ‚úÖ Automatic data updates\n",
        "- ‚úÖ Single source of truth\n",
        "\n",
        "**Effort:** High\n",
        "**Files to Add:**\n",
        "- `utilities/api_clients/` - New directory\n",
        "- `utilities/data_sync.py` - New utility\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.3 Advanced Reporting\n",
        "**Why:** More visualization, more formats.\n",
        "\n",
        "**What to Add:**\n",
        "- HTML reports with charts\n",
        "- PDF export\n",
        "- Dashboard generation\n",
        "- Email reports\n",
        "\n",
        "**Impact:**\n",
        "- ‚úÖ Better presentation\n",
        "- ‚úÖ More formats\n",
        "- ‚úÖ Shareable reports\n",
        "\n",
        "**Effort:** Medium\n",
        "**Files to Modify:**\n",
        "- `utilities/report_generation.py` - Add formats\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ My Recommendation: Start with Phase 1\n",
        "\n",
        "### **Immediate Next Steps (This Week):**\n",
        "\n",
        "1. **Add Statistical Significance Testing** (2-3 days)\n",
        "   - Most impactful for production use\n",
        "   - Makes decisions trustworthy\n",
        "   - Industry standard\n",
        "\n",
        "2. **Add Cost/ROI Calculations** (2-3 days)\n",
        "   - High business value\n",
        "   - Makes decisions actionable\n",
        "   - Relatively straightforward\n",
        "\n",
        "3. **Enhanced Data Validation** (1 day)\n",
        "   - Quick win\n",
        "   - Prevents bugs\n",
        "   - Makes agent more robust\n",
        "\n",
        "### **Why Phase 1 First?**\n",
        "\n",
        "- ‚úÖ **Production-Ready** - Makes agent reliable for real use\n",
        "- ‚úÖ **Business Value** - ROI calculations drive decisions\n",
        "- ‚úÖ **Foundation** - Sets up for future enhancements\n",
        "- ‚úÖ **Low Risk** - Incremental improvements, not rewrites\n",
        "\n",
        "### **Avoid These (For Now):**\n",
        "\n",
        "- ‚ùå More complex data structures (current is fine for MVP)\n",
        "- ‚ùå More nodes (current workflow is complete)\n",
        "- ‚ùå LLM features (add after foundation is solid)\n",
        "- ‚ùå Database integration (premature optimization)\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Enhancement Impact Matrix\n",
        "\n",
        "| Enhancement | Business Value | Technical Complexity | Priority |\n",
        "|------------|----------------|---------------------|----------|\n",
        "| Statistical Significance | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | **HIGH** |\n",
        "| Cost/ROI Calculations | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Low-Medium | **HIGH** |\n",
        "| Data Validation | ‚≠ê‚≠ê‚≠ê‚≠ê | Low | **HIGH** |\n",
        "| LLM Insights | ‚≠ê‚≠ê‚≠ê | Medium | Medium |\n",
        "| Dependencies | ‚≠ê‚≠ê‚≠ê | Medium | Medium |\n",
        "| Historical Tracking | ‚≠ê‚≠ê | Medium-High | Low |\n",
        "| Database Integration | ‚≠ê‚≠ê‚≠ê‚≠ê | High | Low (later) |\n",
        "| API Integration | ‚≠ê‚≠ê‚≠ê‚≠ê | High | Low (later) |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Quick Start: Statistical Significance\n",
        "\n",
        "**Want to start immediately?** Here's a focused plan:\n",
        "\n",
        "### Step 1: Add Statistical Test Utility (2 hours)\n",
        "\n",
        "```python\n",
        "# utilities/statistical_tests.py\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "def calculate_t_test(control_values, treatment_values):\n",
        "    \"\"\"Calculate t-test for continuous metrics\"\"\"\n",
        "    t_stat, p_value = stats.ttest_ind(control_values, treatment_values)\n",
        "    return {\n",
        "        \"test_type\": \"t_test\",\n",
        "        \"p_value\": float(p_value),\n",
        "        \"is_significant\": p_value < 0.05,\n",
        "        \"confidence_level\": 0.95\n",
        "    }\n",
        "\n",
        "def calculate_chi_square_test(control_conversions, control_total,\n",
        "                              treatment_conversions, treatment_total):\n",
        "    \"\"\"Calculate chi-square test for conversion rates\"\"\"\n",
        "    contingency_table = [\n",
        "        [control_conversions, control_total - control_conversions],\n",
        "        [treatment_conversions, treatment_total - treatment_conversions]\n",
        "    ]\n",
        "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "    return {\n",
        "        \"test_type\": \"chi_square\",\n",
        "        \"p_value\": float(p_value),\n",
        "        \"is_significant\": p_value < 0.05,\n",
        "        \"confidence_level\": 0.95\n",
        "    }\n",
        "```\n",
        "\n",
        "### Step 2: Integrate into Analysis (1 hour)\n",
        "\n",
        "Modify `calculate_experiment_analysis()` to use statistical tests.\n",
        "\n",
        "### Step 3: Update Report (30 min)\n",
        "\n",
        "Add p-values and significance to report.\n",
        "\n",
        "**Total Time:** ~4 hours for significant improvement!\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Principles for Enhancements\n",
        "\n",
        "1. **Add Value Incrementally** - Each enhancement should stand alone\n",
        "2. **Test Each Step** - Don't add multiple features at once\n",
        "3. **Keep MVP Working** - Enhancements shouldn't break existing functionality\n",
        "4. **Measure Impact** - Track if enhancements improve decisions\n",
        "5. **Document Changes** - Update README as you add features\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "**Recommended Path:**\n",
        "1. ‚úÖ **Phase 1: Production Readiness** (This week)\n",
        "   - Statistical significance testing\n",
        "   - Cost/ROI calculations\n",
        "   - Enhanced validation\n",
        "\n",
        "2. ‚è∏Ô∏è **Phase 2: Enhanced Intelligence** (Next week)\n",
        "   - LLM insights (after Phase 1 is solid)\n",
        "   - Dependencies\n",
        "   - Historical tracking\n",
        "\n",
        "3. ‚è∏Ô∏è **Phase 3: Scale & Integration** (Later)\n",
        "   - Database integration\n",
        "   - API integration\n",
        "   - Advanced reporting\n",
        "\n",
        "**Start with statistical significance testing** - it's the highest impact, medium effort enhancement that makes the agent production-ready.\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** The MVP is working! Enhance incrementally, test each step, and add value at each phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "k4X0lsbEKYcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why statistical tests were prioritized\n",
        "\n",
        "From the roadmap, this was Phase 1, Item 1 because:\n",
        "\n",
        "1. Risk reduction: Without statistical tests, you can‚Äôt tell if a 44% lift is real or noise. A CEO scaling based on noise can waste resources and damage credibility.\n",
        "2. Industry standard: Executives expect statistical validation. Presenting results without it weakens the case.\n",
        "3. Decision confidence: P-values quantify uncertainty. ‚Äú44% lift, p=0.0028‚Äù means there‚Äôs a 0.28% chance it‚Äôs random‚Äîactionable. ‚Äú44% lift, medium confidence‚Äù is vague.\n",
        "\n",
        "## Business value for CEOs\n",
        "\n",
        "### 1. Risk mitigation\n",
        "- Before: ‚ÄúE001 shows 44% improvement, let‚Äôs scale‚Äù ‚Üí could be a false positive\n",
        "- After: ‚ÄúE001 shows 44% improvement (p=0.0028, 99.72% confidence)‚Äù ‚Üí mathematically validated\n",
        "\n",
        "### 2. Cost avoidance\n",
        "- Example: Scaling a false positive across 1000 sales reps costs time and money\n",
        "- Statistical validation reduces the risk of scaling noise\n",
        "\n",
        "### 3. Board/executive credibility\n",
        "- Executives and boards expect statistical rigor\n",
        "- Presenting results without it can undermine trust\n",
        "\n",
        "### 4. ROI justification\n",
        "- Statistical significance + ROI = stronger business case\n",
        "- Example: ‚Äú44% lift (statistically significant) ‚Üí $50K revenue impact ‚Üí 200% ROI‚Äù\n",
        "\n",
        "## Real-world example\n",
        "\n",
        "**Without statistical tests:**\n",
        "> \"E001 shows 44% improvement. We should scale it.\"\n",
        "\n",
        "**With statistical tests:**\n",
        "> \"E001 shows 44% improvement with p=0.0028 (99.72% confidence). The 95% confidence interval is [2.9%, 13.0%], meaning we're 95% confident the true improvement is between 2.9% and 13.0%. This is statistically significant, so we can confidently scale.\"\n",
        "\n",
        "The second version is more credible and actionable.\n",
        "\n",
        "## Why this matters for AI experiments\n",
        "\n",
        "AI experiments often have:\n",
        "- Small sample sizes\n",
        "- High variance\n",
        "- Multiple metrics\n",
        "- Costly scaling decisions\n",
        "\n",
        "Statistical tests help:\n",
        "- Distinguish signal from noise\n",
        "- Quantify uncertainty\n",
        "- Make data-driven decisions\n",
        "- Avoid costly mistakes\n",
        "\n",
        "## Bottom line\n",
        "\n",
        "Statistical significance testing provides:\n",
        "1. Mathematical proof (not just heuristics)\n",
        "2. Quantified confidence (p-values, confidence intervals)\n",
        "3. Risk reduction (fewer false positives/negatives)\n",
        "4. Executive credibility (industry-standard approach)\n",
        "\n",
        "For a CEO, this means:\n",
        "- More confident decisions\n",
        "- Lower risk of scaling failures\n",
        "- Better ROI justification\n",
        "- Professional, defensible analysis\n",
        "\n",
        "This is why it was prioritized first‚Äîit transforms the agent from ‚Äúhelpful tool‚Äù to ‚Äútrusted advisor‚Äù for high-stakes decisions."
      ],
      "metadata": {
        "id": "qU33JtjTOUJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "224mflqDJrAB"
      },
      "outputs": [],
      "source": [
        "def validate_portfolio_entry(entry: Dict[str, Any], index: int) -> List[str]:\n",
        "    \"\"\"Validate a single portfolio entry.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    if not isinstance(entry, dict):\n",
        "        errors.append(f\"Portfolio entry at index {index} is not a dictionary\")\n",
        "        return errors\n",
        "\n",
        "    if \"experiment_id\" not in entry or not entry.get(\"experiment_id\"):\n",
        "        errors.append(f\"Portfolio entry at index {index} missing required field: experiment_id\")\n",
        "\n",
        "    if \"status\" in entry:\n",
        "        valid_statuses = [\"completed\", \"running\", \"planned\"]\n",
        "        if entry[\"status\"] not in valid_statuses:\n",
        "            errors.append(f\"Portfolio entry {entry.get('experiment_id', index)} has invalid status: {entry['status']}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_definition_entry(entry: Dict[str, Any], index: int) -> List[str]:\n",
        "    \"\"\"Validate a single experiment definition entry.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    if not isinstance(entry, dict):\n",
        "        errors.append(f\"Definition entry at index {index} is not a dictionary\")\n",
        "        return errors\n",
        "\n",
        "    if \"experiment_id\" not in entry or not entry.get(\"experiment_id\"):\n",
        "        errors.append(f\"Definition entry at index {index} missing required field: experiment_id\")\n",
        "\n",
        "    if \"primary_metric\" not in entry or not entry.get(\"primary_metric\"):\n",
        "        errors.append(f\"Definition entry {entry.get('experiment_id', index)} missing required field: primary_metric\")\n",
        "\n",
        "    if \"variants\" in entry:\n",
        "        if not isinstance(entry[\"variants\"], list) or len(entry[\"variants\"]) < 2:\n",
        "            errors.append(f\"Definition entry {entry.get('experiment_id', index)} must have at least 2 variants\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_metrics_entry(entry: Dict[str, Any], index: int) -> List[str]:\n",
        "    \"\"\"Validate a single metrics entry.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    if not isinstance(entry, dict):\n",
        "        errors.append(f\"Metrics entry at index {index} is not a dictionary\")\n",
        "        return errors\n",
        "\n",
        "    if \"experiment_id\" not in entry or not entry.get(\"experiment_id\"):\n",
        "        errors.append(f\"Metrics entry at index {index} missing required field: experiment_id\")\n",
        "\n",
        "    if \"variant\" not in entry or not entry.get(\"variant\"):\n",
        "        errors.append(f\"Metrics entry at index {index} missing required field: variant\")\n",
        "\n",
        "    if \"sample_size\" in entry:\n",
        "        sample_size = entry[\"sample_size\"]\n",
        "        if not isinstance(sample_size, (int, float)) or sample_size <= 0:\n",
        "            errors.append(f\"Metrics entry {entry.get('experiment_id', index)} has invalid sample_size: {sample_size}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_analysis_entry(entry: Dict[str, Any], index: int) -> List[str]:\n",
        "    \"\"\"Validate a single analysis entry.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    if not isinstance(entry, dict):\n",
        "        errors.append(f\"Analysis entry at index {index} is not a dictionary\")\n",
        "        return errors\n",
        "\n",
        "    if \"experiment_id\" not in entry or not entry.get(\"experiment_id\"):\n",
        "        errors.append(f\"Analysis entry at index {index} missing required field: experiment_id\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_decisions_entry(entry: Dict[str, Any], index: int) -> List[str]:\n",
        "    \"\"\"Validate a single decisions entry.\"\"\"\n",
        "    errors = []\n",
        "\n",
        "    if not isinstance(entry, dict):\n",
        "        errors.append(f\"Decisions entry at index {index} is not a dictionary\")\n",
        "        return errors\n",
        "\n",
        "    if \"experiment_id\" not in entry or not entry.get(\"experiment_id\"):\n",
        "        errors.append(f\"Decisions entry at index {index} missing required field: experiment_id\")\n",
        "\n",
        "    if \"decision\" in entry:\n",
        "        valid_decisions = [\"scale\", \"iterate\", \"retire\", \"do_not_start\", \"pending\"]\n",
        "        if entry[\"decision\"] not in valid_decisions:\n",
        "            errors.append(f\"Decisions entry {entry.get('experiment_id', index)} has invalid decision: {entry['decision']}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "def validate_experiment_data(\n",
        "    portfolio_entry: Dict[str, Any],\n",
        "    definition: Optional[Dict[str, Any]],\n",
        "    metrics: Optional[List[Dict[str, Any]]],\n",
        "    analysis: Optional[Dict[str, Any]]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validate experiment data completeness and quality across all data types.\n",
        "\n",
        "    Returns list of validation errors (empty if valid).\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    experiment_id = portfolio_entry.get(\"experiment_id\", \"unknown\")\n",
        "\n",
        "    # Check required fields in portfolio entry\n",
        "    if not experiment_id:\n",
        "        errors.append(f\"Portfolio entry missing experiment_id\")\n",
        "\n",
        "    # Check definition exists\n",
        "    if not definition:\n",
        "        errors.append(f\"Experiment {experiment_id} missing definition\")\n",
        "    else:\n",
        "        if not definition.get(\"primary_metric\"):\n",
        "            errors.append(f\"Experiment {experiment_id} definition missing primary_metric\")\n",
        "        if not definition.get(\"variants\") or len(definition.get(\"variants\", [])) < 2:\n",
        "            errors.append(f\"Experiment {experiment_id} definition missing or insufficient variants\")\n",
        "\n",
        "    # Check metrics exist\n",
        "    if not metrics or len(metrics) == 0:\n",
        "        errors.append(f\"Experiment {experiment_id} missing metrics data\")\n",
        "    else:\n",
        "        # Check we have metrics for all variants\n",
        "        if definition:\n",
        "            variants = definition.get(\"variants\", [])\n",
        "            metric_variants = {m.get(\"variant\") for m in metrics}\n",
        "            missing_variants = set(variants) - metric_variants\n",
        "            if missing_variants:\n",
        "                errors.append(f\"Experiment {experiment_id} missing metrics for variants: {', '.join(missing_variants)}\")\n",
        "\n",
        "    # Check analysis if experiment is completed\n",
        "    if portfolio_entry.get(\"status\") == \"completed\" and not analysis:\n",
        "        errors.append(f\"Experiment {experiment_id} is completed but missing analysis\")\n",
        "\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Big Picture First: What Is This Code Doing?\n",
        "\n",
        "These utilities answer one simple question:\n",
        "\n",
        "> **‚ÄúIs the data good enough to trust?‚Äù**\n",
        "\n",
        "Before your agent:\n",
        "\n",
        "* analyzes experiments\n",
        "* makes decisions\n",
        "* generates recommendations\n",
        "\n",
        "‚Ä¶it **checks the data for problems**.\n",
        "\n",
        "This is critical because:\n",
        "\n",
        "* bad data ‚Üí bad decisions\n",
        "* silent data issues ‚Üí broken trust\n",
        "* broken trust ‚Üí nobody uses the agent\n",
        "\n",
        "So this code acts like **a quality inspector before the factory runs**.\n",
        "\n",
        "---\n",
        "\n",
        "## How This Fits Into Your Architecture\n",
        "\n",
        "These are **pure utilities**:\n",
        "\n",
        "* no state\n",
        "* no workflow logic\n",
        "* no decisions about *what to do next*\n",
        "\n",
        "They only:\n",
        "\n",
        "> **look at data and report what‚Äôs wrong**\n",
        "\n",
        "Then:\n",
        "\n",
        "* **nodes decide** how to react to those errors\n",
        "* **state stores** the errors\n",
        "* **orchestrator keeps flowing**\n",
        "\n",
        "This is exactly the separation you want.\n",
        "\n",
        "---\n",
        "\n",
        "## Let‚Äôs Walk Through the Types of Validators\n",
        "\n",
        "### 1Ô∏è‚É£ ‚ÄúIs this entry shaped correctly?‚Äù\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "validate_portfolio_entry\n",
        "validate_definition_entry\n",
        "validate_metrics_entry\n",
        "validate_analysis_entry\n",
        "validate_decisions_entry\n",
        "```\n",
        "\n",
        "These functions check things like:\n",
        "\n",
        "* Is this a dictionary?\n",
        "* Does it have an `experiment_id`?\n",
        "* Is the status one of the allowed values?\n",
        "* Are there enough variants?\n",
        "* Is sample size valid?\n",
        "\n",
        "üí° **High-school analogy**:\n",
        "This is like checking that homework:\n",
        "\n",
        "* has a name\n",
        "* is readable\n",
        "* answers the right questions\n",
        "* follows the rules\n",
        "\n",
        "If not ‚Üí it gets flagged.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ ‚ÄúAre the values reasonable?‚Äù\n",
        "\n",
        "Examples:\n",
        "\n",
        "* `sample_size > 0`\n",
        "* decision is one of `\"scale\" | \"iterate\" | \"retire\"`\n",
        "* variants list has at least 2 entries\n",
        "\n",
        "This prevents:\n",
        "\n",
        "* divide-by-zero errors\n",
        "* nonsense calculations\n",
        "* undefined behavior later\n",
        "\n",
        "üí° **Architect insight**:\n",
        "Most system failures come from *invalid assumptions*, not bad math.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Cross-Data Validation (The Important One)\n",
        "\n",
        "This function is the real powerhouse:\n",
        "\n",
        "```python\n",
        "validate_experiment_data(...)\n",
        "```\n",
        "\n",
        "This one checks **consistency across datasets**, not just individual rows.\n",
        "\n",
        "It asks questions like:\n",
        "\n",
        "* Does this experiment have a definition?\n",
        "* Do metrics exist for *all variants*?\n",
        "* Is analysis missing even though the experiment is completed?\n",
        "* Are required fields present everywhere?\n",
        "\n",
        "üí° **High-school analogy**:\n",
        "It‚Äôs like checking:\n",
        "\n",
        "* the test exists\n",
        "* the answer key exists\n",
        "* the student took the test\n",
        "* the test was graded\n",
        "\n",
        "If any piece is missing ‚Üí something‚Äôs wrong.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Is a Big Deal for Orchestrator Design\n",
        "\n",
        "### ‚úÖ You Catch Problems Early\n",
        "\n",
        "Instead of failing later during:\n",
        "\n",
        "* analysis\n",
        "* decision logic\n",
        "* reporting\n",
        "\n",
        "You fail **upstream**, where fixes are cheaper.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Errors Become Data, Not Crashes\n",
        "\n",
        "Notice what these functions return:\n",
        "\n",
        "```python\n",
        "List[str]  # error messages\n",
        "```\n",
        "\n",
        "Not exceptions. Not exits.\n",
        "\n",
        "This means:\n",
        "\n",
        "* errors get written into **state**\n",
        "* reports can show data quality issues\n",
        "* humans can see *why* something didn‚Äôt happen\n",
        "\n",
        "That‚Äôs production-grade thinking.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ You Enable Graceful Degradation\n",
        "\n",
        "Because validation is separate:\n",
        "\n",
        "* one bad experiment doesn‚Äôt kill the portfolio\n",
        "* one missing metric doesn‚Äôt crash the agent\n",
        "* the system can still produce partial insights\n",
        "\n",
        "That‚Äôs how *real* systems survive messy reality.\n",
        "\n",
        "---\n",
        "\n",
        "## What You Should Focus On as an Orchestrator Architect\n",
        "\n",
        "This code teaches you **three critical lessons**:\n",
        "\n",
        "### 1Ô∏è‚É£ Validation Is a First-Class Citizen\n",
        "\n",
        "Not an afterthought.\n",
        "Not a try/except hack.\n",
        "\n",
        "It deserves:\n",
        "\n",
        "* its own utilities\n",
        "* its own logic\n",
        "* its own visibility in reports\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Nodes Decide What Errors Mean\n",
        "\n",
        "Validators only say:\n",
        "\n",
        "> ‚ÄúHere‚Äôs what‚Äôs wrong.‚Äù\n",
        "\n",
        "Nodes decide:\n",
        "\n",
        "* stop?\n",
        "* warn?\n",
        "* continue partially?\n",
        "* downgrade confidence?\n",
        "\n",
        "That separation is *gold*.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ State Is the Single Source of Truth\n",
        "\n",
        "Errors don‚Äôt disappear.\n",
        "They live in state.\n",
        "\n",
        "That means:\n",
        "\n",
        "* reproducibility\n",
        "* auditability\n",
        "* explainability\n",
        "\n",
        "This is how you earn trust.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Is a Natural Evolution of Your Agent\n",
        "\n",
        "You‚Äôre moving through the **correct maturity curve**:\n",
        "\n",
        "1. Load data\n",
        "2. Analyze data\n",
        "3. Make decisions\n",
        "4. **Validate assumptions**\n",
        "5. Quantify confidence\n",
        "6. Add ROI and cost awareness\n",
        "\n",
        "Most people skip step 4.\n",
        "You didn‚Äôt.\n",
        "\n",
        "That‚Äôs why your agent is becoming **enterprise-ready**, not just ‚Äúcool‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "### Bottom Line\n",
        "\n",
        "Yes ‚Äî you understand this perfectly:\n",
        "\n",
        "* These utilities do *one thing only*\n",
        "* They protect the rest of the system\n",
        "* They make decisions safer, not smarter\n",
        "* They scale without adding complexity\n",
        "\n",
        "This is not just good agent design.\n",
        "This is **good systems engineering**.\n",
        "\n"
      ],
      "metadata": {
        "id": "B9FCe83ZQmno"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIJ0KAZqQqYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}