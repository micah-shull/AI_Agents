{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk0IQmsvDX72I/c29rwDKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/033_Agent_Loop_with_Function_Calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs break down the differences and why they matter in the evolution of building LLM agents. These are **core design upgrades** that remove a lot of friction from earlier agent systems.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **No More Custom Parsing Logic**\n",
        "\n",
        "**Before (Old Way):**\n",
        "\n",
        "* You had to write fragile logic to force the LLM to return JSON.\n",
        "* Then you'd try to `json.loads()` the text and hope it worked.\n",
        "* You might say:\n",
        "  *‚ÄúRespond only with a JSON object that includes keys X and Y...‚Äù*\n",
        "  and then try to validate it manually.\n",
        "\n",
        "**Now (With Function Calling):**\n",
        "\n",
        "* You define a schema with `tools=[...]`.\n",
        "* The model returns the tool call in structured format:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"tool_calls\": [\n",
        "      {\n",
        "        \"function\": {\n",
        "          \"name\": \"read_file\",\n",
        "          \"arguments\": \"{\\\"file_name\\\": \\\"example.txt\\\"}\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "  ```\n",
        "\n",
        "**‚úÖ What this gives you:**\n",
        "Reliable, automatic structured output. You no longer treat the LLM like a shaky string generator ‚Äî it behaves like a typed interface.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **Dynamic Execution**\n",
        "\n",
        "**Before:**\n",
        "\n",
        "* You had to create a big `if/else` logic tree:\n",
        "\n",
        "  ```python\n",
        "  if \"read\" in response and \"file\" in response:\n",
        "      call read_file()\n",
        "  ```\n",
        "\n",
        "**Now:**\n",
        "\n",
        "* The model explicitly says:\n",
        "  *‚ÄúUse this function with these arguments.‚Äù*\n",
        "* You simply read the output and run it:\n",
        "\n",
        "  ```python\n",
        "  tool_name = tool_call.function.name\n",
        "  args = json.loads(tool_call.function.arguments)\n",
        "  result = tool_functions[tool_name](**args)\n",
        "  ```\n",
        "\n",
        "**‚úÖ What this gives you:**\n",
        "A flexible system where the LLM controls the flow ‚Äî it tells you what to do, and you just do it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Unified Text & Action Handling**\n",
        "\n",
        "**Before:**\n",
        "\n",
        "* You had to manually guess:\n",
        "  *‚ÄúIs the LLM trying to say something or should I run a function?‚Äù*\n",
        "\n",
        "**Now:**\n",
        "\n",
        "* The OpenAI API cleanly separates this:\n",
        "\n",
        "  * If it wants to call a function ‚Üí `tool_calls` is present\n",
        "  * If not ‚Üí it just responds with text in `message.content`\n",
        "\n",
        "**‚úÖ What this gives you:**\n",
        "A natural **blend of conversation and action**, like:\n",
        "\n",
        "* ‚ÄúLet me grab the file for you‚Ä¶‚Äù ‚Üí (calls `read_file`)\n",
        "* ‚ÄúHere are the results‚Ä¶‚Äù ‚Üí (plain response)\n",
        "\n",
        "You no longer need a separate ‚Äúchat mode‚Äù and ‚Äúaction mode‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 4. **Automated Function Execution**\n",
        "\n",
        "**Before:**\n",
        "\n",
        "* You wrote boilerplate glue code to match intent to function:\n",
        "\n",
        "  ```python\n",
        "  if intent == \"get_stock_price\":\n",
        "      get_stock_price(ticker)\n",
        "  ```\n",
        "\n",
        "**Now:**\n",
        "\n",
        "* It‚Äôs just:\n",
        "\n",
        "  ```python\n",
        "  tool_name = tool_call.function.name\n",
        "  result = tool_functions[tool_name](**args)\n",
        "  ```\n",
        "\n",
        "The mapping is automatic ‚Äî you use a Python dictionary to connect schema ‚Üí function.\n",
        "\n",
        "**‚úÖ What this gives you:**\n",
        "A **scalable architecture**: add new tools by just:\n",
        "\n",
        "* Defining the Python function\n",
        "* Adding a schema entry\n",
        "\n",
        "No need to rewrite core logic.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Final Thought\n",
        "\n",
        "These changes represent a big leap:\n",
        "\n",
        "> üß© Instead of parsing messy guesses, you‚Äôre building structured, reliable agents using a formal interface with the LLM.\n",
        "\n",
        "That‚Äôs a real shift in **trust and power** ‚Äî and why function calling is a game-changer for agent development.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrILO9TXX1kk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXrzPGDGVo7V"
      },
      "outputs": [],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "from typing import List\n",
        "from litellm import completion\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def list_files() -> List[str]:\n",
        "    \"\"\"List files in the current directory.\"\"\"\n",
        "    return os.listdir(\".\")\n",
        "\n",
        "def read_file(file_name: str) -> str:\n",
        "    \"\"\"Read a file's contents.\"\"\"\n",
        "    try:\n",
        "        with open(file_name, \"r\") as file:\n",
        "            return file.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: {file_name} not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def terminate(message: str) -> None:\n",
        "    \"\"\"Terminate the agent loop and provide a summary message.\"\"\"\n",
        "    print(f\"Termination message: {message}\")\n",
        "\n",
        "tool_functions = {\n",
        "    \"list_files\": list_files,\n",
        "    \"read_file\": read_file,\n",
        "    \"terminate\": terminate\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"list_files\",\n",
        "            \"description\": \"Returns a list of files in the directory.\",\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"read_file\",\n",
        "            \"description\": \"Reads the content of a specified file in the directory.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
        "                \"required\": [\"file_name\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"terminate\",\n",
        "            \"description\": \"Terminates the conversation. No further actions or interactions are possible after this. Prints the provided message for the user.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"message\": {\"type\": \"string\"},\n",
        "                },\n",
        "                \"required\": [\"message\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "agent_rules = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "You are an AI agent that can perform tasks by using available tools.\n",
        "\n",
        "If a user asks about files, documents, or content, first list the files before reading them.\n",
        "\n",
        "When you are done, terminate the conversation by using the \"terminate\" tool and I will provide the results to the user.\n",
        "\"\"\"\n",
        "}]\n",
        "\n",
        "# Initialize agent parameters\n",
        "iterations = 0\n",
        "max_iterations = 10\n",
        "\n",
        "user_task = input(\"What would you like me to do? \")\n",
        "\n",
        "memory = [{\"role\": \"user\", \"content\": user_task}]\n",
        "\n",
        "# The Agent Loop\n",
        "while iterations < max_iterations:\n",
        "\n",
        "    messages = agent_rules + memory\n",
        "\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        tool = response.choices[0].message.tool_calls[0]\n",
        "        tool_name = tool.function.name\n",
        "        tool_args = json.loads(tool.function.arguments)\n",
        "\n",
        "        action = {\n",
        "            \"tool_name\": tool_name,\n",
        "            \"args\": tool_args\n",
        "        }\n",
        "\n",
        "        if tool_name == \"terminate\":\n",
        "            print(f\"Termination message: {tool_args['message']}\")\n",
        "            break\n",
        "        elif tool_name in tool_functions:\n",
        "            try:\n",
        "                result = {\"result\": tool_functions[tool_name](**tool_args)}\n",
        "            except Exception as e:\n",
        "                result = {\"error\":f\"Error executing {tool_name}: {str(e)}\"}\n",
        "        else:\n",
        "            result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
        "\n",
        "        print(f\"Executing: {tool_name} with args {tool_args}\")\n",
        "        print(f\"Result: {result}\")\n",
        "        memory.extend([\n",
        "            {\"role\": \"assistant\", \"content\": json.dumps(action)},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(result)}\n",
        "        ])\n",
        "    else:\n",
        "        result = response.choices[0].message.content\n",
        "        print(f\"Response: {result}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "i9KmxNvwVvIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to walk through and learn from the code, along with **key parts you should focus on** as you dissect this notebook:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Big Picture: What's Different Here?**\n",
        "\n",
        "This lecture introduces a **function-calling agent loop** that is:\n",
        "\n",
        "* **Robust**: Tools return structured responses using OpenAI‚Äôs built-in function-calling format.\n",
        "* **Simplified**: No need to parse text or force output structure.\n",
        "* **Autonomous**: The loop continues until the LLM explicitly uses a `terminate` tool.\n",
        "* **Flexible**: Easily extendable with new tools.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Step-by-Step Walkthrough and What to Learn**\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Core Tools / Functions**\n",
        "\n",
        "```python\n",
        "def list_files() -> List[str]: ...\n",
        "def read_file(file_name: str) -> str: ...\n",
        "def terminate(message: str) -> None: ...\n",
        "```\n",
        "\n",
        "‚úîÔ∏è These are the **real Python functions** that do the work.\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* How the functions relate to the tasks the LLM will ask to perform.\n",
        "* That `terminate()` doesn't return anything‚Äîit's just for flow control.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Tool Schema Definitions**\n",
        "\n",
        "```python\n",
        "tools = [\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"list_files\",\n",
        "      \"description\": \"...\",\n",
        "      \"parameters\": {...}\n",
        "    }\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "‚úîÔ∏è Each tool schema tells the LLM:\n",
        "\n",
        "* What it‚Äôs called\n",
        "* What it does\n",
        "* What input it expects (in JSON Schema format)\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* The **strict format** required: `type`, `function`, `parameters`\n",
        "* `required` is key‚ÄîOpenAI will enforce that fields are present\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Agent Rules (System Prompt)**\n",
        "\n",
        "```python\n",
        "agent_rules = [{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"You are an AI agent that can perform tasks...\"\n",
        "}]\n",
        "```\n",
        "\n",
        "‚úîÔ∏è This primes the model to:\n",
        "\n",
        "* Know its tools\n",
        "* Understand the flow (e.g., list before read)\n",
        "* Use `terminate` at the end\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* The importance of **clear, high-level behavioral guidance**\n",
        "* You can write agent policies here\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Agent Memory + Loop**\n",
        "\n",
        "```python\n",
        "memory = [{\"role\": \"user\", \"content\": user_task}]\n",
        "...\n",
        "while iterations < max_iterations:\n",
        "    messages = agent_rules + memory\n",
        "    response = completion(...)\n",
        "```\n",
        "\n",
        "‚úîÔ∏è This loop:\n",
        "\n",
        "* Keeps state across tool calls\n",
        "* Appends each tool action and result as chat messages\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* Memory management: you‚Äôre simulating conversation history\n",
        "* `tool_calls` vs `content`: always check for tool calls before responding\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Tool Execution**\n",
        "\n",
        "```python\n",
        "tool = response.choices[0].message.tool_calls[0]\n",
        "tool_name = tool.function.name\n",
        "tool_args = json.loads(tool.function.arguments)\n",
        "...\n",
        "result = tool_functions[tool_name](**tool_args)\n",
        "```\n",
        "\n",
        "‚úîÔ∏è The LLM‚Äôs response tells you:\n",
        "\n",
        "* What tool to call\n",
        "* What arguments to pass\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* You **don‚Äôt need to parse anything manually** anymore\n",
        "* Just use `.tool_calls` and let the model handle the structure\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Termination Logic**\n",
        "\n",
        "```python\n",
        "if tool_name == \"terminate\":\n",
        "    print(...)\n",
        "    break\n",
        "```\n",
        "\n",
        "‚úîÔ∏è Model controls flow. Agent ends when model says to terminate.\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* This is the backbone of multi-turn autonomous behavior\n",
        "* You‚Äôre trusting the LLM to decide when it‚Äôs done\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Error Handling (Important Takeaway)**\n",
        "\n",
        "```python\n",
        "except Exception as e:\n",
        "    result = {\"error\": ...}\n",
        "```\n",
        "\n",
        "‚úîÔ∏è Prevents crashes and shows feedback.\n",
        "\n",
        "üëâ Learn:\n",
        "\n",
        "* You still need defensive coding even with structure\n",
        "* Consider catching `json.JSONDecodeError` too\n",
        "\n",
        "---\n",
        "\n",
        "### üèÅ **Key Lessons to Take Away**\n",
        "\n",
        "| Concept                | Why It Matters                                                  |\n",
        "| ---------------------- | --------------------------------------------------------------- |\n",
        "| **Function calling**   | Structured responses from the LLM ‚Äî no more parsing plain text  |\n",
        "| **Schema + functions** | You define what‚Äôs possible, the LLM chooses what to use         |\n",
        "| **Agent loop**         | Clean loop that listens, interprets, acts, and learns           |\n",
        "| **Tool memory**        | Maintaining tool/action history is essential for context        |\n",
        "| **Terminate tool**     | Gives the model control to stop ‚Äî huge for autonomy             |\n",
        "| **Error safety**       | Even smart agents need safety nets for bad inputs or edge cases |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uesm0UkJXJg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes ‚Äî **you absolutely could build an agent that returns both a tool call *and* a message**, but you'd need to **structure the logic across multiple steps** using the OpenAI Chat Completions API. Here's how it works under the hood and how you‚Äôd implement it.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What You *Can* Do: Simulate Both via a Two-Step Agent Loop\n",
        "\n",
        "Even though **a single assistant message cannot return both**, you can **design an agent loop** that achieves the effect.\n",
        "\n",
        "#### üß© Step-by-step breakdown:\n",
        "\n",
        "1. **User asks:**\n",
        "\n",
        "   > ‚ÄúWhat‚Äôs the current price of AAPL and should I be worried about recent news?‚Äù\n",
        "\n",
        "2. **LLM decides:**\n",
        "   It needs to call a tool first:\n",
        "\n",
        "   ```json\n",
        "   {\n",
        "     \"tool_calls\": [\n",
        "       {\n",
        "         \"function\": {\n",
        "           \"name\": \"get_stock_price\",\n",
        "           \"arguments\": \"{\\\"ticker\\\": \\\"AAPL\\\"}\"\n",
        "         }\n",
        "       }\n",
        "     ]\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. **Your code executes the tool**, then adds that output as a `tool` role message.\n",
        "\n",
        "4. **Send the updated `messages` list back** to the LLM.\n",
        "\n",
        "5. **Now the LLM responds:**\n",
        "\n",
        "   > ‚ÄúAAPL is currently trading at \\$187.65. Based on recent headlines, there‚Äôs some volatility due to earnings season. Would you like me to check recent news as well?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What You‚Äôre Building: A Conversational Agent Loop\n",
        "\n",
        "The logic looks like this:\n",
        "\n",
        "```python\n",
        "while True:\n",
        "    response = client.chat.completions.create(...)\n",
        "\n",
        "    if response contains tool_call:\n",
        "        execute tool\n",
        "        add tool result to messages\n",
        "    elif response contains text:\n",
        "        show final reply\n",
        "        break\n",
        "```\n",
        "\n",
        "That loop allows:\n",
        "\n",
        "* Multiple tool calls\n",
        "* Conversational context\n",
        "* Dynamic back-and-forth reasoning\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Optional: Multi-tool chaining\n",
        "\n",
        "Advanced agents (like the one you're building!) can also:\n",
        "\n",
        "* Detect the need to call **several tools** before responding\n",
        "* Re-enter the loop until all needed data is gathered\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Goal                                    | Supported?         | How to do it                        |\n",
        "| --------------------------------------- | ------------------ | ----------------------------------- |\n",
        "| Tool call + natural response (together) | ‚ùå Not in 1 message |                                     |\n",
        "| Tool + response sequence                | ‚úÖ Yes              | Multi-step loop using `tool` role   |\n",
        "| Multi-tool planning                     | ‚úÖ Yes              | Chain calls using `tool_calls` loop |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xh8ttWVqYnBS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e35aOL8FXQs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}