{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK32lXgAau52sjRnaPesUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/294_HITL_Routing_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Routing decision utilities for HITL Orchestrator"
      ],
      "metadata": {
        "id": "jOywvhUWE1um"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpvbuLDSEzmM"
      },
      "outputs": [],
      "source": [
        "\"\"\"Routing decision utilities for HITL Orchestrator\"\"\"\n",
        "\n",
        "from typing import Dict, Any, Optional, List\n",
        "\n",
        "\n",
        "def apply_routing_policy(\n",
        "    risk_level: str,\n",
        "    confidence_score: float,\n",
        "    routing_policy: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Apply routing policy to determine routing decision for a task.\n",
        "\n",
        "    Args:\n",
        "        risk_level: Task risk level (\"low\", \"medium\", \"high\")\n",
        "        confidence_score: Agent confidence score (0.0-1.0)\n",
        "        routing_policy: Routing policy dictionary with rules\n",
        "\n",
        "    Returns:\n",
        "        Routing decision dictionary with:\n",
        "        - routing_decision: \"auto_approve\" | \"human_review\" | \"escalate\"\n",
        "        - assigned_human_role: Optional[str]\n",
        "        - rule_applied: str (rule_id)\n",
        "        - reasoning: str\n",
        "    \"\"\"\n",
        "    rules = routing_policy.get(\"rules\", [])\n",
        "\n",
        "    # Sort rules by priority (lower number = higher priority)\n",
        "    sorted_rules = sorted(rules, key=lambda r: r.get(\"priority\", 999))\n",
        "\n",
        "    # Find first matching rule\n",
        "    for rule in sorted_rules:\n",
        "        conditions = rule.get(\"conditions\", {})\n",
        "\n",
        "        # Check risk level condition\n",
        "        if \"risk_level\" in conditions:\n",
        "            if risk_level != conditions[\"risk_level\"]:\n",
        "                continue\n",
        "\n",
        "        # Check confidence condition\n",
        "        if \"min_confidence\" in conditions:\n",
        "            min_confidence = conditions[\"min_confidence\"]\n",
        "            if confidence_score < min_confidence:\n",
        "                continue\n",
        "\n",
        "        # Rule matches - apply it\n",
        "        action = rule.get(\"action\")\n",
        "        assigned_role = rule.get(\"assigned_human_role\")\n",
        "        rule_id = rule.get(\"rule_id\", \"unknown\")\n",
        "\n",
        "        # Build reasoning\n",
        "        reasoning_parts = []\n",
        "        if \"risk_level\" in conditions:\n",
        "            reasoning_parts.append(f\"Risk level: {risk_level}\")\n",
        "        if \"min_confidence\" in conditions:\n",
        "            reasoning_parts.append(f\"Confidence: {confidence_score:.2f} >= {conditions['min_confidence']}\")\n",
        "        reasoning = \", \".join(reasoning_parts) if reasoning_parts else \"Default rule\"\n",
        "\n",
        "        return {\n",
        "            \"routing_decision\": action,\n",
        "            \"assigned_human_role\": assigned_role,\n",
        "            \"rule_applied\": rule_id,\n",
        "            \"reasoning\": reasoning\n",
        "        }\n",
        "\n",
        "    # No rule matched - default to human review\n",
        "    return {\n",
        "        \"routing_decision\": \"human_review\",\n",
        "        \"assigned_human_role\": \"domain_reviewer\",\n",
        "        \"rule_applied\": \"default\",\n",
        "        \"reasoning\": \"No matching rule found, defaulting to human review\"\n",
        "    }\n",
        "\n",
        "\n",
        "def make_routing_decision(\n",
        "    task_id: str,\n",
        "    risk_level: str,\n",
        "    confidence_score: float,\n",
        "    routing_policy: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Make routing decision for a single task.\n",
        "\n",
        "    Args:\n",
        "        task_id: Task identifier\n",
        "        risk_level: Task risk level\n",
        "        confidence_score: Agent confidence score\n",
        "        routing_policy: Routing policy\n",
        "\n",
        "    Returns:\n",
        "        Complete routing decision dictionary\n",
        "    \"\"\"\n",
        "    decision = apply_routing_policy(risk_level, confidence_score, routing_policy)\n",
        "\n",
        "    return {\n",
        "        \"task_id\": task_id,\n",
        "        \"risk_level\": risk_level,\n",
        "        \"confidence_score\": confidence_score,\n",
        "        **decision\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# üß† Big Picture: What This Code Does\n",
        "\n",
        "This file answers **one single question**:\n",
        "\n",
        "> **‚ÄúGiven a task‚Äôs risk and the AI‚Äôs confidence, who should decide?‚Äù**\n",
        "\n",
        "That‚Äôs it.\n",
        "\n",
        "Everything else in the orchestrator exists to *support* this moment.\n",
        "\n",
        "---\n",
        "\n",
        "# üß© Mental Model (Very Important)\n",
        "\n",
        "Think of the routing policy as a **rulebook** and this code as a **referee**.\n",
        "\n",
        "* The rulebook says *what should happen*\n",
        "* The referee checks the rules **in order**\n",
        "* The referee stops at the **first rule that applies**\n",
        "* The referee explains the decision\n",
        "\n",
        "No guessing. No learning. No magic.\n",
        "\n",
        "---\n",
        "\n",
        "# Part 1: `apply_routing_policy`\n",
        "\n",
        "## üéØ ‚ÄúRead the rules and pick one‚Äù\n",
        "\n",
        "```python\n",
        "def apply_routing_policy(...)\n",
        "```\n",
        "\n",
        "### What this function is responsible for\n",
        "\n",
        "This function:\n",
        "\n",
        "* does **NOT** know about task IDs\n",
        "* does **NOT** know about humans yet\n",
        "* does **NOT** finalize anything\n",
        "\n",
        "It only answers:\n",
        "\n",
        "> ‚ÄúBased on risk and confidence, what does the policy say?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ Step 1: Get the rules\n",
        "\n",
        "```python\n",
        "rules = routing_policy.get(\"rules\", [])\n",
        "```\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "* Pull the list of rules out of the policy\n",
        "* If there are none, use an empty list (fail safely)\n",
        "\n",
        "---\n",
        "\n",
        "## ü•á Step 2: Sort rules by priority\n",
        "\n",
        "```python\n",
        "sorted_rules = sorted(...)\n",
        "```\n",
        "\n",
        "### Why this matters conceptually\n",
        "\n",
        "Rules are **not equal**.\n",
        "\n",
        "Some are more important:\n",
        "\n",
        "* ‚ÄúHigh risk ‚Üí escalate‚Äù should beat everything\n",
        "* ‚ÄúLow risk + high confidence ‚Üí auto‚Äù comes later\n",
        "\n",
        "This sorting guarantees:\n",
        "\n",
        "> **More serious rules are checked first**\n",
        "\n",
        "This avoids accidental automation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Step 3: Check rules one by one\n",
        "\n",
        "```python\n",
        "for rule in sorted_rules:\n",
        "```\n",
        "\n",
        "This is like reading a checklist:\n",
        "\n",
        "> ‚ÄúDoes this rule apply?\n",
        "> No? Next.\n",
        "> Yes? Stop.‚Äù\n",
        "\n",
        "The first match wins.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Step 4: Check risk level\n",
        "\n",
        "```python\n",
        "if risk_level != conditions[\"risk_level\"]:\n",
        "    continue\n",
        "```\n",
        "\n",
        "Plain English:\n",
        "\n",
        "* If the rule cares about risk\n",
        "* And the task‚Äôs risk doesn‚Äôt match\n",
        "* Skip the rule\n",
        "\n",
        "This keeps rules **specific and safe**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Step 5: Check confidence\n",
        "\n",
        "```python\n",
        "if confidence_score < min_confidence:\n",
        "    continue\n",
        "```\n",
        "\n",
        "Plain English:\n",
        "\n",
        "* If the rule requires confidence\n",
        "* And the AI isn‚Äôt confident enough\n",
        "* Skip the rule\n",
        "\n",
        "This is the **core safety mechanism**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 6: Apply the rule\n",
        "\n",
        "Once both checks pass:\n",
        "\n",
        "```python\n",
        "return {\n",
        "  routing_decision,\n",
        "  assigned_human_role,\n",
        "  rule_applied,\n",
        "  reasoning\n",
        "}\n",
        "```\n",
        "\n",
        "### Why this return is powerful\n",
        "\n",
        "You‚Äôre not just saying *what* happened.\n",
        "You‚Äôre also saying:\n",
        "\n",
        "* which rule fired\n",
        "* why it fired\n",
        "\n",
        "That‚Äôs **explainable AI without AI**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† The ‚ÄúReasoning‚Äù field (very important)\n",
        "\n",
        "This line:\n",
        "\n",
        "```python\n",
        "reasoning = \"Risk level: low, Confidence: 0.91 >= 0.80\"\n",
        "```\n",
        "\n",
        "Exists for **humans**, not machines.\n",
        "\n",
        "It answers:\n",
        "\n",
        "> ‚ÄúWhy did the system do this?‚Äù\n",
        "\n",
        "This is how trust is built.\n",
        "\n",
        "---\n",
        "\n",
        "## üö® Fallback: No rule matched\n",
        "\n",
        "```python\n",
        "return human_review\n",
        "```\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "> **When in doubt, ask a human.**\n",
        "\n",
        "This is the single most important safety principle in the entire system.\n",
        "\n",
        "---\n",
        "\n",
        "# Part 2: `make_routing_decision`\n",
        "\n",
        "## üßæ ‚ÄúWrap it up with context‚Äù\n",
        "\n",
        "```python\n",
        "def make_routing_decision(...)\n",
        "```\n",
        "\n",
        "This function:\n",
        "\n",
        "* calls the rule engine\n",
        "* attaches task identity\n",
        "* produces a complete decision record\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "* the referee making the call\n",
        "* then writing it down on the scoreboard\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why separate these two functions?\n",
        "\n",
        "* `apply_routing_policy` = **pure logic**\n",
        "* `make_routing_decision` = **system integration**\n",
        "\n",
        "This separation makes your system:\n",
        "\n",
        "* testable\n",
        "* reusable\n",
        "* understandable\n",
        "\n",
        "Good agents are built from **small, honest parts**.\n",
        "\n",
        "---\n",
        "\n",
        "# üéØ Big Takeaway (Most Important)\n",
        "\n",
        "This code is teaching you a deep idea:\n",
        "\n",
        "> **Autonomy is not about intelligence ‚Äî it‚Äôs about permission.**\n",
        "\n",
        "The AI doesn‚Äôt decide because it‚Äôs ‚Äúsmart‚Äù.\n",
        "It decides because:\n",
        "\n",
        "* risk is acceptable\n",
        "* confidence is high\n",
        "* rules allow it\n",
        "\n",
        "That‚Äôs how real-world AI works.\n",
        "\n"
      ],
      "metadata": {
        "id": "mTqpUH8qslZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0FS4s5JsodP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}