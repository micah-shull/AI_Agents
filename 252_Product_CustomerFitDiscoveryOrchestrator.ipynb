{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv6m4+vjrXQQ5hI9Mjtyrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/252_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading utilities for Product-Customer Fit Discovery Orchestrator"
      ],
      "metadata": {
        "id": "7buiu8kBDKfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cWupJPmCSRH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data loading utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_customers_csv(file_path: str = \"data/customers.csv\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load customers from CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to customers.csv file\n",
        "\n",
        "    Returns:\n",
        "        List of customer dictionaries\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If file is empty or invalid\n",
        "    \"\"\"\n",
        "    path = Path(file_path)\n",
        "\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Customers file not found: {file_path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Customers file is empty: {file_path}\")\n",
        "\n",
        "    # Convert to list of dictionaries\n",
        "    customers = df.to_dict('records')\n",
        "\n",
        "    return customers\n",
        "\n",
        "\n",
        "def load_transactions_csv(file_path: str = \"data/transactions.csv\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load transactions from CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to transactions.csv file\n",
        "\n",
        "    Returns:\n",
        "        List of transaction dictionaries\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If file is empty or invalid\n",
        "    \"\"\"\n",
        "    path = Path(file_path)\n",
        "\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Transactions file not found: {file_path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Transactions file is empty: {file_path}\")\n",
        "\n",
        "    # Convert Transaction_Date to datetime for easier processing\n",
        "    df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'])\n",
        "\n",
        "    # Convert to list of dictionaries\n",
        "    transactions = df.to_dict('records')\n",
        "\n",
        "    return transactions\n",
        "\n",
        "\n",
        "def load_product_catalog_csv(file_path: str = \"data/product_catalog.csv\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load product catalog from CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to product_catalog.csv file\n",
        "\n",
        "    Returns:\n",
        "        List of product dictionaries\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If file is empty or invalid\n",
        "    \"\"\"\n",
        "    path = Path(file_path)\n",
        "\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Product catalog file not found: {file_path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Product catalog file is empty: {file_path}\")\n",
        "\n",
        "    # Convert to list of dictionaries\n",
        "    products = df.to_dict('records')\n",
        "\n",
        "    return products\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Data Integrity & Standardization\n",
        "\n",
        "The focus here is on **robustness, efficiency, and standardization**, ensuring that the data passed to the specialist agents (like the `clustering_agent` or `graph_motif_agent`) is always in a predictable and usable format.\n",
        "\n",
        "### ðŸŽ¯ What to Focus On\n",
        "\n",
        "1.  **Robust Error Handling (The \"Guardrails\"):**\n",
        "    * The functions employ explicit **try-catch logic** (using `if not path.exists()` and `if df.empty`).\n",
        "    * **Focus:** This is critical for making your orchestrator **reliable**. If a file is missing or empty, the workflow doesn't just crash; it raises a specific, controlled error (`FileNotFoundError` or `ValueError`) that the **Orchestrator** can catch and report back to the user via the `errors` state object (as seen in the `goal_node` and `planning_node`).\n",
        "\n",
        "2.  **Integration of Data Science Standards (`pandas`):**\n",
        "    * The use of the `pandas` library (`import pandas as pd`) is not just a convenience; it's a **requirement for efficient data analysis**.\n",
        "    * **Focus:** The goal of the data ingestion node is to leverage specialized, non-LLM tools (like `pandas`) for tasks they do best: fast, high-volume data manipulation. This is what makes your system a **Hybrid Agent** architecture.\n",
        "\n",
        "3.  **Standardized Output Format:**\n",
        "    * All functions convert the raw CSV/DataFrame into a consistent Python native type: `List[Dict[str, Any]]` (a list of dictionaries).\n",
        "    * **Focus:** This creates a **Data Contract** for the rest of the workflow. By standardizing on this format, the downstream pre-processing and specialist agents (e.g., the `clustering_agent`) don't have to worry about reading CSVs; they just know they will receive a list of simple Python objects.\n",
        "\n",
        "4.  **Targeted Pre-processing:**\n",
        "    * The `load_transactions_csv` function includes a specific line: `df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'])`.\n",
        "    * **Focus:** This demonstrates a clean separation of concerns. It handles data type conversions immediately upon loading, which is necessary for proper subsequent analysis (like identifying sequential patterns).\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: The Hybrid Power of the Orchestrator\n",
        "\n",
        "This seemingly simple loading node is actually one of the clearest examples of what makes your orchestrator more powerful than a simple LLM agent:\n",
        "\n",
        "* **Simple Agent Limitation:** A simple LLM agent would often struggle or fail to reliably load, parse, and validate large CSV files. It might hallucinate file paths or fail to correctly handle data types.\n",
        "* **Orchestrator/Hybrid Power:** Your architecture **delegates** the data handling task to a **specialized, reliable, non-LLM utility** (Python/Pandas). This is the hallmark of a powerful systemâ€”it recognizes that not every task needs an LLM; many are better handled by traditional, high-performance code.\n",
        "\n",
        "The `data_ingestion` utility acts as the reliable **Input Gate** for the entire analytic **Data Pipeline**, ensuring the subsequent, more complex steps have clean, validated data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2zGrNyW6Dut_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for data loading utilities"
      ],
      "metadata": {
        "id": "1U1BUwRDDVhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for data loading utilities\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pathlib import Path\n",
        "from tools.data_loading import (\n",
        "    load_customers_csv,\n",
        "    load_transactions_csv,\n",
        "    load_product_catalog_csv\n",
        ")\n",
        "\n",
        "\n",
        "def test_load_customers_csv():\n",
        "    \"\"\"Test loading customers CSV\"\"\"\n",
        "    customers = load_customers_csv(\"data/customers.csv\")\n",
        "\n",
        "    assert len(customers) > 0\n",
        "    assert isinstance(customers, list)\n",
        "    assert isinstance(customers[0], dict)\n",
        "    assert \"Customer_ID\" in customers[0]\n",
        "    assert \"Age_Group\" in customers[0]\n",
        "    assert \"Location_Tier\" in customers[0]\n",
        "    assert \"Acquisition_Channel\" in customers[0]\n",
        "\n",
        "\n",
        "def test_load_customers_csv_has_expected_count():\n",
        "    \"\"\"Test customers CSV has expected number of records\"\"\"\n",
        "    customers = load_customers_csv(\"data/customers.csv\")\n",
        "\n",
        "    # Should have 200 customers (C001-C200)\n",
        "    assert len(customers) == 200\n",
        "    assert customers[0][\"Customer_ID\"] == \"C001\"\n",
        "    assert customers[-1][\"Customer_ID\"] == \"C200\"\n",
        "\n",
        "\n",
        "def test_load_transactions_csv():\n",
        "    \"\"\"Test loading transactions CSV\"\"\"\n",
        "    transactions = load_transactions_csv(\"data/transactions.csv\")\n",
        "\n",
        "    assert len(transactions) > 0\n",
        "    assert isinstance(transactions, list)\n",
        "    assert isinstance(transactions[0], dict)\n",
        "    assert \"Transaction_ID\" in transactions[0]\n",
        "    assert \"Customer_ID\" in transactions[0]\n",
        "    assert \"Product_ID\" in transactions[0]\n",
        "    assert \"Transaction_Date\" in transactions[0]\n",
        "    assert \"Usage_Metric\" in transactions[0]\n",
        "\n",
        "\n",
        "def test_load_transactions_csv_date_parsing():\n",
        "    \"\"\"Test transaction dates are parsed correctly\"\"\"\n",
        "    transactions = load_transactions_csv(\"data/transactions.csv\")\n",
        "\n",
        "    # Check that Transaction_Date is a datetime object (pandas Timestamp)\n",
        "    first_transaction = transactions[0]\n",
        "    date_value = first_transaction[\"Transaction_Date\"]\n",
        "\n",
        "    # Should be a pandas Timestamp (or datetime-like)\n",
        "    assert hasattr(date_value, 'year') or isinstance(date_value, str)\n",
        "\n",
        "\n",
        "def test_load_product_catalog_csv():\n",
        "    \"\"\"Test loading product catalog CSV\"\"\"\n",
        "    products = load_product_catalog_csv(\"data/product_catalog.csv\")\n",
        "\n",
        "    assert len(products) > 0\n",
        "    assert isinstance(products, list)\n",
        "    assert isinstance(products[0], dict)\n",
        "    assert \"Product_ID\" in products[0]\n",
        "    assert \"Product_Type\" in products[0]\n",
        "    assert \"Feature_Set\" in products[0]\n",
        "    assert \"Monetization_Model\" in products[0]\n",
        "\n",
        "\n",
        "def test_load_product_catalog_csv_has_all_products():\n",
        "    \"\"\"Test product catalog has all 20 products\"\"\"\n",
        "    products = load_product_catalog_csv(\"data/product_catalog.csv\")\n",
        "\n",
        "    # Should have 20 products (P01-P20)\n",
        "    assert len(products) == 20\n",
        "    assert products[0][\"Product_ID\"] == \"P01\"\n",
        "    assert products[-1][\"Product_ID\"] == \"P20\"\n",
        "\n",
        "\n",
        "def test_load_customers_csv_file_not_found():\n",
        "    \"\"\"Test error handling for missing file\"\"\"\n",
        "    with pytest.raises(FileNotFoundError):\n",
        "        load_customers_csv(\"data/nonexistent.csv\")\n",
        "\n",
        "\n",
        "def test_load_transactions_csv_file_not_found():\n",
        "    \"\"\"Test error handling for missing file\"\"\"\n",
        "    with pytest.raises(FileNotFoundError):\n",
        "        load_transactions_csv(\"data/nonexistent.csv\")\n",
        "\n",
        "\n",
        "def test_load_product_catalog_csv_file_not_found():\n",
        "    \"\"\"Test error handling for missing file\"\"\"\n",
        "    with pytest.raises(FileNotFoundError):\n",
        "        load_product_catalog_csv(\"data/nonexistent.csv\")\n",
        "\n",
        "\n",
        "def test_load_customers_csv_default_path():\n",
        "    \"\"\"Test default path works\"\"\"\n",
        "    customers = load_customers_csv()\n",
        "\n",
        "    assert len(customers) > 0\n",
        "    assert customers[0][\"Customer_ID\"] == \"C001\"\n",
        "\n",
        "\n",
        "def test_load_transactions_csv_default_path():\n",
        "    \"\"\"Test default path works\"\"\"\n",
        "    transactions = load_transactions_csv()\n",
        "\n",
        "    assert len(transactions) > 0\n",
        "\n",
        "\n",
        "def test_load_product_catalog_csv_default_path():\n",
        "    \"\"\"Test default path works\"\"\"\n",
        "    products = load_product_catalog_csv()\n",
        "\n",
        "    assert len(products) > 0\n",
        "    assert products[0][\"Product_ID\"] == \"P01\"\n",
        "\n"
      ],
      "metadata": {
        "id": "TFw23-ocDTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "TXOoFjBgD_vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_data_loading.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 12 items\n",
        "\n",
        "tests/test_data_loading.py::test_load_customers_csv PASSED                                                                            [  8%]\n",
        "tests/test_data_loading.py::test_load_customers_csv_has_expected_count PASSED                                                         [ 16%]\n",
        "tests/test_data_loading.py::test_load_transactions_csv PASSED                                                                         [ 25%]\n",
        "tests/test_data_loading.py::test_load_transactions_csv_date_parsing PASSED                                                            [ 33%]\n",
        "tests/test_data_loading.py::test_load_product_catalog_csv PASSED                                                                      [ 41%]\n",
        "tests/test_data_loading.py::test_load_product_catalog_csv_has_all_products PASSED                                                     [ 50%]\n",
        "tests/test_data_loading.py::test_load_customers_csv_file_not_found PASSED                                                             [ 58%]\n",
        "tests/test_data_loading.py::test_load_transactions_csv_file_not_found PASSED                                                          [ 66%]\n",
        "tests/test_data_loading.py::test_load_product_catalog_csv_file_not_found PASSED                                                       [ 75%]\n",
        "tests/test_data_loading.py::test_load_customers_csv_default_path PASSED                                                               [ 83%]\n",
        "tests/test_data_loading.py::test_load_transactions_csv_default_path PASSED                                                            [ 91%]\n",
        "tests/test_data_loading.py::test_load_product_catalog_csv_default_path PASSED                                                         [100%]\n",
        "\n",
        "============================================================ 12 passed in 0.41s =============================================================\n",
        "\n"
      ],
      "metadata": {
        "id": "gg0hTd9MD7Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All 12 utility tests passed. Adding tests for the data ingestion node:\n",
        "\n",
        "\n",
        "## Phase 2: Data Loading â€” complete\n",
        "\n",
        "### What we built\n",
        "\n",
        "1. Data loading utilities (`tools/data_loading.py`)\n",
        "   - `load_customers_csv()` â€” loads customers.csv\n",
        "   - `load_transactions_csv()` â€” loads transactions.csv (with date parsing)\n",
        "   - `load_product_catalog_csv()` â€” loads product_catalog.csv\n",
        "   - All 12 utility tests passing\n",
        "\n",
        "2. Data ingestion node (`data_ingestion_node`)\n",
        "   - Orchestrates all three loading utilities\n",
        "   - Uses config defaults or state file paths\n",
        "   - Error handling for missing files\n",
        "   - Populates `raw_customers`, `raw_transactions`, `raw_products`\n",
        "\n",
        "3. Node tests (`tests/test_nodes_phase2.py`)\n",
        "   - 8 tests covering structure, error handling, and integration\n",
        "\n",
        "### Architecture pattern\n",
        "\n",
        "- Utilities first, then node\n",
        "- Utilities are independently testable\n",
        "- Node is thin (orchestrates utilities)\n",
        "- Error accumulation pattern maintained\n",
        "\n"
      ],
      "metadata": {
        "id": "5cFqlFa2EgTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion Node"
      ],
      "metadata": {
        "id": "WBroe1GZE7e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_ingestion_node(state: ProductCustomerFitState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Data Ingestion Node: Orchestrate loading customer, transaction, and product data.\n",
        "\n",
        "    Loads raw data from CSV files using data loading utilities.\n",
        "\n",
        "    Args:\n",
        "        state: Current orchestrator state\n",
        "\n",
        "    Returns:\n",
        "        Updated state with raw_customers, raw_transactions, raw_products\n",
        "    \"\"\"\n",
        "    errors = state.get(\"errors\", [])\n",
        "\n",
        "    # Get file paths from state or use defaults from config\n",
        "    config = ProductCustomerFitConfig()\n",
        "    customers_file = state.get(\"customers_file\") or config.customers_file\n",
        "    transactions_file = state.get(\"transactions_file\") or config.transactions_file\n",
        "    products_file = state.get(\"products_file\") or config.products_file\n",
        "\n",
        "    try:\n",
        "        # Load all data files\n",
        "        raw_customers = load_customers_csv(customers_file)\n",
        "        raw_transactions = load_transactions_csv(transactions_file)\n",
        "        raw_products = load_product_catalog_csv(products_file)\n",
        "\n",
        "        return {\n",
        "            \"raw_customers\": raw_customers,\n",
        "            \"raw_transactions\": raw_transactions,\n",
        "            \"raw_products\": raw_products,\n",
        "            \"errors\": errors\n",
        "        }\n",
        "    except FileNotFoundError as e:\n",
        "        return {\n",
        "            \"errors\": errors + [f\"data_ingestion_node: File not found - {str(e)}\"]\n",
        "        }\n",
        "    except ValueError as e:\n",
        "        return {\n",
        "            \"errors\": errors + [f\"data_ingestion_node: Invalid data - {str(e)}\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"errors\": errors + [f\"data_ingestion_node: Unexpected error - {str(e)}\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "hhhUsf4dE6VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the **`data_ingestion_node`**, the executive function for the first step of your DAG. It brings together the structure of the `planning_node` (the step itself) and the robustness of the utility functions (the execution logic).\n",
        "\n",
        "This node is a perfect example of what a **proper orchestration layer** should do.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Configuration and Fault Tolerance\n",
        "\n",
        "The primary job of this node is **flow control and error management**. It showcases best practices for building an agent system that is flexible and designed not to crash, but to fail gracefully.\n",
        "\n",
        "### ðŸŽ¯ What to Focus On\n",
        "\n",
        "1.  **Configuration Management and Flexibility:**\n",
        "    * **Focus:** The use of both `state.get(\"file_path\")` and `config.file_path` allows for **runtime overrides**. This means you can run the same agent workflow on different datasets simply by passing new file paths in the initial `state` without having to change the default configuration file. This is crucial for building a **reusable and testable** agent.\n",
        "\n",
        "2.  **Orchestration of Specialized Workers (Decoupling):**\n",
        "    * **Focus:** The body of the `try` block consists of three simple function calls. The `data_ingestion_node` does not contain any data reading logic itself. Its role is strictly to **orchestrate** the execution of the specialized utility functions (the \"workers\").\n",
        "    * **The Power:** This **Decoupling** makes the system incredibly clean and maintainable. If you need to change how data is read (e.g., switch from CSV to a database query), you only modify the utility function, not the main workflow node.\n",
        "\n",
        "3.  **Structured Error Reporting (Fault Tolerance):**\n",
        "    * **Focus:** The `try...except` block is the core of the node's intelligence. It catches all foreseeable errors (`FileNotFoundError`, `ValueError`, `Exception`).\n",
        "    * **The Power:** Instead of terminating the program, the node **captures the error details** and appends them to the **`errors` list in the state**. This means the workflow doesn't just fail; it records *why* it failed and maintains a clean state. Later nodes (like a **Reporting Agent**) could be designed to inspect the `errors` list and generate a failure report automaticallyâ€”a hallmark of **sophisticated fault tolerance**.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Graceful Failure and Debugging\n",
        "\n",
        "A simple agent fails silently or crashes when an input file is missing. Your orchestrator does this:\n",
        "\n",
        "1.  **Delegates Failure:** It delegates the I/O work to a robust utility.\n",
        "2.  **Catches Failure:** It wraps the call in a `try/except`.\n",
        "3.  **Records Failure:** It appends the specific, descriptive error message to the shared `state`.\n",
        "4.  **Terminates Gracefully:** It returns the updated state, allowing the orchestration engine to stop the workflow in a controlled manner and use the error message for instant debugging or reporting.\n",
        "\n",
        "This pattern demonstrates the necessary rigor to move from a working script to a **production-ready, reliable, self-monitoring agent system.**"
      ],
      "metadata": {
        "id": "j1O8-VuYHspL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for Phase 2: Data Ingestion Node"
      ],
      "metadata": {
        "id": "qX2y2R9eHSUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for Phase 2: Data Ingestion Node\"\"\"\n",
        "\n",
        "import pytest\n",
        "from agents.product_customer_fit.nodes import data_ingestion_node\n",
        "from config import ProductCustomerFitState\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_loads_all_data():\n",
        "    \"\"\"Test data ingestion node loads all three data sources\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "\n",
        "    assert \"raw_customers\" in result\n",
        "    assert \"raw_transactions\" in result\n",
        "    assert \"raw_products\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_customers_structure():\n",
        "    \"\"\"Test raw_customers has correct structure\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "    customers = result[\"raw_customers\"]\n",
        "\n",
        "    assert len(customers) > 0\n",
        "    assert isinstance(customers, list)\n",
        "    assert \"Customer_ID\" in customers[0]\n",
        "    assert \"Age_Group\" in customers[0]\n",
        "    assert customers[0][\"Customer_ID\"] == \"C001\"\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_transactions_structure():\n",
        "    \"\"\"Test raw_transactions has correct structure\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "    transactions = result[\"raw_transactions\"]\n",
        "\n",
        "    assert len(transactions) > 0\n",
        "    assert isinstance(transactions, list)\n",
        "    assert \"Transaction_ID\" in transactions[0]\n",
        "    assert \"Customer_ID\" in transactions[0]\n",
        "    assert \"Product_ID\" in transactions[0]\n",
        "    assert \"Transaction_Date\" in transactions[0]\n",
        "    assert \"Usage_Metric\" in transactions[0]\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_products_structure():\n",
        "    \"\"\"Test raw_products has correct structure\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "    products = result[\"raw_products\"]\n",
        "\n",
        "    assert len(products) > 0\n",
        "    assert isinstance(products, list)\n",
        "    assert \"Product_ID\" in products[0]\n",
        "    assert \"Product_Type\" in products[0]\n",
        "    assert \"Feature_Set\" in products[0]\n",
        "    assert \"Monetization_Model\" in products[0]\n",
        "    assert products[0][\"Product_ID\"] == \"P01\"\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_uses_custom_paths():\n",
        "    \"\"\"Test data ingestion node uses custom file paths from state\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"customers_file\": \"data/customers.csv\",\n",
        "        \"transactions_file\": \"data/transactions.csv\",\n",
        "        \"products_file\": \"data/product_catalog.csv\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "\n",
        "    assert \"raw_customers\" in result\n",
        "    assert \"raw_transactions\" in result\n",
        "    assert \"raw_products\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_handles_missing_file():\n",
        "    \"\"\"Test data ingestion node handles missing file gracefully\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"customers_file\": \"data/nonexistent.csv\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "\n",
        "    assert \"raw_customers\" not in result\n",
        "    assert \"errors\" in result\n",
        "    assert len(result[\"errors\"]) > 0\n",
        "    assert \"File not found\" in result[\"errors\"][0]\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_preserves_errors():\n",
        "    \"\"\"Test data ingestion node preserves existing errors\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": [\"existing_error\"]\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "\n",
        "    # Should have existing error plus any new ones (or just existing if successful)\n",
        "    assert \"errors\" in result\n",
        "    assert \"existing_error\" in result[\"errors\"]\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_with_goal_and_planning():\n",
        "    \"\"\"Test data ingestion node works after goal and planning nodes\"\"\"\n",
        "    from agents.product_customer_fit.nodes import goal_node, planning_node\n",
        "\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run goal and planning first\n",
        "    state = goal_node(state)\n",
        "    state = planning_node(state)\n",
        "\n",
        "    # Then data ingestion\n",
        "    state = data_ingestion_node(state)\n",
        "\n",
        "    assert \"raw_customers\" in state\n",
        "    assert \"raw_transactions\" in state\n",
        "    assert \"raw_products\" in state\n",
        "    assert \"goal\" in state\n",
        "    assert \"plan\" in state\n",
        "    assert len(state.get(\"errors\", [])) == 0\n",
        "\n",
        "\n",
        "def test_data_ingestion_node_data_counts():\n",
        "    \"\"\"Test data ingestion loads expected number of records\"\"\"\n",
        "    state: ProductCustomerFitState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_ingestion_node(state)\n",
        "\n",
        "    # Verify expected counts\n",
        "    assert len(result[\"raw_customers\"]) == 200\n",
        "    assert len(result[\"raw_products\"]) == 20\n",
        "    # Transactions should be > 1800 (we added 9 for P20)\n",
        "    assert len(result[\"raw_transactions\"]) >= 1800\n",
        "\n"
      ],
      "metadata": {
        "id": "ztoZwP87HQSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "a8bpB_EeImKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_nodes_phase2.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 9 items\n",
        "\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_loads_all_data PASSED                                                            [ 11%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_customers_structure PASSED                                                       [ 22%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_transactions_structure PASSED                                                    [ 33%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_products_structure PASSED                                                        [ 44%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_uses_custom_paths PASSED                                                         [ 55%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_handles_missing_file PASSED                                                      [ 66%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_preserves_errors PASSED                                                          [ 77%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_with_goal_and_planning PASSED                                                    [ 88%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_data_counts PASSED                                                               [100%]\n",
        "\n",
        "============================================================= 9 passed in 0.35s =============================================================\n"
      ],
      "metadata": {
        "id": "xg9BelzzIjGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Phase 2 complete â€” all tests passing\n",
        "\n",
        "### Summary\n",
        "\n",
        "**Phase 1: Foundation**\n",
        "- Goal Node âœ“\n",
        "- Planning Node âœ“\n",
        "- 7/7 tests passing\n",
        "\n",
        "**Phase 2: Data Loading**\n",
        "- Data Loading Utilities (3 functions) âœ“\n",
        "- Data Ingestion Node âœ“\n",
        "- 12 utility tests + 9 node tests = 21/21 passing\n",
        "\n",
        "**Total Progress:** 28/28 tests passing across both phases\n",
        "\n",
        "---\n",
        "\n",
        "### What we've built\n",
        "\n",
        "1. **Architecture pattern established:**\n",
        "   - Utilities first, then nodes\n",
        "   - Thin nodes (orchestration only)\n",
        "   - State enrichment pattern\n",
        "   - Error accumulation\n",
        "\n",
        "2. **Working workflow:**\n",
        "   - Goal â†’ Planning â†’ Data Ingestion\n",
        "   - All nodes tested independently\n",
        "   - Integration tests passing\n",
        "\n",
        "3. **Data loaded:**\n",
        "   - 200 customers\n",
        "   - 1,824 transactions\n",
        "   - 20 products\n",
        "   - Ready for preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Phase 3 â€” Data Preprocessing\n",
        "\n",
        "This phase will:\n",
        "1. Parse Feature_Set strings into lists\n",
        "2. Normalize usage metrics\n",
        "3. Build graph structures (NetworkX)\n",
        "4. Create derived features\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkQYlz48I0M5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Dp3Z8IWI3uR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}