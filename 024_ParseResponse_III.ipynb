{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMm3a5Dl11rfk8AdrLAjexT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/024_ParseResponse_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🧠 Planning + Parsing\n",
        "\n",
        "### 1. **LLMs don't execute — they plan.**\n",
        "\n",
        "* The LLM returns a **structured action plan**, not code or results.\n",
        "* Your Python code interprets and executes those steps.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Structured outputs are key to reliability.**\n",
        "\n",
        "* Markdown code blocks (like ` ```action `) and strict JSON formats make parsing easy.\n",
        "* Avoiding prose ensures your agent can work **without guesswork**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **You are building a “tool-using brain.”**\n",
        "\n",
        "* LLMs decide which tools to use, and with what args.\n",
        "* Your code is the executor — like giving the brain a body.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Few-shot examples dramatically improve results.**\n",
        "\n",
        "* Showing the LLM *how* to respond increases consistency.\n",
        "* Define your tool schema + show examples in your system prompt.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Agents can scale up quickly — and costs can too.**\n",
        "\n",
        "* One LLM call per plan = affordable.\n",
        "* One LLM call per file = 🧨 60K tokens/day if you’re not careful.\n",
        "* Plan your architecture with batching, reuse, and simplicity in mind.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Simulated tools are the best way to learn.**\n",
        "\n",
        "* You don’t need real summaries or file moves at first.\n",
        "* Simulate → debug → validate → then implement.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Db9YM0rnilj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧠 **What’s the next level of parsing?**\n",
        "\n",
        "### ➕ Add complexity to the return structure:\n",
        "\n",
        "* Multiple actions\n",
        "* Conditional steps\n",
        "* Multi-tool reasoning\n",
        "\n",
        "This simulates real-world agent orchestration.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔥 Recommended Next Step:\n",
        "\n",
        "# **🧩 Multi-Tool Planner Agent**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Agent Goal:\n",
        "\n",
        "You give a natural language task like:\n",
        "\n",
        "> “Summarize each file in docs\\_folder and then move it into a topic folder.”\n",
        "\n",
        "The LLM returns **a list of structured steps**, like:\n",
        "\n",
        "````markdown\n",
        "```action\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"summarize_file\",\n",
        "    \"args\": { \"filename\": \"lecture_01.txt\" }\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"move_file\",\n",
        "    \"args\": {\n",
        "      \"filename\": \"lecture_01.txt\",\n",
        "      \"target_folder\": \"Memory_Management\"\n",
        "    }\n",
        "  }\n",
        "]\n",
        "````\n",
        "\n",
        "\n",
        "\n",
        "### ✅ What you’ll build:\n",
        "- LLM returns a list of actions (instead of one)\n",
        "- You parse it with `json.loads()` into a **list of dicts**\n",
        "- You loop through each step and execute the corresponding tool (summarize, move, rename, tag…)\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 What this teaches you:\n",
        "\n",
        "| Concept | Why it matters |\n",
        "|--------|----------------|\n",
        "| ✅ Multi-step reasoning | Required for real planning agents |\n",
        "| ✅ Tool orchestration | Foundation of LangChain / OpenAgents style design |\n",
        "| ✅ Dynamic routing | Agent decides what function to run |\n",
        "| ✅ Multi-action parsing | Real-world LLMs often return batches of actions |\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Tools You'll Simulate\n",
        "\n",
        "| Tool | Args |\n",
        "|------|------|\n",
        "| `summarize_file` | `filename: str` |\n",
        "| `move_file` | `filename: str`, `target_folder: str` |\n",
        "| `tag_file` (optional) | `filename: str`, `tag: str` |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m-NnvLK-Sswa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO0cRXcqNPYR",
        "outputId": "1966f5c4-cc41-48fc-d007-dac1233a2f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/765.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m757.8/765.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Coded Actions"
      ],
      "metadata": {
        "id": "4moByI4o14L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "source_dir = \"/content/docs_folder\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    raise FileNotFoundError(f\"📁 Directory not found: {source_dir}\")\n",
        "\n",
        "# List and build full file paths\n",
        "file_list = [\n",
        "    os.path.join(source_dir, f)\n",
        "    for f in os.listdir(source_dir)\n",
        "    if os.path.isfile(os.path.join(source_dir, f))\n",
        "]\n",
        "\n",
        "# Display the found files\n",
        "print(\"📂 Files found:\")\n",
        "for file in file_list:\n",
        "    print(\"  -\", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylHl6sdMNUGv",
        "outputId": "dc968f19-76bf-4fbd-8d74-5491f1ed09fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Files found:\n",
            "  - /content/docs_folder/001_PArse_the Response.txt\n",
            "  - /content/docs_folder/006_Agent Loop with Function Calling.txt\n",
            "  - /content/docs_folder/000_Prompting for Agents -GAIL.txt\n",
            "  - /content/docs_folder/002_Execute_the_Action.txt\n",
            "  - /content/docs_folder/005_Using Function Calling Capabilities with LLMs.txt\n",
            "  - /content/docs_folder/003_gent Feedback and Memory.txt\n",
            "  - /content/docs_folder/004_AGENT_Tools.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Coded Actions\n"
      ],
      "metadata": {
        "id": "eLNp2JpZTWTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the multi action prompt\n",
        "def build_multi_action_prompt(file_list):\n",
        "    file_names = [os.path.basename(f) for f in file_list]\n",
        "    file_list_str = \"\\n\".join(f\"- {name}\" for name in file_names)\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an agent planner that creates a list of tool-based actions.\\n\"\n",
        "        \"You always respond with a JSON list of tool invocations inside a markdown code block, like this:\\n\\n\"\n",
        "        \"```action\\n\"\n",
        "        \"[\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"tool_name\\\": \\\"summarize_file\\\",\\n\"\n",
        "        \"    \\\"args\\\": { \\\"filename\\\": \\\"lecture_01.txt\\\" }\\n\"\n",
        "        \"  },\\n\"\n",
        "        \"  {\\n\"\n",
        "        \"    \\\"tool_name\\\": \\\"move_file\\\",\\n\"\n",
        "        \"    \\\"args\\\": { \\\"filename\\\": \\\"lecture_01.txt\\\", \\\"target_folder\\\": \\\"Memory_Management\\\" }\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"]\\n\"\n",
        "        \"```\\n\\n\"\n",
        "        \"Available tools:\\n\"\n",
        "        \"- summarize_file: takes {\\\"filename\\\": str}\\n\"\n",
        "        \"- move_file: takes {\\\"filename\\\": str, \\\"target_folder\\\": str}\"\n",
        "    )\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"I want to summarize and organize the following files:\\n\\n{file_list_str}\\n\\n\"\n",
        "        \"Please create an ordered list of tool actions that I should perform.\"\n",
        "    )\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=700\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "messages = build_multi_action_prompt(file_list)\n",
        "llm_output = generate_response(messages)\n",
        "\n",
        "print(\"🤖 LLM Response:\\n\")\n",
        "print(llm_output)\n",
        "\n",
        "\n",
        "# 📦 Parse the Markdown Block\n",
        "def extract_markdown_block(text: str, tag: str = \"action\") -> str:\n",
        "    pattern = rf\"```{tag}\\s*(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        raise ValueError(f\"Missing markdown block with tag '{tag}'\")\n",
        "\n",
        "# ✅ Load it into a Python list\n",
        "def parse_multi_action_plan(llm_response):\n",
        "    try:\n",
        "        json_block = extract_markdown_block(llm_response, tag=\"action\")\n",
        "        return json.loads(json_block)\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to parse multi-action plan:\", e)\n",
        "        return []\n",
        "\n",
        "# Agent Executes Each Action\n",
        "def simulate_tool(tool_name, args):\n",
        "    if tool_name == \"summarize_file\":\n",
        "        print(f\"📝 Summarizing: {args['filename']}\")\n",
        "    elif tool_name == \"move_file\":\n",
        "        print(f\"📂 Moving '{args['filename']}' → '{args['target_folder']}'\")\n",
        "    else:\n",
        "        print(f\"⚠️ Unknown tool: {tool_name}\")\n",
        "\n",
        "plan = parse_multi_action_plan(generate_response(build_multi_action_prompt(file_list)))\n",
        "\n",
        "print(\"\\n🛠️ Executing agent plan...\\n\")\n",
        "for step in plan:\n",
        "    simulate_tool(step[\"tool_name\"], step[\"args\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GWUAzlWNUDg",
        "outputId": "7921009b-09f7-48cc-e593-a78b9a53c40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 LLM Response:\n",
            "\n",
            "```action\n",
            "[\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"000_Prompting for Agents -GAIL.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"001_PArse_the Response.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"002_Execute_the_Action.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"003_gent Feedback and Memory.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"004_AGENT_Tools.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"005_Using Function Calling Capabilities with LLMs.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"summarize_file\",\n",
            "    \"args\": { \"filename\": \"006_Agent Loop with Function Calling.txt\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"000_Prompting for Agents -GAIL.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"001_PArse_the Response.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"002_Execute_the_Action.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"003_gent Feedback and Memory.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"004_AGENT_Tools.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"005_Using Function Calling Capabilities with LLMs.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  },\n",
            "  {\n",
            "    \"tool_name\": \"move_file\",\n",
            "    \"args\": { \"filename\": \"006_Agent Loop with Function Calling.txt\", \"target_folder\": \"Organized_Summaries\" }\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "🛠️ Executing agent plan...\n",
            "\n",
            "📝 Summarizing: 000_Prompting for Agents -GAIL.txt\n",
            "📝 Summarizing: 001_PArse_the Response.txt\n",
            "📝 Summarizing: 002_Execute_the_Action.txt\n",
            "📝 Summarizing: 003_gent Feedback and Memory.txt\n",
            "📝 Summarizing: 004_AGENT_Tools.txt\n",
            "📝 Summarizing: 005_Using Function Calling Capabilities with LLMs.txt\n",
            "📝 Summarizing: 006_Agent Loop with Function Calling.txt\n",
            "📂 Moving '000_Prompting for Agents -GAIL.txt' → 'Summaries'\n",
            "📂 Moving '001_PArse_the Response.txt' → 'Summaries'\n",
            "📂 Moving '002_Execute_the_Action.txt' → 'Summaries'\n",
            "📂 Moving '003_gent Feedback and Memory.txt' → 'Summaries'\n",
            "📂 Moving '004_AGENT_Tools.txt' → 'Summaries'\n",
            "📂 Moving '005_Using Function Calling Capabilities with LLMs.txt' → 'Summaries'\n",
            "📂 Moving '006_Agent Loop with Function Calling.txt' → 'Summaries'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧩 Why Is the Output Inside Triple Backticks?\n",
        "\n",
        "This:\n",
        "\n",
        "````python\n",
        "\"```action\\n\"\n",
        "\"[\\n\"\n",
        "\"  {...},\\n\"\n",
        "\"  {...}\\n\"\n",
        "\"]\\n\"\n",
        "\"```\\n\"\n",
        "````\n",
        "\n",
        "...is a **Markdown code block**, and it's used **on purpose** to:\n",
        "\n",
        "### ✅ 1. Make the Output **Easy to Parse**\n",
        "\n",
        "By putting the JSON inside:\n",
        "\n",
        "````markdown\n",
        "```action\n",
        "[\n",
        "  { \"tool_name\": ..., \"args\": {...} },\n",
        "  ...\n",
        "]\n",
        "```\n",
        "````\n",
        "\n",
        "…we can use **regex** to reliably extract it later:\n",
        "\n",
        "````python\n",
        "pattern = r\"```action\\s*(.*?)```\"\n",
        "````\n",
        "\n",
        "This ensures we can **locate and isolate** the JSON from other surrounding LLM content (like extra commentary, formatting, etc).\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. Prevent the LLM from Adding Confusing Output\n",
        "\n",
        "LLMs sometimes add:\n",
        "\n",
        "* Explanations: \"Here’s your requested JSON:\"\n",
        "* Markdown formatting\n",
        "* Apologies or extra text\n",
        "\n",
        "Putting the plan in a ` ```action ` block tells the LLM:\n",
        "\n",
        "> “Only include tool instructions in *this* block. Everything else goes outside.”\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3. Allow Multiple Blocks in One Output\n",
        "\n",
        "You might later use:\n",
        "\n",
        "* ` ```query ` for a search\n",
        "* ` ```plan ` for high-level strategy\n",
        "* ` ```action ` for tool execution\n",
        "\n",
        "This lets you extract **only the part you care about**, using tags.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Why the Word `\"action\"`?\n",
        "\n",
        "It's a **custom tag**.\n",
        "\n",
        "You could use any tag (like `\"json\"`, `\"plan\"`, `\"result\"`), but by calling it `\"action\"`, you're signaling:\n",
        "\n",
        "> “This block contains a list of tool-based steps the agent should take.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary\n",
        "\n",
        "| Purpose                          | Benefit                              |\n",
        "| -------------------------------- | ------------------------------------ |\n",
        "| ✅ Wrap in \\`\\`\\`action           | Lets you extract only the tool plan  |\n",
        "| 🧠 Use markdown code block       | Keeps JSON clean and separate        |\n",
        "| 🧹 Parse cleanly with regex      | No guessing or extra formatting      |\n",
        "| 🧱 Supports multi-block messages | Easy to separate structured sections |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aD40CaKcccEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The ` ```action ` block acts like a **labelled container** or **named index**, so that later in your agent pipeline you can say:\n",
        "\n",
        "> “Hey, go find the part of the message that contains the **action plan** — and only that part.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🔖 Analogy\n",
        "\n",
        "Think of the LLM’s output as a full document. Inside it, you're marking sections like this:\n",
        "\n",
        "````\n",
        "Here’s what I want to do:\n",
        "\n",
        "```action\n",
        "[ { \"tool_name\": ..., \"args\": {...} } ]\n",
        "````\n",
        "\n",
        "Here’s why I’m doing it:\n",
        "\n",
        "```reasoning\n",
        "I want to group similar files together to help the user find them faster.\n",
        "```\n",
        "\n",
        "````\n",
        "\n",
        "Now you can write extractors like:\n",
        "\n",
        "```python\n",
        "extract_markdown_block(response, tag=\"action\")\n",
        "extract_markdown_block(response, tag=\"reasoning\")\n",
        "````\n",
        "\n",
        "Each tag gives you **structured access** to different parts of the LLM’s reasoning or output.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Benefits\n",
        "\n",
        "| Concept                                    | Benefit                                               |\n",
        "| ------------------------------------------ | ----------------------------------------------------- |\n",
        "| 🔖 Tags like `action`, `reasoning`, `plan` | Create a schema for multi-step agent logic            |\n",
        "| 🧼 Markdown code block                     | Keeps JSON clean and parseable                        |\n",
        "| 📍 Acts like a labeled container           | You can always find it later with a regex             |\n",
        "| 🧠 Reliable LLM parsing                    | Avoids accidental formatting errors, stray text, etc. |\n",
        "\n"
      ],
      "metadata": {
        "id": "qyw0DoYNc2w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧰 `\"Available tools\"` Section — Why It's Included\n",
        "\n",
        "This part:\n",
        "\n",
        "```python\n",
        "\"Available tools:\\n\"\n",
        "\"- summarize_file: takes {\\\"filename\\\": str}\\n\"\n",
        "\"- move_file: takes {\\\"filename\\\": str, \\\"target_folder\\\": str}\"\n",
        "```\n",
        "\n",
        "...is a **reference guide** for the LLM. You're giving it a **tool schema** — like API documentation — so it knows:\n",
        "\n",
        "* What tools it can choose from\n",
        "* What arguments are required for each tool\n",
        "* The correct structure and spelling for the tool names and arguments\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Why Does the LLM Need This?\n",
        "\n",
        "LLMs are good at language but **don’t know what tools your agent supports** unless you tell them.\n",
        "\n",
        "So this section does two things:\n",
        "\n",
        "| Purpose                         | How                                                         |\n",
        "| ------------------------------- | ----------------------------------------------------------- |\n",
        "| 🛠️ Defines the “menu” of tools | Gives the names and argument shapes                         |\n",
        "| 🔐 Prevents hallucination       | Keeps LLM from inventing tools like `\"summarize_and_email\"` |\n",
        "\n",
        "You’re saying:\n",
        "\n",
        "> “Hey LLM, you can ONLY use `summarize_file` and `move_file`, and here’s exactly how they work.”\n",
        "\n",
        "This guides the LLM to produce output like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"move_file\",\n",
        "  \"args\": {\n",
        "    \"filename\": \"001.txt\",\n",
        "    \"target_folder\": \"Summaries\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 It's Like an API Spec\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "* `\"Available tools\"` = **OpenAPI schema**\n",
        "* `\"tool_name\"` + `\"args\"` = **API call**\n",
        "* LLM = **caller**\n",
        "* Your agent = **executor**\n",
        "\n",
        "So you're giving it a **safe, structured list** to work from.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Could You Leave It Out?\n",
        "\n",
        "Technically yes, but:\n",
        "\n",
        "* LLM might guess wrong tool names\n",
        "* Or add invalid arguments\n",
        "* Or format the JSON incorrectly\n",
        "\n",
        "💡 Including `\"Available tools\"` boosts **reliability** and **correct formatting** by a lot — especially when the tool list grows.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "| Element                  | Purpose                                  |\n",
        "| ------------------------ | ---------------------------------------- |\n",
        "| `\"action\"` tag           | Marks the output section to extract      |\n",
        "| `\"Available tools\"`      | Informs the LLM how to respond correctly |\n",
        "| `\"tool_name\"` + `\"args\"` | Defines a single action to take          |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zJcBA6kfduMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 🤖 **LLMs are powerful at language, but imprecise by default. Agents require precision.**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Why Parsing Is Hard Without Structure\n",
        "\n",
        "LLMs are trained to write like this:\n",
        "\n",
        "> \"Sure! To summarize `lecture_01.txt`, I suggest moving it to the folder `Summaries`. Let me know if you'd like help renaming it.\"\n",
        "\n",
        "That’s natural language — great for humans, **terrible for machines**.\n",
        "\n",
        "Now imagine trying to extract:\n",
        "\n",
        "* What file to summarize\n",
        "* What folder to move it to\n",
        "* What order those should happen in\n",
        "\n",
        "🧠 You’d need complex NLP parsing logic… and still risk breaking.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Why the Structured Format Fixes That\n",
        "\n",
        "By enforcing:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"move_file\",\n",
        "  \"args\": {\n",
        "    \"filename\": \"lecture_01.txt\",\n",
        "    \"target_folder\": \"Summaries\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "...inside a markdown block like:\n",
        "\n",
        "````\n",
        "```action\n",
        "[ ... ]\n",
        "````\n",
        "\n",
        "You’re telling the LLM:\n",
        "\n",
        "- “Don’t talk.”\n",
        "- “Don’t explain.”\n",
        "- “Just give me machine-readable instructions.”\n",
        "\n",
        "That’s **exactly** what enables reliable, automated parsing.\n",
        "\n",
        "---\n",
        "\n",
        "## 💥 Without the `\"Available tools\"` List...\n",
        "\n",
        "The LLM might return:\n",
        "- `\"moveFile\"` instead of `\"move_file\"`\n",
        "- `\"folder\": \"Summaries\"` instead of `\"target_folder\"`\n",
        "- Or invent a tool like `\"summarize_and_move\"`\n",
        "\n",
        "Parsing would fail, or your agent might break or do the wrong thing.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 So Yes, You're Spot On:\n",
        "\n",
        "| Design Element | Why It Matters |\n",
        "|----------------|----------------|\n",
        "| ✅ Parsing | Lets your agent *act* on the LLM’s output |\n",
        "| 🚫 English prose | Looks pretty, but not actionable |\n",
        "| ✅ Structured JSON in markdown | Cleanly extractable and reliable |\n",
        "| ✅ Explicit tool schema | Prevents LLM hallucinations or misuse |\n",
        "| 🚫 Leaving it to the LLM’s guess | Breaks reliability and makes agents flaky |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kGe-zDfjeTUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🧠 Step-by-Step: What Each Function Does\n",
        "\n",
        "---\n",
        "\n",
        "### 1. ✅ `extract_markdown_block(text, tag=\"action\")`\n",
        "\n",
        "This function:\n",
        "\n",
        "* Searches the LLM response for a **markdown block** like:\n",
        "\n",
        "  ````\n",
        "  ```action\n",
        "  { ... JSON ... }\n",
        "  ````\n",
        "\n",
        "  ```\n",
        "  ```\n",
        "* It extracts only the part between the backticks, using the `tag` (`action`) to target the correct block.\n",
        "\n",
        "🔍 **Why?** LLMs often wrap structured data in markdown so you can parse it out cleanly without extra fluff.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ✅ `parse_multi_action_plan(llm_response)`\n",
        "\n",
        "This function:\n",
        "\n",
        "* Uses `extract_markdown_block()` to isolate the structured block.\n",
        "* Then runs `json.loads()` to convert it from **JSON text → Python data structure** (a list of dicts).\n",
        "\n",
        "📥 Input:\n",
        "LLM output (as raw string)\n",
        "\n",
        "📤 Output:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"tool_name\": \"summarize_file\", \"args\": {\"filename\": \"file1.txt\"}},\n",
        "  {\"tool_name\": \"move_file\", \"args\": {\"filename\": \"file1.txt\", \"target_folder\": \"Summaries\"}}\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. ✅ `simulate_tool(tool_name, args)`\n",
        "\n",
        "This is your **agent's executor.**\n",
        "\n",
        "Right now it's just simulating real behavior:\n",
        "\n",
        "| Tool               | Simulated Action                                |\n",
        "| ------------------ | ----------------------------------------------- |\n",
        "| `\"summarize_file\"` | Prints `📝 Summarizing: filename.txt`           |\n",
        "| `\"move_file\"`      | Prints `📂 Moving 'filename' → 'target_folder'` |\n",
        "| unknown            | Prints warning: unknown tool                    |\n",
        "\n",
        "Later, you could replace these print statements with:\n",
        "\n",
        "* Real summarization using OpenAI\n",
        "* Real file moving via `shutil.move()`\n",
        "* Web scraping, emailing, database calls — anything!\n",
        "\n",
        "---\n",
        "\n",
        "### 4. ✅ Looping Through the Plan\n",
        "\n",
        "This is the **agent loop**:\n",
        "\n",
        "```python\n",
        "for step in plan:\n",
        "    simulate_tool(step[\"tool_name\"], step[\"args\"])\n",
        "```\n",
        "\n",
        "Each step in the plan is:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"tool_name\": \"move_file\",\n",
        "  \"args\": {\n",
        "    \"filename\": \"lecture_01.txt\",\n",
        "    \"target_folder\": \"Memory\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "So the loop says:\n",
        "\n",
        "> “Tool = `move_file`, args = `{filename: ..., target_folder: ...}` — now go run that.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Agent Logic Summary\n",
        "\n",
        "```python\n",
        "LLM ➜ markdown block ➜ parsed JSON ➜ tool name + args ➜ tool execution\n",
        "```\n",
        "\n",
        "The LLM **thinks**.\n",
        "Your Python code **does**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yr8XyX8bba7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents can get expensive surprisingly fast**, especially when:\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ LLM Agents Cost Risks\n",
        "\n",
        "| Risk                                     | What It Means                                                       |\n",
        "| ---------------------------------------- | ------------------------------------------------------------------- |\n",
        "| 🧾 **Long prompts**                      | Big file lists, long context, detailed instructions                 |\n",
        "| 📤 **Large outputs**                     | Structured responses (JSON, plans, chains) can be verbose           |\n",
        "| 🔁 **Multiple LLM calls per agent loop** | If your agent plans, checks, reasons, executes… that adds up fast   |\n",
        "| 🪜 **Recursive reasoning**               | Agents calling agents (e.g., one LLM interprets another's response) |\n",
        "| 🧠 **Stateful conversations**            | Chat history gets long = big context every time                     |\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Real Token Cost Breakdown (Estimates)\n",
        "\n",
        "| Action                                   | Token Cost                 |\n",
        "| ---------------------------------------- | -------------------------- |\n",
        "| 10 filenames listed in prompt            | \\~150–200 tokens           |\n",
        "| Structured plan per file                 | \\~40–100 tokens per action |\n",
        "| 10 files with 2 actions each             | \\~800–2000+ tokens         |\n",
        "| One agent loop with planning + execution | \\~2–4k tokens total        |\n",
        "| One detailed summarization per file      | 500–1500 tokens each       |\n",
        "\n",
        "So if you’re summarizing + organizing 10–20 files in a planner loop…\n",
        "👉 You're easily at **20,000–40,000 tokens** just for one session.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Key Insight\n",
        "\n",
        "> **Simple apps are cheap. Agents are powerful — but power has a price.**\n",
        "\n",
        "Agents bring more:\n",
        "\n",
        "* Flexibility\n",
        "* Reasoning\n",
        "* Control flow\n",
        "* Autonomy\n",
        "  …but those advantages come at the cost of **longer prompts**, **richer output**, and **multi-turn processing**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Cost Management Tips\n",
        "\n",
        "| Strategy                                          | Result                             |\n",
        "| ------------------------------------------------- | ---------------------------------- |\n",
        "| 🔹 Use `gpt-3.5-turbo` for general planning       | 90% of the time it's “good enough” |\n",
        "| 🔹 Log token usage per run (`usage.total_tokens`) | Monitor & alert on large jobs      |\n",
        "| 🔹 Save and reuse plans                           | Only re-call LLM if inputs change  |\n",
        "| 🔹 Use local code for filtering, clustering       | Let LLM do only what Python can't  |\n",
        "| 🔹 Use batching or a queueing system              | Avoid massive prompts in one go    |\n",
        "\n",
        "\n",
        "## 🤔 So Why Was It More Expensive?\n",
        "\n",
        "Likely because of:\n",
        "\n",
        "### 1. **Prompt Size**\n",
        "\n",
        "* If you have many files, the `file_list_str` is long.\n",
        "* More input tokens = higher cost.\n",
        "\n",
        "### 2. **LLM Output Size**\n",
        "\n",
        "* The model returns a JSON list of actions, which can be long if there are lots of files.\n",
        "* More output tokens = higher cost.\n",
        "\n",
        "### 3. **Model Choice**\n",
        "\n",
        "* Even `gpt-4o-mini` is more expensive than `gpt-3.5-turbo`, though it's faster and smarter.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Ways to Save on Cost\n",
        "\n",
        "| Strategy                     | How                                                                      |\n",
        "| ---------------------------- | ------------------------------------------------------------------------ |\n",
        "| 🧹 **Summarize fewer files** | Only pass a subset of files                                              |\n",
        "| 🔄 **Batch the LLM calls**   | Break files into chunks of 5–10                                          |\n",
        "| ⚡ **Use a cheaper model**    | Switch to `gpt-3.5-turbo` if accuracy is acceptable                      |\n",
        "| 🧠 **Preprocess locally**    | Use local Python (e.g., keywords) to cluster/group files before LLM call |\n",
        "| 📉 **Log & cache results**   | Save plans so you don’t re-query the LLM for the same files repeatedly   |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Optional Optimization\n",
        "\n",
        "You could break this into a batching loop like:\n",
        "\n",
        "```python\n",
        "from math import ceil\n",
        "\n",
        "batch_size = 5\n",
        "batches = [file_list[i:i + batch_size] for i in range(0, len(file_list), batch_size)]\n",
        "\n",
        "all_steps = []\n",
        "\n",
        "for batch in batches:\n",
        "    messages = build_multi_action_prompt(batch)\n",
        "    llm_output = generate_response(messages)\n",
        "    plan = parse_multi_action_plan(llm_output)\n",
        "    all_steps.extend(plan)\n",
        "```\n",
        "\n",
        "This breaks up big prompts and avoids a huge single LLM call — which helps with:\n",
        "\n",
        "* Cost\n",
        "* Speed\n",
        "* Token limit risks\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Bottom Line\n",
        "\n",
        "* ✅ You're doing the right thing so far (1 call total).\n",
        "* 🧠 As scale grows, batching and optimization matter.\n",
        "* 💰 You’re building cost-aware, production-capable agents — huge milestone!\n",
        "\n"
      ],
      "metadata": {
        "id": "79ZJHeCFYeL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt + Validation in the Real World\n",
        "\n",
        "In production you *do both*: prompt the model to emit structured JSON **and** validate what you get. Prompts **steer**; validation **guarantees**. Models are probabilistic, can drift, and occasionally ignore formatting—so you need a contract (schema) that your Python enforces before any tool runs.\n",
        "\n",
        "Why this combo works:\n",
        "\n",
        "* **Reliability:** Prompting raises the success rate; schema validation catches the misses.\n",
        "* **Safety:** You never execute unknown tools or bad args.\n",
        "* **Maintainability:** Clear errors (from validation) feed back into retries and prompt tuning.\n",
        "* **Determinism:** Structured data lets you route, log, and chain actions predictably.\n",
        "\n",
        "Here’s a tiny drop-in hardening for your multi-action plan:\n",
        "\n",
        "````python\n",
        "import json, re, os\n",
        "from typing import List, Union, Literal\n",
        "from pydantic import BaseModel, ValidationError\n",
        "\n",
        "# 1) Slightly tougher extractor (accepts ```action or ```action json)\n",
        "def extract_markdown_block(text: str, tag: str = \"action\") -> str:\n",
        "    pattern = re.compile(rf\"```{tag}(?:\\s+json)?\\s*(.*?)```\", re.DOTALL | re.IGNORECASE)\n",
        "    m = pattern.search(text)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Missing markdown block with tag '{tag}'\")\n",
        "    return m.group(1).strip()\n",
        "\n",
        "# 2) Schemas (the contract)\n",
        "class SummarizeArgs(BaseModel):\n",
        "    filename: str\n",
        "\n",
        "class MoveArgs(BaseModel):\n",
        "    filename: str\n",
        "    target_folder: str\n",
        "\n",
        "class SummarizeAction(BaseModel):\n",
        "    tool_name: Literal[\"summarize_file\"]\n",
        "    args: SummarizeArgs\n",
        "\n",
        "class MoveAction(BaseModel):\n",
        "    tool_name: Literal[\"move_file\"]\n",
        "    args: MoveArgs\n",
        "\n",
        "PlanItem = Union[SummarizeAction, MoveAction]\n",
        "\n",
        "# 3) Parse + validate + semantic checks\n",
        "def parse_multi_action_plan(llm_response: str, allowed_files: set[str], max_actions: int = 50) -> List[dict]:\n",
        "    block = extract_markdown_block(llm_response, tag=\"action\")\n",
        "    try:\n",
        "        raw = json.loads(block)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Invalid JSON in action block: {e}\") from e\n",
        "\n",
        "    if not isinstance(raw, list):\n",
        "        raise ValueError(\"Expected a JSON list of actions.\")\n",
        "\n",
        "    if len(raw) > max_actions:\n",
        "        raise ValueError(f\"Plan too long (> {max_actions} actions).\")\n",
        "\n",
        "    validated: List[dict] = []\n",
        "    for i, item in enumerate(raw, 1):\n",
        "        try:\n",
        "            model = SummarizeAction.model_validate(item) if item.get(\"tool_name\") == \"summarize_file\" \\\n",
        "                    else MoveAction.model_validate(item) if item.get(\"tool_name\") == \"move_file\" \\\n",
        "                    else None\n",
        "            if model is None:\n",
        "                raise ValueError(f\"Unknown tool '{item.get('tool_name')}'.\")\n",
        "        except ValidationError as ve:\n",
        "            raise ValueError(f\"Schema error at action #{i}: {ve.errors()}\") from ve\n",
        "\n",
        "        # semantic checks\n",
        "        fn = os.path.basename(model.args.filename)\n",
        "        if fn not in allowed_files:\n",
        "            raise ValueError(f\"Unknown or disallowed filename at action #{i}: {fn}\")\n",
        "\n",
        "        validated.append(model.model_dump())\n",
        "\n",
        "    return validated\n",
        "````\n",
        "\n",
        "**Usage pattern in prod:**\n",
        "\n",
        "1. Prompt for JSON (as you do).\n",
        "2. Extract → `json.loads` → **Pydantic validate** (+ semantic checks).\n",
        "3. On failure: **retry once** with stricter instructions/lower temperature; else surface a clean error.\n",
        "\n",
        "That’s the pragmatic, reliable way to turn LLM plans into safe, executable agent actions.\n"
      ],
      "metadata": {
        "id": "npAD5ZmW3oPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Prompt-only formatting is brittle.**\n",
        "\n",
        "In production, you *constrain and validate* every time.\n",
        "\n",
        "## The reliability ladder (strongest → weakest)\n",
        "\n",
        "1. **Native tool/function calling** (API returns `{name, arguments}`): no free-text to parse; still validate.\n",
        "2. **JSON mode + schema / constrained decoding**: model is steered to emit valid JSON of a known shape; still validate.\n",
        "3. **Pydantic (or JSON Schema) validation**: enforce types, required fields, enums, ranges; reject bad outputs.\n",
        "4. **Fenced JSON + parser** (` ```action { ... } ``` `): workable, but needs schema validation + retry.\n",
        "5. **Prompt-only “please format like …”**: OK for demos; too flaky for automation.\n",
        "\n",
        "## Why prompt-only loses\n",
        "\n",
        "* Models drift; prompts evolve; occasional hallucinated prose sneaks in.\n",
        "* Minor changes (temperature, context length, upstream model updates) break formatting.\n",
        "* No types, no bounds, no whitelist → higher risk to tools with side effects.\n",
        "\n",
        "## What to ship\n",
        "\n",
        "* **Constrain → Parse → Validate → Retry (≤2) → Fallback/log.**\n",
        "* Keep a **schema version** in outputs; add **semantic checks** (e.g., `b != 0`).\n",
        "* Always log validation failures to improve prompts/schemas.\n",
        "\n",
        "## When prompt-only is acceptable\n",
        "\n",
        "* Prototypes, internal tools with a human-in-the-loop, or low-stakes outputs.\n",
        "\n",
        "**Bottom line:** Use the strongest mechanism available (tool calling or JSON mode), and **always** layer Pydantic/Schema validation on top. Prompts help—but contracts and validation make agents reliable.\n"
      ],
      "metadata": {
        "id": "dlzLCAgg38By"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validate Forever.**\n",
        "Treat *all* LLM output as untrusted input—during dev **and** in production. Even with JSON mode or tool-calling you still validate, because models drift, prompts evolve, and edge cases show up in the wild.\n",
        "\n",
        "## What to validate (two layers)\n",
        "\n",
        "1. **Structural (syntactic)**\n",
        "\n",
        "* Valid JSON / matches schema (Pydantic/JSON Schema)\n",
        "* Required keys present, correct types\n",
        "* Enums/allowlists for `tool_name`\n",
        "* Length limits (e.g., summary ≤ 120 words), regexes, etc.\n",
        "\n",
        "2. **Semantic (business rules)**\n",
        "\n",
        "* `divide`: `b != 0`\n",
        "* File ops: paths stay in allowed directory\n",
        "* URLs on an allowlist; no network where forbidden\n",
        "* Budget/date bounds, idempotency tokens, etc.\n",
        "\n",
        "## Runtime policy (keep this in prod)\n",
        "\n",
        "* **Validate → execute**; on failure: **retry once** (maybe twice) with stricter instructions/lower temperature.\n",
        "* If still invalid: **graceful fallback** (return a safe error or a simpler baseline behavior).\n",
        "* **Always log** invalid outputs, retry counts, and tool failures; set alerts if rates spike.\n",
        "\n",
        "## Observability you want\n",
        "\n",
        "* Metrics: `llm_parse_invalid_rate`, `validation_errors_by_field`, `tool_call_success_rate`, latency, token/cost.\n",
        "* Attach `schema_version` to every response so you can evolve safely.\n",
        "* Circuit breaker or feature flag to tighten/relax validation without redeploy.\n",
        "\n",
        "## Tiny pattern (prod-safe wrapper)\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import Literal, Dict\n",
        "\n",
        "class Action(BaseModel):\n",
        "    schema_version: int = Field(1, const=True)\n",
        "    tool_name: Literal[\"multiply\", \"divide\", \"search\"]\n",
        "    args: Dict\n",
        "\n",
        "def validate_and_dispatch(raw_text: str):\n",
        "    try:\n",
        "        block = extract_markdown_block(raw_text, \"action\")\n",
        "        action = Action.model_validate_json(block)  # structural\n",
        "        # semantic checks\n",
        "        if action.tool_name == \"divide\" and action.args.get(\"b\") == 0:\n",
        "            raise ValueError(\"Division by zero.\")\n",
        "        return dispatch(action.tool_name, action.args)\n",
        "    except (ValidationError, ValueError) as e:\n",
        "        # log error, optionally retry once with stricter prompt/JSON mode\n",
        "        return {\"error\": f\"Validation failed: {e}\"}\n",
        "```\n",
        "\n",
        "## Bottom line\n",
        "\n",
        "* **Validation is not a dev-only safety net—it’s the production contract.**\n",
        "* Keep it **always-on**, layer structural + semantic checks, cap retries, and monitor. This is what turns LLM glue code into a reliable agent system.\n"
      ],
      "metadata": {
        "id": "fUO1Odou4Hll"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1xpKm_aNUAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}