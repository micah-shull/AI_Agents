{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBjpGCyQ2S3Yx15IG14eNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/203_Evaluations_as_a_Service_(EaaS)_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GOAL Node"
      ],
      "metadata": {
        "id": "MHgHJPNsYMWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWwLOTvMYCQr"
      },
      "outputs": [],
      "source": [
        "\"\"\"Goal Node - Defines evaluation objective\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "from config import EaaSState, EaaSConfig\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize config\n",
        "config = EaaSConfig()\n",
        "\n",
        "\n",
        "def goal_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Define the evaluation goal based on target agents and evaluation config.\n",
        "\n",
        "    Reads: target_agents, evaluation_config\n",
        "    Writes: goal\n",
        "    \"\"\"\n",
        "    logger.info(\"üéØ Defining evaluation goal...\")\n",
        "\n",
        "    try:\n",
        "        target_agents = state.get(\"target_agents\", [])\n",
        "        evaluation_config = state.get(\"evaluation_config\", {})\n",
        "\n",
        "        # Extract criteria from config or use defaults\n",
        "        criteria = evaluation_config.get(\"criteria\", config.default_criteria)\n",
        "\n",
        "        # Build goal structure\n",
        "        goal = {\n",
        "            \"objective\": \"Evaluate target agents against test scenarios\",\n",
        "            \"target_agents\": [agent.get(\"id\", \"unknown\") for agent in target_agents],\n",
        "            \"criteria\": criteria,\n",
        "            \"evaluation_type\": \"automated_testing\",\n",
        "            \"expected_outcomes\": {\n",
        "                \"accuracy\": \"Measure correctness of agent outputs\",\n",
        "                \"safety\": \"Verify safe responses to sensitive inputs\",\n",
        "                \"latency\": \"Track response time performance\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        state[\"goal\"] = goal\n",
        "        logger.info(f\"‚úÖ Goal defined for {len(target_agents)} agent(s) with criteria: {criteria}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in goal_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning Node"
      ],
      "metadata": {
        "id": "a_cCj6IQYYyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Planning Node - Creates execution plan\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def planning_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Create execution plan based on goal.\n",
        "\n",
        "    Reads: goal\n",
        "    Writes: plan\n",
        "    \"\"\"\n",
        "    logger.info(\"üìã Creating execution plan...\")\n",
        "\n",
        "    try:\n",
        "        goal = state.get(\"goal\", {})\n",
        "\n",
        "        # Template-based plan (MVP: fixed structure)\n",
        "        plan = [\n",
        "            {\"step\": 1, \"action\": \"ingest_data\", \"description\": \"Load test scenarios and ground truth\"},\n",
        "            {\"step\": 2, \"action\": \"generate_scenarios\", \"description\": \"Generate additional test scenarios if needed\"},\n",
        "            {\"step\": 3, \"action\": \"execute_evaluations\", \"description\": \"Run test scenarios through target agents\"},\n",
        "            {\"step\": 4, \"action\": \"score_results\", \"description\": \"Score and analyze agent performance\"},\n",
        "            {\"step\": 5, \"action\": \"generate_report\", \"description\": \"Generate evaluation report\"}\n",
        "        ]\n",
        "\n",
        "        state[\"plan\"] = plan\n",
        "        logger.info(f\"‚úÖ Plan created with {len(plan)} steps\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in planning_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "1e9sMpgdYZ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "n1_sQhpEYh0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Data Ingestion Node - Loads test scenarios and ground truth\"\"\"\n",
        "\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def data_ingestion_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Load test scenarios from data file.\n",
        "\n",
        "    Reads: test_data_path\n",
        "    Writes: evaluation_data\n",
        "    \"\"\"\n",
        "    logger.info(\"üì• Ingesting evaluation data...\")\n",
        "\n",
        "    try:\n",
        "        test_data_path = state.get(\"test_data_path\")\n",
        "\n",
        "        if not test_data_path:\n",
        "            error_msg = \"test_data_path is required\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Load and parse the data file\n",
        "        data_file = Path(test_data_path)\n",
        "        if not data_file.exists():\n",
        "            error_msg = f\"Test data file not found: {test_data_path}\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Read file content\n",
        "        content = data_file.read_text()\n",
        "\n",
        "        # Handle Python assignment format (e.g., \"classification_cases = [...]\")\n",
        "        # Extract JSON array from the content\n",
        "        if \"=\" in content:\n",
        "            # Find the JSON array part\n",
        "            start_idx = content.find(\"[\")\n",
        "            end_idx = content.rfind(\"]\") + 1\n",
        "            if start_idx != -1 and end_idx > start_idx:\n",
        "                json_content = content[start_idx:end_idx]\n",
        "            else:\n",
        "                json_content = content\n",
        "        else:\n",
        "            json_content = content\n",
        "\n",
        "        # Parse JSON\n",
        "        test_scenarios = json.loads(json_content)\n",
        "\n",
        "        # Extract task types\n",
        "        task_types = list(set(scenario.get(\"task_type\", \"unknown\") for scenario in test_scenarios))\n",
        "\n",
        "        # Build evaluation_data structure\n",
        "        evaluation_data = {\n",
        "            \"test_scenarios\": test_scenarios,\n",
        "            \"metadata\": {\n",
        "                \"total_scenarios\": len(test_scenarios),\n",
        "                \"task_types\": task_types,\n",
        "                \"source_file\": str(test_data_path)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        state[\"evaluation_data\"] = evaluation_data\n",
        "        logger.info(f\"‚úÖ Loaded {len(test_scenarios)} test scenarios (types: {task_types})\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        error_msg = f\"JSON parsing error: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in data_ingestion_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "-vr7emkcYjOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario Generation Node"
      ],
      "metadata": {
        "id": "DIrKszzVYpU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Scenario Generation Node - Generates additional test scenarios if needed\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def scenario_generation_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Generate additional test scenarios if needed.\n",
        "\n",
        "    MVP: Skip generation if data is provided, just pass through.\n",
        "    Future: Use LLM to generate synthetic scenarios.\n",
        "\n",
        "    Reads: evaluation_data, goal\n",
        "    Writes: generated_scenarios\n",
        "    \"\"\"\n",
        "    logger.info(\"üîß Generating additional scenarios...\")\n",
        "\n",
        "    try:\n",
        "        evaluation_data = state.get(\"evaluation_data\", {})\n",
        "        test_scenarios = evaluation_data.get(\"test_scenarios\", [])\n",
        "\n",
        "        # MVP: If we have test data, skip generation\n",
        "        # Future: Generate additional scenarios using LLM\n",
        "        if len(test_scenarios) > 0:\n",
        "            logger.info(\"‚úÖ Test data provided, skipping scenario generation (MVP)\")\n",
        "            state[\"generated_scenarios\"] = []\n",
        "        else:\n",
        "            # Would generate scenarios here in future version\n",
        "            state[\"generated_scenarios\"] = []\n",
        "            logger.info(\"‚ö†Ô∏è No test data provided, but generation not implemented yet (MVP)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in scenario_generation_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "l5f89nXcYrm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Execution Node"
      ],
      "metadata": {
        "id": "ymV76PrcYxS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluation Execution Node - Runs test scenarios through target agents\"\"\"\n",
        "\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _run_agent(agent: Dict[str, Any], input_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run a single agent on input text.\n",
        "\n",
        "    MVP: Simple mock implementation - just returns a placeholder.\n",
        "    Future: Support actual agent execution (API calls, function calls, etc.)\n",
        "    \"\"\"\n",
        "    # MVP: Mock execution - just return a placeholder response\n",
        "    # In real implementation, this would:\n",
        "    # - Call agent endpoint/function\n",
        "    # - Handle timeouts, retries\n",
        "    # - Capture actual output\n",
        "\n",
        "    time.sleep(0.1)  # Simulate latency\n",
        "\n",
        "    # Mock response based on agent type\n",
        "    agent_type = agent.get(\"type\", \"unknown\")\n",
        "    if agent_type == \"classification\":\n",
        "        # Mock classification: return first label (would be actual agent output)\n",
        "        return \"positive\"  # Placeholder\n",
        "    elif agent_type == \"safety\":\n",
        "        # Mock safety: return safe (would be actual agent output)\n",
        "        return \"safe\"  # Placeholder\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def evaluation_execution_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Execute test scenarios through target agents.\n",
        "\n",
        "    Reads: evaluation_data, generated_scenarios, target_agents\n",
        "    Writes: evaluation_results\n",
        "    \"\"\"\n",
        "    logger.info(\"üöÄ Executing evaluations...\")\n",
        "\n",
        "    try:\n",
        "        target_agents = state.get(\"target_agents\", [])\n",
        "        evaluation_data = state.get(\"evaluation_data\", {})\n",
        "        generated_scenarios = state.get(\"generated_scenarios\", [])\n",
        "\n",
        "        # Combine all scenarios\n",
        "        all_scenarios = evaluation_data.get(\"test_scenarios\", []) + generated_scenarios\n",
        "\n",
        "        if len(target_agents) == 0:\n",
        "            error_msg = \"No target agents provided\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        if len(all_scenarios) == 0:\n",
        "            error_msg = \"No test scenarios available\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Execute each scenario through each agent\n",
        "        evaluation_results = []\n",
        "\n",
        "        for agent in target_agents:\n",
        "            agent_id = agent.get(\"id\", \"unknown\")\n",
        "            logger.info(f\"  Evaluating agent: {agent_id}\")\n",
        "\n",
        "            for scenario in all_scenarios:\n",
        "                scenario_id = scenario.get(\"id\", \"unknown\")\n",
        "                input_text = scenario.get(\"input\", \"\")\n",
        "                expected_output = scenario.get(\"expected_output\", \"\")\n",
        "\n",
        "                # Run agent\n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    actual_output = _run_agent(agent, input_text)\n",
        "                    latency_ms = int((time.time() - start_time) * 1000)\n",
        "                    errors = []\n",
        "                except Exception as e:\n",
        "                    actual_output = None\n",
        "                    latency_ms = 0\n",
        "                    errors = [str(e)]\n",
        "\n",
        "                # Record result\n",
        "                result = {\n",
        "                    \"agent_id\": agent_id,\n",
        "                    \"scenario_id\": scenario_id,\n",
        "                    \"input\": input_text,\n",
        "                    \"actual_output\": actual_output,\n",
        "                    \"expected_output\": expected_output,\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"latency_ms\": latency_ms,\n",
        "                    \"errors\": errors\n",
        "                }\n",
        "\n",
        "                evaluation_results.append(result)\n",
        "\n",
        "        state[\"evaluation_results\"] = evaluation_results\n",
        "        logger.info(f\"‚úÖ Executed {len(evaluation_results)} evaluations across {len(target_agents)} agent(s)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in evaluation_execution_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "sEDtkxDZYzqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring Node"
      ],
      "metadata": {
        "id": "IWoSuhOsY9uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Scoring Node - Scores and analyzes evaluation results\"\"\"\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any, List\n",
        "from config import EaaSState\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def _calculate_accuracy(results: List[Dict[str, Any]]) -> float:\n",
        "    \"\"\"Calculate accuracy score (correct / total)\"\"\"\n",
        "    if len(results) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    correct = sum(1 for r in results if r.get(\"actual_output\") == r.get(\"expected_output\"))\n",
        "    return correct / len(results)\n",
        "\n",
        "\n",
        "def _calculate_latency_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"Calculate latency percentiles\"\"\"\n",
        "    latencies = [r.get(\"latency_ms\", 0) for r in results if r.get(\"latency_ms\", 0) > 0]\n",
        "\n",
        "    if len(latencies) == 0:\n",
        "        return {\"p50\": 0, \"p95\": 0, \"avg\": 0}\n",
        "\n",
        "    sorted_latencies = sorted(latencies)\n",
        "    p50_idx = int(len(sorted_latencies) * 0.5)\n",
        "    p95_idx = int(len(sorted_latencies) * 0.95)\n",
        "\n",
        "    return {\n",
        "        \"p50\": sorted_latencies[p50_idx] if p50_idx < len(sorted_latencies) else sorted_latencies[-1],\n",
        "        \"p95\": sorted_latencies[p95_idx] if p95_idx < len(sorted_latencies) else sorted_latencies[-1],\n",
        "        \"avg\": sum(sorted_latencies) / len(sorted_latencies)\n",
        "    }\n",
        "\n",
        "\n",
        "def scoring_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Score and analyze evaluation results.\n",
        "\n",
        "    Reads: evaluation_results, evaluation_config\n",
        "    Writes: scores, drift_detection, failure_analysis\n",
        "    \"\"\"\n",
        "    logger.info(\"üìä Scoring evaluation results...\")\n",
        "\n",
        "    try:\n",
        "        evaluation_results = state.get(\"evaluation_results\", [])\n",
        "        evaluation_config = state.get(\"evaluation_config\", {})\n",
        "\n",
        "        if len(evaluation_results) == 0:\n",
        "            error_msg = \"No evaluation results to score\"\n",
        "            logger.error(error_msg)\n",
        "            state.setdefault(\"errors\", []).append(error_msg)\n",
        "            return state\n",
        "\n",
        "        # Group results by agent\n",
        "        scores = {}\n",
        "\n",
        "        # Get unique agent IDs\n",
        "        agent_ids = set(r.get(\"agent_id\") for r in evaluation_results)\n",
        "\n",
        "        for agent_id in agent_ids:\n",
        "            agent_results = [r for r in evaluation_results if r.get(\"agent_id\") == agent_id]\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = _calculate_accuracy(agent_results)\n",
        "\n",
        "            # Calculate latency metrics\n",
        "            latency_metrics = _calculate_latency_metrics(agent_results)\n",
        "\n",
        "            # Calculate scenario-level scores\n",
        "            scenario_scores = []\n",
        "            for result in agent_results:\n",
        "                correct = result.get(\"actual_output\") == result.get(\"expected_output\")\n",
        "                scenario_scores.append({\n",
        "                    \"scenario_id\": result.get(\"scenario_id\"),\n",
        "                    \"correct\": correct,\n",
        "                    \"score\": 1.0 if correct else 0.0\n",
        "                })\n",
        "\n",
        "            # Calculate overall score (simple average for MVP)\n",
        "            overall_score = accuracy  # MVP: just use accuracy\n",
        "\n",
        "            scores[agent_id] = {\n",
        "                \"overall_score\": overall_score,\n",
        "                \"accuracy\": accuracy,\n",
        "                \"latency_p50\": latency_metrics[\"p50\"],\n",
        "                \"latency_p95\": latency_metrics[\"p95\"],\n",
        "                \"latency_avg\": latency_metrics[\"avg\"],\n",
        "                \"scenario_scores\": scenario_scores,\n",
        "                \"total_scenarios\": len(agent_results)\n",
        "            }\n",
        "\n",
        "        state[\"scores\"] = scores\n",
        "\n",
        "        # MVP: Empty drift detection and failure analysis\n",
        "        state[\"drift_detection\"] = {}\n",
        "        state[\"failure_analysis\"] = []\n",
        "\n",
        "        logger.info(f\"‚úÖ Scored {len(scores)} agent(s)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in scoring_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "bt4HOG2oY-vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report Node"
      ],
      "metadata": {
        "id": "YAHdBvNiZBX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Report Node - Generates evaluation report\"\"\"\n",
        "\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "from config import EaaSState, EaaSConfig\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize config\n",
        "config = EaaSConfig()\n",
        "\n",
        "\n",
        "def report_node(state: EaaSState) -> EaaSState:\n",
        "    \"\"\"\n",
        "    Generate evaluation report.\n",
        "\n",
        "    Reads: scores, evaluation_results, goal\n",
        "    Writes: evaluation_report, report_file_path\n",
        "    \"\"\"\n",
        "    logger.info(\"üìù Generating evaluation report...\")\n",
        "\n",
        "    try:\n",
        "        scores = state.get(\"scores\", {})\n",
        "        evaluation_results = state.get(\"evaluation_results\", [])\n",
        "        goal = state.get(\"goal\", {})\n",
        "\n",
        "        # MVP: Simple markdown report (no template for now)\n",
        "        report_lines = [\n",
        "            \"# Evaluation Report\",\n",
        "            \"\",\n",
        "            f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            \"\",\n",
        "            \"## Summary\",\n",
        "            \"\",\n",
        "            f\"Evaluated **{len(scores)} agent(s)** across **{len(evaluation_results)} test scenario(s)**.\",\n",
        "            \"\",\n",
        "            \"## Agent Scores\",\n",
        "            \"\"\n",
        "        ]\n",
        "\n",
        "        # Add scores for each agent\n",
        "        for agent_id, agent_scores in scores.items():\n",
        "            report_lines.extend([\n",
        "                f\"### {agent_id}\",\n",
        "                \"\",\n",
        "                f\"- **Overall Score:** {agent_scores.get('overall_score', 0):.2%}\",\n",
        "                f\"- **Accuracy:** {agent_scores.get('accuracy', 0):.2%}\",\n",
        "                f\"- **Latency (P50):** {agent_scores.get('latency_p50', 0)}ms\",\n",
        "                f\"- **Latency (P95):** {agent_scores.get('latency_p95', 0)}ms\",\n",
        "                f\"- **Total Scenarios:** {agent_scores.get('total_scenarios', 0)}\",\n",
        "                \"\"\n",
        "            ])\n",
        "\n",
        "        report_lines.extend([\n",
        "            \"## Detailed Results\",\n",
        "            \"\",\n",
        "            \"| Agent | Scenario | Input | Expected | Actual | Correct |\",\n",
        "            \"|-------|----------|-------|----------|--------|---------|\"\n",
        "        ])\n",
        "\n",
        "        # Add detailed results (limit to first 10 for readability)\n",
        "        for result in evaluation_results[:10]:\n",
        "            agent_id = result.get(\"agent_id\", \"unknown\")\n",
        "            scenario_id = result.get(\"scenario_id\", \"unknown\")\n",
        "            input_text = result.get(\"input\", \"\")[:50] + \"...\" if len(result.get(\"input\", \"\")) > 50 else result.get(\"input\", \"\")\n",
        "            expected = result.get(\"expected_output\", \"\")\n",
        "            actual = result.get(\"actual_output\", \"\")\n",
        "            correct = \"‚úÖ\" if expected == actual else \"‚ùå\"\n",
        "\n",
        "            report_lines.append(\n",
        "                f\"| {agent_id} | {scenario_id} | {input_text} | {expected} | {actual} | {correct} |\"\n",
        "            )\n",
        "\n",
        "        if len(evaluation_results) > 10:\n",
        "            report_lines.append(f\"\\n*... and {len(evaluation_results) - 10} more results*\")\n",
        "\n",
        "        report_markdown = \"\\n\".join(report_lines)\n",
        "        state[\"evaluation_report\"] = report_markdown\n",
        "\n",
        "        # Save report to file\n",
        "        reports_dir = Path(config.evaluation_reports_dir)\n",
        "        reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        report_file = reports_dir / f\"evaluation_report_{timestamp}.md\"\n",
        "        report_file.write_text(report_markdown)\n",
        "\n",
        "        state[\"report_file_path\"] = str(report_file)\n",
        "        logger.info(f\"‚úÖ Report generated: {report_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in report_node: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        state.setdefault(\"errors\", []).append(error_msg)\n",
        "\n",
        "    return state\n",
        "\n"
      ],
      "metadata": {
        "id": "sdRCH8CPZCRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoke Test"
      ],
      "metadata": {
        "id": "22hLysPyZJSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Smoke Test Runner for EaaS Agent\n",
        "Tests nodes manually in sequence before LangGraph wiring\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(__file__).parent.parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from config import EaaSState\n",
        "from nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_ingestion_node,\n",
        "    scenario_generation_node,\n",
        "    evaluation_execution_node,\n",
        "    scoring_node,\n",
        "    report_node\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def test_linear_flow():\n",
        "    \"\"\"Test all nodes in sequence\"\"\"\n",
        "\n",
        "    # Initialize state\n",
        "    state: EaaSState = {\n",
        "        \"target_agents\": [\n",
        "            {\n",
        "                \"id\": \"agent_001\",\n",
        "                \"name\": \"Sentiment Classifier\",\n",
        "                \"type\": \"classification\"\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"agent_002\",\n",
        "                \"name\": \"Safety Checker\",\n",
        "                \"type\": \"safety\"\n",
        "            }\n",
        "        ],\n",
        "        \"evaluation_config\": {\n",
        "            \"criteria\": [\"accuracy\", \"safety\", \"latency\"],\n",
        "            \"thresholds\": {\n",
        "                \"accuracy\": 0.8,\n",
        "                \"safety\": 0.95,\n",
        "                \"latency_ms\": 2000\n",
        "            }\n",
        "        },\n",
        "        \"test_data_path\": \"data/classification_cases.json\",  # Start with classification\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üß™ EaaS Agent Smoke Test\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Test goal_node\n",
        "    print(\"1Ô∏è‚É£ Testing goal_node...\")\n",
        "    state = goal_node(state)\n",
        "    assert \"goal\" in state, \"Goal node should add 'goal' to state\"\n",
        "    assert state[\"goal\"][\"objective\"] is not None, \"Goal should have objective\"\n",
        "    print(f\"   ‚úÖ Goal defined: {state['goal']['objective']}\\n\")\n",
        "\n",
        "    # Test planning_node\n",
        "    print(\"2Ô∏è‚É£ Testing planning_node...\")\n",
        "    state = planning_node(state)\n",
        "    assert \"plan\" in state, \"Planning node should add 'plan' to state\"\n",
        "    assert len(state[\"plan\"]) > 0, \"Plan should have steps\"\n",
        "    print(f\"   ‚úÖ Plan created with {len(state['plan'])} steps\\n\")\n",
        "\n",
        "    # Test data_ingestion_node\n",
        "    print(\"3Ô∏è‚É£ Testing data_ingestion_node...\")\n",
        "    state = data_ingestion_node(state)\n",
        "    assert \"evaluation_data\" in state, \"Data ingestion should add 'evaluation_data'\"\n",
        "    assert \"test_scenarios\" in state[\"evaluation_data\"], \"Should have test_scenarios\"\n",
        "    scenarios = state[\"evaluation_data\"][\"test_scenarios\"]\n",
        "    print(f\"   ‚úÖ Loaded {len(scenarios)} test scenarios\\n\")\n",
        "\n",
        "    # Test scenario_generation_node\n",
        "    print(\"4Ô∏è‚É£ Testing scenario_generation_node...\")\n",
        "    state = scenario_generation_node(state)\n",
        "    assert \"generated_scenarios\" in state, \"Should add 'generated_scenarios'\"\n",
        "    print(f\"   ‚úÖ Scenario generation complete\\n\")\n",
        "\n",
        "    # Test evaluation_execution_node\n",
        "    print(\"5Ô∏è‚É£ Testing evaluation_execution_node...\")\n",
        "    state = evaluation_execution_node(state)\n",
        "    assert \"evaluation_results\" in state, \"Should add 'evaluation_results'\"\n",
        "    results = state[\"evaluation_results\"]\n",
        "    print(f\"   ‚úÖ Executed {len(results)} evaluations\\n\")\n",
        "\n",
        "    # Test scoring_node\n",
        "    print(\"6Ô∏è‚É£ Testing scoring_node...\")\n",
        "    state = scoring_node(state)\n",
        "    assert \"scores\" in state, \"Should add 'scores'\"\n",
        "    scores = state[\"scores\"]\n",
        "    print(f\"   ‚úÖ Scored {len(scores)} agent(s)\\n\")\n",
        "\n",
        "    # Print score summary\n",
        "    for agent_id, agent_scores in scores.items():\n",
        "        print(f\"   üìä {agent_id}:\")\n",
        "        print(f\"      Accuracy: {agent_scores.get('accuracy', 0):.2%}\")\n",
        "        print(f\"      Overall: {agent_scores.get('overall_score', 0):.2%}\")\n",
        "    print()\n",
        "\n",
        "    # Test report_node\n",
        "    print(\"7Ô∏è‚É£ Testing report_node...\")\n",
        "    state = report_node(state)\n",
        "    assert \"evaluation_report\" in state, \"Should add 'evaluation_report'\"\n",
        "    assert \"report_file_path\" in state, \"Should add 'report_file_path'\"\n",
        "    print(f\"   ‚úÖ Report generated: {state['report_file_path']}\\n\")\n",
        "\n",
        "    # Final summary\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ All nodes passed smoke test!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüìÑ Report saved to: {state['report_file_path']}\")\n",
        "\n",
        "    if state.get(\"errors\"):\n",
        "        print(f\"\\n‚ö†Ô∏è  {len(state['errors'])} error(s) encountered:\")\n",
        "        for error in state[\"errors\"]:\n",
        "            print(f\"   - {error}\")\n",
        "    else:\n",
        "        print(\"\\n‚ú® No errors encountered!\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        final_state = test_linear_flow()\n",
        "        print(\"\\nüéâ Smoke test completed successfully!\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"\\n‚ùå Assertion failed: {e}\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during smoke test: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "BHvlfa0jZKsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "AF2uBhhTZbb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "micahshull@Micahs-iMac LG_Cursor_026 % cd /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_026 && mkdir -p agents nodes templates utils tests/test_data output/evaluation_reports\n",
        "micahshull@Micahs-iMac LG_Cursor_026 % cd /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_026 && python3 tests/test_mvp_runner.py\n",
        "\n",
        "============================================================\n",
        "üß™ EaaS Agent Smoke Test\n",
        "============================================================\n",
        "\n",
        "1Ô∏è‚É£ Testing goal_node...\n",
        "INFO: üéØ Defining evaluation goal...\n",
        "INFO: ‚úÖ Goal defined for 2 agent(s) with criteria: ['accuracy', 'safety', 'latency']\n",
        "   ‚úÖ Goal defined: Evaluate target agents against test scenarios\n",
        "\n",
        "2Ô∏è‚É£ Testing planning_node...\n",
        "INFO: üìã Creating execution plan...\n",
        "INFO: ‚úÖ Plan created with 5 steps\n",
        "   ‚úÖ Plan created with 5 steps\n",
        "\n",
        "3Ô∏è‚É£ Testing data_ingestion_node...\n",
        "INFO: üì• Ingesting evaluation data...\n",
        "INFO: ‚úÖ Loaded 10 test scenarios (types: ['classification'])\n",
        "   ‚úÖ Loaded 10 test scenarios\n",
        "\n",
        "4Ô∏è‚É£ Testing scenario_generation_node...\n",
        "INFO: üîß Generating additional scenarios...\n",
        "INFO: ‚úÖ Test data provided, skipping scenario generation (MVP)\n",
        "   ‚úÖ Scenario generation complete\n",
        "\n",
        "5Ô∏è‚É£ Testing evaluation_execution_node...\n",
        "INFO: üöÄ Executing evaluations...\n",
        "INFO:   Evaluating agent: agent_001\n",
        "INFO:   Evaluating agent: agent_002\n",
        "INFO: ‚úÖ Executed 20 evaluations across 2 agent(s)\n",
        "   ‚úÖ Executed 20 evaluations\n",
        "\n",
        "6Ô∏è‚É£ Testing scoring_node...\n",
        "INFO: üìä Scoring evaluation results...\n",
        "INFO: ‚úÖ Scored 2 agent(s)\n",
        "   ‚úÖ Scored 2 agent(s)\n",
        "\n",
        "   üìä agent_002:\n",
        "      Accuracy: 0.00%\n",
        "      Overall: 0.00%\n",
        "   üìä agent_001:\n",
        "      Accuracy: 40.00%\n",
        "      Overall: 40.00%\n",
        "\n",
        "7Ô∏è‚É£ Testing report_node...\n",
        "INFO: üìù Generating evaluation report...\n",
        "INFO: ‚úÖ Report generated: output/evaluation_reports/evaluation_report_20251117_145817.md\n",
        "   ‚úÖ Report generated: output/evaluation_reports/evaluation_report_20251117_145817.md\n",
        "\n",
        "============================================================\n",
        "‚úÖ All nodes passed smoke test!\n",
        "============================================================\n",
        "\n",
        "üìÑ Report saved to: output/evaluation_reports/evaluation_report_20251117_145817.md\n",
        "\n",
        "‚ú® No errors encountered!\n",
        "\n",
        "üéâ Smoke test completed successfully!\n"
      ],
      "metadata": {
        "id": "LGqzMHdmZdab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Report\n",
        "\n",
        "**Generated:** 2025-11-17 14:58:17\n",
        "\n",
        "## Summary\n",
        "\n",
        "Evaluated **2 agent(s)** across **20 test scenario(s)**.\n",
        "\n",
        "## Agent Scores\n",
        "\n",
        "### agent_002\n",
        "\n",
        "- **Overall Score:** 0.00%\n",
        "- **Accuracy:** 0.00%\n",
        "- **Latency (P50):** 101ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "### agent_001\n",
        "\n",
        "- **Overall Score:** 40.00%\n",
        "- **Accuracy:** 40.00%\n",
        "- **Latency (P50):** 105ms\n",
        "- **Latency (P95):** 105ms\n",
        "- **Total Scenarios:** 10\n",
        "\n",
        "## Detailed Results\n",
        "\n",
        "| Agent | Scenario | Input | Expected | Actual | Correct |\n",
        "|-------|----------|-------|----------|--------|---------|\n",
        "| agent_001 | c001 | I absolutely loved the new dashboard ‚Äì it‚Äôs so muc... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c002 | This update is terrible, nothing works the way it ... | negative | positive | ‚ùå |\n",
        "| agent_001 | c003 | It‚Äôs fine, I guess. Not really better or worse tha... | neutral | positive | ‚ùå |\n",
        "| agent_001 | c004 | Thank you so much for fixing this so quickly, I re... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c005 | I‚Äôm really frustrated that I keep getting logged o... | negative | positive | ‚ùå |\n",
        "| agent_001 | c006 | The results are okay, but there‚Äôs still room for i... | neutral | positive | ‚ùå |\n",
        "| agent_001 | c007 | This new feature saves me at least an hour every d... | positive | positive | ‚úÖ |\n",
        "| agent_001 | c008 | I don‚Äôt really care about this change. | neutral | positive | ‚ùå |\n",
        "| agent_001 | c009 | This is completely unusable; I‚Äôm going back to the... | negative | positive | ‚ùå |\n",
        "| agent_001 | c010 | Nice job on the redesign ‚Äì it looks clean and intu... | positive | positive | ‚úÖ |\n",
        "\n",
        "*... and 10 more results*"
      ],
      "metadata": {
        "id": "K1kbYr7nZ0r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Smoke test passed. All 7 nodes executed successfully.\n",
        "\n",
        "## What we built\n",
        "\n",
        "1. Goal node ‚Äî defines evaluation objectives\n",
        "2. Planning node ‚Äî creates execution plan\n",
        "3. Data ingestion node ‚Äî loads test scenarios (handles Python assignment format)\n",
        "4. Scenario generation node ‚Äî placeholder for future generation\n",
        "5. Evaluation execution node ‚Äî runs tests through target agents (mock for MVP)\n",
        "6. Scoring node ‚Äî calculates accuracy, latency metrics\n",
        "7. Report node ‚Äî generates markdown report\n",
        "\n",
        "## Architecture highlights\n",
        "\n",
        "- Linear orchestration flow\n",
        "- State management with TypedDict\n",
        "- Incremental testing (smoke test before LangGraph)\n",
        "- Error handling in each node\n",
        "- Logging for debugging\n",
        "\n",
        "## Current status\n",
        "\n",
        "The low accuracy (40% for agent_001, 0% for agent_002) is expected because we're using mock agents that return placeholder values. This is fine for MVP and demonstrates the architecture.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "1. Wire into LangGraph ‚Äî create the StateGraph workflow (final step)\n",
        "2. Improve mock agents ‚Äî make them more realistic for testing\n",
        "3. Test with safety cases ‚Äî run with `safety_cases.json`\n",
        "4. Add real agent integration ‚Äî connect to actual agent endpoints/functions\n",
        "\n"
      ],
      "metadata": {
        "id": "Lc8KQx-LaLdS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5NMF9DKZ2dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}