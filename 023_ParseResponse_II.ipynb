{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpKY7B7pW1vfv7Nj9qxVQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/023_ParseResponse_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ‚ÄúShould we use an LLM for something we can do easily with Python?‚Äù\n",
        "\n",
        "Short answer: **No ‚Äî not if it‚Äôs deterministic and cheap to do in code.**\n",
        "But let‚Äôs unpack the bigger picture, because this is where **agent design maturity** really starts to show.\n",
        "\n",
        "This is a **crucial point** in understanding how to design intelligent systems with agents.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† When to Use an LLM vs. Traditional Code\n",
        "\n",
        "| Task Type                                   | Use Python                        | Use LLM                              |\n",
        "| ------------------------------------------- | --------------------------------- | ------------------------------------ |\n",
        "| File listing, renaming, matching extensions | ‚úÖ Yes ‚Äî fast, deterministic, free | ‚ùå No ‚Äî overkill, costly, error-prone |\n",
        "| Sorting files by type/date                  | ‚úÖ Yes                             | ‚ùå No                                 |\n",
        "| Inferring *intent* from natural language    | ‚ùå No                              | ‚úÖ Yes                                |\n",
        "| Reasoning across vague instructions         | ‚ùå No                              | ‚úÖ Yes                                |\n",
        "| Multi-step planning                         | ‚ùå No                              | ‚úÖ Yes                                |\n",
        "| Responding conversationally to user         | ‚ùå No                              | ‚úÖ Yes                                |\n",
        "| Matching user instruction to a tool         | ‚ö†Ô∏è Possible but complex           | ‚úÖ Easy with prompting                |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key distinction:\n",
        "\n",
        "> **Use LLMs for language understanding and planning.**\n",
        "> **Use code for everything deterministic and efficient.**\n",
        "\n",
        "So:\n",
        "\n",
        "* ‚úÖ Use Python to **list files**, **filter extensions**, **move files**\n",
        "* ‚úÖ Use LLM to **interpret user intent** from:\n",
        "\n",
        "  > ‚ÄúHey assistant, could you move all the PDFs and Word docs into the project folder?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Real-world hybrid approach\n",
        "\n",
        "Let‚Äôs say the user types:\n",
        "\n",
        "> ‚ÄúCan you move all lecture notes and RAG files to the organized folder?‚Äù\n",
        "\n",
        "That‚Äôs vague. What are:\n",
        "\n",
        "* ‚ÄúLecture notes‚Äù? `.txt`? `.pdf`?\n",
        "* ‚ÄúRAG files‚Äù? Specific filenames? `.json`?\n",
        "\n",
        "‚úÖ **LLM interprets that**\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"move_files\",\n",
        "  \"args\": {\n",
        "    \"file_type\": [\"pdf\", \"json\"],\n",
        "    \"source_dir\": \"docs_folder\",\n",
        "    \"target_dir\": \"organized_folder\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Then üêç **Python executes it efficiently**:\n",
        "\n",
        "* `os.listdir()`, `shutil.move()`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö°Ô∏è Rule of Thumb for Agent Design\n",
        "\n",
        "> ‚ÄúLLMs should tell you *what* to do.\n",
        "> Python should handle *how* to do it.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ So in our notebook:\n",
        "\n",
        "* Use Python to list the files ‚úÖ\n",
        "* Use the LLM to turn vague language into structured commands ‚úÖ\n",
        "* Use code to execute the plan ‚úÖ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SEvHVRFt-Ur_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hubcRDvB79A-",
        "outputId": "86841aa7-b9ec-490c-abf4-69110dc328b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "source_dir = \"/content/docs_folder\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    raise FileNotFoundError(f\"üìÅ Directory not found: {source_dir}\")\n",
        "\n",
        "# List and build full file paths\n",
        "file_list = [\n",
        "    os.path.join(source_dir, f)\n",
        "    for f in os.listdir(source_dir)\n",
        "    if os.path.isfile(os.path.join(source_dir, f))\n",
        "]\n",
        "\n",
        "# Display the found files\n",
        "print(\"üìÇ Files found:\")\n",
        "for file in file_list:\n",
        "    print(\"  -\", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VszbSNVj8G2c",
        "outputId": "d8e94b94-fdeb-49bf-e854-0b5a4c886b36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Files found:\n",
            "  - /content/docs_folder/001_PArse_the Response.txt\n",
            "  - /content/docs_folder/006_Agent Loop with Function Calling.txt\n",
            "  - /content/docs_folder/000_Prompting for Agents -GAIL.txt\n",
            "  - /content/docs_folder/002_Execute_the_Action.txt\n",
            "  - /content/docs_folder/005_Using Function Calling Capabilities with LLMs.txt\n",
            "  - /content/docs_folder/003_gent Feedback and Memory.txt\n",
            "  - /content/docs_folder/004_AGENT_Tools.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí• You‚Äôre now stepping into the **hybrid agent zone**, where Python handles mechanics (like reading files), and the LLM handles higher-order tasks like **summarization, classification, and reasoning.**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Extract filename + summarize contents\n",
        "\n",
        "Let‚Äôs break it down using the rule:\n",
        "\n",
        "| Step                        | Who should handle it         | Why                             |\n",
        "| --------------------------- | ---------------------------- | ------------------------------- |\n",
        "| List files                  | ‚úÖ Python                     | Fast, local, predictable        |\n",
        "| Read file contents          | ‚úÖ Python                     | Straightforward, cheap          |\n",
        "| Summarize contents          | ‚úÖ LLM                        | Needs understanding of language |\n",
        "| Group or tag files by theme | ‚úÖ LLM                        | Requires semantic inference     |\n",
        "| Move based on theme/summary | üß© LLM (plan) ‚Üí Python (act) | Hybrid action loop              |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Example Flow\n",
        "\n",
        "User says:\n",
        "\n",
        "> ‚ÄúSummarize all the files in `docs_folder` and group them by topic.‚Äù\n",
        "\n",
        "Your agent does:\n",
        "\n",
        "1. **List the files** ‚Üí Python\n",
        "2. **Read contents (first N tokens)** ‚Üí Python\n",
        "3. **For each file, send to LLM**:\n",
        "\n",
        "```python\n",
        "Summarize this file:\n",
        "Filename: lecture_01.txt\n",
        "Content:\n",
        "\"Today we covered memory management for LLMs...\"\n",
        "```\n",
        "\n",
        "4. LLM replies:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"summary\": \"Introduction to memory handling in agents.\",\n",
        "  \"topic\": \"Memory / LLM internals\"\n",
        "}\n",
        "```\n",
        "\n",
        "5. Your agent stores or displays those summaries, or moves the files into folders by topic.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why this is a smart LLM use\n",
        "\n",
        "LLMs shine when:\n",
        "\n",
        "* You need **semantic understanding** (‚ÄúWhat is this about?‚Äù)\n",
        "* You want **language tasks** like summarization, classification, insight extraction\n",
        "* The logic is **subjective or fuzzy**, not strict rules\n",
        "\n",
        "And they **struggle** when:\n",
        "\n",
        "* You could use a loop\n",
        "* You can `if`/`else` your way through it\n",
        "* You want performance or cost-efficiency\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OBJbVFWt-1hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs build a **File Summarizer Agent** that:\n",
        "\n",
        "1. Lists files from `/content/docs_folder`\n",
        "2. Reads the content of each file\n",
        "3. Sends it to the LLM with a prompt like:\n",
        "\n",
        "   > \"Summarize and identify the topic of this file.\"\n",
        "4. Stores the results in a structured format\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 1: List files + read contents\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "SOURCE_DIR = \"/content/docs_folder\"\n",
        "\n",
        "# List files\n",
        "file_paths = [\n",
        "    os.path.join(SOURCE_DIR, f)\n",
        "    for f in os.listdir(SOURCE_DIR)\n",
        "    if os.path.isfile(os.path.join(SOURCE_DIR, f))\n",
        "]\n",
        "\n",
        "print(f\"üìÅ Found {len(file_paths)} files:\")\n",
        "for file in file_paths:\n",
        "    print(\"  -\", os.path.basename(file))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 2: Read file content safely\n",
        "\n",
        "Let‚Äôs read a *limited* number of characters to keep the LLM input short and cost-efficient:\n",
        "\n",
        "```python\n",
        "def read_file_preview(file_path, max_chars=1500):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "            return content[:max_chars]\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 3: Create the summarization prompt\n",
        "\n",
        "```python\n",
        "def build_summary_prompt(filename, content):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a file summarization assistant. For each file, return a short summary and a topic label.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Filename: {filename}\\n\\nContent:\\n{content}\"}\n",
        "    ]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 4: Call the LLM for each file\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 5: Process all files and collect summaries\n",
        "\n",
        "```python\n",
        "summaries = []\n",
        "\n",
        "for path in file_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    content = read_file_preview(path)\n",
        "    prompt = build_summary_prompt(filename, content)\n",
        "\n",
        "    print(f\"\\nüìÑ Summarizing: {filename}...\")\n",
        "    summary_response = generate_response(prompt)\n",
        "    print(\"üß† LLM Summary:\\n\", summary_response)\n",
        "\n",
        "    summaries.append({\n",
        "        \"filename\": filename,\n",
        "        \"summary\": summary_response\n",
        "    })\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 6: Review or save summaries\n",
        "\n",
        "To view as a table (optional, for Colab):\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(summaries)\n",
        "df.head()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lU06O1OhIF0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import textwrap\n",
        "\n",
        "# ‚úÖ Step 1: List files + read contents\n",
        "SOURCE_DIR = \"/content/docs_folder\"\n",
        "\n",
        "# List files\n",
        "file_paths = [\n",
        "    os.path.join(SOURCE_DIR, f)\n",
        "    for f in os.listdir(SOURCE_DIR)\n",
        "    if os.path.isfile(os.path.join(SOURCE_DIR, f))\n",
        "]\n",
        "\n",
        "print(f\"üìÅ Found {len(file_paths)} files:\")\n",
        "for file in file_paths:\n",
        "    print(\"  -\", os.path.basename(file))\n",
        "\n",
        "# Read file content safely\n",
        "def read_file_preview(file_path, max_chars=1500):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "            return content[:max_chars]\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "\n",
        "# ‚úÖ Step 3: Create the summarization prompt\n",
        "def build_summary_prompt(filename, content):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a file summarization assistant. For each file, return a short summary and a topic label.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Filename: {filename}\\n\\nContent:\\n{content}\"}\n",
        "    ]\n",
        "\n",
        "# ‚úÖ Step 4: Call the LLM for each file\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ‚úÖ Step 5: Process all files and collect summaries\n",
        "\n",
        "summaries = []\n",
        "\n",
        "for path in file_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    content = read_file_preview(path)\n",
        "    prompt = build_summary_prompt(filename, content)\n",
        "\n",
        "    print(f\"\\nüìÑ Summarizing: {filename}...\")\n",
        "\n",
        "    summary_response = generate_response(prompt)\n",
        "\n",
        "    print(\"üß† LLM Summary:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(textwrap.fill(summary_response, width=80))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    summaries.append({\n",
        "        \"filename\": filename,\n",
        "        \"summary\": summary_response\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_DX1tbd-ho5",
        "outputId": "b32549bd-8ed4-401a-c01c-877c9f2e160b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Found 7 files:\n",
            "  - 001_PArse_the Response.txt\n",
            "  - 006_Agent Loop with Function Calling.txt\n",
            "  - 000_Prompting for Agents -GAIL.txt\n",
            "  - 002_Execute_the_Action.txt\n",
            "  - 005_Using Function Calling Capabilities with LLMs.txt\n",
            "  - 003_gent Feedback and Memory.txt\n",
            "  - 004_AGENT_Tools.txt\n",
            "\n",
            "üìÑ Summarizing: 001_PArse_the Response.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** The document explains the process of parsing responses generated by\n",
            "a language model (LLM). It outlines how to extract an action and its parameters\n",
            "from the output, specifically expecting a JSON format. The parsing is done using\n",
            "a provided function that checks for the presence and validity of an action block\n",
            "and handles errors accordingly.  **Topic Label:** LLM Response Parsing\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 006_Agent Loop with Function Calling.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** The document discusses simplifying the AI agent loop using function\n",
            "calling, which allows for structured responses from language models (LLMs). This\n",
            "approach reduces complexity in parsing responses, enabling direct interpretation\n",
            "and execution of function calls without the need for manual extraction from\n",
            "unstructured text. The article highlights how this method streamlines the agent\n",
            "loop through native support for structured outputs.  **Topic Label:** AI\n",
            "Development / Function Calling\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 000_Prompting for Agents -GAIL.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** The document outlines the concept of programmatic prompting for\n",
            "agents, focusing on the automatic sending of prompts to large language models\n",
            "(LLMs) and the importance of memory management in maintaining context during the\n",
            "agent's decision-making process. It includes an example of a function to\n",
            "interact with an LLM, highlighting the format of system and user messages used\n",
            "in the prompting process.  **Topic Label:** AI Agent Development\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 002_Execute_the_Action.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary**: The document outlines the execution phase of an agent's action\n",
            "processing, detailing how the agent executes specific functions based on a\n",
            "parsed tool name and its corresponding arguments. It provides examples of\n",
            "actions like listing files, reading a file, handling errors, and terminating the\n",
            "process, emphasizing the importance of this step in interacting with the\n",
            "environment and integrating feedback into the agent's memory.  **Topic Label**:\n",
            "Action Execution Logic\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 005_Using Function Calling Capabilities with LLMs.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** The document discusses the use of function calling capabilities in\n",
            "large language models (LLMs) for improving interactions with AI agents. It\n",
            "highlights the challenges of ensuring structured and reliable output from\n",
            "models, which often struggle with generating well-formed JSON. To address this,\n",
            "function calling APIs provide a solution by allowing the definition of tools\n",
            "using JSON Schema, ensuring that outputs are structured and predictable. The\n",
            "model can either return a structured function call or a standard text response,\n",
            "eliminating the need for complex prompt engineering.  **Topic Label:** AI and\n",
            "Machine Learning\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 003_gent Feedback and Memory.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** The document explains how an agent updates its memory after\n",
            "performing actions, maintaining a record of user interactions, actions taken,\n",
            "and their outcomes to enhance future decision-making. It details how the agent\n",
            "captures responses and action results through structured roles, ensuring\n",
            "continuity and dynamic refinement of behavior. Additionally, it mentions the\n",
            "importance of deciding whether to continue or terminate after each action based\n",
            "on task status.  **Topic Label:** Agent Memory Management\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÑ Summarizing: 004_AGENT_Tools.txt...\n",
            "üß† LLM Summary:\n",
            "------------------------------------------------------------\n",
            "**Summary:** This document discusses the design of an AI agent's tools,\n",
            "emphasizing the importance of clear definitions, structured metadata, and naming\n",
            "conventions. It includes an example of an AI agent that automates the\n",
            "documentation process for Python files by reading their content and writing\n",
            "corresponding documentation. Additionally, it outlines the need for structured\n",
            "tool definitions, illustrating how to define a basic tool in Python that lists\n",
            "Python files, and hints at using JSON Schema for parameter definitions.  **Topic\n",
            "Label:** AI Agent Tools and Development\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === üì¶ Imports ===\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# === üîë Load API Key ===\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# === üìÇ Step 1: List files ===\n",
        "SOURCE_DIR = \"/content/docs_folder\"\n",
        "\n",
        "file_paths = [\n",
        "    os.path.join(SOURCE_DIR, f)\n",
        "    for f in os.listdir(SOURCE_DIR)\n",
        "    if os.path.isfile(os.path.join(SOURCE_DIR, f))\n",
        "]\n",
        "\n",
        "# === üìÑ Step 2: Read preview of each file ===\n",
        "def read_file_preview(file_path, max_chars=1500):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()[:max_chars]\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "\n",
        "# === ü§ñ Step 3: Build summarization prompt ===\n",
        "def build_summary_prompt(filename, content):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a file summarization assistant. For each file, return a short summary AND a topic label in JSON format inside a markdown code block like this:\\n\\n```action\\n{\\n  \\\"summary\\\": \\\"...\\\",\\n  \\\"topic\\\": \\\"...\\\"\\n}\\n```\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Filename: {filename}\\n\\nContent:\\n{content}\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# === üì§ Step 4: Send to OpenAI ===\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# === üîç Step 5: Extract markdown block ===\n",
        "def extract_markdown_block(text: str, tag: str = \"action\") -> str:\n",
        "    pattern = rf\"```{tag}\\s*(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        raise ValueError(f\"Missing markdown block with tag '{tag}'\")\n",
        "\n",
        "# === üß† Step 6: Parse the structured response ===\n",
        "def parse_summary_response(response_text):\n",
        "    try:\n",
        "        block = extract_markdown_block(response_text, tag=\"action\")\n",
        "        return json.loads(block)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"summary\": response_text.strip(),\n",
        "            \"topic\": \"Unknown (parse failed)\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# === üîÅ Step 7: Process all files ===\n",
        "summaries = []\n",
        "\n",
        "for path in file_paths:\n",
        "    filename = os.path.basename(path)\n",
        "    content = read_file_preview(path)\n",
        "    prompt = build_summary_prompt(filename, content)\n",
        "\n",
        "    print(f\"\\nüìÑ Summarizing: {filename}...\")\n",
        "    llm_response = generate_response(prompt)\n",
        "\n",
        "    print(\"üß† Raw LLM Output:\")\n",
        "    print(textwrap.fill(llm_response, width=80))\n",
        "\n",
        "    parsed = parse_summary_response(llm_response)\n",
        "\n",
        "    print(\"\\n‚úÖ Parsed Result:\")\n",
        "    print(f\"  Summary: {parsed.get('summary')}\")\n",
        "    print(f\"  Topic:   {parsed.get('topic')}\")\n",
        "\n",
        "    summaries.append({\n",
        "        \"filename\": filename,\n",
        "        \"summary\": parsed.get(\"summary\"),\n",
        "        \"topic\": parsed.get(\"topic\"),\n",
        "        \"raw\": llm_response\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX9bDMPVIXDa",
        "outputId": "da46a89b-181f-4bfb-a658-b3bbe87eebf5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Summarizing: 001_PArse_the Response.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The document outlines a method for parsing responses\n",
            "from a language model to extract actionable items formatted in JSON. It\n",
            "emphasizes the importance of a structured output for execution, detailing a\n",
            "function to handle valid and invalid responses.\",   \"topic\": \"Response Parsing\"\n",
            "} ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The document outlines a method for parsing responses from a language model to extract actionable items formatted in JSON. It emphasizes the importance of a structured output for execution, detailing a function to handle valid and invalid responses.\n",
            "  Topic:   Response Parsing\n",
            "\n",
            "üìÑ Summarizing: 006_Agent Loop with Function Calling.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The document discusses how function calling can\n",
            "simplify the AI agent loop by allowing for native support of structured\n",
            "responses from large language models (LLMs). This innovation reduces complexity\n",
            "in parsing responses and handling actions, making it unnecessary to engineer\n",
            "strict output formats or validate errors from the model. The text includes a\n",
            "code snippet demonstrating how function calling can be implemented to streamline\n",
            "operations.\",   \"topic\": \"AI Development\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The document discusses how function calling can simplify the AI agent loop by allowing for native support of structured responses from large language models (LLMs). This innovation reduces complexity in parsing responses and handling actions, making it unnecessary to engineer strict output formats or validate errors from the model. The text includes a code snippet demonstrating how function calling can be implemented to streamline operations.\n",
            "  Topic:   AI Development\n",
            "\n",
            "üìÑ Summarizing: 000_Prompting for Agents -GAIL.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"This document discusses the concept of programmatic\n",
            "prompting for agents, which allows automated interaction with language models\n",
            "(LLMs) rather than manual input by humans. It outlines essential capabilities\n",
            "needed for agents, specifically programmatic prompting and memory management.\n",
            "The text provides a code example that demonstrates how to send prompts to an LLM\n",
            "and highlights the importance of the system message in setting the model's\n",
            "behavior.\",   \"topic\": \"AI/Automation\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: This document discusses the concept of programmatic prompting for agents, which allows automated interaction with language models (LLMs) rather than manual input by humans. It outlines essential capabilities needed for agents, specifically programmatic prompting and memory management. The text provides a code example that demonstrates how to send prompts to an LLM and highlights the importance of the system message in setting the model's behavior.\n",
            "  Topic:   AI/Automation\n",
            "\n",
            "üìÑ Summarizing: 002_Execute_the_Action.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The text describes the execution phase of an agent's\n",
            "actions where it parses responses to call specific functions based on a tool\n",
            "name and arguments. Each predefined tool corresponds to a particular function,\n",
            "allowing the agent to perform tasks like listing files, reading a file, or\n",
            "handling errors. The execution is crucial as it transforms decisions into actual\n",
            "interactions with the environment.\",   \"topic\": \"Agent Action Execution\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The text describes the execution phase of an agent's actions where it parses responses to call specific functions based on a tool name and arguments. Each predefined tool corresponds to a particular function, allowing the agent to perform tasks like listing files, reading a file, or handling errors. The execution is crucial as it transforms decisions into actual interactions with the environment.\n",
            "  Topic:   Agent Action Execution\n",
            "\n",
            "üìÑ Summarizing: 005_Using Function Calling Capabilities with LLMs.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The document discusses the use of LLM function calling\n",
            "APIs to facilitate structured execution and integration with AI agents. It\n",
            "highlights how these APIs produce predictable JSON outputs, enhancing the\n",
            "reliability of interactions while reducing the need for complex prompt\n",
            "engineering.\",   \"topic\": \"AI Function Calling\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The document discusses the use of LLM function calling APIs to facilitate structured execution and integration with AI agents. It highlights how these APIs produce predictable JSON outputs, enhancing the reliability of interactions while reducing the need for complex prompt engineering.\n",
            "  Topic:   AI Function Calling\n",
            "\n",
            "üìÑ Summarizing: 003_gent Feedback and Memory.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The document explains how an agent updates its memory\n",
            "after executing actions, retaining context and improving future decision-making.\n",
            "Memory is continuously updated with both the agent's responses and the action\n",
            "results in a structured format. Additionally, it discusses the importance of\n",
            "maintaining a history of exchanges for refining the agent's behavior and how the\n",
            "agent decides whether to continue or terminate its task based on the outcomes.\",\n",
            "\"topic\": \"Agent Memory and Feedback\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The document explains how an agent updates its memory after executing actions, retaining context and improving future decision-making. Memory is continuously updated with both the agent's responses and the action results in a structured format. Additionally, it discusses the importance of maintaining a history of exchanges for refining the agent's behavior and how the agent decides whether to continue or terminate its task based on the outcomes.\n",
            "  Topic:   Agent Memory and Feedback\n",
            "\n",
            "üìÑ Summarizing: 004_AGENT_Tools.txt...\n",
            "üß† Raw LLM Output:\n",
            "```action {   \"summary\": \"The document discusses how to effectively define tools\n",
            "for an AI agent, emphasizing the significance of naming, parameters, and\n",
            "structured metadata. It includes an example of an AI agent automating\n",
            "documentation for Python code by listing files and writing documentation. The\n",
            "document outlines steps for defining a tool using structured metadata and JSON\n",
            "schema for parameters.\",   \"topic\": \"AI Tool Development\" } ```\n",
            "\n",
            "‚úÖ Parsed Result:\n",
            "  Summary: The document discusses how to effectively define tools for an AI agent, emphasizing the significance of naming, parameters, and structured metadata. It includes an example of an AI agent automating documentation for Python code by listing files and writing documentation. The document outlines steps for defining a tool using structured metadata and JSON schema for parameters.\n",
            "  Topic:   AI Tool Development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs compare **what we did before (no parsing)** with **what we‚Äôre doing now (structured parsing)** ‚Äî and why this change matters so much in the context of building agents.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± **Previous version (no parsing):**\n",
        "\n",
        "### üß† LLM returned natural language like:\n",
        "\n",
        "```\n",
        "\"This file introduces concepts of agent memory. It covers how LLMs are stateless...\"\n",
        "```\n",
        "\n",
        "### ‚úÖ Code behavior:\n",
        "\n",
        "* Printed the LLM's response directly with `print()`\n",
        "* Stored it as a `\"summary\"` in a dictionary or list\n",
        "* Easy for **humans to read**\n",
        "* ‚ùå But difficult for the agent to do anything structured with\n",
        "\n",
        "---\n",
        "\n",
        "## üß© **Current version (with parsing):**\n",
        "\n",
        "### üß† LLM returns structured JSON *inside* a markdown code block:\n",
        "\n",
        "````markdown\n",
        "```action\n",
        "{\n",
        "  \"summary\": \"Overview of agent memory handling.\",\n",
        "  \"topic\": \"Memory Management\"\n",
        "}\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Code behavior:\n",
        "- Uses `extract_markdown_block()` to isolate the JSON\n",
        "- Uses `json.loads()` to turn it into a Python dictionary\n",
        "- Now we get **structured fields**:\n",
        "  - `summary = parsed[\"summary\"]`\n",
        "  - `topic = parsed[\"topic\"]`\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **Key Differences**\n",
        "\n",
        "| Feature | No Parsing (Old) | With Parsing (New) |\n",
        "|--------|------------------|---------------------|\n",
        "| LLM Output Format | Free-form text | Markdown-wrapped JSON |\n",
        "| Parsing | None | Extracted with regex, parsed with `json.loads()` |\n",
        "| Data Structure | Plain string | Dictionary with `\"summary\"` and `\"topic\"` |\n",
        "| Machine-readability | ‚ùå Not actionable | ‚úÖ Agent-ready |\n",
        "| Flexibility | Good for reading | Good for automation |\n",
        "| Error handling | Minimal | Can detect and recover from malformed output |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why Parsing Matters for Agents\n",
        "\n",
        "When you want your agent to:\n",
        "- **Group files by topic**\n",
        "- **Move or rename them**\n",
        "- **Search summaries**\n",
        "- **Build a dashboard**\n",
        "- **Chain summaries into downstream tasks**\n",
        "\n",
        "‚Ä¶ then **structured data is everything**.\n",
        "\n",
        "Parsing gives you **clear keys** (`\"summary\"`, `\"topic\"`, etc.) that your system can work with.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Final takeaway\n",
        "\n",
        "> **Unparsed text is great for humans. Parsed JSON is great for agents.**\n",
        "\n",
        "You‚Äôve just leveled up from ‚Äúprint the answer‚Äù to ‚Äúuse the answer.‚Äù That‚Äôs agent territory. üëè\n",
        "\n",
        "Would you like to now:\n",
        "- Export these summaries to a CSV?\n",
        "- Organize files by topic?\n",
        "- Chain this into a vector index for RAG?\n",
        "\n"
      ],
      "metadata": {
        "id": "FT2auiEkLQIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# üìÇ Base folder where we‚Äôll organize by topic\n",
        "TARGET_ROOT = \"/content/organized_files\"\n",
        "\n",
        "# Create root folder if not exists\n",
        "os.makedirs(TARGET_ROOT, exist_ok=True)\n",
        "\n",
        "for summary in summaries:\n",
        "    filename = summary[\"filename\"]\n",
        "    topic = summary[\"topic\"] or \"Uncategorized\"\n",
        "\n",
        "    # Clean topic string for use as a folder name\n",
        "    topic_folder = os.path.join(TARGET_ROOT, topic.replace(\" \", \"_\"))\n",
        "    os.makedirs(topic_folder, exist_ok=True)\n",
        "\n",
        "    # Build source and destination paths\n",
        "    source_path = os.path.join(SOURCE_DIR, filename)\n",
        "    target_path = os.path.join(topic_folder, filename)\n",
        "\n",
        "    try:\n",
        "        shutil.move(source_path, target_path)\n",
        "        print(f\"‚úÖ Moved '{filename}' ‚Üí {topic_folder}\")\n",
        "    except Exception as e:\n",
        "        print(f\"üö´ Failed to move '{filename}': {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6KjFk6IXAX",
        "outputId": "eb4e949c-9efd-413b-e94f-edafdeb23394"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Moved '001_PArse_the Response.txt' ‚Üí /content/organized_files/Response_Parsing\n",
            "‚úÖ Moved '006_Agent Loop with Function Calling.txt' ‚Üí /content/organized_files/AI_Development\n",
            "‚úÖ Moved '000_Prompting for Agents -GAIL.txt' ‚Üí /content/organized_files/AI/Automation\n",
            "‚úÖ Moved '002_Execute_the_Action.txt' ‚Üí /content/organized_files/Agent_Action_Execution\n",
            "‚úÖ Moved '005_Using Function Calling Capabilities with LLMs.txt' ‚Üí /content/organized_files/AI_Function_Calling\n",
            "‚úÖ Moved '003_gent Feedback and Memory.txt' ‚Üí /content/organized_files/Agent_Memory_and_Feedback\n",
            "‚úÖ Moved '004_AGENT_Tools.txt' ‚Üí /content/organized_files/AI_Tool_Development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_directory_tree(start_path, indent=\"\"):\n",
        "    for item in sorted(os.listdir(start_path)):\n",
        "        item_path = os.path.join(start_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"{indent}üìÅ {item}/\")\n",
        "            print_directory_tree(item_path, indent + \"    \")\n",
        "        else:\n",
        "            print(f\"{indent}üìÑ {item}\")\n",
        "\n",
        "print(\"\\nüì¶ Organized Folder Structure:\\n\")\n",
        "print_directory_tree(\"/content/organized_files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mXknsJIW9y",
        "outputId": "8b607af5-8505-4166-d2c3-c434d79841a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Organized Folder Structure:\n",
            "\n",
            "üìÅ AI/\n",
            "    üìÅ Automation/\n",
            "        üìÑ 000_Prompting for Agents -GAIL.txt\n",
            "üìÅ AI_Development/\n",
            "    üìÑ 006_Agent Loop with Function Calling.txt\n",
            "üìÅ AI_Function_Calling/\n",
            "    üìÑ 005_Using Function Calling Capabilities with LLMs.txt\n",
            "üìÅ AI_Tool_Development/\n",
            "    üìÑ 004_AGENT_Tools.txt\n",
            "üìÅ Agent_Action_Execution/\n",
            "    üìÑ 002_Execute_the_Action.txt\n",
            "üìÅ Agent_Memory_and_Feedback/\n",
            "    üìÑ 003_gent Feedback and Memory.txt\n",
            "üìÅ Response_Parsing/\n",
            "    üìÑ 001_PArse_the Response.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† **Concepts & Principles Learned**\n",
        "\n",
        "### 1. **LLMs are Stateless by Default**\n",
        "\n",
        "* LLMs don‚Äôt retain memory across calls.\n",
        "* Developers must manage the **conversation history** manually to provide context.\n",
        "* You learned to alternate `{\"role\": \"user\"}` and `{\"role\": \"assistant\"}` entries to simulate memory.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Stateful Memory in Agent Loops**\n",
        "\n",
        "* Built a manual memory buffer to retain past interactions.\n",
        "* Saw how memory growth hits **token limits**, requiring summarization or truncation strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Parsing: The Core of Agent Execution**\n",
        "\n",
        "* LLMs must return structured responses so agents can act.\n",
        "* Markdown-wrapped JSON (e.g. `action { ... } `) is a common and powerful pattern.\n",
        "* You implemented:\n",
        "\n",
        "  * `extract_markdown_block()`\n",
        "  * `parse_action()` with fallback error handling\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Rule of Thumb for Agents**\n",
        "\n",
        "> ‚ö°Ô∏è ‚ÄúLLMs tell you *what* to do. Python handles *how* to do it.‚Äù\n",
        "\n",
        "* This principle guided your design choices.\n",
        "* Python handled reading, moving, listing files.\n",
        "* LLM handled intent extraction, summarization, and classification.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **Hands-On Agent Components Built**\n",
        "\n",
        "### ‚úÖ Calculator Agent\n",
        "\n",
        "* Used structured LLM output to perform arithmetic\n",
        "* Parsed tool name + arguments\n",
        "* Demonstrated the need for markdown + JSON parsing\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ File Summarization Agent\n",
        "\n",
        "* Read files from `/content/docs_folder`\n",
        "* Sent contents to an LLM with a summarization prompt\n",
        "* Parsed responses into:\n",
        "\n",
        "  * `\"summary\"`\n",
        "  * `\"topic\"`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ File Organizer Agent\n",
        "\n",
        "* Took the parsed topic from each summary\n",
        "* Automatically created subfolders by topic\n",
        "* Moved files into their topic folders using `shutil.move()`\n",
        "* Printed out the final folder tree for verification\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Now Understand Deeply\n",
        "\n",
        "| Skill                       | Description                                                |\n",
        "| --------------------------- | ---------------------------------------------------------- |\n",
        "| ‚úÖ Prompt Engineering        | You structured prompts for structured responses            |\n",
        "| ‚úÖ LLM-Oriented Parsing      | Extracting actionable JSON from LLM responses              |\n",
        "| ‚úÖ Memory Management         | Simulating stateful context across chat calls              |\n",
        "| ‚úÖ Language vs Code Boundary | Using LLMs for fuzzy logic, Python for precise execution   |\n",
        "| ‚úÖ File I/O + Agent Actions  | Integrated real-world Python file ops into an agent loop   |\n",
        "| ‚úÖ Agent Loops               | Built pipelines where the agent perceives ‚Üí reasons ‚Üí acts |\n",
        "\n"
      ],
      "metadata": {
        "id": "Pfhr6oujMzDO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-vevxL0IW69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ba8Lv_7zIW4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}