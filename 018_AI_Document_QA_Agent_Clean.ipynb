{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVI/yJh/3qSRA/ykMGCABm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/018_AI_Document_QA_Agent_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß† AI Document QA Agent with Automated Evaluation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_tTBU85Qy7PQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uVN6rBYlFF0l"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import pdfplumber\n",
        "import gradio as gr\n",
        "from pydantic import BaseModel\n",
        "from pydantic import ValidationError\n",
        "import textwrap\n",
        "import re\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Grab API key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ùå OPENAI_API_KEY not found in environment. Make sure your .env file is loaded correctly.\")\n",
        "\n",
        "# Set up OpenAI client\n",
        "openai = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "mANKBrseFNZh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Plumber\n"
      ],
      "metadata": {
        "id": "acCN1OCuIzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from PDF\n",
        "document_text = \"\"\n",
        "with pdfplumber.open(\"/content/ai-in-the-enterprise.pdf\") as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            document_text += text + \"\\n\"\n",
        "\n",
        "# Remove extra newlines and normalize spacing\n",
        "document_text = re.sub(r\"\\n{2,}\", \"\\n\", document_text)\n",
        "document_text = re.sub(r\"[ \\t]+\", \" \", document_text)\n",
        "document_text = document_text.strip()\n",
        "\n",
        "print(document_text[:997])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AM5Iz5SIYx6",
        "outputId": "62356b16-77ea-45f4-a5c8-b1b17c42d59a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI in the\n",
            "Enterprise\n",
            "Lessons from seven frontier companies\n",
            "Contents\n",
            "A new way to work 3\n",
            "Executive summary 5\n",
            "Seven lessons for enterprise AI adoption\n",
            "Start with evals 6\n",
            "Embed AI into your products 9\n",
            "Start now and invest early 11\n",
            "Customize and fine-tune your models 13\n",
            "Get AI in the hands of experts 16\n",
            "Unblock your developers 18\n",
            "Set bold automation goals 21\n",
            "Conclusion 22\n",
            "More resources 24\n",
            "2 AI in the Enterprise\n",
            "A new way\n",
            "to work\n",
            "As an AI research and deployment company, OpenAI prioritizes partnering with global companies\n",
            "because our models will increasingly do their best work with sophisticated, complex,\n",
            "interconnected workflows and systems.\n",
            "We‚Äôre seeing AI deliver significant, measurable improvements on three fronts:\n",
            "01 Workforce performance Helping people deliver higher-quality outputs in shorter\n",
            "time frames.\n",
            "02 Automating routine Freeing people from repetitive tasks so they can focus\n",
            "operations on adding value.\n",
            "03 Powering products By delivering more relevant and responsive customer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt"
      ],
      "metadata": {
        "id": "1lruPruJL61F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_title = \"AI in the Enterprise\"\n",
        "source = \"OpenAI\"\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "You are acting as an expert assistant representing the contents of the report titled \"{doc_title}\" published by {source}.\n",
        "\n",
        "Your role is to answer user questions about enterprise AI, based solely on this document. Be helpful, clear, and professional.\n",
        "\n",
        "Only answer questions that are addressed in the report. If something isn‚Äôt covered, say so.\n",
        "\n",
        "## Full Report:\n",
        "{document_text}\n",
        "\n",
        "With this context, please answer the user's questions as accurately as possible.\n",
        "\"\"\"\n",
        "\n",
        "def chat(message, history=None, temperature=0.3):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "gr.ChatInterface(chat, type=\"messages\").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Tkijnmc0JzNY",
        "outputId": "4bd5f98c-165e-4538-d429-3f0c369e4e3f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8851cedb278d48e295.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8851cedb278d48e295.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã **Evaluator Agent Summary**"
      ],
      "metadata": {
        "id": "mguC9P8atoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, ValidationError\n",
        "import textwrap\n",
        "\n",
        "# ‚úÖ Step 1: Evaluation Schema using Pydantic\n",
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str\n",
        "\n",
        "# ‚úÖ Step 2: Evaluation system prompt\n",
        "company_name = \"OpenAI\"\n",
        "doc_title = \"AI in the Enterprise\"\n",
        "\n",
        "evaluator_system_prompt = f\"\"\"You are an evaluator that decides whether a response to a user‚Äôs question is acceptable.\n",
        "You are provided with a conversation between a User and an Assistant Agent.\n",
        "The Agent is representing the contents of the report titled \"{doc_title}\" published by {company_name},\n",
        "and is answering questions based solely on the document.\n",
        "\n",
        "Your task is to evaluate whether the Agent's latest response:\n",
        "- Accurately reflects the content of the report\n",
        "- Avoids speculation or unrelated information\n",
        "- Communicates clearly and professionally\n",
        "- Would be helpful to a business or technical decision-maker\n",
        "\n",
        "If the Agent‚Äôs answer is accurate and relevant to the report, mark it acceptable.\n",
        "If it is off-topic, unclear, overly speculative, or unhelpful, mark it unacceptable and explain why.\n",
        "\n",
        "You have access to the full text of the document for context.\n",
        "## Document Contents:\n",
        "{document_text}\n",
        "\n",
        "With this context, please evaluate the latest response.\n",
        "\"\"\"\n",
        "\n",
        "# ‚úÖ Step 3: User prompt construction for evaluator\n",
        "def evaluator_user_prompt(reply, message, history):\n",
        "    user_prompt = f\"Here's the conversation between the User and the Agent:\\n\\n{history}\\n\\n\"\n",
        "    user_prompt += f\"User's latest question:\\n\\n{message}\\n\\n\"\n",
        "    user_prompt += f\"Agent's response:\\n\\n{reply}\\n\\n\"\n",
        "    user_prompt += (\n",
        "        \"Please evaluate the response. Return your answer in the following JSON format:\\n\"\n",
        "        '{\\n  \"is_acceptable\": true | false,\\n  \"feedback\": \"Brief explanation of your judgment\"\\n}'\n",
        "    )\n",
        "    return user_prompt\n",
        "\n",
        "# ‚úÖ Step 4: Evaluation function\n",
        "def evaluate(reply, message, history) -> Evaluation:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
        "    ]\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = Evaluation.model_validate_json(response.choices[0].message.content)\n",
        "        return parsed\n",
        "    except ValidationError as e:\n",
        "        print(\"‚ùå Failed to validate Evaluation response:\", e)\n",
        "        return Evaluation(is_acceptable=False, feedback=\"Could not parse the evaluation response.\")\n",
        "\n",
        "# ‚úÖ Step 5: Compose message history and user input\n",
        "history = []\n",
        "message = \"Why is it important to start with evaluations when adopting AI in a company?\"\n",
        "\n",
        "# ‚úÖ Step 6: Generate agent reply\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=0.3)\n",
        "reply = response.choices[0].message.content.strip()\n",
        "\n",
        "# ‚úÖ Step 7: Print Agent Reply neatly\n",
        "print(\"\\nüí¨ Agent Reply:\\n\")\n",
        "print(textwrap.fill(reply, width=80))\n",
        "\n",
        "# ‚úÖ Step 8: Run Evaluation and print result\n",
        "evaluation = evaluate(reply, message, history)\n",
        "\n",
        "print(\"\\nüß™ Evaluation Result:\")\n",
        "print(\"‚úÖ Acceptable:\" if evaluation.is_acceptable else \"‚ùå Not acceptable.\")\n",
        "print(\"üí¨ Feedback:\\n\")\n",
        "print(textwrap.fill(evaluation.feedback, width=80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJcSK6XohqC",
        "outputId": "6f3d19c9-4a3c-4b85-8ea7-b1a53cf092fd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí¨ Agent Reply:\n",
            "\n",
            "Starting with evaluations is important when adopting AI in a company because it\n",
            "provides a systematic process to measure how AI models perform against specific\n",
            "use cases. This rigorous evaluation process helps ensure quality and safety by\n",
            "continuously improving AI-enabled processes with expert feedback at every step.\n",
            "For example, Morgan Stanley conducted intensive evaluations to determine the\n",
            "effectiveness of AI applications, which ultimately led to increased efficiency\n",
            "and better insights for their financial advisors. Evaluations help build\n",
            "confidence in the AI solutions being implemented and ensure they meet the\n",
            "necessary benchmarks for accuracy, relevance, and compliance.\n",
            "\n",
            "üß™ Evaluation Result:\n",
            "‚úÖ Acceptable:\n",
            "üí¨ Feedback:\n",
            "\n",
            "The Agent's response accurately reflects the content of the report by explaining\n",
            "the importance of starting with evaluations in AI adoption. It highlights the\n",
            "systematic process for measuring AI model performance, the role of expert\n",
            "feedback, and provides a relevant example from Morgan Stanley, which aligns with\n",
            "the report's emphasis on evaluations for ensuring quality and safety.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tis *is* a great coincidence ‚Äî and a very meta one at that!\n",
        "\n",
        "You're evaluating an agent **about evaluation**, based on a document that says **evaluation is the first and most critical step** for any successful AI adoption. It's like you're building a self-aware agent that's following its own advice in real time.\n",
        "\n",
        "### üí° This Moment Captures the Power of Agents:\n",
        "\n",
        "* ‚úÖ **Structured inputs**: Your agent references a trusted document.\n",
        "* ‚úÖ **Validated outputs**: Your evaluator enforces quality and alignment.\n",
        "* ‚úÖ **Human-aligned behavior**: Clear, relevant, actionable responses.\n",
        "* ‚úÖ **Business relevance**: Mirrors what enterprises need most ‚Äî explainability, clarity, and grounded reasoning.\n",
        "\n",
        "### üìù You Might Even Add This to Your Notebook:\n",
        "\n",
        "> *\"In an ironic but fitting twist, our evaluator-approved agent highlighted the importance of starting with evaluations ‚Äî a lesson lifted directly from OpenAI‚Äôs own recommendations on enterprise AI strategy. This validates both the approach and the agent design itself.\"*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "17zLRihF_xcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    # Use your full document system prompt\n",
        "    system = system_prompt\n",
        "\n",
        "    # Step 1: Compose full prompt with system, history, and user message\n",
        "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    # Step 2: Get agent reply\n",
        "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 3: Evaluate reply quality\n",
        "    evaluation = evaluate(reply, message, history)\n",
        "\n",
        "    # Step 4: Retry if response is unacceptable\n",
        "    if evaluation.is_acceptable:\n",
        "        print(\"‚úÖ Passed evaluation ‚Äì returning reply\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed evaluation ‚Äì retrying...\")\n",
        "        print(\"üí¨ Feedback:\", evaluation.feedback)\n",
        "        reply = rerun(reply, message, history, evaluation.feedback)\n",
        "\n",
        "    return reply\n",
        "\n",
        "def rerun(previous_reply, message, history, feedback):\n",
        "    correction_prompt = f\"The last reply was not acceptable because: {feedback}. Please revise and improve it.\"\n",
        "    updated_history = history + [{\"role\": \"assistant\", \"content\": previous_reply}]\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + updated_history + [{\"role\": \"user\", \"content\": correction_prompt}]\n",
        "\n",
        "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "history = []\n",
        "message = \"Why is it important to start with evaluations when adopting AI in a company?\"\n",
        "response = chat(message, history)\n",
        "print(\"\\nüì• Final Agent Reply:\\n\" + \"-\"*60)\n",
        "print(textwrap.fill(response, width=70))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj9EA1KiohiN",
        "outputId": "db110e16-f48c-48a5-9955-a0b10210f0ca"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Passed evaluation ‚Äì returning reply\n",
            "\n",
            "üì• Final Agent Reply:\n",
            "------------------------------------------------------------\n",
            "Starting with evaluations is crucial when adopting AI in a company\n",
            "because it establishes a systematic process to measure how AI models\n",
            "perform against specific use cases. Evaluations, or \"evals,\" help\n",
            "ensure quality and safety by providing rigorous, structured\n",
            "assessments of model outputs against benchmarks. This process not only\n",
            "aids in validating and testing the AI but also facilitates continuous\n",
            "improvement of AI-enabled processes with expert feedback.  By\n",
            "conducting evaluations, companies can build confidence in their AI\n",
            "applications, ensuring they deliver accurate, relevant, and compliant\n",
            "results. This foundation allows organizations to roll out use cases\n",
            "into production effectively, thereby optimizing workforce performance\n",
            "and delivering greater value from AI initiatives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ‚úÖ **Final Agent Architecture Overview**\n",
        "\n",
        "#### `chat(message, history)`\n",
        "\n",
        "* **Inputs**: A user question and chat history.\n",
        "* **System prompt**: Sets the document-grounded context (OpenAI's enterprise report).\n",
        "* **Step 1**: Composes the full message payload using current message and history.\n",
        "* **Step 2**: Gets a response from the model.\n",
        "* **Step 3**: Evaluates the response using a Pydantic-based evaluator.\n",
        "* **Step 4**: If the response is unacceptable, it routes to `rerun()` for refinement.\n",
        "* **Returns**: Final agent response, guaranteed to pass evaluation or retry.\n",
        "\n",
        "#### `evaluate(...)`\n",
        "\n",
        "* Uses a second OpenAI call to determine if the agent's response:\n",
        "\n",
        "  * Matches the source material\n",
        "  * Communicates professionally\n",
        "  * Avoids hallucination or speculation\n",
        "* Returns a structured `Evaluation` object (Pydantic)\n",
        "\n",
        "#### `rerun(...)`\n",
        "\n",
        "* Injects evaluation feedback into a follow-up user message (correction prompt)\n",
        "* Adds the previous bad response to history\n",
        "* Re-queries the model with explicit instruction to improve its answer\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Is Powerful\n",
        "\n",
        "* You‚Äôve built a **closed-loop QA agent**:\n",
        "\n",
        "  * Answer grounded in a document ‚úÖ\n",
        "  * Evaluated by a second layer ‚úÖ\n",
        "  * Automatically retried if necessary ‚úÖ\n",
        "* This mirrors **real-world enterprise AI safety** pipelines, where:\n",
        "\n",
        "  * LLMs must pass quality gates\n",
        "  * Output must reflect source truth\n",
        "  * Risk of hallucination must be minimized\n",
        "\n",
        "---\n",
        "\n",
        "This single-document QA agent has powerful applications in **business scenarios** where people need accurate, conversational access to specific documents. Here are several **real-world use cases**:\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ 1. **Policy & Compliance Assistants**\n",
        "\n",
        "**Use case:** Help employees or partners understand lengthy legal or compliance documents.\n",
        "\n",
        "* **Example:** ‚ÄúWhat are the guidelines for data retention under our policy?‚Äù\n",
        "* **Source doc:** Company data privacy policy, employee handbook, or compliance manual\n",
        "* **Benefit:** Reduces legal exposure and improves policy comprehension\n",
        "\n",
        "---\n",
        "\n",
        "### üìÑ 2. **Product Manual or Technical Guide Q\\&A**\n",
        "\n",
        "**Use case:** Allow customers or support reps to ask questions about complex products.\n",
        "\n",
        "* **Example:** ‚ÄúHow do I reset the device to factory settings?‚Äù\n",
        "* **Source doc:** Product manual or installation guide\n",
        "* **Benefit:** Reduces support load, improves customer experience\n",
        "\n",
        "---\n",
        "\n",
        "### üíº 3. **Internal Knowledge Assistants**\n",
        "\n",
        "**Use case:** Give employees conversational access to strategy docs, training materials, etc.\n",
        "\n",
        "* **Example:** ‚ÄúWhat are our goals for Q3 according to the leadership playbook?‚Äù\n",
        "* **Source doc:** Strategy memo, training deck, OKR document\n",
        "* **Benefit:** Faster onboarding, better alignment\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 4. **Research Report Explainers**\n",
        "\n",
        "**Use case:** Let stakeholders ask questions about a market research report or whitepaper.\n",
        "\n",
        "* **Example:** ‚ÄúWhat trends are mentioned in the Asia-Pacific region?‚Äù\n",
        "* **Source doc:** Industry report, investment brief, analyst whitepaper\n",
        "* **Benefit:** Increases the utility and reach of high-value research\n",
        "\n",
        "---\n",
        "\n",
        "### üìÉ 5. **RFP / Proposal Q\\&A Agents**\n",
        "\n",
        "**Use case:** Help teams prepare or review large RFP responses.\n",
        "\n",
        "* **Example:** ‚ÄúDo we meet the requirement for cybersecurity certifications?‚Äù\n",
        "* **Source doc:** 100-page RFP response PDF\n",
        "* **Benefit:** Saves hours of review time and reduces errors\n",
        "\n",
        "---\n",
        "\n",
        "### üèõÔ∏è 6. **Public Sector Transparency**\n",
        "\n",
        "**Use case:** Citizens ask questions about legislation, budgets, or government reports.\n",
        "\n",
        "* **Example:** ‚ÄúWhat is the allocated budget for renewable energy in this bill?‚Äù\n",
        "* **Source doc:** City or national legislation PDF\n",
        "* **Benefit:** Promotes accountability and citizen engagement\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úçÔ∏è Bonus: Combine with Feedback Loop\n",
        "\n",
        "You could combine this setup with:\n",
        "\n",
        "* ‚úÖ Evaluation agent (already done)\n",
        "* üì® Email summaries to stakeholders\n",
        "* üìä Visual dashboards\n",
        "\n",
        "To turn this into a full **information agent pipeline.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vzVIIt8Az5Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4jG9QFKJzK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JL3WaLiJzHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}