{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWPC/dgVyQ2ttLRicsh4ns",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/038_JW_Modular_AgentDesign_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 🧩 Agent Design — Compact Recipe (GAME: Goals · Actions · Memory · Environment)\n",
        "\n",
        "> What you’ll get below:\n",
        ">\n",
        "> 1. tiny types,\n",
        "> 2. `Action` + `ActionRegistry`,\n",
        "> 3. a safe-ish `Environment`,\n",
        "> 4. an `AgentLanguage` (fenced-JSON variant),\n",
        "> 5. a couple of file tools,\n",
        "> 6. wire it all into your `Agent` (the class you pasted),\n",
        "> 7. a one-liner to run.\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Minimal types\n",
        "\n",
        "```python\n",
        "from typing import List, Dict, Any, Callable, Protocol\n",
        "import json, os, re, time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Aliases\n",
        "Prompt = List[Dict[str, str]]\n",
        "\n",
        "@dataclass\n",
        "class Goal:\n",
        "    text: str\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self, max_events: int = 50):\n",
        "        self.events: List[Dict[str, Any]] = []\n",
        "        self.max_events = max_events\n",
        "\n",
        "    def add_memory(self, item: Dict[str, Any]):\n",
        "        self.events.append(item)\n",
        "        # keep it small\n",
        "        if len(self.events) > self.max_events:\n",
        "            self.events = self.events[-self.max_events:]\n",
        "\n",
        "    def to_text(self) -> str:\n",
        "        # simple linearization for prompts; keep it compact\n",
        "        lines = []\n",
        "        for e in self.events[-20:]:\n",
        "            role = e.get(\"type\", \"note\")\n",
        "            content = e.get(\"content\", \"\")\n",
        "            lines.append(f\"{role.upper()}: {content}\")\n",
        "        return \"\\n\".join(lines)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Actions (definition + registry)\n",
        "\n",
        "```python\n",
        "class Action:\n",
        "    def __init__(self, name: str, function: Callable, description: str, parameters: Dict[str, Any], terminal: bool=False):\n",
        "        self.name = name\n",
        "        self.function = function          # Python callable to run\n",
        "        self.description = description    # short blurb for the model\n",
        "        self.parameters = parameters      # JSON Schema for args\n",
        "        self.terminal = terminal          # if True, ends the loop\n",
        "\n",
        "    def to_openai_tool(self) -> Dict[str, Any]:\n",
        "        # If you later switch to tool-calling, you'll use this.\n",
        "        return {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": self.name,\n",
        "                \"description\": self.description,\n",
        "                \"parameters\": self.parameters,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def execute(self, **args):\n",
        "        return self.function(**args)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions: Dict[str, Action] = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self.actions:\n",
        "            raise ValueError(f\"Action '{action.name}' already registered\")\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    def get_action(self, name: str):\n",
        "        return self.actions.get(name)\n",
        "\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self.actions.values())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Environment (safe execution gateway)\n",
        "\n",
        "```python\n",
        "class Environment:\n",
        "    \"\"\"\n",
        "    Central place for policy, safety, logging, and normalized results.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dir: str | None = None, allowed_exts={\".txt\", \".md\"}, timeout_s: float = 10.0):\n",
        "        self.base_dir = os.path.abspath(base_dir) if base_dir else None\n",
        "        self.allowed_exts = set(allowed_exts) if allowed_exts else set()\n",
        "        self.timeout_s = timeout_s\n",
        "\n",
        "    def _safe_path(self, filename: str) -> str:\n",
        "        full = os.path.abspath(os.path.join(self.base_dir, filename)) if self.base_dir else os.path.abspath(filename)\n",
        "        if self.base_dir and not full.startswith(self.base_dir + os.sep):\n",
        "            raise PermissionError(\"Path traversal blocked.\")\n",
        "        if self.allowed_exts:\n",
        "            _, ext = os.path.splitext(full)\n",
        "            if ext.lower() not in self.allowed_exts:\n",
        "                raise PermissionError(f\"Extension {ext} not allowed.\")\n",
        "        return full\n",
        "\n",
        "    def execute_action(self, action: Action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start = time.time()\n",
        "        # simple semantic guardrails for file-like tools\n",
        "        guarded_args = dict(args)\n",
        "        if action.name in {\"read_file\", \"search_in_file\"} and \"file_name\" in guarded_args:\n",
        "            guarded_args[\"file_name\"] = self._safe_path(guarded_args[\"file_name\"])\n",
        "\n",
        "        try:\n",
        "            data = action.execute(**guarded_args)\n",
        "            return {\"ok\": True, \"action\": action.name, \"args\": guarded_args, \"data\": data, \"ms\": int((time.time()-start)*1000)}\n",
        "        except Exception as e:\n",
        "            return {\"ok\": False, \"action\": action.name, \"args\": guarded_args, \"error\": str(e), \"ms\": int((time.time()-start)*1000)}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3) AgentLanguage (prompt builder + response parser)\n",
        "\n",
        "This version uses the **fenced-JSON** protocol you learned (easy to swap to OpenAI tool-calling later).\n",
        "\n",
        "````python\n",
        "class AgentLanguage:\n",
        "    \"\"\"\n",
        "    Defines the protocol for speaking with the LLM:\n",
        "    - construct_prompt: how to ask\n",
        "    - parse_response:   how to interpret\n",
        "    \"\"\"\n",
        "    block_example = (\n",
        "        '{\"schema_version\":1,'\n",
        "        '\"tool\":\"<tool-name|final_answer>\",'\n",
        "        '\"args\":{ /* key: value */ },'\n",
        "        '\"terminal\": <true|false>}'\n",
        "    )\n",
        "\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> Prompt:\n",
        "        tools_list = \"\\n\".join(\n",
        "            f\"- {a.name}: {a.description} | params: {list(a.parameters.get('properties', {}).keys())}\"\n",
        "            for a in actions\n",
        "        )\n",
        "        sys = (\n",
        "            \"You are an agent planner. \"\n",
        "            \"Reply ONLY with a single fenced code block labeled action containing STRICT JSON exactly like:\\n\"\n",
        "            f\"```action\\n{self.block_example}\\n```\\n\"\n",
        "            \"No extra text.\"\n",
        "        )\n",
        "        user = (\n",
        "            \"GOALS:\\n\" + \"\\n\".join(f\"- {g.text}\" for g in goals) +\n",
        "            \"\\n\\nTOOLS:\\n\" + tools_list +\n",
        "            \"\\n\\nMEMORY (recent):\\n\" + memory.to_text() +\n",
        "            \"\\n\\nDecide the next step.\"\n",
        "        )\n",
        "        return [{\"role\":\"system\",\"content\":sys}, {\"role\":\"user\",\"content\":user}]\n",
        "\n",
        "    def parse_response(self, response_text: str) -> Dict[str, Any]:\n",
        "        m = re.search(r\"```action(?:\\s+json)?\\s*(\\{.*?\\})\\s*```\", response_text, re.DOTALL | re.IGNORECASE)\n",
        "        if not m:\n",
        "            raise ValueError(\"Missing ```action``` JSON block.\")\n",
        "        try:\n",
        "            data = json.loads(m.group(1))\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Invalid JSON in action block: {e}\")\n",
        "        # minimal schema checks\n",
        "        if \"tool\" not in data or \"args\" not in data:\n",
        "            raise ValueError(\"Missing 'tool' or 'args' in action JSON.\")\n",
        "        if \"terminal\" not in data:\n",
        "            data[\"terminal\"] = False\n",
        "        return data\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Tools (Python functions) + registration\n",
        "\n",
        "```python\n",
        "# --- sample tools ---\n",
        "def list_files() -> list[str]:\n",
        "    return sorted([f for f in os.listdir(\".\") if os.path.isfile(f)])\n",
        "\n",
        "def read_file(file_name: str) -> dict:\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {\"file_name\": file_name, \"content\": f.read()}\n",
        "\n",
        "def search_in_file(file_name: str, search_term: str) -> dict:\n",
        "    matches = []\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, start=1):\n",
        "            if search_term in line:\n",
        "                matches.append({\"line\": i, \"text\": line.rstrip(\"\\n\")})\n",
        "    return {\"file_name\": file_name, \"matches\": matches}\n",
        "\n",
        "# --- a terminal action so the agent can stop ---\n",
        "def final_answer(text: str) -> dict:\n",
        "    return {\"answer\": text}\n",
        "\n",
        "# --- register them ---\n",
        "registry = ActionRegistry()\n",
        "\n",
        "registry.register(Action(\n",
        "    name=\"list_files\",\n",
        "    function=list_files,\n",
        "    description=\"List all files in the current working directory.\",\n",
        "    parameters={\"type\":\"object\", \"properties\":{}, \"required\":[]}\n",
        "))\n",
        "\n",
        "registry.register(Action(\n",
        "    name=\"read_file\",\n",
        "    function=read_file,\n",
        "    description=\"Read the contents of a text file.\",\n",
        "    parameters={\"type\":\"object\",\n",
        "                \"properties\":{\"file_name\":{\"type\":\"string\"}},\n",
        "                \"required\":[\"file_name\"]}\n",
        "))\n",
        "\n",
        "registry.register(Action(\n",
        "    name=\"search_in_file\",\n",
        "    function=search_in_file,\n",
        "    description=\"Find lines containing a term in a file.\",\n",
        "    parameters={\"type\":\"object\",\n",
        "                \"properties\":{\"file_name\":{\"type\":\"string\"},\n",
        "                              \"search_term\":{\"type\":\"string\"}},\n",
        "                \"required\":[\"file_name\",\"search_term\"]}\n",
        "))\n",
        "\n",
        "registry.register(Action(\n",
        "    name=\"final_answer\",\n",
        "    function=final_answer,\n",
        "    description=\"Return the final answer to the user and stop.\",\n",
        "    parameters={\"type\":\"object\",\n",
        "                \"properties\":{\"text\":{\"type\":\"string\"}},\n",
        "                \"required\":[\"text\"]},\n",
        "    terminal=True\n",
        "))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5) LLM caller (dependency-injected)\n",
        "\n",
        "You can start with a stub (for local testing) and swap in your real OpenAI call later.\n",
        "\n",
        "````python\n",
        "# --- stub for local testing (always proposes to list files then stop) ---\n",
        "def fake_generate_response(prompt: Prompt) -> str:\n",
        "    # Always choose list_files first (toy logic)\n",
        "    return \"\"\"```action\n",
        "{\"schema_version\":1,\"tool\":\"list_files\",\"args\":{},\"terminal\":false}\n",
        "```\"\"\"\n",
        "\n",
        "# Example OpenAI wiring (uncomment & adapt when ready):\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI()\n",
        "# def openai_generate_response(prompt: Prompt) -> str:\n",
        "#     resp = client.chat.completions.create(\n",
        "#         model=\"gpt-4o-mini\",\n",
        "#         messages=prompt,\n",
        "#         max_tokens=400\n",
        "#     )\n",
        "#     return resp.choices[0].message.content\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Plug into **your Agent** and run\n",
        "\n",
        "```python\n",
        "# Instantiate pieces (Dependency Injection)\n",
        "goals = [Goal(\"Answer the user's request using available tools\"),\n",
        "         Goal(\"Stop with final_answer when done\")]\n",
        "\n",
        "agent_language = AgentLanguage()\n",
        "env = Environment(base_dir=\".\", allowed_exts={\".txt\", \".md\"})\n",
        "generate = fake_generate_response  # swap to openai_generate_response when ready\n",
        "\n",
        "# Use your Agent class exactly as pasted\n",
        "# (make sure it's already in the notebook above this cell)\n",
        "agent = Agent(goals, agent_language, registry, generate, env)\n",
        "\n",
        "# Kick off a run\n",
        "mem = agent.run(\"What files are here, and then stop with a final answer?\")\n",
        "```\n",
        "\n",
        "> When you swap in the real LLM, it should return an `action` JSON block.\n",
        "> After a tool or two, have it return:\n",
        ">\n",
        "> ```action\n",
        "> {\"schema_version\":1,\"tool\":\"final_answer\",\"args\":{\"text\":\"...\"},\"terminal\":true}\n",
        "> ```\n",
        ">\n",
        "> The loop will stop because `final_answer` has `terminal=True`.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) When you’re ready for production-ish upgrades\n",
        "\n",
        "* **Swap to OpenAI Tool-Calling**: change `AgentLanguage` to build `tools=[a.to_openai_tool()]` and parse `message.tool_calls` instead of fenced JSON.\n",
        "* **Validation**: add Pydantic models per action and validate args before `execute`.\n",
        "* **Retry on parse**: if `parse_response` fails, append a corrective message and retry once at `temperature=0`.\n",
        "* **Observability**: log `prompt → action → result (ok/ms)` in the environment.\n",
        "* **Memory control**: summarize occasionally; or keep only the last N steps.\n",
        "\n",
        "---\n",
        "\n",
        "### TL;DR flow (keep this mental model)\n",
        "\n",
        "```\n",
        "Goals + Memory + Actions --(AgentLanguage.construct_prompt)--> LLM\n",
        "LLM --> {\"tool\": \"...\", \"args\": {...}, \"terminal\": ...}  (fenced JSON)\n",
        "Agent --> Registry.get_action(name)\n",
        "Environment --> execute safely --> {ok,data|error}\n",
        "Agent --> update Memory --> check terminal --> loop or stop\n",
        "```\n",
        "\n",
        "This notebook skeleton lets you **start simple** and **swap pieces later** (LLM caller, prompt protocol, tool set, environment policy) without touching the Agent’s core.\n"
      ],
      "metadata": {
        "id": "m2wCJmDNeq72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 🧭 Agent Dev Notes — What to Focus On & Why\n",
        "\n",
        "## 1) Priorities (in this order)\n",
        "\n",
        "1. **Reliability** → Structured outputs (tool calls/JSON) + schema validation + clear termination.\n",
        "2. **Safety** → Environment guardrails (path/url allowlists, timeouts, rate limits, idempotency).\n",
        "3. **Observability** → Log prompts, tool calls, args (redacted), results, timings, token/cost, errors.\n",
        "4. **Testability** → Dependency injection (fake LLM, fake Environment); unit tests for parse/dispatch.\n",
        "5. **Cost/Latency** → Short prompts, strict schemas, minimal memory context, limited retries.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Design pillars\n",
        "\n",
        "* **Separation of concerns**\n",
        "\n",
        "  * LLM plans; **Environment** executes; **Registry** discovers tools; **AgentLanguage** builds/parses; **Agent** orchestrates.\n",
        "* **Contracts over prose**\n",
        "\n",
        "  * Prefer tool/function calling or JSON mode; otherwise fenced JSON + Pydantic validation.\n",
        "* **Deterministic control**\n",
        "\n",
        "  * Whitelist tool names; validate types/ranges; consistent result envelope `{ok, data|error, ms}`.\n",
        "* **Termination**\n",
        "\n",
        "  * Provide a `final_answer` (or similar) **terminal action**; cap `max_iterations`.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Memory policy\n",
        "\n",
        "* Keep it **small and relevant**: sliding window of recent steps + optional periodic summaries.\n",
        "* Store **structured tool results**; avoid stuffing huge blobs into the prompt.\n",
        "* Consider a “pins” section for facts that must persist across steps.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Prompts (AgentLanguage)\n",
        "\n",
        "* Be **brief and specific**; include:\n",
        "\n",
        "  * goals (bullets),\n",
        "  * tools (name + one-line description + param names),\n",
        "  * minimal recent memory,\n",
        "  * exact output format (schema or tool call).\n",
        "* Add **one tiny example** only if needed; examples cost tokens and can drift.\n",
        "* For retries: lower temperature (→ 0), restate schema, and ask for **JSON only**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Environment guardrails (must-haves)\n",
        "\n",
        "* **Filesystem**: base directory, path traversal checks, allowed extensions.\n",
        "* **Network**: domain allowlist, per-host rate limits, response size caps.\n",
        "* **Runtime**: per-tool timeouts, retry for transient errors, idempotency keys for side effects.\n",
        "* **Security/Privacy**: redact logs, never echo secrets into prompts, validate user-provided paths/URLs.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Validation strategy\n",
        "\n",
        "* **Structural**: JSON parse + Pydantic/JSON Schema (required keys, types, enums).\n",
        "* **Semantic**: business rules (e.g., `b != 0`, filename in allowed dir, nonempty query).\n",
        "* **On failure**: 1 retry (2 max) with stricter instructions; else return a safe error.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Logging & metrics (observability)\n",
        "\n",
        "Track per step:\n",
        "\n",
        "* `tool_name`, `args_hash`, `ok`, `ms`, error message (if any),\n",
        "* tokens in/out, cost (if available),\n",
        "* retry count, invalid-output rate, loop length.\n",
        "  Set alerts if invalid-output or error rates spike.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Do’s & Don’ts\n",
        "\n",
        "### ✅ Do\n",
        "\n",
        "* Inject dependencies (LLM caller, Environment, Registry, AgentLanguage).\n",
        "* Keep schemas tight; version them (`\"schema_version\": 1`).\n",
        "* Normalize tool results (same envelope) to make next-step reasoning easier.\n",
        "* Unit-test `parse_response`, arg validation, and dispatcher with fakes.\n",
        "* Use `max_iterations` and per-turn budgets (time/tokens).\n",
        "\n",
        "### ❌ Don’t\n",
        "\n",
        "* Don’t parse free-form prose if you can use tool calling/JSON mode.\n",
        "* Don’t execute tools without validating args.\n",
        "* Don’t let the LLM choose arbitrary file paths or URLs.\n",
        "* Don’t let memory grow unbounded or include huge raw blobs.\n",
        "* Don’t hard-code providers/paths inside your Agent (avoid the “glued toy” anti-pattern).\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Minimal “production-ish” checklist\n",
        "\n",
        "* [ ] Tool schemas defined; tool names whitelisted.\n",
        "* [ ] Pydantic validation before every execution.\n",
        "* [ ] Environment with allowlists, timeouts, retries, idempotency.\n",
        "* [ ] Terminal action defined and enforced.\n",
        "* [ ] Retry policy (≤2) + lower temp on retry.\n",
        "* [ ] Logs/metrics wired; sensitive data redacted.\n",
        "* [ ] Unit tests with fake LLM + fake Environment.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Quick upgrade roadmap\n",
        "\n",
        "1. Start with fenced JSON → **switch to tool/function calling**.\n",
        "2. Add Pydantic models per tool args + per-tool semantic validators.\n",
        "3. Introduce a **summarizing memory** to keep context small.\n",
        "4. Add budgeting (tokens/time per run) and circuit breakers.\n",
        "5. Move to a vector store/RAG **only if** the tasks require retrieval (don’t over-engineer early).\n",
        "\n",
        "---\n",
        "\n",
        "**Bottom line:** make the LLM’s output **machine-readable**, execute **only through a guarded Environment**, and keep everything **swappable** via dependency injection. Reliability and safety first; flashiness later.\n"
      ],
      "metadata": {
        "id": "mSqEIi09fLPV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h96JlGtNevfD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}