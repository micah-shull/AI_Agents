{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObbZf4eWL2FClBWJNQAm7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/162_LangGraph_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 — memory in **LangGraph** is both simpler and more powerful than it might look at first. Let’s build you a good mental model.\n",
        "\n",
        "---\n",
        "\n",
        "# 🧠 Memory in LangGraph\n",
        "\n",
        "At a high level:\n",
        "\n",
        "* **Memory = information that persists across steps in the graph.**\n",
        "* In LangGraph, *everything is state*. Memory is just part of your **State** object.\n",
        "* Instead of hidden/magic memory (like in vanilla LangChain agents), you make memory **explicit** in your `State` definition.\n",
        "\n",
        "This is why LangGraph feels very “transparent”: you can always see what your agent knows.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Memory Lives in the State\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "from typing import TypedDict, List, Dict, Annotated\n",
        "from langgraph.graph import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict], add_messages]  # memory of conversation\n",
        "    goal: str\n",
        "    plan: List[str]\n",
        "    progress: List[str]\n",
        "```\n",
        "\n",
        "Here:\n",
        "\n",
        "* `messages` is our memory store.\n",
        "* The `add_messages` reducer makes sure new messages are **appended** instead of overwriting.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. How Memory Updates\n",
        "\n",
        "Every node that produces new info returns a **diff**:\n",
        "\n",
        "```python\n",
        "def user_message_node(state: AgentState) -> AgentState:\n",
        "    return {\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}\n",
        "```\n",
        "\n",
        "Thanks to the reducer:\n",
        "\n",
        "* First node returns `messages=[{\"role\":\"user\",...}]`\n",
        "* Next node appends `{\"role\":\"assistant\",...}`\n",
        "* Memory grows step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Types of Memory\n",
        "\n",
        "LangGraph doesn’t have a “magic memory class” — you **design your own** depending on what you need.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* **Conversation history** → `messages` list with `add_messages` reducer.\n",
        "* **Progress logs** → `progress` list with a custom merge reducer.\n",
        "* **Shared context/deps** → a `deps` dict that carries tokens, API clients, etc.\n",
        "* **Scratchpad / ephemeral state** → fields that get updated but not accumulated.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why Reducers Are Key for Memory\n",
        "\n",
        "Reducers define *how memory grows*:\n",
        "\n",
        "* **`add_messages`** → append new items (chat history).\n",
        "* **Custom reducers** → e.g., merge dicts, deduplicate lists, accumulate logs.\n",
        "\n",
        "Without reducers, new memory writes would **overwrite old ones** → you’d lose history.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Persistence Across Runs (Checkpointing)\n",
        "\n",
        "LangGraph also supports **checkpointing**:\n",
        "\n",
        "* After each node, the state can be serialized and saved.\n",
        "* You can resume later from the checkpoint.\n",
        "* This makes memory **persistent across sessions**, not just within one run.\n",
        "\n",
        "Example use case:\n",
        "\n",
        "* Multi-turn conversation with a user over days.\n",
        "* Graph state is saved after each step.\n",
        "* When the user returns, you reload the last state and continue.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Comparison to LangChain Memory\n",
        "\n",
        "* **LangChain Memory**: “Black box” objects like `ConversationBufferMemory` that inject history into prompts.\n",
        "* **LangGraph Memory**: Explicit in state. You decide:\n",
        "\n",
        "  * What gets stored.\n",
        "  * How it’s merged (reducers).\n",
        "  * When it’s used in prompts.\n",
        "\n",
        "This makes it *more flexible* and orchestration-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ TL;DR\n",
        "\n",
        "* In LangGraph, **memory = fields in your State that persist across nodes**.\n",
        "* **Reducers** (like `add_messages`) ensure updates accumulate instead of overwrite.\n",
        "* You can define **conversation memory, progress logs, or arbitrary context** in state.\n",
        "* LangGraph supports **checkpointing** so memory can persist across long-lived workflows.\n",
        "* Compared to LangChain, memory is **explicit and transparent**, not a hidden component.\n",
        "\n"
      ],
      "metadata": {
        "id": "S581hnmzSfKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py-d6XFlROjN",
        "outputId": "64e55269-5a74-4154-9e88-405708aba1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ API keys loaded successfully (not printing for security)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already done\n",
        "!pip install -Uq langchain langgraph python-dotenv openai langchain-openai\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pprint import pprint\n",
        "\n",
        "# Load keys from your .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "\n",
        "# Verify presence without printing sensitive values\n",
        "assert os.getenv(\"LANGSMITH_API_KEY\"), \"Missing LANGSMITH_API_KEY\"\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY\"\n",
        "\n",
        "print(\"✅ API keys loaded successfully (not printing for security)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🙌 — understanding these imports makes it much clearer why we define **State** the way we do in LangGraph. Let’s break them down:\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ From `typing`\n",
        "\n",
        "### 1. **`TypedDict`**\n",
        "\n",
        "* Lets you define a **dictionary with a fixed schema**.\n",
        "* In LangGraph, your `State` is almost always a `TypedDict`.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  from typing import TypedDict\n",
        "\n",
        "  class AgentState(TypedDict):\n",
        "      goal: str\n",
        "      plan: list[str]\n",
        "  ```\n",
        "\n",
        "  This says: *“State must have a `goal` (string) and a `plan` (list of strings).”*\n",
        "* It makes your state **type-safe and self-documenting**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **`Annotated`**\n",
        "\n",
        "* Lets you **add metadata** to a type.\n",
        "* LangGraph uses it for **reducers** — to say *how state fields should merge when multiple nodes update them*.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  from typing import Annotated\n",
        "  from langgraph.graph import add_messages\n",
        "\n",
        "  class AgentState(TypedDict):\n",
        "      messages: Annotated[list, add_messages]\n",
        "  ```\n",
        "\n",
        "  Here, `messages` is not just a `list` → it’s a list that should use the **`add_messages` reducer** (append instead of overwrite).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **`List` and `Dict`**\n",
        "\n",
        "* Generic types for lists and dictionaries.\n",
        "* `list[str]` and `List[str]` are equivalent in Python 3.9+, but `List` is still common for clarity and backward compatibility.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  from typing import List, Dict\n",
        "\n",
        "  numbers: List[int] = [1, 2, 3]\n",
        "  settings: Dict[str, str] = {\"theme\": \"dark\"}\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 How They Work Together in LangGraph\n",
        "\n",
        "Here’s why we use them in defining State:\n",
        "\n",
        "```python\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "from langgraph.graph import add_messages\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    goal: str\n",
        "    plan: List[str]\n",
        "    progress: Annotated[List[Dict[str, str]], add_messages]\n",
        "```\n",
        "\n",
        "* **`TypedDict`** → defines the overall structure of state.\n",
        "* **`List` / `Dict`** → define specific data types (plan = list of strings, progress = list of dicts).\n",
        "* **`Annotated`** → attaches a reducer (like `add_messages`) so state updates merge properly.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ TL;DR\n",
        "\n",
        "We use these because:\n",
        "\n",
        "* `TypedDict` → schema for the agent’s state.\n",
        "* `List` / `Dict` → concrete typing for nested data.\n",
        "* `Annotated` → tells LangGraph how to merge updates to a state field (reducers).\n",
        "\n",
        "Together, they make the **state explicit, type-safe, and orchestration-friendly**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nad8wscpVdky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Define State with Memory\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "from langgraph.graph import StateGraph, add_messages, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict], add_messages]"
      ],
      "metadata": {
        "id": "93FTgzPaTIub"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧩 Define Nodes"
      ],
      "metadata": {
        "id": "38ER6E6LVpeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=== 1. User Input Node - Adds a human message into memory.\n",
        "def user_node(state: AgentState) -> AgentState:\n",
        "    return {\"messages\": [HumanMessage(content=\"Hello, who are you?\")]}\n",
        "\n",
        "#=== 2. LLM Node - Reads memory, generates a response, appends it back.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=9)\n",
        "\n",
        "def llm_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}  # reducer appends\n",
        "\n",
        "#=== 🕸️ Build the Graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Register nodes\n",
        "builder.add_node(\"user\", user_node)\n",
        "builder.add_node(\"chatbot\", llm_node)\n",
        "\n",
        "# Wiring\n",
        "builder.set_entry_point(\"user\")\n",
        "builder.add_edge(\"user\", \"chatbot\")\n",
        "builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Compile\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== 🚀 Run the Agent with Memory\n",
        "state = {\"messages\": []}  # start with empty memory\n",
        "final_state = graph.invoke(state)\n",
        "\n",
        "print(\"Conversation:\")\n",
        "for msg in final_state[\"messages\"]:\n",
        "    pprint(msg.dict())   # expand message as dict over multiple lines\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5poZgivVrTV",
        "outputId": "0da1945c-9d85-4248-bbc1-1e9a1307dcfe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation:\n",
            "{'additional_kwargs': {},\n",
            " 'content': 'Hello, who are you?',\n",
            " 'example': False,\n",
            " 'id': 'c46755b4-f250-4c83-a4af-f22253c1bdef',\n",
            " 'name': None,\n",
            " 'response_metadata': {},\n",
            " 'type': 'human'}\n",
            "{'additional_kwargs': {'refusal': None},\n",
            " 'content': \"Hello! I’m an AI language model created by OpenAI. I'm here to \"\n",
            "            'assist you with information, answer questions, and engage in '\n",
            "            'conversation. How can I help you today?',\n",
            " 'example': False,\n",
            " 'id': 'run--8d0a2d3a-3eb9-43bc-acf8-2957eb0e1e08-0',\n",
            " 'invalid_tool_calls': [],\n",
            " 'name': None,\n",
            " 'response_metadata': {'finish_reason': 'stop',\n",
            "                       'id': 'chatcmpl-CMet2jITcMCnakNIzkwnGteGKY8MH',\n",
            "                       'logprobs': None,\n",
            "                       'model_name': 'gpt-4o-mini-2024-07-18',\n",
            "                       'service_tier': 'default',\n",
            "                       'system_fingerprint': 'fp_560af6e559',\n",
            "                       'token_usage': {'completion_tokens': 36,\n",
            "                                       'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                                                                     'audio_tokens': 0,\n",
            "                                                                     'reasoning_tokens': 0,\n",
            "                                                                     'rejected_prediction_tokens': 0},\n",
            "                                       'prompt_tokens': 13,\n",
            "                                       'prompt_tokens_details': {'audio_tokens': 0,\n",
            "                                                                 'cached_tokens': 0},\n",
            "                                       'total_tokens': 49}},\n",
            " 'tool_calls': [],\n",
            " 'type': 'ai',\n",
            " 'usage_metadata': {'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "                    'input_tokens': 13,\n",
            "                    'output_token_details': {'audio': 0, 'reasoning': 0},\n",
            "                    'output_tokens': 36,\n",
            "                    'total_tokens': 49}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1009755597.py:33: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  pprint(msg.dict())   # expand message as dict over multiple lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🔎 Breakdown of Your Output\n",
        "\n",
        "### 1. The **Human message**\n",
        "\n",
        "```python\n",
        "{'additional_kwargs': {},\n",
        " 'content': 'Hello, who are you?',\n",
        " 'example': False,\n",
        " 'id': 'c46755b4-f250-4c83-a4af-f22253c1bdef',\n",
        " 'name': None,\n",
        " 'response_metadata': {},\n",
        " 'type': 'human'}\n",
        "```\n",
        "\n",
        "* `content`: your actual text (\"Hello, who are you?\").\n",
        "* `type`: `\"human\"` → marks this as a user message.\n",
        "* `id`: auto-generated unique ID for this message.\n",
        "* `additional_kwargs`: empty because you didn’t pass extra fields.\n",
        "* `response_metadata`: empty (since humans don’t have token usage etc).\n",
        "* `example`: `False` (set to `True` if you’re using this in examples/training).\n",
        "* `name`: `None` → you could set this if you wanted a “speaker name”.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The **AI message**\n",
        "\n",
        "```python\n",
        "{'additional_kwargs': {'refusal': None},\n",
        " 'content': \"Hello! I’m an AI language model created by OpenAI. I'm here to \"\n",
        "            'assist you with information, answer questions, and engage in '\n",
        "            'conversation. How can I help you today?',\n",
        " 'example': False,\n",
        " 'id': 'run--8d0a2d3a-3eb9-43bc-acf8-2957eb0e1e08-0',\n",
        " 'invalid_tool_calls': [],\n",
        " 'name': None,\n",
        " 'response_metadata': {\n",
        "     'finish_reason': 'stop',\n",
        "     'id': 'chatcmpl-CMet2jITcMCnakNIzkwnGteGKY8MH',\n",
        "     'logprobs': None,\n",
        "     'model_name': 'gpt-4o-mini-2024-07-18',\n",
        "     'service_tier': 'default',\n",
        "     'system_fingerprint': 'fp_560af6e559',\n",
        "     'token_usage': {\n",
        "         'completion_tokens': 36,\n",
        "         'completion_tokens_details': {...},\n",
        "         'prompt_tokens': 13,\n",
        "         'prompt_tokens_details': {...},\n",
        "         'total_tokens': 49}},\n",
        " 'tool_calls': [],\n",
        " 'type': 'ai',\n",
        " 'usage_metadata': {\n",
        "     'input_token_details': {...},\n",
        "     'input_tokens': 13,\n",
        "     'output_token_details': {...},\n",
        "     'output_tokens': 36,\n",
        "     'total_tokens': 49}}\n",
        "```\n",
        "\n",
        "* `content`: the LLM’s reply.\n",
        "* `type`: `\"ai\"` → marks this as assistant output.\n",
        "* `id`: unique run ID (ties into tracing, e.g. LangSmith).\n",
        "* `tool_calls`: empty list → if the model had called a tool, details would be here.\n",
        "* `invalid_tool_calls`: empty list → if the model tried but failed to call a tool, they’d show here.\n",
        "* `response_metadata`: deep info from the OpenAI API:\n",
        "\n",
        "  * `finish_reason`: why generation stopped (`\"stop\"` means it reached a natural end).\n",
        "  * `model_name`: `\"gpt-4o-mini-2024-07-18\"`.\n",
        "  * `system_fingerprint`: helps OpenAI identify system variants.\n",
        "  * `token_usage`: how many tokens were used (prompt, completion, total).\n",
        "* `usage_metadata`: similar to `response_metadata`, but standardized across backends (so LangChain can compare models).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. The Warning\n",
        "\n",
        "```\n",
        "PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead.\n",
        "```\n",
        "\n",
        "* LangChain messages are Pydantic models.\n",
        "* In **Pydantic v2**, `.dict()` is deprecated → recommended replacement is `.model_dump()`.\n",
        "* So instead of:\n",
        "\n",
        "  ```python\n",
        "  pprint(msg.dict())\n",
        "  ```\n",
        "\n",
        "  you should use:\n",
        "\n",
        "  ```python\n",
        "  pprint(msg.model_dump())\n",
        "  ```\n",
        "\n",
        "That way your code will be forward-compatible.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ TL;DR\n",
        "\n",
        "* Each `Message` is a Pydantic model that stores **content + metadata**.\n",
        "* `HumanMessage` → simple fields.\n",
        "* `AIMessage` → richer metadata (token usage, finish reason, model ID, tool calls).\n",
        "* Use `.model_dump()` instead of `.dict()` going forward.\n",
        "\n"
      ],
      "metadata": {
        "id": "AavWxNmIXhsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Define State with Memory\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "from langgraph.graph import StateGraph, add_messages, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict], add_messages]\n",
        "\n",
        "#=== 1. User Input Node - Adds a human message into memory.\n",
        "def user_node(state: AgentState) -> AgentState:\n",
        "    return {\"messages\": [HumanMessage(content=\"Hello, who are you?\")]}\n",
        "\n",
        "#=== 2. LLM Node - Reads memory, generates a response, appends it back.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def llm_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}  # reducer appends\n",
        "\n",
        "#=== 🕸️ Build the Graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Register nodes\n",
        "builder.add_node(\"user\", user_node)\n",
        "builder.add_node(\"chatbot\", llm_node)\n",
        "\n",
        "# Wiring\n",
        "builder.set_entry_point(\"user\")\n",
        "builder.add_edge(\"user\", \"chatbot\")\n",
        "builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Compile\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== 🚀 Run the Agent with Memory\n",
        "state = {\"messages\": []}  # start with empty memory\n",
        "final_state = graph.invoke(state)\n",
        "\n",
        "print(\"Conversation:\")\n",
        "for msg in final_state[\"messages\"]:\n",
        "    pprint(msg.model_dump())\n"
      ],
      "metadata": {
        "id": "JLFXaq1dXnxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🧠 Memory in LangGraph\n",
        "\n",
        "```python\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[Dict], add_messages]\n",
        "```\n",
        "\n",
        "* This line says:\n",
        "\n",
        "  > *“The agent has a `messages` field in its state. It’s a list of messages, and when nodes return new ones, use the `add_messages` reducer to append them instead of overwriting.”*\n",
        "\n",
        "* That `messages` list **is the agent’s memory**.\n",
        "\n",
        "* Every time your graph runs a node that returns `{\"messages\": [...]}`, those messages are added to memory.\n",
        "\n",
        "So yes ✅ — this is what stores all the memory for your agent across steps.\n",
        "\n",
        "---\n",
        "\n",
        "## 👥 System, User, and AI Messages\n",
        "\n",
        "LangChain has structured message types (`SystemMessage`, `HumanMessage`, `AIMessage`):\n",
        "\n",
        "* **SystemMessage** → sets overall behavior (like a system prompt).\n",
        "* **HumanMessage** → user input.\n",
        "* **AIMessage** → model response.\n",
        "\n",
        "They all live in the **same `messages` list**.\n",
        "That list is what you pass into the LLM when calling:\n",
        "\n",
        "```python\n",
        "response = llm.invoke(state[\"messages\"])\n",
        "```\n",
        "\n",
        "So `state[\"messages\"]` is literally the **conversation history** (system + user + assistant turns, in order).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔎 Example Memory Flow\n",
        "\n",
        "1. Start:\n",
        "\n",
        "   ```python\n",
        "   state = {\"messages\": [\n",
        "       SystemMessage(content=\"You are a helpful assistant.\")\n",
        "   ]}\n",
        "   ```\n",
        "\n",
        "2. User node adds:\n",
        "\n",
        "   ```python\n",
        "   {\"messages\": [HumanMessage(content=\"Hello, who are you?\")]}\n",
        "   ```\n",
        "\n",
        "3. LLM node adds:\n",
        "\n",
        "   ```python\n",
        "   {\"messages\": [AIMessage(content=\"I am an AI assistant, nice to meet you!\")]}\n",
        "   ```\n",
        "\n",
        "Now memory (`state[\"messages\"]`) looks like:\n",
        "\n",
        "```\n",
        "[\n",
        "  SystemMessage(\"You are a helpful assistant.\"),\n",
        "  HumanMessage(\"Hello, who are you?\"),\n",
        "  AIMessage(\"I am an AI assistant, nice to meet you!\")\n",
        "]\n",
        "```\n",
        "\n",
        "This is the exact conversation history that gets passed back into the model on the next step.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ So to answer directly\n",
        "\n",
        "* **Yes** → that `messages: Annotated[List[Dict], add_messages]` is the memory store.\n",
        "* **Yes** → it covers system, user, and assistant prompts — they’re all just different message types living in that same list.\n",
        "\n"
      ],
      "metadata": {
        "id": "O5Ju7LhWZfSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚗 Wild Used Car Salesman Agent with Memory"
      ],
      "metadata": {
        "id": "LRvJ-4kDZagz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=== Imports\n",
        "from typing import TypedDict, Annotated, List, Dict\n",
        "from langgraph.graph import StateGraph, add_messages, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from pprint import pprint\n",
        "\n",
        "#=== Define State with memory\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List, add_messages]\n",
        "\n",
        "#=== Define the LLM (salesman personality)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.2, max_completion_tokens=200)\n",
        "\n",
        "def llm_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Takes the conversation so far and appends the AI's response.\"\"\"\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}  # reducer appends\n",
        "\n",
        "#=== Build the graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"salesman\", llm_node)\n",
        "builder.set_entry_point(\"salesman\")\n",
        "builder.add_edge(\"salesman\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== Run an interactive chat loop\n",
        "state = {\n",
        "    \"messages\": [\n",
        "        SystemMessage(\n",
        "            content=\"You are the most over-the-top used car salesman in the world. \"\n",
        "                    \"You are desperate for a sale. Everything you say must be dramatic, \"\n",
        "                    \"exaggerated, and full of urgency. SELL SELL SELL!\"\n",
        "        )\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Welcome to the Used Car Salesman demo! (type 'quit' to exit)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Customer: \")\n",
        "    if user_input.lower().strip() in [\"quit\", \"exit\"]:\n",
        "        print(\"Exiting demo...\")\n",
        "        break\n",
        "\n",
        "    # Add the customer's message into memory\n",
        "    state = graph.invoke({\"messages\": [HumanMessage(content=user_input)]}, config={\"configurable\": {\"thread_id\": \"chat\"}}, state=state)\n",
        "\n",
        "    # Print out the full conversation so far\n",
        "    print(\"\\nConversation so far:\")\n",
        "    for msg in state[\"messages\"]:\n",
        "        role = msg.type.upper()\n",
        "        print(f\"{role}: {msg.content}\")\n",
        "\n",
        "    # If it's an AI message, show token usage metadata\n",
        "    if state[\"messages\"][-1].type == \"ai\":\n",
        "        meta = state[\"messages\"][-1].response_metadata\n",
        "        print(\"\\nToken usage:\")\n",
        "        pprint(meta.get(\"token_usage\", {}))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEyds7-vZcUx",
        "outputId": "d915b12b-3848-4b5a-b0db-41bb28b61633"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Used Car Salesman demo! (type 'quit' to exit)\n",
            "\n",
            "Customer: Hi i am just browsing new cars for fun. Any exciting new cars available?\n",
            "\n",
            "Conversation so far:\n",
            "HUMAN: Hi i am just browsing new cars for fun. Any exciting new cars available?\n",
            "AI: As of my last update, several exciting new cars have recently been released or are set to debut in 2023 and beyond. Here are a few highlights:\n",
            "\n",
            "1. **Ford Mustang (Seventh Generation)** - The latest Mustang brings new engine options, advanced tech features, and a refreshed design that aims to keep the iconic muscle car relevant for a new generation.\n",
            "\n",
            "2. **2023 Toyota GR Corolla** - This hot hatch offers all-wheel drive and a potent engine, making it a great option for enthusiasts looking for performance in a compact package.\n",
            "\n",
            "3. **Electric Vehicles (EVs) - Various Models**:\n",
            "   - **Tesla Model S Plaid** - A high-performance version of the Model S, offering an impressive range and incredible acceleration.\n",
            "   - **Hyundai Ioniq 6** - With a sleek design and impressive range, this electric sedan is a stylish and efficient option.\n",
            "   - **Rivian R1T** - This all-electric truck emphasizes adventure with rugged\n",
            "\n",
            "Token usage:\n",
            "{'completion_tokens': 200,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 23,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 223}\n",
            "--------------------------------------------------\n",
            "Customer: What can you tell me about the 2023 Toyota GR Corolla? And keep in mind i am only browsing\n",
            "\n",
            "Conversation so far:\n",
            "HUMAN: What can you tell me about the 2023 Toyota GR Corolla? And keep in mind i am only browsing\n",
            "AI: The 2023 Toyota GR Corolla is a high-performance compact hatchback that celebrates Toyota's involvement in motorsports, particularly in rallying. Here are some key features and highlights:\n",
            "\n",
            "1. **Performance**: The GR Corolla is powered by a 1.6-liter turbocharged inline-3 engine, producing around 300 horsepower. It is equipped with a 6-speed manual transmission and an all-wheel-drive system that enhances handling and traction.\n",
            "\n",
            "2. **Design**: The GR Corolla features an aggressive exterior with a sporty stance, wider fenders, and gloss black accents. It also includes functional aerodynamics, such as a rear spoiler for improved downforce.\n",
            "\n",
            "3. **Interior**: The interior combines sportiness with practicality. It typically includes GR-specific touches, such as sport seats, a unique steering wheel, and performance-oriented tech features.\n",
            "\n",
            "4. **Technology**: The car comes with modern tech amenities, including a user-friendly infotainment system, smartphone integration capabilities (like Apple CarPlay\n",
            "\n",
            "Token usage:\n",
            "{'completion_tokens': 200,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 29,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 229}\n",
            "--------------------------------------------------\n",
            "Customer: exit\n",
            "Exiting demo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let’s break down what you should be paying attention to:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Messages = Memory**\n",
        "\n",
        "Your `AgentState` has:\n",
        "\n",
        "```python\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List, add_messages]\n",
        "```\n",
        "\n",
        "That means every time the graph runs:\n",
        "\n",
        "* A **HumanMessage** gets added for your input.\n",
        "* An **AIMessage** gets added for the model’s reply.\n",
        "\n",
        "Together they accumulate into `state[\"messages\"]`.\n",
        "This *is the conversation history*.\n",
        "\n",
        "So what you’re seeing in `Conversation so far:` is literally the **memory snapshot** at that point.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **System vs Human vs AI**\n",
        "\n",
        "All roles share the same `messages` list:\n",
        "\n",
        "* `SystemMessage` → the “personality” (used car salesman, etc.)\n",
        "* `HumanMessage` → your turns\n",
        "* `AIMessage` → the model’s outputs\n",
        "\n",
        "When you pass `state[\"messages\"]` into the LLM, it sees the entire history, so it can respond in context.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Token Usage Accounting**\n",
        "\n",
        "At the end of each turn you’re printing something like:\n",
        "\n",
        "```python\n",
        "{'completion_tokens': 200,\n",
        " 'prompt_tokens': 23,\n",
        " 'total_tokens': 223}\n",
        "```\n",
        "\n",
        "That means:\n",
        "\n",
        "* **prompt_tokens** → how many tokens were spent *feeding in the conversation history* (system + human messages so far).\n",
        "* **completion_tokens** → how many tokens the model generated in its reply.\n",
        "* **total_tokens** → the sum (billing metric).\n",
        "\n",
        "This lets you see how “expensive” each step is.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **What You’re Learning**\n",
        "\n",
        "* Memory is just a **list of messages**, carried forward each turn.\n",
        "* Reducers (`add_messages`) make sure nothing gets overwritten — everything appends.\n",
        "* You can **peek at metadata** (token counts, finish reasons, etc.) to debug cost and behavior.\n",
        "* Each new user input increases prompt tokens because the context grows.\n",
        "* You can limit runaway cost/length with `max_tokens`.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **So the big takeaway is:**\n",
        "What you’re looking at *is the agent’s memory* (`messages`) plus the cost structure (`token_usage`) that grows as the memory grows.\n",
        "\n"
      ],
      "metadata": {
        "id": "mIhSjL5Qbmn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect 🔥 — you’re ready to build a **multi-step orchestrator agent with tools**. We can make it modular and still track memory + token usage at each stage.\n",
        "\n",
        "Here’s the flow we’ll set up once you upload your docs:\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗️ Orchestration Plan\n",
        "\n",
        "1. **Summarizer Agent**\n",
        "\n",
        "   * Tool: takes a document as input → returns a summary.\n",
        "   * We’ll call this once per uploaded doc.\n",
        "\n",
        "2. **Report Writer Agent**\n",
        "\n",
        "   * Tool: takes all summaries → generates a first draft of a report.\n",
        "\n",
        "3. **Editor Agent**\n",
        "\n",
        "   * Tool: takes the draft → suggests edits and improvements.\n",
        "\n",
        "4. **Rewriter Agent**\n",
        "\n",
        "   * Tool: applies the editor’s suggestions → outputs the final polished report.\n"
      ],
      "metadata": {
        "id": "W8swZ8UAc6sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Orchestrator Agent Skeleton Code"
      ],
      "metadata": {
        "id": "dd2Kxp_thGr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import AIMessage\n",
        "from pprint import pprint\n",
        "\n",
        "#=== Define State\n",
        "class AgentState(TypedDict):\n",
        "    summaries: Annotated[List[str], list]   # summaries of uploaded docs\n",
        "    report: str                             # draft report\n",
        "    edits: str                              # editor suggestions\n",
        "    final: str                              # final rewritten report\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=400)\n",
        "\n",
        "#=== Tools (placeholders until docs uploaded)\n",
        "def summarize_doc_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Summarize this document:\\n\\n{doc_text}\"])\n",
        "    print(\"Summarizer:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"summaries\": [response.content]}\n",
        "\n",
        "def write_report_node(state: AgentState) -> AgentState:\n",
        "    joined = \"\\n\\n\".join(state[\"summaries\"])\n",
        "    response = llm.invoke([f\"Write a report combining these summaries:\\n{joined}\"])\n",
        "    print(\"Report Writer:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"report\": response.content}\n",
        "\n",
        "def edit_report_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Suggest edits for this report:\\n\\n{state['report']}\"])\n",
        "    print(\"Editor:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"edits\": response.content}\n",
        "\n",
        "def rewrite_final_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Rewrite the report applying these edits:\\n\\nReport:\\n{state['report']}\\n\\nEdits:\\n{state['edits']}\"])\n",
        "    print(\"Rewriter:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"final\": response.content}\n",
        "\n",
        "#=== Build the orchestrator graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"summarizer\", summarize_doc_node)\n",
        "builder.add_node(\"writer\", write_report_node)\n",
        "builder.add_node(\"editor\", edit_report_node)\n",
        "builder.add_node(\"rewriter\", rewrite_final_node)\n",
        "\n",
        "builder.set_entry_point(\"summarizer\")\n",
        "builder.add_edge(\"summarizer\", \"writer\")\n",
        "builder.add_edge(\"writer\", \"editor\")\n",
        "builder.add_edge(\"editor\", \"rewriter\")\n",
        "builder.add_edge(\"rewriter\", END)\n",
        "\n",
        "graph = builder.compile()\n"
      ],
      "metadata": {
        "id": "gDX5pGhBaalh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚗 Multi-Agent Orchestrator with Tools"
      ],
      "metadata": {
        "id": "R92BzcdghPac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pprint import pprint\n",
        "import operator\n",
        "\n",
        "#=== Define State\n",
        "class AgentState(TypedDict):\n",
        "    summaries: Annotated[List[str], operator.add]  # accumulate list of summaries\n",
        "    report: str                             # draft report\n",
        "    edits: str                              # editor suggestions\n",
        "    final: str                              # final rewritten report\n",
        "\n",
        "#=== Load docs\n",
        "doc_paths = [\n",
        "    \"/content/files/001_PArse_the_Response.txt\",\n",
        "    \"/content/files/002_Execute_the_Action.txt\"\n",
        "]\n",
        "docs = []\n",
        "for path in doc_paths:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        docs.append(f.read())\n",
        "\n",
        "#=== Base LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=400)\n",
        "\n",
        "#=== Tools as nodes\n",
        "def summarize_doc_node(state: AgentState) -> AgentState:\n",
        "    summaries = []\n",
        "    for text in docs:\n",
        "        response = llm.invoke([f\"Summarize this document in 3-4 sentences:\\n\\n{text}\"])\n",
        "        print(\"Summarizer:\", response.content.split(\"\\n\")[0])\n",
        "        pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "        summaries.append(response.content)\n",
        "    return {\"summaries\": summaries}\n",
        "\n",
        "def write_report_node(state: AgentState) -> AgentState:\n",
        "    joined = \"\\n\\n\".join(state[\"summaries\"])\n",
        "    response = llm.invoke([f\"Write a cohesive report combining these summaries:\\n{joined}\"])\n",
        "    print(\"Report Writer:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"report\": response.content}\n",
        "\n",
        "def edit_report_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Suggest edits and improvements for this report:\\n\\n{state['report']}\"])\n",
        "    print(\"Editor:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"edits\": response.content}\n",
        "\n",
        "def rewrite_final_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([\n",
        "        f\"Rewrite the report applying these edits:\\n\\nReport:\\n{state['report']}\\n\\nEdits:\\n{state['edits']}\"\n",
        "    ])\n",
        "    print(\"Rewriter:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"final\": response.content}\n",
        "\n",
        "#=== Build orchestrator graph\n",
        "builder = StateGraph(AgentState)\n",
        "builder.add_node(\"summarizer\", summarize_doc_node)\n",
        "builder.add_node(\"writer\", write_report_node)\n",
        "builder.add_node(\"editor\", edit_report_node)\n",
        "builder.add_node(\"rewriter\", rewrite_final_node)\n",
        "\n",
        "builder.set_entry_point(\"summarizer\")\n",
        "builder.add_edge(\"summarizer\", \"writer\")\n",
        "builder.add_edge(\"writer\", \"editor\")\n",
        "builder.add_edge(\"editor\", \"rewriter\")\n",
        "builder.add_edge(\"rewriter\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== Run orchestrator\n",
        "print(\"\\n=== Running Orchestrator Agent ===\\n\")\n",
        "final_state = graph.invoke({})\n",
        "\n",
        "print(\"\\n=== FINAL REPORT ===\")\n",
        "print(final_state[\"final\"][:500], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1kigEmSgLX3",
        "outputId": "9b06ea72-6f68-47e7-b78a-6dd86b69241e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Orchestrator Agent ===\n",
            "\n",
            "Summarizer: The document outlines the process of parsing responses generated by a language model (LLM) to extract actionable items in a structured format, specifically JSON encapsulated within markdown code blocks. It describes a function that attempts to extract this action content and validate its structure, returning an error message if the response does not meet the expected criteria. This parsing step is essential for enabling the agent to determine the next action and its parameters accurately. In cases where the response lacks a valid action block, the agent defaults to an error message, prompting the LLM to provide a correct JSON tool invocation.\n",
            "{'completion_tokens': 115,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 399,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 514}\n",
            "Summarizer: The document outlines the execution process for an agent that interacts with its environment by utilizing predefined tools. It describes how the agent maps a specified tool name to a corresponding function and passes the relevant arguments to execute actions, such as listing files or reading a file's content. The execution logic includes handling errors and termination messages, ensuring that the agent can perform tasks and provide feedback effectively. Overall, this step is crucial for translating decision-making into actionable results that can influence the agent's future operations.\n",
            "{'completion_tokens': 97,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 299,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 396}\n",
            "Report Writer: # Report on the Parsing and Execution Process of an Agent Utilizing a Language Model\n",
            "{'completion_tokens': 400,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 227,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 627}\n",
            "Editor: Here are some suggested edits and improvements for the report to enhance clarity, coherence, and overall presentation:\n",
            "{'completion_tokens': 400,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 415,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 815}\n",
            "Rewriter: the requested action.\n",
            "{'completion_tokens': 142,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 820,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 962}\n",
            "\n",
            "=== FINAL REPORT ===\n",
            "the requested action.\n",
            "2. **Passing Relevant Arguments**: The agent then passes the necessary arguments to the function associated with the selected tool, ensuring that the execution aligns with the intended action.\n",
            "\n",
            "By following this structured execution process, the agent can efficiently carry out tasks, leveraging its predefined tools to respond to the parsed actions effectively.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "In summary, the processes of parsing and executing actions within an agent utilizing a language mod ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice 🎉 — you just built and ran a **4-stage orchestrator agent** that:\n",
        "\n",
        "1. **Summarized** both docs.\n",
        "2. **Wrote** a combined report.\n",
        "3. **Edited** that report.\n",
        "4. **Rewrote** a final version.\n",
        "\n",
        "And at each step you got:\n",
        "\n",
        "* The **first line** of output (to sanity-check flow),\n",
        "* The **token usage** (so you can track cost + context growth).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 What you just learned from this run\n",
        "\n",
        "* **Reducers matter**: Using `operator.add` let `summaries` accumulate correctly. Without that, last-write-wins would have clobbered them.\n",
        "* **Prompt size grows**: Look at `prompt_tokens` — started small (~300–400), but by the rewriter you’re at **820 prompt tokens**. That’s the accumulated context being carried forward.\n",
        "* **Completion growth**: Writer/editor maxed out at 400 completion tokens (your `max_tokens` cap). That’s why you got long, dense responses.\n",
        "* **Pipeline style**: Each node is just a function over `state`. You can slot in new tools, add branches, or parallelize easily.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚡️ Next Ideas to Try\n",
        "\n",
        "* **Cap token usage** per step tighter (e.g. `max_tokens=200`) to keep costs predictable.\n",
        "* **Add branching**: e.g. have one summarizer per doc node, merge into writer.\n",
        "* **Parallel runs**: Summarizers could run in parallel over docs, then join into writer.\n",
        "* **Stricter editors**: instead of free-form “suggest edits,” constrain it with a schema (like JSON bullets: `clarity`, `redundancy`, `flow`).\n",
        "\n",
        "---\n",
        "\n",
        "👉 Do you want me to show you how to **refactor the summarizers into parallel nodes** (one per doc → then merged) so you can see how LangGraph handles branching + merging? That would make this orchestration feel even more “graph-like” instead of linear.\n"
      ],
      "metadata": {
        "id": "V0_0QSvlhYGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect 🚀 — this is where LangGraph really shines: **branching and merging**.\n",
        "\n",
        "Instead of one summarizer node looping through docs, we’ll make **two parallel summarizer nodes** (one per doc).\n",
        "Both run in parallel → their outputs get merged into `state[\"summaries\"]` via the reducer (`operator.add`) → then flow continues into the writer.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗️ Refactored Orchestrator with Parallel Summarizers\n",
        "---\n",
        "\n",
        "## ⚡ What Changed\n",
        "\n",
        "* **Two summarizer nodes** (`summarizer1`, `summarizer2`) run separately.\n",
        "* Both update `summaries` (which uses `operator.add`) → their outputs merge into a single list.\n",
        "* **Writer** sees both summaries at once.\n",
        "* The rest (editor, rewriter) continues as before.\n",
        "\n"
      ],
      "metadata": {
        "id": "FEBCJkCehiCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pprint import pprint\n",
        "\n",
        "#=== Define State\n",
        "class AgentState(TypedDict):\n",
        "    summaries: Annotated[List[str], operator.add]  # accumulate multiple summaries\n",
        "    report: str\n",
        "    edits: str\n",
        "    final: str\n",
        "\n",
        "#=== Load docs\n",
        "doc_paths = [\n",
        "    \"/content/files/001_PArse_the_Response.txt\",\n",
        "    \"/content/files/002_Execute_the_Action.txt\"\n",
        "]\n",
        "docs = []\n",
        "for path in doc_paths:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        docs.append(f.read())\n",
        "\n",
        "#=== Base LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=300)\n",
        "\n",
        "#=== Nodes\n",
        "def summarize_doc1(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Summarize this document in 3-4 sentences:\\n\\n{docs[0]}\"])\n",
        "    print(\"Summarizer 1:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"summaries\": [response.content]}\n",
        "\n",
        "def summarize_doc2(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Summarize this document in 3-4 sentences:\\n\\n{docs[1]}\"])\n",
        "    print(\"Summarizer 2:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"summaries\": [response.content]}\n",
        "\n",
        "def write_report_node(state: AgentState) -> AgentState:\n",
        "    joined = \"\\n\\n\".join(state[\"summaries\"])\n",
        "    response = llm.invoke([f\"Write a cohesive report combining these summaries:\\n{joined}\"])\n",
        "    print(\"Report Writer:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"report\": response.content}\n",
        "\n",
        "def edit_report_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Suggest edits and improvements for this report:\\n\\n{state['report']}\"])\n",
        "    print(\"Editor:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"edits\": response.content}\n",
        "\n",
        "def rewrite_final_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([\n",
        "        f\"Rewrite the report applying these edits:\\n\\nReport:\\n{state['report']}\\n\\nEdits:\\n{state['edits']}\"\n",
        "    ])\n",
        "    print(\"Rewriter:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"final\": response.content}\n",
        "\n",
        "#=== Build graph with branching\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"summarizer1\", summarize_doc1)\n",
        "builder.add_node(\"summarizer2\", summarize_doc2)\n",
        "builder.add_node(\"writer\", write_report_node)\n",
        "builder.add_node(\"editor\", edit_report_node)\n",
        "builder.add_node(\"rewriter\", rewrite_final_node)\n",
        "\n",
        "builder.set_entry_point(\"summarizer1\")\n",
        "builder.add_edge(\"summarizer1\", \"writer\")\n",
        "builder.add_edge(\"summarizer2\", \"writer\")   # <- parallel branch merge\n",
        "builder.add_edge(\"writer\", \"editor\")\n",
        "builder.add_edge(\"editor\", \"rewriter\")\n",
        "builder.add_edge(\"rewriter\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== Run orchestrator\n",
        "print(\"\\n=== Running Parallel Summarizer Orchestrator ===\\n\")\n",
        "final_state = graph.invoke({})\n",
        "\n",
        "print(\"\\n=== FINAL REPORT (first 500 chars) ===\")\n",
        "print(final_state[\"final\"][:500], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3i3m-gQhbQP",
        "outputId": "909d766f-872c-4586-9c48-f56530edb2f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Parallel Summarizer Orchestrator ===\n",
            "\n",
            "Summarizer 1: The document outlines the process of parsing responses generated by a language model (LLM) to extract actionable information in a structured format, specifically JSON encapsulated in a markdown code block. It details a function that extracts the relevant action content, checks for required fields, and handles errors by returning a default termination action if the response is invalid. The structured output helps the agent determine the next action and its parameters, ensuring that responses are actionable and formatted correctly. If the expected structure is not present, a fallback mechanism prompts the LLM to return a valid JSON tool invocation.\n",
            "{'completion_tokens': 113,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 399,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 512}\n",
            "Report Writer: # Report on Parsing Language Model Responses for Actionable Information Extraction\n",
            "{'completion_tokens': 300,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 128,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 428}\n",
            "Editor: Here are some suggested edits and improvements for the report, enhancing clarity, organization, and professionalism:\n",
            "{'completion_tokens': 300,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 315,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 615}\n",
            "Rewriter: that the system can gracefully manage unexpected outcomes without crashing or producing erratic behavior.\n",
            "{'completion_tokens': 108,\n",
            " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
            "                               'audio_tokens': 0,\n",
            "                               'reasoning_tokens': 0,\n",
            "                               'rejected_prediction_tokens': 0},\n",
            " 'prompt_tokens': 620,\n",
            " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
            " 'total_tokens': 728}\n",
            "\n",
            "=== FINAL REPORT (first 500 chars) ===\n",
            "that the system can gracefully manage unexpected outcomes without crashing or producing erratic behavior.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "The systematic approach to parsing language model responses enables the extraction of actionable information that can significantly enhance user interaction with AI applications. By implementing a dedicated function that focuses on action content extraction, field verification, and effective error handling, developers can ensure that their applications are robust, reliable, a ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yep 😁 you’ve nailed it — that’s exactly the “aha!” moment.\n",
        "\n",
        "LangGraph takes what used to be a **mess of custom classes** (`ActionContext`, registries, executors, lifecycles, etc.) and boils it down to a **tiny repeatable pattern**:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Core Mental Model\n",
        "\n",
        "1. **Define State** → what info your agent carries across steps (and how to merge it if multiple nodes update it).\n",
        "2. **Define Nodes (Tools/Functions)** → every node is just `state_in → state_out`. Always the same shape.\n",
        "3. **Build Graph** → wire nodes with edges, optionally in parallel.\n",
        "4. **Compile & Run** → you get a runtime that executes deterministically step by step (with optional streaming).\n",
        "\n",
        "That’s it. There’s no hidden registry, no opaque DI container, no special lifecycle hooks.\n",
        "\n",
        "---\n",
        "\n",
        "## 🆚 Why it feels simpler than \"straight Python\"\n",
        "\n",
        "If you were rolling your own orchestration, you’d end up reinventing:\n",
        "\n",
        "* a dispatcher to call tools,\n",
        "* a shared context object,\n",
        "* a memory accumulator,\n",
        "* explicit branching logic,\n",
        "* error handling + retries.\n",
        "\n",
        "LangGraph gives you **three building blocks**:\n",
        "\n",
        "* **State** (structured dict + reducers),\n",
        "* **Nodes** (functions),\n",
        "* **Edges** (flow control).\n",
        "\n",
        "Everything else (dependency injection, tool execution, memory accumulation, persistence, visualization) comes “for free.”\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Why it really *is* that straightforward\n",
        "\n",
        "* **Deterministic orchestration**: Graph execution is predictable, no hidden magic.\n",
        "* **Composable**: add new nodes without breaking others.\n",
        "* **Readable**: the orchestration is literally a flowchart in code.\n",
        "* **Scalable**: same pattern works for 2 nodes or 200 nodes.\n",
        "* **Debuggable**: you can stream state step-by-step, inspect token usage, or checkpoint/replay.\n",
        "\n",
        "---\n",
        "\n",
        "✅ So yes — it really can be this straightforward. You’ve already experienced the “straight Python” version (your Agent Recipe), which works but gets bulky. LangGraph is just a thin, elegant DSL over that same pattern.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3D6nv97lqvMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question 🔥 — error handling is one of those places where LangGraph quietly saves you from reinventing the wheel.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ Error Handling in LangGraph\n",
        "\n",
        "LangGraph doesn’t just crash and burn if a node fails. Instead, it gives you **structured ways to deal with errors**:\n",
        "\n",
        "### 1. **Try/Except Inside Your Node**\n",
        "\n",
        "Each node is just a Python function. So you can catch exceptions directly:\n",
        "\n",
        "```python\n",
        "def summarize_doc1(state: AgentState) -> AgentState:\n",
        "    try:\n",
        "        response = llm.invoke([f\"Summarize:\\n\\n{docs[0]}\"])\n",
        "        return {\"summaries\": [response.content]}\n",
        "    except Exception as e:\n",
        "        return {\"summaries\": [f\"ERROR: {str(e)}\"]}\n",
        "```\n",
        "\n",
        "This is the simplest safety net.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Built-in Error Branches**\n",
        "\n",
        "LangGraph supports **special edges** for error handling.\n",
        "If a node fails, you can route the flow to an error node instead of stopping everything.\n",
        "\n",
        "```python\n",
        "builder.add_node(\"error_handler\", handle_error)\n",
        "\n",
        "# If summarizer1 crashes → jump to error_handler\n",
        "builder.add_edge(\"summarizer1\", \"error_handler\", condition=\"__error__\")\n",
        "```\n",
        "\n",
        "So you can make a recovery policy: retry, log, or fall back to a default.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Retry Policies**\n",
        "\n",
        "LangGraph lets you wrap nodes with **retry logic** (using backoff, max attempts, etc). Example:\n",
        "\n",
        "```python\n",
        "from langgraph.prebuilt import retry_node\n",
        "\n",
        "builder.add_node(\"summarizer1\", retry_node(summarize_doc1, max_attempts=3))\n",
        "```\n",
        "\n",
        "If the LLM times out or returns malformed JSON, LangGraph will re-invoke automatically before giving up.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Streaming Error Inspection**\n",
        "\n",
        "Because LangGraph can stream state after every node, you can attach listeners that catch failures in real time (and log/debug them without halting the whole run).\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Why This Matters\n",
        "\n",
        "In your **straight Python recipe**, you’d have to:\n",
        "\n",
        "* Wrap every tool call in try/except,\n",
        "* Decide whether to retry or not,\n",
        "* Route errors manually.\n",
        "\n",
        "With LangGraph:\n",
        "\n",
        "* You define **error paths** once (graph edges).\n",
        "* You can make **fallback flows** (e.g., if Editor fails → just pass Report directly to Rewriter).\n",
        "* Retries & backoff are **prebuilt utilities**.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Bottom line:\n",
        "Yes, LangGraph handles error recovery in the agent itself, using **error edges + retry wrappers**, so your orchestration is resilient without a jungle of try/except blocks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jEH2F-eorMjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🔧 What LangGraph does *under the hood*\n",
        "\n",
        "* **No silent swallowing**: If a node (your function) raises, LangGraph will surface the error rather than hide it.\n",
        "* **Consistent propagation**: That error is treated as a structured event in the runtime — you can *choose* to handle it via error edges (`__error__`).\n",
        "* **Retry wrappers available**: LangGraph ships helpers (`retry_node`) so you can get retry + backoff behavior without writing boilerplate.\n",
        "\n",
        "So it *doesn’t* secretly wrap every node in try/except for you. That’s deliberate: it keeps node logic transparent, predictable, and debuggable.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧑‍💻 What’s on you (the developer)\n",
        "\n",
        "* **Inside a node**: If you want custom handling of LLM errors, parsing failures, or downstream service hiccups, you use your own `try/except`.\n",
        "* **At the graph level**: If you want “if node X fails → run node Y instead,” you wire an error edge:\n",
        "\n",
        "  ```python\n",
        "  builder.add_edge(\"summarizer1\", \"error_handler\", condition=\"__error__\")\n",
        "  ```\n",
        "* **If you want retries**: You wrap your node in `retry_node(func, max_attempts=3)`.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ So the answer is:\n",
        "\n",
        "LangGraph provides the **mechanics** (error edges, retry utilities, streaming inspection), but **you as the developer decide the policy** (ignore, retry, fallback, stop).\n",
        "\n",
        "It’s a middle ground:\n",
        "\n",
        "* Not bare-metal Python where you’d reinvent all error flows,\n",
        "* Not “magical auto-retry everything” that hides bugs,\n",
        "* Instead: explicit but ergonomic error orchestration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TBKxFwsZ6kF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔧 Example: Error Handling with __error__ Edge\n",
        "\n",
        "\n",
        "## 📝 What’s Happening\n",
        "\n",
        "* **`summarizer1` fails** → raises an exception.\n",
        "* LangGraph detects that → follows the `__error__` edge → runs `fallback_summary`.\n",
        "* Fallback writes a safe default: `\"(FAILED TO SUMMARIZE DOC 1)\"`.\n",
        "* **Execution continues** as normal: both summaries are merged, then Writer → Editor → Rewriter.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Now your orchestrator won’t blow up on a single failure. Instead, it gracefully recovers and produces a final report with a placeholder for the failed part.\n"
      ],
      "metadata": {
        "id": "gd2k0HFK6pUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pprint import pprint\n",
        "\n",
        "#=== Define State\n",
        "class AgentState(TypedDict):\n",
        "    summaries: Annotated[List[str], operator.add]\n",
        "    report: str\n",
        "    edits: str\n",
        "    final: str\n",
        "\n",
        "#=== Dummy docs\n",
        "docs = [\"DOC 1 CONTENT\", \"DOC 2 CONTENT\"]\n",
        "\n",
        "#=== Base LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=200)\n",
        "\n",
        "#=== Nodes\n",
        "def summarize_doc1(state: AgentState) -> AgentState:\n",
        "    # simulate failure for demonstration\n",
        "    raise ValueError(\"Simulated failure in summarizer1\")\n",
        "\n",
        "def summarize_doc2(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Summarize doc2:\\n\\n{docs[1]}\"])\n",
        "    print(\"Summarizer2:\", response.content.split(\"\\n\")[0])\n",
        "    pprint(response.response_metadata.get(\"token_usage\", {}))\n",
        "    return {\"summaries\": [response.content]}\n",
        "\n",
        "def fallback_summary(state: AgentState) -> AgentState:\n",
        "    print(\"⚠️ Fallback triggered for summarizer1\")\n",
        "    return {\"summaries\": [\"(FAILED TO SUMMARIZE DOC 1)\"]}\n",
        "\n",
        "def write_report_node(state: AgentState) -> AgentState:\n",
        "    joined = \"\\n\\n\".join(state[\"summaries\"])\n",
        "    response = llm.invoke([f\"Write a cohesive report:\\n{joined}\"])\n",
        "    print(\"Writer:\", response.content.split(\"\\n\")[0])\n",
        "    return {\"report\": response.content}\n",
        "\n",
        "def edit_report_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Suggest edits for:\\n\\n{state['report']}\"])\n",
        "    print(\"Editor:\", response.content.split(\"\\n\")[0])\n",
        "    return {\"edits\": response.content}\n",
        "\n",
        "def rewrite_final_node(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke([f\"Rewrite report with edits:\\n\\nReport:\\n{state['report']}\\n\\nEdits:\\n{state['edits']}\"])\n",
        "    print(\"Rewriter:\", response.content.split(\"\\n\")[0])\n",
        "    return {\"final\": response.content}\n",
        "\n",
        "#=== Build Graph\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"summarizer1\", summarize_doc1)\n",
        "builder.add_node(\"summarizer2\", summarize_doc2)\n",
        "builder.add_node(\"fallback_summary\", fallback_summary)\n",
        "builder.add_node(\"writer\", write_report_node)\n",
        "builder.add_node(\"editor\", edit_report_node)\n",
        "builder.add_node(\"rewriter\", rewrite_final_node)\n",
        "\n",
        "builder.set_entry_point(\"summarizer1\")\n",
        "builder.add_edge(\"summarizer1\", \"writer\")            # normal path\n",
        "builder.add_edge(\"summarizer2\", \"writer\")            # normal path\n",
        "builder.add_edge(\"summarizer1\", \"fallback_summary\", condition=\"__error__\")  # error path\n",
        "builder.add_edge(\"fallback_summary\", \"writer\")       # continue after fallback\n",
        "builder.add_edge(\"writer\", \"editor\")\n",
        "builder.add_edge(\"editor\", \"rewriter\")\n",
        "builder.add_edge(\"rewriter\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "#=== Run Orchestrator\n",
        "print(\"\\n=== Running Orchestrator with Error Handling ===\\n\")\n",
        "final_state = graph.invoke({})\n",
        "\n",
        "print(\"\\n=== FINAL REPORT (first 300 chars) ===\")\n",
        "print(final_state[\"final\"][:300], \"...\")\n"
      ],
      "metadata": {
        "id": "Q2txV-TghrWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 — that’s the beauty of LangGraph. You don’t need to bolt on some mysterious “error handler framework.” You literally just **drop in a retry wrapper or an error edge**. It’s modular and explicit — like LEGO blocks. 🧱\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 Adding Retry to a Node\n",
        "\n",
        "LangGraph ships a helper called `retry_node` that wraps your node function with retry logic. Example:\n",
        "\n",
        "```python\n",
        "from langgraph.prebuilt import retry_node\n",
        "\n",
        "# Wrap summarizer1 with retry (max 3 attempts, exponential backoff)\n",
        "builder.add_node(\"summarizer1\", retry_node(summarize_doc1, max_attempts=3))\n",
        "```\n",
        "\n",
        "That’s it — no extra boilerplate.\n",
        "If `summarize_doc1` fails:\n",
        "\n",
        "* It will retry up to 3 times (with exponential backoff delays).\n",
        "* If it *still* fails → LangGraph triggers the `__error__` edge you already defined → your fallback node runs.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗️ Combined Error + Retry Flow\n",
        "\n",
        "```python\n",
        "# summarizer1 node is wrapped with retry\n",
        "builder.add_node(\"summarizer1\", retry_node(summarize_doc1, max_attempts=3))\n",
        "builder.add_node(\"summarizer2\", summarize_doc2)\n",
        "builder.add_node(\"fallback_summary\", fallback_summary)\n",
        "\n",
        "# error edge routes failures to fallback\n",
        "builder.add_edge(\"summarizer1\", \"fallback_summary\", condition=\"__error__\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Why it feels “LEGO-like”\n",
        "\n",
        "* **Retry wrapper** = swap one line: `summarize_doc1` → `retry_node(summarize_doc1)`.\n",
        "* **Error fallback** = one extra edge: `condition=\"__error__\"`.\n",
        "* No new classes, no global error registry, no hidden hooks. Just nodes and edges.\n",
        "\n",
        "---\n",
        "\n",
        "✅ So yes — it’s **straightforward, explicit, and composable**. You can literally point to your graph and say:\n",
        "\n",
        "* This node retries 3 times.\n",
        "* If it still fails, flow jumps here.\n",
        "* Then execution continues as normal.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QZr_njIa7N94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right? 🤩 That’s the magic — LangGraph is basically the **orchestrator’s toolkit distilled**.\n",
        "\n",
        "Instead of you writing:\n",
        "\n",
        "* a retry manager,\n",
        "* a dispatcher for tools,\n",
        "* a context/memory system,\n",
        "* and a bunch of custom error flows…\n",
        "\n",
        "LangGraph just says:\n",
        "\n",
        "* **State** = everything you care about,\n",
        "* **Nodes** = functions that read/write state,\n",
        "* **Edges** = what happens next (including error paths),\n",
        "* **Reducers** = how to merge state when multiple things update at once,\n",
        "* **retry_node** = resilience baked in.\n",
        "\n",
        "That’s it. The rest (persistence, streaming, visualization) is extras you can plug in when you want.\n",
        "\n",
        "---\n",
        "\n",
        "### Why it feels like the “best option” for orchestrators:\n",
        "\n",
        "* ✅ *Explicit but compact* → orchestration logic is visible in a graph, not hidden in class hierarchies.\n",
        "* ✅ *Scales gracefully* → same pattern works for 3 nodes or 300 nodes.\n",
        "* ✅ *Batteries included* → retries, error edges, streaming, persistence.\n",
        "* ✅ *No boilerplate* → you focus on your tools/logic, not plumbing.\n",
        "* ✅ *Composable* → swap out nodes, add branches, or parallelize without re-architecting.\n",
        "\n",
        "---\n",
        "\n",
        "So yeah — you got it. It really is **that straightforward** and **that powerful**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ArbBwHYR8Ghg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wNZytQW7sgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}