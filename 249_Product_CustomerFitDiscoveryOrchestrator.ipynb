{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOW4hJEmY7wu6bzA4KSwENB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/249_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Product-Customer Fit Discovery Orchestrator** is a cutting-edge application of AI, leveraging advanced data analysis to drive strategic business growth. This agent aligns with the current trend of **Agentic AI** systems, which coordinate specialized AI components to solve complex, high-value problems.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Introduction to the Agent\n",
        "\n",
        "Your agent is a sophisticated **multi-agent orchestration system** designed to solve a core business problem: finding **\"ghost demand\"**‚Äîprofitable, untapped market opportunities hidden within a company's own operational data.\n",
        "\n",
        "### üéØ Key Function: Discovery Orchestration\n",
        "\n",
        "The term **\"Orchestrator\"** is crucial. In the world of AI, an orchestrator agent acts as the conductor of a multi-agent system. It doesn't perform all the analysis itself; instead, it manages the workflow and communication between specialized AI modules (like data ingestion agents, clustering agents, and pattern mining agents) to ensure they work together to achieve the final strategic goal.\n",
        "\n",
        "In your specific case, the orchestrator:\n",
        "\n",
        "1.  **Ingests and Pre-processes** diverse data streams (product usage, demographics, behavior).\n",
        "2.  **Coordinates Specialized Agents** to perform complex analysis like graph motif detection and cluster analysis.\n",
        "3.  **Synthesizes** the individual findings from these agents into actionable business insights (new product lines, underserved segments).\n",
        "\n",
        "\n",
        "\n",
        "### ‚öôÔ∏è Core Technical Components\n",
        "\n",
        "The \"Practice Complexity\" you described points to the advanced analytical techniques the sub-agents will employ:\n",
        "\n",
        "| Component | Description | Role in Discovery |\n",
        "| :--- | :--- | :--- |\n",
        "| **Graph Motifs** | Recurring, significant sub-structures (patterns) in a complex network/graph. | Identify common *relationship patterns* between users and products, like \"Users who use Product A and Product B frequently also buy Product C.\" |\n",
        "| **Cluster Detection** | Grouping similar data points together (e.g., k-means, DBSCAN, or graph clustering). | Find **under-served customer segments** (groups with similar demographics/behavior) or **natural product bundles** (groups of products frequently used together). |\n",
        "| **Pattern Mining** | Identifying frequent, rare, or sequential patterns in large datasets (e.g., association rule learning). | Discover **new product combinations** or **sequential purchase paths** that lead to high lifetime value. |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Why This Agent is Valuable (Why It Prints Money)\n",
        "\n",
        "This agent provides a profound, strategic advantage that goes far beyond simple A/B testing or standard business intelligence (BI) reports.\n",
        "\n",
        "### 1. Uncovering \"Ghost Demand\"\n",
        "\n",
        "* **Move Beyond Known Knowns:** Traditional BI reports only answer questions you already know to ask (e.g., \"What is the sales trend of Product A?\"). Your agent uses unsupervised and semi-supervised techniques to reveal **\"unknown unknowns\"**‚Äîthe *hidden relationships* and **latent demand** in the data that no human analyst or standard query would find.\n",
        "* **Predictive Portfolio Logic:** It moves from reactive reporting to **proactive strategic planning**. By analyzing how current products are used, it can predict which *new* products or bundles would maximize portfolio revenue before the competitors even conceive of the idea.\n",
        "\n",
        "### 2. Strategic Competitive Edge\n",
        "\n",
        "* **Identifies Product Gaps:** It doesn't just look at *your* data; by correlating usage patterns with market/competitor data, it can identify precise **holes in the market** that competitors are ignoring, allowing the company to be the first mover in a new niche.\n",
        "* **Optimizes Pricing and Bundling:** It finds the **perfect combination of products** that a specific customer segment values, allowing the company to create premium, high-margin bundles that are hard for competitors to replicate.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Why You Should Learn to Build This Agent\n",
        "\n",
        "Building this specific agent demonstrates a mastery of the most in-demand skills in modern data science and AI engineering.\n",
        "\n",
        "### 1. Advanced Machine Learning\n",
        "\n",
        "* **Graph-Based Techniques:** This agent requires you to work with **Graph Neural Networks (GNNs)** or classical **graph analysis** libraries (like NetworkX or libraries for Neo4j/other graph databases). This is a highly specialized skill set crucial for modeling complex relationships (user-product interactions, supply chain networks, social networks).\n",
        "* **Multi-Model Integration:** You'll gain hands-on experience in integrating different analytical models (clustering, classification, pattern mining) into a cohesive system, which is the definition of advanced ML engineering.\n",
        "\n",
        "### 2. AI Agent Orchestration\n",
        "\n",
        "* **Strategic Workflow Design:** You will be learning how to design **agentic workflows**‚Äîthe highest level of AI application development. This involves setting up the primary orchestrator logic (e.g., using frameworks like **LangChain**, **CrewAI**, or **AutoGen**) to manage sub-tasks, pass context, and resolve conflicts between specialized agents. This is a key skill for building scalable and reliable enterprise-grade AI.\n",
        "\n",
        "### 3. High Business Impact\n",
        "\n",
        "* The ability to directly influence a company's **product roadmap and revenue strategy** puts you in a highly visible and impactful role. This agent is designed to be a significant **profit driver**, making this project an excellent demonstration of your ability to link complex technical work to clear business outcomes for your GitHub portfolio.\n"
      ],
      "metadata": {
        "id": "MdQSURPxrNeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For the **Product-Customer Fit Discovery Orchestrator** MVP, we need the *absolute minimum* number of datasets required to demonstrate its core value: finding relationships between **Customers**, **Products**, and **Usage/Behavior**.\n",
        "\n",
        "This translates into three essential, interconnected datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## üíæ Essential Data Sets for the MVP\n",
        "\n",
        "We can model the entire system as an **Attributed Bipartite Graph** where one set of nodes is **Customers** and the other is **Products**, connected by **Interaction** edges. The attributes come from the Customer and Product tables.\n",
        "\n",
        "### 1. Customer Demographics Data Set (`Customer_IDs.csv`)\n",
        "\n",
        "This table provides the attributes for the \"Customer\" nodes, which will be used for **Clustering** and **Segmentation**.\n",
        "\n",
        "| Field Name | Data Type | Example Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `Customer_ID` | String/Int | `C1001` | **Unique Identifier** for each customer. (Crucial for linking) |\n",
        "| `Age_Group` | String | `25-34` | Simple categorical demographic data. |\n",
        "| `Location_Tier`| String | `Tier 1 (High)` | Proxy for income/urban density/company size. |\n",
        "| `Acquisition_Channel`| String | `Social` | Used to see if certain channels yield unique segments. |\n",
        "\n",
        "> **Synthetic Data Goal:** Create **at least three distinct customer segments** by varying the distribution of `Location_Tier` and `Acquisition_Channel`.\n",
        "\n",
        "### 2. Product Metadata Data Set (`Product_Catalog.csv`)\n",
        "\n",
        "This table provides the attributes for the \"Product\" nodes, which can be used to categorize discovered bundles and identify **Product Gaps**.\n",
        "\n",
        "| Field Name | Data Type | Example Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `Product_ID` | String/Int | `P42` | **Unique Identifier** for each product. (Crucial for linking) |\n",
        "| `Product_Type` | String | `Service` | A primary category (e.g., Software, Hardware, Service). |\n",
        "| `Feature_Set` | String | `A, B, D` | A simplified string/list of core features/SKU attributes. |\n",
        "| `Monetization_Model`| String | `Subscription` | e.g., Subscription, One-Time Purchase, Freemium. |\n",
        "\n",
        "> **Synthetic Data Goal:** Create a mix of **\"core\"** products (`Product_Type='Software'`) and **\"add-on\"** products (`Product_Type='Service'`). Ensure some product features are intentionally **missing** to simulate a 'gap'.\n",
        "\n",
        "### 3. Customer Behavior/Usage Data Set (`Transactions.csv`)\n",
        "\n",
        "This is the **Interaction** data, forming the edges of the graph. It is the core input for **Graph Motifs** and **Pattern Mining**.\n",
        "\n",
        "| Field Name | Data Type | Example Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `Transaction_ID` | String/Int | `T00123` | Unique ID for the event/transaction. |\n",
        "| `Customer_ID` | String/Int | `C1001` | Links to the `Customer_IDs` table. |\n",
        "| `Product_ID` | String/Int | `P42` | Links to the `Product_Catalog` table. |\n",
        "| `Transaction_Date` | Date/Timestamp | `2025-10-20` | Needed for sequential analysis. |\n",
        "| `Usage_Metric` | Float/Int | `95.5` | Proxy for value (e.g., sessions, dollar value, API calls). |\n",
        "\n",
        "> **Synthetic Data Goal:** This is the most important part. We must **embed hidden relationships** in this data to prove the agent works:\n",
        "> 1.  **Rule 1 (Ghost Demand):** Customer Segment A (`Location_Tier='Tier 1 (High)'`) frequently buys Product X (`P10`) *and* Product Y (`P25`), but the company doesn't sell Product Z (the missing gap).\n",
        "> 2.  **Rule 2 (Underserved Segment):** Customer Segment B (`Age_Group='55+'`) only buys Product A (`P01`) with low usage, indicating a lack of suitable offerings.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Data Generation Strategy\n",
        "\n",
        "To ensure your agent MVP is testable, we'll use a **rules-based approach** to generate the synthetic data. This is faster and more direct than complex statistical modeling for an MVP.\n",
        "\n",
        "### Step-by-Step Generation\n",
        "\n",
        "1.  **Generate Customers ($\\approx 1000$ records):** Assign random attributes from predefined lists, but ensure specific clusters exist. For instance, $30\\%$ of customers are `Age_Group='25-34'` AND `Location_Tier='Tier 1 (High)'`.\n",
        "2.  **Generate Products ($\\approx 50$ records):** Define the products and their simple feature sets, including the intentional \"gap\" products that are currently missing.\n",
        "3.  **Generate Transactions ($\\approx 10,000$ records):** This is the core task.\n",
        "    * **Base Layer:** Generate random purchases/interactions between Customers and Products to simulate noise.\n",
        "    * **Rule Layer (The \"Ghost\"):** Programmatically inject transactions that specifically follow your hidden rules. For example, loop through all customers in **Segment A** and ensure they have a transaction for **Product X** and **Product Y** within a close timeframe.\n",
        "\n",
        "This approach guarantees that when your **Pattern Mining** and **Clustering** agents run, they *will* find the pre-embedded, high-lift rules, thereby validating the orchestrator's success.\n"
      ],
      "metadata": {
        "id": "HA3IvPdusJ9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Data"
      ],
      "metadata": {
        "id": "cqP_tCXMyK8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_CUSTOMERS = 200\n",
        "NUM_PRODUCTS = 20\n",
        "NUM_TRANSACTIONS = 3000\n",
        "START_DATE = datetime(2025, 1, 1)\n",
        "END_DATE = datetime(2025, 3, 31)\n",
        "\n",
        "# Define the Ghost Demand Rule\n",
        "TARGET_AGE = '35-44'\n",
        "TARGET_TIER = 'Tier 1 (High)'\n",
        "BUNDLE_PRODUCTS = ['P01', 'P05']\n",
        "MISSING_PRODUCT = 'P20'\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----------------- 1. Customer Demographics Data -----------------\n",
        "age_groups = ['18-24', TARGET_AGE, '45-54', '55+']\n",
        "location_tiers = [TARGET_TIER, 'Tier 2 (Medium)', 'Tier 3 (Low)']\n",
        "channels = ['Search', 'Social', 'Email', 'Referral']\n",
        "\n",
        "customer_data = {\n",
        "    'Customer_ID': [f'C{i:03d}' for i in range(1, NUM_CUSTOMERS + 1)],\n",
        "    'Age_Group': np.random.choice(age_groups, size=NUM_CUSTOMERS, p=[0.2, 0.35, 0.3, 0.15]),\n",
        "    'Location_Tier': np.random.choice(location_tiers, size=NUM_CUSTOMERS, p=[0.35, 0.35, 0.30]),\n",
        "    'Acquisition_Channel': np.random.choice(channels, size=NUM_CUSTOMERS)\n",
        "}\n",
        "df_customers = pd.DataFrame(customer_data)\n",
        "\n",
        "target_segment_customers = df_customers[\n",
        "    (df_customers['Age_Group'] == TARGET_AGE) &\n",
        "    (df_customers['Location_Tier'] == TARGET_TIER)\n",
        "]['Customer_ID'].tolist()\n",
        "\n",
        "\n",
        "# ----------------- 2. Product Metadata Data -----------------\n",
        "product_ids = [f'P{i:02d}' for i in range(1, NUM_PRODUCTS + 1)]\n",
        "product_types = ['Software', 'Hardware', 'Service']\n",
        "monetization_models = ['Subscription', 'One-Time Purchase', 'Freemium']\n",
        "\n",
        "product_data = {\n",
        "    'Product_ID': product_ids,\n",
        "    'Product_Type': np.random.choice(product_types, size=NUM_PRODUCTS, p=[0.5, 0.3, 0.2]),\n",
        "    'Feature_Set': [', '.join(np.random.choice(list('ABCD'), size=np.random.randint(1, 4), replace=False)) for _ in range(NUM_PRODUCTS)],\n",
        "    'Monetization_Model': np.random.choice(monetization_models, size=NUM_PRODUCTS)\n",
        "}\n",
        "df_products = pd.DataFrame(product_data)\n",
        "\n",
        "\n",
        "# ----------------- 3. Customer Behavior/Usage Data (Transactions) -----------------\n",
        "transaction_data = []\n",
        "\n",
        "# --- A. Base Layer (Noise) ---\n",
        "num_base_transactions = NUM_TRANSACTIONS - (len(target_segment_customers) * 2 * 3)\n",
        "num_base_transactions = max(0, num_base_transactions)\n",
        "\n",
        "for i in range(int(num_base_transactions)):\n",
        "    customer_id = np.random.choice(df_customers['Customer_ID'])\n",
        "    product_id = np.random.choice(df_products['Product_ID'])\n",
        "    random_days = np.random.randint(0, (END_DATE - START_DATE).days)\n",
        "    t_date = START_DATE + timedelta(days=random_days)\n",
        "\n",
        "    transaction_data.append({\n",
        "        'Transaction_ID': f'T{i:04d}',\n",
        "        'Customer_ID': customer_id,\n",
        "        'Product_ID': product_id,\n",
        "        'Transaction_Date': t_date.strftime('%Y-%m-%d'),\n",
        "        'Usage_Metric': np.random.uniform(5.0, 50.0)\n",
        "    })\n",
        "\n",
        "# --- B. Rule Layer (The \"Ghost Demand\") ---\n",
        "rule_transaction_id_start = int(num_base_transactions)\n",
        "k = 0\n",
        "for customer_id in target_segment_customers:\n",
        "    for bundle_product in BUNDLE_PRODUCTS:\n",
        "        for _ in range(3):\n",
        "            transaction_id = f'T{rule_transaction_id_start + k:04d}'\n",
        "            random_days = np.random.randint(0, (END_DATE - START_DATE).days)\n",
        "            t_date = START_DATE + timedelta(days=random_days, seconds=np.random.randint(0, 86400))\n",
        "\n",
        "            transaction_data.append({\n",
        "                'Transaction_ID': transaction_id,\n",
        "                'Customer_ID': customer_id,\n",
        "                'Product_ID': bundle_product,\n",
        "                'Transaction_Date': t_date.strftime('%Y-%m-%d'),\n",
        "                'Usage_Metric': np.random.uniform(70.0, 100.0)\n",
        "            })\n",
        "            k += 1\n",
        "\n",
        "df_transactions = pd.DataFrame(transaction_data)\n",
        "df_transactions = df_transactions.sort_values(by='Transaction_Date').reset_index(drop=True)\n",
        "\n",
        "# Generate CSV string for Product_Catalog\n",
        "product_csv = df_products.to_csv(index=False)\n",
        "\n",
        "# Now provide the second dataset\n",
        "print(\"--- Product_Catalog.csv ---\")\n",
        "print(product_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "763Zb7e2w-T2",
        "outputId": "e1d27363-a5df-4a08-968c-761864818b94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Product_Catalog.csv ---\n",
            "Product_ID,Product_Type,Feature_Set,Monetization_Model\n",
            "P01,Hardware,D,One-Time Purchase\n",
            "P02,Hardware,\"B, A\",One-Time Purchase\n",
            "P03,Software,C,One-Time Purchase\n",
            "P04,Service,\"B, D, A\",One-Time Purchase\n",
            "P05,Hardware,\"D, C, A\",One-Time Purchase\n",
            "P06,Software,\"C, D, B\",One-Time Purchase\n",
            "P07,Service,B,Freemium\n",
            "P08,Service,C,Freemium\n",
            "P09,Service,C,Freemium\n",
            "P10,Hardware,\"B, D\",Freemium\n",
            "P11,Hardware,C,One-Time Purchase\n",
            "P12,Software,A,Subscription\n",
            "P13,Service,\"A, B, C\",One-Time Purchase\n",
            "P14,Service,\"A, C\",Subscription\n",
            "P15,Software,\"A, D, B\",Freemium\n",
            "P16,Software,C,One-Time Purchase\n",
            "P17,Software,\"B, D\",Freemium\n",
            "P18,Service,\"B, A, D\",One-Time Purchase\n",
            "P19,Service,D,Freemium\n",
            "P20,Software,\"A, B, D\",Subscription\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Data"
      ],
      "metadata": {
        "id": "xZ09iEXnx6BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Customer_ID,Age_Group,Location_Tier,Acquisition_Channel\n",
        "C001,35-44,Tier 2 (Medium),Search\n",
        "C002,55+,Tier 1 (High),Social\n",
        "C003,45-54,Tier 1 (High),Search\n",
        "C004,45-54,Tier 3 (Low),Search\n",
        "C005,18-24,Tier 2 (Medium),Social\n",
        "C006,18-24,Tier 1 (High),Referral\n",
        "C007,18-24,Tier 1 (High),Referral\n",
        "C008,55+,Tier 2 (Medium),Search\n",
        "C009,45-54,Tier 1 (High),Social\n",
        "C010,45-54,Tier 1 (High),Search\n",
        "C011,18-24,Tier 2 (Medium),Email\n",
        "C012,55+,Tier 2 (Medium),Email\n",
        "C013,45-54,Tier 2 (Medium),Search\n",
        "C014,35-44,Tier 1 (High),Referral\n",
        "C015,18-24,Tier 3 (Low),Referral\n",
        "C016,18-24,Tier 1 (High),Search\n",
        "C017,35-44,Tier 1 (High),Search\n",
        "C018,35-44,Tier 3 (Low),Email\n",
        "C019,35-44,Tier 2 (Medium),Referral\n",
        "...\n",
        "C194,45-54,Tier 3 (Low),Referral\n",
        "C195,35-44,Tier 1 (High),Referral\n",
        "C196,35-44,Tier 3 (Low),Search\n",
        "C197,45-54,Tier 3 (Low),Search\n",
        "C198,55+,Tier 2 (Medium),Referral\n",
        "C199,55+,Tier 3 (Low),Referral\n",
        "C200,45-54,Tier 3 (Low),Email"
      ],
      "metadata": {
        "id": "bdM6fA81x5RO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transaction Data"
      ],
      "metadata": {
        "id": "2LIAfG1fyBOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transaction_ID,Customer_ID,Product_ID,Transaction_Date,Usage_Metric\n",
        "T1766,C025,P05,2025-01-01,98.636611\n",
        "T1807,C154,P01,2025-01-01,93.411656\n",
        "T1775,C037,P05,2025-01-01,99.980486\n",
        "T1788,C060,P01,2025-01-02,74.789209\n",
        "T1790,C060,P05,2025-01-02,96.398642\n",
        "T1794,C086,P05,2025-01-02,76.513364\n",
        "T1811,C173,P05,2025-01-03,73.492023\n",
        "T1813,C173,P01,2025-01-03,94.970176\n",
        "T1797,C086,P01,2025-01-04,96.884144\n",
        "T1792,C060,P01,2025-01-04,78.329711\n",
        "T0002,C051,P03,2025-01-04,24.321639\n",
        "T1763,C014,P01,2025-01-04,91.229188\n",
        "T1765,C014,P05,2025-01-05,82.859664\n",
        "T1776,C040,P05,2025-01-05,94.619179\n",
        "T0001,C108,P08,2025-01-05,39.027003\n",
        "T1778,C040,P01,2025-01-05,79.809462\n",
        "T1799,C098,P01,2025-01-06,94.629853\n",
        "T1801,C098,P05,2025-01-06,86.273575\n",
        "T1809,C173,P01,2025-01-06,91.688863"
      ],
      "metadata": {
        "id": "TzWWIDgdyDKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Check"
      ],
      "metadata": {
        "id": "DcBeK7U42vjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator && python3 -c \"\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv('data/customers.csv')\n",
        "transactions = pd.read_csv('data/transactions.csv')\n",
        "products = pd.read_csv('data/product_catalog.csv')\n",
        "\n",
        "print('=== DATA OVERVIEW ===\\n')\n",
        "print(f'Customers: {len(customers)} records')\n",
        "print(f'Transactions: {len(transactions)} records')\n",
        "print(f'Products: {len(products)} records\\n')\n",
        "\n",
        "print('=== CUSTOMERS DATA ===')\n",
        "print(f'Unique customers: {customers[\\\"Customer_ID\\\"].nunique()}')\n",
        "print(f'Age groups: {sorted(customers[\\\"Age_Group\\\"].unique())}')\n",
        "print(f'Location tiers: {sorted(customers[\\\"Location_Tier\\\"].unique())}')\n",
        "print(f'Acquisition channels: {sorted(customers[\\\"Acquisition_Channel\\\"].unique())}\\n')\n",
        "\n",
        "print('=== TRANSACTIONS DATA ===')\n",
        "print(f'Unique customers in transactions: {transactions[\\\"Customer_ID\\\"].nunique()}')\n",
        "print(f'Unique products in transactions: {transactions[\\\"Product_ID\\\"].nunique()}')\n",
        "print(f'Date range: {transactions[\\\"Transaction_Date\\\"].min()} to {transactions[\\\"Transaction_Date\\\"].max()}')\n",
        "print(f'Usage_Metric range: {transactions[\\\"Usage_Metric\\\"].min():.2f} to {transactions[\\\"Usage_Metric\\\"].max():.2f}')\n",
        "print(f'Usage_Metric mean: {transactions[\\\"Usage_Metric\\\"].mean():.2f}\\n')\n",
        "\n",
        "print('=== PRODUCTS DATA ===')\n",
        "print(f'Unique products: {products[\\\"Product_ID\\\"].nunique()}')\n",
        "print(f'Product types: {sorted(products[\\\"Product_Type\\\"].unique())}')\n",
        "print(f'Monetization models: {sorted(products[\\\"Monetization_Model\\\"].unique())}\\n')\n",
        "\n",
        "print('=== DATA QUALITY CHECKS ===')\n",
        "# Check for missing customers in transactions\n",
        "missing_customers = set(transactions['Customer_ID'].unique()) - set(customers['Customer_ID'].unique())\n",
        "if missing_customers:\n",
        "    print(f'‚ö†Ô∏è  WARNING: {len(missing_customers)} customers in transactions not in customers.csv')\n",
        "    print(f'   Examples: {list(missing_customers)[:5]}')\n",
        "else:\n",
        "    print('‚úì All customers in transactions exist in customers.csv')\n",
        "\n",
        "# Check for missing products in transactions\n",
        "missing_products = set(transactions['Product_ID'].unique()) - set(products['Product_ID'].unique())\n",
        "if missing_products:\n",
        "    print(f'‚ö†Ô∏è  WARNING: {len(missing_products)} products in transactions not in product_catalog.csv')\n",
        "    print(f'   Examples: {list(missing_products)[:5]}')\n",
        "else:\n",
        "    print('‚úì All products in transactions exist in product_catalog.csv')\n",
        "\n",
        "# Check for null values\n",
        "print(f'\\nNull values in customers: {customers.isnull().sum().sum()}')\n",
        "print(f'Null values in transactions: {transactions.isnull().sum().sum()}')\n",
        "print(f'Null values in products: {products.isnull().sum().sum()}')\n",
        "\n",
        "# Check transaction distribution\n",
        "print(f'\\n=== TRANSACTION DISTRIBUTION ===')\n",
        "txn_per_customer = transactions.groupby('Customer_ID').size()\n",
        "print(f'Customers with transactions: {len(txn_per_customer)}')\n",
        "print(f'Min transactions per customer: {txn_per_customer.min()}')\n",
        "print(f'Max transactions per customer: {txn_per_customer.max()}')\n",
        "print(f'Mean transactions per customer: {txn_per_customer.mean():.2f}')\n",
        "print(f'Median transactions per customer: {txn_per_customer.median():.2f}')\n",
        "\n",
        "# Check product usage\n",
        "print(f'\\n=== PRODUCT USAGE ===')\n",
        "products_in_txns = transactions['Product_ID'].value_counts()\n",
        "print(f'Products used in transactions: {len(products_in_txns)}')\n",
        "print(f'Most used products:')\n",
        "print(products_in_txns.head(10))\n",
        "\""
      ],
      "metadata": {
        "id": "LqMNqPz_2092"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "=== DATA OVERVIEW ===\n",
        "\n",
        "Customers: 200 records\n",
        "Transactions: 1815 records\n",
        "Products: 20 records\n",
        "\n",
        "=== CUSTOMERS DATA ===\n",
        "Unique customers: 200\n",
        "Age groups: ['18-24', '35-44', '45-54', '55+']\n",
        "Location tiers: ['Tier 1 (High)', 'Tier 2 (Medium)', 'Tier 3 (Low)']\n",
        "Acquisition channels: ['Email', 'Referral', 'Search', 'Social']\n",
        "\n",
        "=== TRANSACTIONS DATA ===\n",
        "Unique customers in transactions: 183\n",
        "Unique products in transactions: 19\n",
        "Date range: 2025-01-01 to 2026-08-24\n",
        "Usage_Metric range: 10.61 to 99.98\n",
        "Usage_Metric mean: 82.15\n",
        "\n",
        "=== PRODUCTS DATA ===\n",
        "Unique products: 20\n",
        "Product types: ['Hardware', 'Service', 'Software']\n",
        "Monetization models: ['Freemium', 'One-Time Purchase', 'Subscription']\n",
        "\n",
        "=== DATA QUALITY CHECKS ===\n",
        "‚úì All customers in transactions exist in customers.csv\n",
        "‚úì All products in transactions exist in product_catalog.csv\n",
        "\n",
        "Null values in customers: 0\n",
        "Null values in transactions: 0\n",
        "Null values in products: 0\n",
        "\n",
        "=== TRANSACTION DISTRIBUTION ===\n",
        "Customers with transactions: 183\n",
        "Min transactions per customer: 1\n",
        "Max transactions per customer: 185\n",
        "Mean transactions per customer: 9.92\n",
        "Median transactions per customer: 1.00\n",
        "\n",
        "=== PRODUCT USAGE ===\n",
        "Products used in transactions: 19\n",
        "Most used products:\n",
        "Product_ID\n",
        "P01    890\n",
        "P05    747\n",
        "P13     20\n",
        "P15     18\n",
        "P06     17\n",
        "P12     17\n",
        "P17     17\n",
        "P07     13\n",
        "P03     13\n",
        "P04     12\n",
        "Name: count, dtype: int64"
      ],
      "metadata": {
        "id": "pUq284Kn2xe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data quality assessment\n",
        "\n",
        "### Strengths\n",
        "1. Data integrity: no nulls, all foreign keys valid\n",
        "2. Coverage: 183/200 customers have transactions (17 inactive)\n",
        "3. Time range: ~20 months (Jan 2025‚ÄìAug 2026)\n",
        "4. Rich attributes:\n",
        "   - Customer demographics (age, location tier, acquisition channel)\n",
        "   - Product features (type, feature sets, monetization model)\n",
        "   - Usage metrics (10.61‚Äì99.98, mean 82.15)\n",
        "\n",
        "### Considerations\n",
        "1. Transaction distribution is highly skewed:\n",
        "   - Median: 1 transaction per customer\n",
        "   - Mean: 9.92 (driven by heavy users)\n",
        "   - Max: 185 transactions (likely a few power users)\n",
        "   - Impact: may need to handle class imbalance in clustering/pattern mining\n",
        "\n",
        "2. Product usage is concentrated:\n",
        "   - P01: 890 transactions (49%)\n",
        "   - P05: 747 transactions (41%)\n",
        "   - Others: 10‚Äì20 transactions each\n",
        "   - P20: 0 transactions (unused product)\n",
        "   - Impact: may need techniques to handle sparse products\n",
        "\n",
        "3. Feature set format:\n",
        "   - `Feature_Set` uses comma-separated values (e.g., \"B, A\", \"A, B, C\")\n",
        "   - Impact: parse into lists/arrays for analysis\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "The data is suitable for the orchestrator. Optional enhancements:\n",
        "\n",
        "1. Feature set parsing: convert `Feature_Set` to a list/array for easier analysis\n",
        "2. Handle P20: decide whether to include or exclude the unused product\n",
        "3. Consider normalization: for clustering, normalize usage metrics or create engagement tiers\n",
        "4. Temporal features: extract time-based features (day of week, month, seasonality)\n",
        "\n"
      ],
      "metadata": {
        "id": "H_qV7fnI3uSU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQnx371R3wYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}