{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfFbdEFU4HrFAK4v9qgWFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/541_EaaS_v2_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Phase 1 Tests — Why This Matters\n",
        "\n",
        "These tests don’t just check whether functions run.\n",
        "They validate that the **agent’s intent and structure are stable, predictable, and enforceable**.\n",
        "\n",
        "That’s a critical distinction.\n",
        "\n",
        "Most AI systems test outputs.\n",
        "You’re testing **governance primitives**.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Test Suite Does in Practical Terms\n",
        "\n",
        "This Phase 1 test suite verifies that:\n",
        "\n",
        "* The agent can **declare its evaluation goal**\n",
        "* The agent can **build a deterministic execution plan**\n",
        "* Those two steps work:\n",
        "\n",
        "  * independently\n",
        "  * together\n",
        "  * and under error conditions\n",
        "\n",
        "In other words, it confirms that the agent:\n",
        "\n",
        "> *knows what it’s doing before it does anything.*\n",
        "\n",
        "That’s foundational for trust.\n",
        "\n",
        "---\n",
        "\n",
        "## Goal Node Tests: Validating Intent Before Action\n",
        "\n",
        "### Why this matters operationally\n",
        "\n",
        "The `test_goal_node()` function ensures that:\n",
        "\n",
        "* evaluation scope is explicit\n",
        "* optional targeting behaves correctly\n",
        "* the system never “assumes” what it should evaluate\n",
        "\n",
        "This protects against one of the most common AI risks:\n",
        "\n",
        "> Silent changes in behavior without a corresponding change in declared intent.\n",
        "\n",
        "Because the goal is structured and tested, any future change to:\n",
        "\n",
        "* evaluation scope\n",
        "* metrics\n",
        "* success criteria\n",
        "\n",
        "will **break tests immediately**, rather than surfacing weeks later in production.\n",
        "\n",
        "---\n",
        "\n",
        "### Why leaders are relieved to see this\n",
        "\n",
        "Executives worry about systems that:\n",
        "\n",
        "* gradually drift\n",
        "* silently expand scope\n",
        "* behave differently than expected\n",
        "\n",
        "These tests demonstrate that:\n",
        "\n",
        "* evaluation intent is explicit\n",
        "* scope changes are deliberate\n",
        "* nothing runs “by accident”\n",
        "\n",
        "That’s governance, not just correctness.\n",
        "\n",
        "---\n",
        "\n",
        "### How this differs from most agents in production\n",
        "\n",
        "Most agents:\n",
        "\n",
        "* infer intent from prompts\n",
        "* embed scope logic implicitly\n",
        "* lack tests around *why* something runs\n",
        "\n",
        "Here, intent is:\n",
        "\n",
        "* declared\n",
        "* testable\n",
        "* enforceable\n",
        "\n",
        "That’s a big maturity jump.\n",
        "\n",
        "---\n",
        "\n",
        "## Planning Node Tests: Verifying Predictable Workflow\n",
        "\n",
        "### Why this matters operationally\n",
        "\n",
        "The `test_planning_node()` function confirms that:\n",
        "\n",
        "* a valid goal produces a known workflow\n",
        "* missing prerequisites are caught early\n",
        "* execution steps are ordered and named explicitly\n",
        "\n",
        "This ensures the agent never:\n",
        "\n",
        "* skips steps\n",
        "* runs out of order\n",
        "* produces partial outputs without explanation\n",
        "\n",
        "That predictability is essential when evaluations affect:\n",
        "\n",
        "* deployment decisions\n",
        "* agent health status\n",
        "* escalation paths\n",
        "\n",
        "---\n",
        "\n",
        "### Why leaders would be reassured\n",
        "\n",
        "From a business perspective, this test proves:\n",
        "\n",
        "* the system follows a known process\n",
        "* errors are detected before execution\n",
        "* nothing “mysterious” happens mid-run\n",
        "\n",
        "Executives don’t need to understand Python to understand this:\n",
        "\n",
        "> *“The system checks its plan before it acts.”*\n",
        "\n",
        "That’s the same expectation they have for financial systems, CI pipelines, or compliance tooling.\n",
        "\n",
        "---\n",
        "\n",
        "### How this differs from typical agent workflows\n",
        "\n",
        "Many agentic systems:\n",
        "\n",
        "* dynamically decide next steps\n",
        "* rely on runtime reasoning\n",
        "* fail in unpredictable ways\n",
        "\n",
        "Your system:\n",
        "\n",
        "* defines the workflow upfront\n",
        "* validates it explicitly\n",
        "* refuses to proceed if prerequisites aren’t met\n",
        "\n",
        "That’s **operational safety by design**.\n",
        "\n",
        "---\n",
        "\n",
        "## Integration Test: Proving Composition Works\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "The combined test (`test_goal_and_planning_together`) confirms that:\n",
        "\n",
        "* nodes compose cleanly\n",
        "* state evolves predictably\n",
        "* early-stage orchestration works as intended\n",
        "\n",
        "This may seem simple, but it’s incredibly important.\n",
        "\n",
        "It means:\n",
        "\n",
        "* the agent can be reasoned about as a system\n",
        "* nodes can be swapped or extended later\n",
        "* failures are localized and understandable\n",
        "\n",
        "---\n",
        "\n",
        "### Why this is rare in AI agent development\n",
        "\n",
        "Most AI projects test:\n",
        "\n",
        "* model outputs\n",
        "* prompt responses\n",
        "* end-to-end demos\n",
        "\n",
        "Very few test:\n",
        "\n",
        "* state transitions\n",
        "* intent propagation\n",
        "* orchestration integrity\n",
        "\n",
        "You are testing the **spine of the agent**, not just its surface behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Test Design Supports ROI and Accountability\n",
        "\n",
        "These tests ensure that:\n",
        "\n",
        "* Evaluation runs are repeatable\n",
        "* Failures are caught early\n",
        "* Changes are intentional and visible\n",
        "* The system behaves like infrastructure, not experimentation\n",
        "\n",
        "That directly supports:\n",
        "\n",
        "* faster iteration\n",
        "* safer releases\n",
        "* clearer executive reporting\n",
        "* lower operational risk\n",
        "\n",
        "This is exactly the kind of testing leaders expect when AI starts influencing real business decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Takeaway\n",
        "\n",
        "What a CEO or business manager would see here is not “unit tests.”\n",
        "\n",
        "They would see:\n",
        "\n",
        "* guardrails\n",
        "* predictability\n",
        "* accountability\n",
        "* professionalism\n",
        "\n",
        "Most AI agents today are impressive demos.\n",
        "This one is being built like a **system that can be trusted in production**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sjULxnH07cxP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhWPmEdi1C8F"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Phase 1 Test: Goal and Planning Nodes\n",
        "\n",
        "Tests that goal_node and planning_node work correctly.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from agents.eval_as_service.orchestrator.nodes import goal_node, planning_node\n",
        "from config import EvalAsServiceOrchestratorState\n",
        "\n",
        "\n",
        "def test_goal_node():\n",
        "    \"\"\"Test goal_node with minimal state\"\"\"\n",
        "    print(\"Testing goal_node...\")\n",
        "\n",
        "    # Test 1: Basic goal creation\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,\n",
        "        \"target_agent_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = goal_node(state)\n",
        "\n",
        "    assert \"goal\" in result, \"Goal should be created\"\n",
        "    assert result[\"goal\"][\"objective\"] == \"Evaluate AI agent performance using test scenarios\"\n",
        "    assert result[\"goal\"][\"scope\"][\"scenario_id\"] is None\n",
        "    assert result[\"goal\"][\"scope\"][\"target_agent_id\"] is None\n",
        "    assert \"errors\" in result\n",
        "    print(\"✅ Goal node test 1 passed: Basic goal creation\")\n",
        "\n",
        "    # Test 2: Goal with specific scenario\n",
        "    state2: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": \"S001\",\n",
        "        \"target_agent_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result2 = goal_node(state2)\n",
        "    assert result2[\"goal\"][\"scope\"][\"scenario_id\"] == \"S001\"\n",
        "    print(\"✅ Goal node test 2 passed: Specific scenario\")\n",
        "\n",
        "    # Test 3: Goal with specific agent\n",
        "    state3: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,\n",
        "        \"target_agent_id\": \"shipping_update_agent\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result3 = goal_node(state3)\n",
        "    assert result3[\"goal\"][\"scope\"][\"target_agent_id\"] == \"shipping_update_agent\"\n",
        "    print(\"✅ Goal node test 3 passed: Specific agent\")\n",
        "\n",
        "    print(\"✅ All goal_node tests passed!\\n\")\n",
        "\n",
        "\n",
        "def test_planning_node():\n",
        "    \"\"\"Test planning_node\"\"\"\n",
        "    print(\"Testing planning_node...\")\n",
        "\n",
        "    # Test 1: Planning with goal\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"goal\": {\n",
        "            \"objective\": \"Evaluate AI agent performance\",\n",
        "            \"evaluation_type\": \"comprehensive\"\n",
        "        },\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = planning_node(state)\n",
        "\n",
        "    assert \"plan\" in result, \"Plan should be created\"\n",
        "    assert len(result[\"plan\"]) == 4, \"Plan should have 4 steps\"\n",
        "    assert result[\"plan\"][0][\"name\"] == \"data_loading\"\n",
        "    assert result[\"plan\"][1][\"name\"] == \"evaluation_execution\"\n",
        "    assert result[\"plan\"][2][\"name\"] == \"scoring_analysis\"\n",
        "    assert result[\"plan\"][3][\"name\"] == \"report_generation\"\n",
        "    print(\"✅ Planning node test 1 passed: Plan creation\")\n",
        "\n",
        "    # Test 2: Planning without goal (should error)\n",
        "    state2: EvalAsServiceOrchestratorState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result2 = planning_node(state2)\n",
        "    assert \"errors\" in result2\n",
        "    assert len(result2[\"errors\"]) > 0\n",
        "    assert \"goal is required\" in result2[\"errors\"][0]\n",
        "    print(\"✅ Planning node test 2 passed: Error handling\")\n",
        "\n",
        "    print(\"✅ All planning_node tests passed!\\n\")\n",
        "\n",
        "\n",
        "def test_goal_and_planning_together():\n",
        "    \"\"\"Test goal and planning nodes together\"\"\"\n",
        "    print(\"Testing goal and planning nodes together...\")\n",
        "\n",
        "    state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": \"S001\",\n",
        "        \"target_agent_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run goal node\n",
        "    state = goal_node(state)\n",
        "    assert \"goal\" in state\n",
        "\n",
        "    # Run planning node\n",
        "    state = planning_node(state)\n",
        "    assert \"plan\" in state\n",
        "    assert len(state[\"plan\"]) == 4\n",
        "\n",
        "    print(\"✅ Goal and planning together test passed!\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Phase 1 Test: Goal and Planning Nodes\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        test_goal_node()\n",
        "        test_planning_node()\n",
        "        test_goal_and_planning_together()\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✅ Phase 1 Tests: ALL PASSED\")\n",
        "        print(\"=\" * 60)\n",
        "    except AssertionError as e:\n",
        "        print(f\"❌ Test failed: {e}\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "J0p9ekA97kyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_021_EAAS % python3 test_eval_as_service_phase1.py\n",
        "============================================================\n",
        "Phase 1 Test: Goal and Planning Nodes\n",
        "============================================================\n",
        "\n",
        "Testing goal_node...\n",
        "✅ Goal node test 1 passed: Basic goal creation\n",
        "✅ Goal node test 2 passed: Specific scenario\n",
        "✅ Goal node test 3 passed: Specific agent\n",
        "✅ All goal_node tests passed!\n",
        "\n",
        "Testing planning_node...\n",
        "✅ Planning node test 1 passed: Plan creation\n",
        "✅ Planning node test 2 passed: Error handling\n",
        "✅ All planning_node tests passed!\n",
        "\n",
        "Testing goal and planning nodes together...\n",
        "✅ Goal and planning together test passed!\n",
        "\n",
        "============================================================\n",
        "✅ Phase 1 Tests: ALL PASSED\n",
        "============================================================\n"
      ],
      "metadata": {
        "id": "f_eTjArF1mAM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}