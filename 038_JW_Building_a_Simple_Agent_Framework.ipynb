{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkIwDwWCJh2qIOLmtleRPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/038_Building_a_Simple_Agent_Frameworkipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Building a Simple Agent Framework\n",
        "\n",
        "Now, we are going to put the components together into a **reusable agent class**. This class encapsulates the GAME components and provides a clean interface for running the agent loop. We can create different agents simply by changing the **Goals**, **Actions**, **Memory**, and **Environment** (GAME) without modifying the core loop."
      ],
      "metadata": {
        "id": "Q82Y6AK8Cm1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2spKWItNCJDf"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"user\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† What Is `AgentLanguage`?\n",
        "\n",
        "The `AgentLanguage` class is a **core abstraction** responsible for:\n",
        "\n",
        "* üìù **Formatting the prompt** that gets sent to the LLM\n",
        "* üì¶ **Parsing the model‚Äôs response** into structured tool calls\n",
        "\n",
        "It acts as the ‚Äúlanguage layer‚Äù that determines how your agent **talks to** and **interprets responses from** the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How `AgentLanguage` Is Used in the Agent Loop\n",
        "\n",
        "### üîπ Step 1: Constructing the Prompt\n",
        "\n",
        "When the agent loop begins, it first constructs the LLM prompt like this:\n",
        "\n",
        "```python\n",
        "def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "    return self.agent_language.construct_prompt(\n",
        "        actions=actions.get_actions(),\n",
        "        environment=self.environment,\n",
        "        goals=goals,\n",
        "        memory=memory\n",
        "    )\n",
        "```\n",
        "\n",
        "This method builds a **structured input** for the model using four key components:\n",
        "\n",
        "| Component      | Description                                                             |\n",
        "| -------------- | ----------------------------------------------------------------------- |\n",
        "| üß≠ Goals       | What the agent is trying to accomplish                                  |\n",
        "| üõ†Ô∏è Actions    | The tools or functions available to the agent                           |\n",
        "| üß† Memory      | Prior messages, file contents, and context relevant to the current task |\n",
        "| üåç Environment | Constraints, settings, or metadata about where the agent is running     |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Parsing Responses: AgentLanguage Again\n",
        "\n",
        "Later in the loop, after receiving the LLM response, `AgentLanguage` also handles **interpreting that response**:\n",
        "\n",
        "```python\n",
        "action, invocation = self.agent_language.parse_action_response(response)\n",
        "```\n",
        "\n",
        "If you're using **OpenAI function calling**, this is typically as simple as extracting the structured `tool_calls` block.\n",
        "\n",
        "But by **decoupling** the formatting/parsing logic into `AgentLanguage`, you can:\n",
        "\n",
        "* üîÅ Swap out OpenAI for another LLM format\n",
        "* üìú Use natural language responses instead of tool calls (for simulation)\n",
        "* üß™ Test your agent without hard-coding the LLM's output logic\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Why This Matters\n",
        "\n",
        "This modular `AgentLanguage` component lets your agent:\n",
        "\n",
        "* Speak **different languages** (OpenAI function calling, plain text, etc.)\n",
        "* Be tested or simulated in different contexts\n",
        "* Separate **communication logic** from **business logic**\n",
        "\n",
        "So even though we use OpenAI tools most of the time, this abstraction gives you a path to scale, extend, and simulate more flexibly.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üß† **Memory Context**\n",
        "\n",
        "**Definition**:\n",
        "Memory is a record of everything the agent has \"seen\" or \"done\" so far in the current session.\n",
        "\n",
        "### üîç Why it matters:\n",
        "\n",
        "* It helps the agent **maintain continuity** over multiple steps.\n",
        "* Without it, the agent would be stateless ‚Äî it would forget past actions, results, or user instructions.\n",
        "\n",
        "### üß© What's typically stored:\n",
        "\n",
        "* The **user‚Äôs original request**\n",
        "* Past **LLM responses**\n",
        "* Previous **tool invocations**\n",
        "* Results of those tools (e.g., content from a file it read)\n",
        "\n",
        "### ‚úÖ Example:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"type\": \"user\", \"content\": \"Refactor the data processing script.\"},\n",
        "  {\"type\": \"assistant\", \"content\": \"I'll begin by listing files in the 'scripts/' directory.\"},\n",
        "  {\"type\": \"user\", \"content\": \"['clean.py', 'process.py']\"},\n",
        "  {\"type\": \"assistant\", \"content\": \"Reading process.py now.\"},\n",
        "  {\"type\": \"user\", \"content\": \"# contents of process.py...\"}\n",
        "]\n",
        "```\n",
        "\n",
        "This history allows the agent to ask follow-up questions, avoid repeating steps, and remember relevant files or outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## üåç **Environment Info**\n",
        "\n",
        "**Definition**:\n",
        "A description of the **context** in which the agent is operating. This is static info about the world the agent is working in.\n",
        "\n",
        "### üì¶ Typical contents:\n",
        "\n",
        "* \"You are working in a local Python project folder.\"\n",
        "* \"You have access to functions like `list_files()` and `read_file()`.\"\n",
        "* \"You do *not* have internet access.\"\n",
        "\n",
        "### üîê Why it matters:\n",
        "\n",
        "It **guides the agent‚Äôs reasoning boundaries**:\n",
        "\n",
        "* Prevents hallucinating actions it cannot perform (like calling an API when it's not available).\n",
        "* Sets correct assumptions about what data/tools are available.\n",
        "\n",
        "### ‚úÖ Example:\n",
        "\n",
        "```text\n",
        "You are operating in a local development environment.\n",
        "You can read and write files, but cannot access the internet.\n",
        "Use the tools provided below to complete the task.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Summary\n",
        "\n",
        "| Component        | Purpose                                           | Keeps Agent Aware Of     |\n",
        "| ---------------- | ------------------------------------------------- | ------------------------ |\n",
        "| Memory Context   | Conversation and execution history                | What‚Äôs already been done |\n",
        "| Environment Info | Operational boundaries and available capabilities | What‚Äôs *possible* to do  |\n",
        "\n"
      ],
      "metadata": {
        "id": "qn60vwmVDnHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ Step 2: Generating a Response\n",
        "\n",
        "After the prompt is constructed, the agent sends it to the language model:\n",
        "\n",
        "```python\n",
        "def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "    response = self.generate_response(full_prompt)\n",
        "    return response\n",
        "```\n",
        "\n",
        "### üîß What's Happening?\n",
        "\n",
        "* `generate_response()` is an **injected function** defined during initialization.\n",
        "* It handles the actual call to the LLM.\n",
        "* This abstraction allows the agent framework to stay **model-agnostic**.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ Why This Matters\n",
        "\n",
        "This design provides flexibility:\n",
        "\n",
        "* You can use **LiteLLM**, **OpenAI**, **Anthropic**, or any LLM.\n",
        "* You don‚Äôt have to change the core agent loop to switch models.\n",
        "* Great for mocking, testing, or deploying in environments with different LLMs.\n"
      ],
      "metadata": {
        "id": "0r7MRBjWE26f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 3: Parsing the Response\n",
        "\n",
        "```python\n",
        "action, invocation = self.get_action(response)\n",
        "```\n",
        "\n",
        "* `AgentLanguage` parses the model‚Äôs reply.\n",
        "* `ActionRegistry` retrieves the action definition.\n",
        "* `invocation` contains the tool name and argument values.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What Is `AgentLanguage`?\n",
        "\n",
        "`AgentLanguage` is a **custom abstraction** used in this lecture's framework to:\n",
        "\n",
        "> Define how the LLM communicates actions to the agent.\n",
        "\n",
        "It **controls how the LLM should express tool calls** in text, and how we **parse** those calls from its output.\n",
        "\n",
        "### Think of `AgentLanguage` as:\n",
        "\n",
        "* A bridge between **natural language** (from the LLM) and **structured commands** (the agent executes).\n",
        "* A reusable module that **parses the LLM's output** into something your code can understand and act on.\n",
        "\n",
        "### It likely provides:\n",
        "\n",
        "* `format_action(action_name, args)` ‚Üí Returns a prompt string to tell the LLM how to call the tool.\n",
        "* `parse_action_response(response_text)` ‚Üí Extracts `action_name` and `args` from the LLM‚Äôs text.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† So What Is `get_action()` Doing?\n",
        "\n",
        "In the agent loop:\n",
        "\n",
        "```python\n",
        "action, invocation = self.get_action(response)\n",
        "```\n",
        "\n",
        "Here‚Äôs what‚Äôs happening:\n",
        "\n",
        "1. **`response`** = the raw LLM output (text).\n",
        "2. `self.get_action()` internally calls something like:\n",
        "\n",
        "   ```python\n",
        "   return self.language.parse_action_response(response)\n",
        "   ```\n",
        "3. That parsing breaks the response into:\n",
        "\n",
        "   * `action`: the name of the tool (e.g., `\"read_python_file\"`)\n",
        "   * `invocation`: the actual argument dictionary (e.g., `{\"file_name\": \"main.py\"}`)\n",
        "\n",
        "So **you don‚Äôt see `parse_response()` defined**, because it's likely wrapped inside this `AgentLanguage` object‚Äôs method.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Where Does This Fit in the Loop?\n",
        "\n",
        "The structure is roughly:\n",
        "\n",
        "```python\n",
        "prompt = self.construct_prompt(...)\n",
        "response = llm_call(prompt)\n",
        "action, invocation = agent_language.parse_action_response(response)\n",
        "tool_fn = registry[action]\n",
        "tool_fn(**invocation)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary\n",
        "\n",
        "| Term            | Role                                                    |\n",
        "| --------------- | ------------------------------------------------------- |\n",
        "| `AgentLanguage` | Class that defines LLM input/output formatting          |\n",
        "| `get_action()`  | Uses `AgentLanguage` to extract tool call from response |\n",
        "| `invocation`    | Parsed tool arguments ready to call in Python           |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IqSSAFSSE9Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üîß Step 4: Executing the Action\n",
        "\n",
        "Once the tool and arguments are known, the agent performs the action:\n",
        "\n",
        "```python\n",
        "result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "```\n",
        "\n",
        "### üõ†Ô∏è Execution Happens in the Environment\n",
        "\n",
        "* The `Environment` class is responsible for **carrying out the action**.\n",
        "* This might involve:\n",
        "\n",
        "  * Calling APIs\n",
        "  * Accessing files\n",
        "  * Running computations\n",
        "  * Querying databases\n",
        "\n",
        "### üß© Separation of Concerns\n",
        "\n",
        "* The `ActionRegistry` knows **what** can be done.\n",
        "* The `Environment` knows **how** to do it in the current context.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6BAWzUJTV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### üß† Step 5: Updating Memory\n",
        "\n",
        "After the agent executes an action, it needs to **record what happened**‚Äîboth the decision it made and the result of that action.\n",
        "\n",
        "```python\n",
        "def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "    \"\"\"\n",
        "    Update memory with the agent's decision and the environment's response.\n",
        "    \"\"\"\n",
        "    new_memories = [\n",
        "        {\"type\": \"assistant\", \"content\": response},             # What the LLM said\n",
        "        {\"type\": \"user\", \"content\": json.dumps(result)}         # What the environment returned\n",
        "    ]\n",
        "    for m in new_memories:\n",
        "        memory.add_memory(m)\n",
        "```\n",
        "\n",
        "### üí° Why Update Memory?\n",
        "\n",
        "* Keeps a **chronological record** of what the agent did and why.\n",
        "* Memory becomes **part of the next prompt**, allowing the agent to reason across multiple steps.\n",
        "* Helps the agent **avoid repetition**, reuse past insights, and improve coherence.\n",
        "\n",
        "### üß† What Gets Stored?\n",
        "\n",
        "1. The **LLM‚Äôs response** (usually a tool call or text-based reasoning).\n",
        "2. The **environment‚Äôs result** (i.e., output from the executed action).\n",
        "\n",
        "Together, this builds a conversational history that gets looped back into the LLM‚Äôs input, making the agent smarter over time.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Here's what happens regarding token usage and cost:\n",
        "\n",
        "#### üíæ 1. **Memory is added to the prompt**\n",
        "\n",
        "Each time the agent loops:\n",
        "\n",
        "* It **constructs a new prompt**.\n",
        "* This prompt includes:\n",
        "\n",
        "  * The agent‚Äôs goals.\n",
        "  * Available tool definitions (tool schemas).\n",
        "  * The full **memory history** so far (unless truncated).\n",
        "  * Possibly some environmental context.\n",
        "\n",
        "#### üîÑ 2. **All of that is sent to the LLM**\n",
        "\n",
        "* OpenAI (and most LLM APIs) **charge based on token input and output**.\n",
        "* So every piece of memory stored ‚Äî whether it‚Äôs the user's request, the agent's tool call, or the tool's result ‚Äî will be **converted into tokens** and **counted toward the prompt token budget**.\n",
        "\n",
        "#### üí∏ 3. **You pay for it**\n",
        "\n",
        "* You're billed for:\n",
        "\n",
        "  * All **tokens in the request** (the full prompt).\n",
        "  * All **tokens in the response** (e.g., a tool call or text reply).\n",
        "* In long-running loops, the **memory can grow large**, increasing prompt size and cost quickly.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† How to Manage This\n",
        "\n",
        "* üîÑ **Summarize or compress memory**: Store only the essential decisions and results.\n",
        "* üìâ **Truncate old context**: Only keep the last N exchanges.\n",
        "* üß± **Structured memory**: Instead of dumping raw results, store simplified records (e.g., `\"Searched files A, B, found match in B\"`).\n",
        "* üß† **Hybrid memory**: Use a vector store or database to retrieve memory selectively, instead of including all memory in every call.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HyrXBAa1J4Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚õî Step 6: Termination Check\n",
        "\n",
        "```python\n",
        "if self.should_terminate(response): break\n",
        "```\n",
        "\n",
        "* Uses `action_def.terminal` to see if the agent should stop.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why We Need a Termination Check\n",
        "\n",
        "Agents usually run in loops:\n",
        "\n",
        "1. Construct a prompt.\n",
        "2. Call the LLM.\n",
        "3. Decide what to do.\n",
        "4. Execute a tool.\n",
        "5. Update memory.\n",
        "6. Repeat...\n",
        "\n",
        "But this can‚Äôt go on forever. At some point, the agent must decide:\n",
        "\n",
        "> ‚ÄúI‚Äôve done everything I needed to do. Time to stop.‚Äù\n",
        "\n",
        "That‚Äôs where the **`should_terminate()`** check comes in.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What the Code Does\n",
        "\n",
        "```python\n",
        "def should_terminate(self, response: str) -> bool:\n",
        "    action_def, _ = self.get_action(response)\n",
        "    return action_def.terminal\n",
        "```\n",
        "\n",
        "Here‚Äôs what this means, step by step:\n",
        "\n",
        "1. **`get_action(response)`**\n",
        "   ‚Üí This parses the model‚Äôs response and returns:\n",
        "\n",
        "   * `action_def`: the tool definition (from the registry)\n",
        "   * `invocation`: the tool name + arguments\n",
        "\n",
        "2. **`action_def.terminal`**\n",
        "   ‚Üí This is a Boolean property on the tool definition:\n",
        "\n",
        "   * If `True`, this tool is a **terminal action**, i.e. ‚ÄúI'm done.‚Äù\n",
        "   * If `False`, the loop should continue.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example: ‚Äúterminate\\_agent‚Äù Tool\n",
        "\n",
        "You might define a tool like this:\n",
        "\n",
        "```python\n",
        "Action(\n",
        "    name=\"terminate_agent\",\n",
        "    description=\"Indicates that the task is complete and the agent should shut down.\",\n",
        "    parameters={},\n",
        "    terminal=True  # <--- THIS is what causes the loop to end\n",
        ")\n",
        "```\n",
        "\n",
        "When the model selects this action, `should_terminate()` will return `True`, and the loop exits.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Design Is Powerful\n",
        "\n",
        "* ‚úÖ It gives the **LLM control** over when it's done ‚Äî like finishing a conversation.\n",
        "* ‚úÖ It keeps the agent loop generic ‚Äî you don‚Äôt hardcode ‚Äúwhen‚Äù to stop.\n",
        "* ‚úÖ You can have **multiple terminal tools**, like `terminate`, `cancel`, or `complete_task`.\n",
        "\n"
      ],
      "metadata": {
        "id": "lh8XTfqaLOoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## üîÅ Information Flow in One Loop Iteration\n",
        "\n",
        "1. **Memory**: Supplies past conversations and results.\n",
        "2. **Goals**: Define what the agent is trying to achieve.\n",
        "3. **ActionRegistry**: Tells what the agent *can* do.\n",
        "4. **AgentLanguage**: Converts everything into a structured prompt.\n",
        "5. **LLM**: Chooses an action and returns tool + args.\n",
        "6. **Environment**: Executes the selected action.\n",
        "7. **Memory**: Gets updated with the result and decision.\n",
        "8. **Loop**: Repeats or ends.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Creating Specialized Agents (Examples)\n",
        "\n",
        "### üìö Research Agent\n",
        "\n",
        "```python\n",
        "research_agent = Agent(\n",
        "    goals=[Goal(\"Find and summarize information on topic X\")],\n",
        "    agent_language=ResearchLanguage(),\n",
        "    action_registry=ActionRegistry([SearchAction(), SummarizeAction(), ...]),\n",
        "    generate_response=openai_call,\n",
        "    environment=WebEnvironment()\n",
        ")\n",
        "```\n",
        "\n",
        "### üßë‚Äçüíª Coding Agent\n",
        "\n",
        "```python\n",
        "coding_agent = Agent(\n",
        "    goals=[Goal(\"Write and debug Python code for task Y\")],\n",
        "    agent_language=CodingLanguage(),\n",
        "    action_registry=ActionRegistry([WriteCodeAction(), TestCodeAction(), ...]),\n",
        "    generate_response=anthropic_call,\n",
        "    environment=DevEnvironment()\n",
        ")\n",
        "```\n",
        "\n",
        "Each agent uses the **same loop** but behaves entirely differently due to their unique GAME components.\n",
        "\n"
      ],
      "metadata": {
        "id": "F3Lc4bRHCba-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdmUlQhTCl5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
