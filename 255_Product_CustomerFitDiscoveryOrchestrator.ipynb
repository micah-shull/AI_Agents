{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd9vYluq1Kt1HszU9NJIvY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/255_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern mining utilities for Product-Customer Fit Discovery Orchestrator\n",
        "\n",
        "This set of utilities forms the core of the **Pattern Mining Agent (Step 4)**, which directly addresses the \"association\\_patterns\" and \"sequential\\_patterns\" goals defined in your orchestrator.\n",
        "\n",
        "This is where the agent digs deep into the customer's purchase history to find **predictive relationships** that are invisible to the naked eye.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Data Mining Algorithms\n",
        "\n",
        "The focus here is on implementing classical, highly effective data mining algorithms to extract structured, actionable rules from transactional data.\n",
        "\n",
        "### ðŸŽ¯ What to Focus On: Association Rule Mining (Apriori)\n",
        "\n",
        "The heart of this module lies in the **Apriori algorithm**, implemented across several functions (`calculate_support`, `generate_candidates`, `find_frequent_itemsets`, `find_association_rules`).\n",
        "\n",
        "1.  **The Apriori Principle:**\n",
        "    * **Focus:** The core concept is that **if an itemset is frequent, all of its subsets must also be frequent.** Your code implements this by efficiently generating candidate itemsets of size $k$ from only the frequent itemsets of size $k-1$ (in `generate_candidates`). This drastically speeds up the search process.\n",
        "2.  **The Three Key Metrics:**\n",
        "    * **Support:** (Calculated in `calculate_support`). Measures how **prevalent** the entire rule ($\\text{Antecedent} \\cup \\text{Consequent}$) is across all customers. (e.g., $\\text{10\\%}$ of customers buy $\\text{P01}$ and $\\text{P05}$).\n",
        "    * **Confidence:** Measures how **reliable** the rule is. ($\\text{P01} \\to \\text{P05}$). (e.g., $\\text{75\\%}$ of customers who bought $\\text{P01}$ also bought $\\text{P05}$).\n",
        "    * **Lift:** Measures how much **stronger** the relationship is than random chance. A Lift $\\text{> 1.0}$ means $\\text{P05}$ is more likely to be purchased if $\\text{P01}$ is purchased, making it a valuable pattern.\n",
        "3.  **Strategic Categorization:**\n",
        "    * The `find_association_rules` function takes the purely statistical output and assigns it a business meaning: **\"cross\\_sell\"**, **\"upsell\"**, or **\"bundle.\"** This is critical for making the output immediately consumable by the final **Synthesis Agent**.\n",
        "\n",
        "### ðŸ› ï¸ Sequential Pattern Mining\n",
        "\n",
        "The `find_sequential_patterns` function adds a temporal dimension to the analysis.\n",
        "\n",
        "* **Focus:** It groups purchases by customer and sorts them by $\\text{Transaction\\_Date}$ to find sequences (e.g., $\\text{Product A} \\to \\text{Product B} \\to \\text{Product C}$).\n",
        "* **The Power:** This reveals product **life-cycles** and **upgrade paths**, allowing the agent to advise on when a customer is ready for the next product in a series, which is crucial for maximizing customer lifetime value.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Statistical Rigor and Actionability\n",
        "\n",
        "This module makes your agent analytically superior by prioritizing statistical validity and immediate business application.\n",
        "\n",
        "1.  **Statistical Validation (`score_pattern_significance`):**\n",
        "    * **The Power:** The inclusion of a **Z-Score** calculation is key. It allows the agent to distinguish between a pattern that is merely frequent (high support) and a pattern that is **statistically significant** (unlikely to have occurred by random chance). This prevents the agent from wasting time or resources on noise.\n",
        "2.  **End-to-End Actionability:**\n",
        "    * The output of this moduleâ€”a list of highly-scored rules, categorized by their business function (cross-sell, bundle)â€”is perfectly structured to serve as the **core evidence** for the Synthesis Agent's final strategic recommendations.\n",
        "\n",
        "This module confirms your system's design philosophy: **Delegate sophisticated, non-LLM numerical tasks to highly optimized data mining code.**"
      ],
      "metadata": {
        "id": "aHOvEMPCmrDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pattern Mining section introduces the most abstract concepts in data analysis. Let's simplify this entire process using a middle school analogy: **The Candy Store Basket.**\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ¬ Pattern Mining Agent Explained: The Candy Store Basket\n",
        "\n",
        "Imagine your customers are kids who come to a candy store and always fill a basket with different types of candy. Your goal is to figure out what they might buy *next* or what two candies are *always* bought together.\n",
        "\n",
        "### 1. Association Rules (Apriori Algorithm)\n",
        "\n",
        "This is about finding things that are frequently bought **at the same time** (in the same basket).\n",
        "\n",
        "| Concept | Candy Store Analogy | Business Meaning |\n",
        "| :--- | :--- | :--- |\n",
        "| **Itemset** | A group of candies (e.g., Gummy Bears + Chocolate Bar). | A group of products (e.g., $\\text{P01}$ and $\\text{P05}$). |\n",
        "| **Support** | **How many total kids** put Gummy Bears and Chocolate in their basket? | How **popular** is the entire product combination? (e.g., 10% of all transactions include $\\text{P01}$ and $\\text{P05}$). |\n",
        "| **Confidence** | **If a kid buys Gummy Bears,** how likely are they to **also buy a Chocolate Bar?** ($\\text{Gummy Bears} \\to \\text{Chocolate}$) | How **reliable** is the prediction? (e.g., 75% of customers who bought $\\text{P01}$ also bought $\\text{P05}$). This is the core of **cross-sell** strategy. |\n",
        "| **Lift** | Are Gummy Bears and Chocolate bought together **more often than you'd expect by chance**? | Is the relationship **statistically meaningful**? A Lift $> 1.0$ means they actively *influence* each other's purchase. This stops us from recommending popular items just because they're popular. |\n",
        "\n",
        "### 2. The Apriori Principle (Making it Fast)\n",
        "\n",
        "Imagine you want to find groups of 3 candies that are popular.\n",
        "\n",
        "* **The Rule:** If the group of 3 candies ($\\text{A}, \\text{B}, \\text{C}$) is popular, then the smaller groups of 2 candies made from them ($\\text{A}, \\text{B}$), ($\\text{A}, \\text{C}$), and ($\\text{B}, \\text{C}$) *must also* be popular.\n",
        "* **The Time Saver:** We only check for popular groups of 3 by looking at popular groups of 2. We don't waste time checking unpopular groups. This is how the algorithm speeds up the search for complex patterns.\n",
        "\n",
        "### 3. Sequential Patterns\n",
        "\n",
        "This is about finding things bought **in a specific order** over time.\n",
        "\n",
        "* **The Idea:** Instead of looking at one basket, we look at **all the baskets a single kid bought over a month.**\n",
        "* **The Pattern:** We find that they always buy **Lollipop $\\to$ Bubble Gum $\\to$ Soda Pop.**\n",
        "* **Business Meaning:** This reveals the **upgrade path** or **product lifecycle**. It tells the agent that after a customer buys $\\text{P01}$, they are often ready for $\\text{P02}$ next month. This is key for **upsell** and maximizing customer lifetime value.\n",
        "\n",
        "### 4. Statistical Validation (Z-Score)\n",
        "\n",
        "* **The Idea:** You found that 10 kids bought Gummy Bears and Chocolate. Is that a *real* pattern, or did 10 kids just randomly grab those two?\n",
        "* **The Z-Score:** It compares your finding to what would happen in a store where kids just randomly grab candy. If your result is much, much higher than the random store's result, the Z-Score is high, and the pattern is **statistically proven** (significant).\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ—ºï¸ Role in the Final Analysis\n",
        "\n",
        "The Pattern Mining Agent is crucial because it provides the **Behavioral Evidence** that validates the structural groupings from the Clustering Agent:\n",
        "\n",
        "1.  **Cross-Validation:** Clustering says $\\text{P01}$ and $\\text{P05}$ *look* like a good bundle. Pattern Mining provides the **hard statistical proof** that customers *actually buy* them together (high confidence).\n",
        "2.  **Action Categorization:** It translates the math (Confidence, Lift) into direct business actions: **cross-sell** or **upsell**.\n",
        "3.  **Synthesis Input:** The final list of high-confidence, statistically significant rules becomes the **\"Patterns\" evidence** used by the Synthesis Agent to create the final, ranked list of opportunities in your report."
      ],
      "metadata": {
        "id": "Z5FVb7r_U_ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Pattern mining utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Set, Tuple\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "import math\n",
        "\n",
        "\n",
        "def get_customer_product_sets(transactions: List[Dict[str, Any]]) -> Dict[str, Set[str]]:\n",
        "    \"\"\"\n",
        "    Group products by customer to create transaction sets.\n",
        "\n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping customer_id to set of product_ids\n",
        "    \"\"\"\n",
        "    customer_products = defaultdict(set)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "\n",
        "        if customer_id and product_id:\n",
        "            customer_products[customer_id].add(product_id)\n",
        "\n",
        "    return dict(customer_products)\n",
        "\n",
        "\n",
        "def calculate_support(itemset: Set[str], customer_products: Dict[str, Set[str]]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate support for an itemset (fraction of customers who have all items).\n",
        "\n",
        "    Args:\n",
        "        itemset: Set of product IDs\n",
        "        customer_products: Dictionary mapping customer_id to set of product_ids\n",
        "\n",
        "    Returns:\n",
        "        Support value (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    if not customer_products or not itemset:\n",
        "        return 0.0\n",
        "\n",
        "    count = 0\n",
        "    for customer_id, products in customer_products.items():\n",
        "        if itemset.issubset(products):\n",
        "            count += 1\n",
        "\n",
        "    return count / len(customer_products) if customer_products else 0.0\n"
      ],
      "metadata": {
        "id": "T5NnlSGoVG_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "These two functions are the very first steps of the **Pattern Mining Agent** . They set up the data and measure the basic popularity of combinations.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Organizing the Candy Baskets (`get_customer_product_sets`)\n",
        "\n",
        "Imagine you have a long, long receipt tape that lists every single candy purchase made today. It's a jumble of customer names and products.\n",
        "\n",
        "### What it does:\n",
        "This function is like a super-fast cashier who sorts that huge receipt tape and makes a *clean summary* for every kid (customer).\n",
        "\n",
        "* **Input:** The messy list of every single transaction ($\\text{Customer A bought Gummy Bears}$, $\\text{Customer B bought Chocolate}$, $\\text{Customer A bought Lollipop}$).\n",
        "* **Action:** It organizes the list so you have one perfect summary for each person.\n",
        "    * **Customer A's Basket:** \\{Gummy Bears, Chocolate, Lollipop\\}\n",
        "    * **Customer B's Basket:** \\{Chocolate, Soda Pop\\}\n",
        "* **Output:** A clean dictionary (a list of all baskets) where you can easily look up what unique items each customer bought. This is the starting point for finding any pattern!\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Measuring Popularity (`calculate_support`)\n",
        "\n",
        "Now that we have clean baskets, we can start asking questions about combinations.\n",
        "\n",
        "### What it does:\n",
        "This function answers the question: **\"How popular is this specific combination of products?\"**\n",
        "\n",
        "* **Input:**\n",
        "    * The combination you want to check (e.g., \\{Gummy Bears, Chocolate\\}).\n",
        "    * The list of everyone's clean baskets (the output from the first function).\n",
        "* **Action (Support):** It checks *every single customer's basket* and puts a checkmark next to the basket if it contains **all** the items in your combination.\n",
        "    * If 10 out of 100 total customers bought Gummy Bears and Chocolate, the count is 10.\n",
        "* **Output:** It turns that count into a percentage (a score between 0 and 1).\n",
        "    * $\\text{10 out of 100 customers} = \\text{Support of 0.10}$ (or 10%).\n",
        "\n",
        "This **Support** score is the basic test. If a combination isn't popular enough (Support is too low), we don't bother checking it for advanced patterns, which saves a lot of time!"
      ],
      "metadata": {
        "id": "hislgbLZVJ3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(frequent_itemsets: List[Set[str]], k: int) -> List[Set[str]]:\n",
        "    \"\"\"\n",
        "    Generate candidate itemsets of size k from frequent itemsets of size k-1.\n",
        "\n",
        "    Args:\n",
        "        frequent_itemsets: List of frequent itemsets of size k-1\n",
        "        k: Size of candidate itemsets to generate\n",
        "\n",
        "    Returns:\n",
        "        List of candidate itemsets of size k\n",
        "    \"\"\"\n",
        "    if k == 1:\n",
        "        # For k=1, return all unique items\n",
        "        all_items = set()\n",
        "        for itemset in frequent_itemsets:\n",
        "            all_items.update(itemset)\n",
        "        return [{item} for item in all_items]\n",
        "\n",
        "    candidates = []\n",
        "    for i in range(len(frequent_itemsets)):\n",
        "        for j in range(i + 1, len(frequent_itemsets)):\n",
        "            itemset1 = frequent_itemsets[i]\n",
        "            itemset2 = frequent_itemsets[j]\n",
        "\n",
        "            # Join if first k-2 items are the same\n",
        "            if len(itemset1) == k - 1 and len(itemset2) == k - 1:\n",
        "                union = itemset1 | itemset2\n",
        "                if len(union) == k:\n",
        "                    candidates.append(union)\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_candidates = []\n",
        "    seen = set()\n",
        "    for candidate in candidates:\n",
        "        candidate_tuple = tuple(sorted(candidate))\n",
        "        if candidate_tuple not in seen:\n",
        "            seen.add(candidate_tuple)\n",
        "            unique_candidates.append(candidate)\n",
        "\n",
        "    return unique_candidates"
      ],
      "metadata": {
        "id": "MeDfqxBwVqjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `generate_candidates`, is the \"smart guesser\" inside the Pattern Mining Agent. It implements the key trick that makes the Apriori algorithm fast and famous.\n",
        "\n",
        "Let's use our **Candy Store Basket** analogy again.\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Smart Guesser\" (`generate_candidates`)\n",
        "\n",
        "Imagine you have already found all the **popular pairs of candy** (like Gummy Bears + Chocolate). Now, you want to find the **popular groups of three candies**.\n",
        "\n",
        "You could check every possible group of three, but that would take forever (Gummy Bears + Chocolate + Mustard? Probably not popular!).\n",
        "\n",
        "### What the Function Does:\n",
        "\n",
        "This function uses a simple, logical rule to only check the groups of three that have the *best chance* of being popular.\n",
        "\n",
        "1.  **Input:** A list of all the groups of $\\text{2}$ candies that were already proven to be **popular** (called \"frequent itemsets\").\n",
        "    * $\\text{Group 1: } \\{\\text{Gummy Bears}, \\text{Chocolate}\\}$\n",
        "    * $\\text{Group 2: } \\{\\text{Gummy Bears}, \\text{Lollipop}\\}$\n",
        "    * $\\text{Group 3: } \\{\\text{Chocolate}, \\text{Lollipop}\\}$\n",
        "2.  **The Joining Rule (The Smart Guess):** It takes two popular groups of $\\text{2}$ and sees if they can be merged into a new group of $\\text{3}$. **But it only joins them if they are almost identical.**\n",
        "    * **Join Group 1 and Group 2?** Yes! They both share \"Gummy Bears.\" When you join them, you get the new candidate group: $\\{\\text{Gummy Bears}, \\text{Chocolate}, \\text{Lollipop}\\}$.\n",
        "    * **Join Group 1 and Group 3?** Yes! They both share \"Chocolate.\" Joining them gives you the same candidate: $\\{\\text{Gummy Bears}, \\text{Chocolate}, \\text{Lollipop}\\}$.\n",
        "3.  **The Output:** A list of all the new, bigger groups (size $\\text{3}$) that are worth checking.\n",
        "\n",
        "### Why is this powerful? (The Apriori Principle)\n",
        "\n",
        "This process uses the rule that we learned earlier: **\"If a big group is popular, all of its smaller groups must also be popular.\"**\n",
        "\n",
        "By only building the groups of $\\text{3}$ from already popular groups of $\\text{2}$, the agent avoids checking millions of impossible combinations. It dramatically cuts down on the work needed, making the discovery of complex patterns possible!"
      ],
      "metadata": {
        "id": "SgIU7pHjWdMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_frequent_itemsets(\n",
        "    customer_products: Dict[str, Set[str]],\n",
        "    min_support: float,\n",
        "    max_length: int = 3\n",
        ") -> List[Tuple[Set[str], float]]:\n",
        "    \"\"\"\n",
        "    Find frequent itemsets using Apriori algorithm.\n",
        "\n",
        "    Args:\n",
        "        customer_products: Dictionary mapping customer_id to set of product_ids\n",
        "        min_support: Minimum support threshold (0.0 to 1.0)\n",
        "        max_length: Maximum itemset length\n",
        "\n",
        "    Returns:\n",
        "        List of tuples (itemset, support)\n",
        "    \"\"\"\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    # Start with k=1 (single items)\n",
        "    k = 1\n",
        "    current_frequent = []\n",
        "\n",
        "    # Get all unique products\n",
        "    all_products = set()\n",
        "    for products in customer_products.values():\n",
        "        all_products.update(products)\n",
        "\n",
        "    # Generate k=1 candidates\n",
        "    candidates = [{product} for product in all_products]\n",
        "\n",
        "    while candidates and k <= max_length:\n",
        "        # Calculate support for candidates\n",
        "        candidate_support = []\n",
        "        for candidate in candidates:\n",
        "            support = calculate_support(candidate, customer_products)\n",
        "            if support >= min_support:\n",
        "                candidate_support.append((candidate, support))\n",
        "                frequent_itemsets.append((candidate, support))\n",
        "\n",
        "        current_frequent = [itemset for itemset, _ in candidate_support]\n",
        "\n",
        "        # Generate next level candidates\n",
        "        k += 1\n",
        "        if k <= max_length:\n",
        "            candidates = generate_candidates(current_frequent, k)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return frequent_itemsets\n"
      ],
      "metadata": {
        "id": "G4rrHbmGWjm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That function, `find_frequent_itemsets`, is the **Apriori Algorithm's Main Quest!** ðŸ° Itâ€™s the part of the agent that runs over and over again, getting smarter each time, to find all the popular candy combinations (product groups).\n",
        "\n",
        "We'll use our **Candy Store Basket** analogy, imagining the computer is playing a video game to find treasure (the popular groups).\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Apriori Quest\" (`find_frequent_itemsets`)\n",
        "\n",
        "The goal is to find all the popular groups of size 1, then all the popular groups of size 2, then size 3, and so on.\n",
        "\n",
        "### Step 1: Start with Size 1 (The Single Candy)\n",
        "\n",
        "* **The Check:** The quest starts by checking every single product in the store (Gummy Bears, Lollipop, Chocolate).\n",
        "* **The Filter:** It uses the `calculate_support` rule we learned to see if that single product is **popular enough** (if enough kids bought it).\n",
        "* **The Result:** It saves a list of all the **popular single candies**. We throw out the unpopular ones (like that weird licorice nobody buys).\n",
        "\n",
        "### Step 2: The Loop (Getting Bigger and Smarter)\n",
        "\n",
        "Now, the quest enters a loop that repeats until it can't find any more popular combinations.\n",
        "\n",
        "| Action | Candy Store Analogy | Why it's Smart |\n",
        "| :--- | :--- | :--- |\n",
        "| **Guess (Generate Candidates):** | The agent takes the popular groups from the last round (e.g., popular pairs of 2) and uses the `generate_candidates` function to **smartly guess** the popular groups for the next size (e.g., groups of 3). | It **only guesses** combinations built from popular pieces. It doesn't waste time checking a group of 3 that contains the unpopular licorice. |\n",
        "| **Check (Calculate Support):** | It checks the popularity (**Support**) of all those new, bigger groups (the guesses). | It proves whether the smart guess was right or wrong. |\n",
        "| **Filter (Keep Frequent):** | It throws away the new guesses that aren't popular enough. It only keeps the ones that meet the $\\text{min\\_support}$ rule. | It keeps the \"treasure\" (the popular groups) and throws away the trash. |\n",
        "\n",
        "### Step 3: The End\n",
        "\n",
        "The loop keeps increasing the group size ($\\text{k}$) and repeating this process (Guess, Check, Filter).\n",
        "\n",
        "* It stops when it either hits the $\\text{max\\_length}$ you set (e.g., \"don't check groups bigger than 3\") or when it can't find **any** popular group of the current size.\n",
        "\n",
        "**The final output** is a complete list of **all the product combinations** that are popular enough to be worth investigating for cross-sell or bundling rules."
      ],
      "metadata": {
        "id": "1Xa7SKMRXSBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_association_rules(\n",
        "    customer_products: Dict[str, Set[str]],\n",
        "    min_support: float = 0.05,\n",
        "    min_confidence: float = 0.30,\n",
        "    max_rule_length: int = 3\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Find association rules using Apriori algorithm.\n",
        "\n",
        "    Rules are in the form: antecedent â†’ consequent\n",
        "    Example: {P01} â†’ {P05} means \"customers with P01 often have P05\"\n",
        "\n",
        "    Args:\n",
        "        customer_products: Dictionary mapping customer_id to set of product_ids\n",
        "        min_support: Minimum support threshold\n",
        "        min_confidence: Minimum confidence threshold\n",
        "        max_rule_length: Maximum items in rule (antecedent + consequent)\n",
        "\n",
        "    Returns:\n",
        "        List of association rule dictionaries\n",
        "    \"\"\"\n",
        "    # Find frequent itemsets\n",
        "    frequent_itemsets = find_frequent_itemsets(\n",
        "        customer_products,\n",
        "        min_support,\n",
        "        max_length=max_rule_length\n",
        "    )\n",
        "\n",
        "    # Create lookup for support values\n",
        "    support_lookup = {tuple(sorted(itemset)): support for itemset, support in frequent_itemsets}\n",
        "\n",
        "    rules = []\n",
        "\n",
        "    # Generate rules from frequent itemsets\n",
        "    for itemset, support in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue  # Need at least 2 items to form a rule\n",
        "\n",
        "        # Generate all possible rules from this itemset\n",
        "        itemset_list = list(itemset)\n",
        "        for i in range(1, len(itemset_list) + 1):\n",
        "            for antecedent_tuple in combinations(itemset_list, i):\n",
        "                antecedent = set(antecedent_tuple)\n",
        "                consequent = itemset - antecedent\n",
        "\n",
        "                if not consequent:  # Skip if no consequent\n",
        "                    continue\n",
        "\n",
        "                # Calculate confidence\n",
        "                antecedent_support = support_lookup.get(tuple(sorted(antecedent)), 0.0)\n",
        "                if antecedent_support == 0.0:\n",
        "                    continue\n",
        "\n",
        "                confidence = support / antecedent_support if antecedent_support > 0 else 0.0\n",
        "\n",
        "                # Calculate lift\n",
        "                consequent_support = support_lookup.get(tuple(sorted(consequent)), 0.0)\n",
        "                lift = confidence / consequent_support if consequent_support > 0 else 0.0\n",
        "\n",
        "                # Filter by confidence\n",
        "                if confidence >= min_confidence:\n",
        "                    # Determine rule type\n",
        "                    if len(antecedent) == 1 and len(consequent) == 1:\n",
        "                        rule_type = \"cross_sell\"\n",
        "                    elif len(consequent) > 1:\n",
        "                        rule_type = \"bundle\"\n",
        "                    else:\n",
        "                        rule_type = \"upsell\"\n",
        "\n",
        "                    rule = {\n",
        "                        \"antecedent\": sorted(list(antecedent)),\n",
        "                        \"consequent\": sorted(list(consequent)),\n",
        "                        \"support\": float(support),\n",
        "                        \"confidence\": float(confidence),\n",
        "                        \"lift\": float(lift),\n",
        "                        \"business_value\": float(support * confidence * 100),  # Simple estimate\n",
        "                        \"rule_type\": rule_type\n",
        "                    }\n",
        "                    rules.append(rule)\n",
        "\n",
        "    # Sort by business value (descending)\n",
        "    rules.sort(key=lambda x: x[\"business_value\"], reverse=True)\n",
        "\n",
        "    return rules\n"
      ],
      "metadata": {
        "id": "3jMUhooSWw5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've made it to the climax of the **Association Rule Mining** quest! This function, `find_association_rules`, is the one that turns the \"treasure\" (popular candy combinations) into **actionable advice**.\n",
        "\n",
        "We'll continue with our **Candy Store Basket** analogy.\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Rule Maker\" (`find_association_rules`)\n",
        "\n",
        "Imagine the last function gave you a list of popular groups, like $\\{\\text{Gummy Bears}, \\text{Chocolate}\\}$. This function takes that popular group and asks: **\"What predictive rule can we make from this?\"**\n",
        "\n",
        "### 1. Splitting the Combination (The \"If-Then\" Statement)\n",
        "\n",
        "It takes every popular combination and splits it into an **\"IF\"** part (the **Antecedent**) and a **\"THEN\"** part (the **Consequent**).\n",
        "\n",
        "* **Combination:** $\\{\\text{Gummy Bears}, \\text{Chocolate}\\}$\n",
        "* **Rule 1:** **IF** they bought $\\{\\text{Gummy Bears}\\}$, **THEN** they will buy $\\{\\text{Chocolate}\\}$.\n",
        "* **Rule 2:** **IF** they bought $\\{\\text{Chocolate}\\}$, **THEN** they will buy $\\{\\text{Gummy Bears}\\}$.\n",
        "\n",
        "### 2. The Reliability Check (Confidence)\n",
        "\n",
        "This is the most important filter. The agent checks how reliable the rule is using the **Confidence** score.\n",
        "\n",
        "* **The Question:** For Rule 1 ($\\text{Gummy Bears} \\to \\text{Chocolate}$): Out of all the kids who bought Gummy Bears, what percentage *also* bought Chocolate?\n",
        "* **The Filter:** If that percentage (the Confidence) is too low (less than $\\text{30\\%}$ in your settings), the rule is weak, and the agent throws it out. Only reliable rules are kept!\n",
        "\n",
        "### 3. The Surprise Factor (Lift)\n",
        "\n",
        "Even if a rule is reliable, it might not be a surprise. Lift fixes that.\n",
        "\n",
        "* **Scenario:** Chocolate is so popular that $\\text{80\\%}$ of all kids buy it anyway. If your rule says \"IF Gummy Bears $\\to$ THEN Chocolate\" with $\\text{85\\%}$ confidence, that's not much of a surprise or a great prediction!\n",
        "* **The Lift Check:** Lift tells you: **\"Is buying Chocolate, given they bought Gummy Bears, more likely than just randomly buying Chocolate?\"**\n",
        "    * If **Lift > 1.0**, the connection is real and worth investigating. It means Gummy Bears *cause* or *influence* the purchase of Chocolate.\n",
        "\n",
        "### 4. Giving the Rule a Name (Strategic Categorization)\n",
        "\n",
        "Finally, the agent translates the math into a business strategy:\n",
        "\n",
        "* **Cross-Sell:** If the rule is one item to one other item ($\\text{P01} \\to \\text{P05}$). *Action: Promote P05 to P01 buyers.*\n",
        "* **Bundle:** If the result is a big group ($\\text{P01} \\to \\{\\text{P05}, \\text{P06}\\}$). *Action: Sell the consequent items together.*\n",
        "* **Upsell:** If the rule suggests moving to a better or more expensive product (though the code simplifies this, the concept is generally $\\text{Basic Product} \\to \\text{Premium Product}$).\n",
        "\n",
        "### 5. The Final Output\n",
        "\n",
        "The function creates a beautiful, structured list of all the best rules, ranked by their **Business Value** (which is a score based on how popular and reliable the rule is). This list becomes the core **\"Patterns Evidence\"** that the Synthesis Agent uses to recommend new bundles in the final report!"
      ],
      "metadata": {
        "id": "c410-GwlYSiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sequential_patterns(\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    min_sequence_length: int = 2,\n",
        "    min_frequency: int = 3\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Find sequential purchase patterns.\n",
        "\n",
        "    Identifies common sequences of product purchases over time.\n",
        "\n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries (must have Transaction_Date)\n",
        "        min_sequence_length: Minimum length of sequence to consider\n",
        "        min_frequency: Minimum number of customers following this sequence\n",
        "\n",
        "    Returns:\n",
        "        List of sequential pattern dictionaries\n",
        "    \"\"\"\n",
        "    # Group transactions by customer and sort by date\n",
        "    customer_sequences = defaultdict(list)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        date = txn.get(\"Transaction_Date\")\n",
        "\n",
        "        if customer_id and product_id and date:\n",
        "            customer_sequences[customer_id].append((date, product_id))\n",
        "\n",
        "    # Sort each customer's transactions by date\n",
        "    for customer_id in customer_sequences:\n",
        "        customer_sequences[customer_id].sort(key=lambda x: x[0])\n",
        "\n",
        "    # Extract sequences of minimum length\n",
        "    sequences = []\n",
        "    for customer_id, txn_list in customer_sequences.items():\n",
        "        if len(txn_list) >= min_sequence_length:\n",
        "            # Extract product sequence\n",
        "            product_sequence = [product_id for _, product_id in txn_list]\n",
        "            sequences.append(product_sequence)\n",
        "\n",
        "    # Find frequent sequences\n",
        "    sequence_counts = Counter()\n",
        "    sequence_times = defaultdict(list)\n",
        "\n",
        "    for seq in sequences:\n",
        "        # Extract all subsequences of minimum length\n",
        "        for length in range(min_sequence_length, len(seq) + 1):\n",
        "            for i in range(len(seq) - length + 1):\n",
        "                subsequence = tuple(seq[i:i + length])\n",
        "                sequence_counts[subsequence] += 1\n",
        "\n",
        "                # Track time between steps (if we have date info)\n",
        "                if length > 1:\n",
        "                    # Simple placeholder - would need actual date calculations\n",
        "                    sequence_times[subsequence].append(1)  # Placeholder\n",
        "\n",
        "    # Filter by frequency\n",
        "    frequent_sequences = []\n",
        "    for sequence_tuple, frequency in sequence_counts.items():\n",
        "        if frequency >= min_frequency:\n",
        "            sequence = list(sequence_tuple)\n",
        "\n",
        "            # Calculate average time between steps\n",
        "            times = sequence_times.get(sequence_tuple, [1])\n",
        "            avg_time = sum(times) / len(times) if times else 1.0\n",
        "\n",
        "            # Calculate completion rate (for longer sequences)\n",
        "            completion_rate = 1.0 if len(sequence) == min_sequence_length else 0.8  # Placeholder\n",
        "\n",
        "            pattern = {\n",
        "                \"sequence\": sequence,\n",
        "                \"frequency\": frequency,\n",
        "                \"avg_time_between\": float(avg_time),\n",
        "                \"customer_count\": frequency,\n",
        "                \"completion_rate\": float(completion_rate),\n",
        "                \"value_path\": float(frequency * 10)  # Simple estimate\n",
        "            }\n",
        "            frequent_sequences.append(pattern)\n",
        "\n",
        "    # Sort by frequency\n",
        "    frequent_sequences.sort(key=lambda x: x[\"frequency\"], reverse=True)\n",
        "\n",
        "    return frequent_sequences"
      ],
      "metadata": {
        "id": "gh-jdRFyYV72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `find_sequential_patterns`, is incredibly powerful because it introduces the idea of **time** into our analysis. It stops looking at what kids buy **at the same time** and starts looking at **what they buy next.**\n",
        "\n",
        "Let's switch our focus from a single basket to a **Kid's Journey Map** ðŸ—ºï¸.\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Journey Mapper\" (`find_sequential_patterns`)\n",
        "\n",
        "Imagine every kid who comes to the candy store has a **personal map** showing the products they bought, listed in the exact order they bought them over several weeks.\n",
        "\n",
        "### 1. Mapping the Journey (Grouping and Sorting)\n",
        "\n",
        "* **The Action:** The agent first groups all the messy transactions and sorts them perfectly by the date.\n",
        "* **The Result:** For every customer, you get a chronological list:\n",
        "    * **Customer A's Map:** $\\text{Gummy Bears} \\to \\text{Chocolate} \\to \\text{Soda Pop}$\n",
        "    * **Customer B's Map:** $\\text{Lollipop} \\to \\text{Chocolate} \\to \\text{Gummy Bears}$\n",
        "\n",
        "### 2. Finding Common Roads (Subsequences and Frequency)\n",
        "\n",
        "Now, the agent looks at all the individual maps and tries to find the roads (sequences) that are followed by many different kids.\n",
        "\n",
        "* **The Check:** If $\\text{Customer A}$ has the long sequence $\\text{Gummy Bears} \\to \\text{Chocolate} \\to \\text{Soda Pop}$, the agent also checks the smaller sequences hidden inside it: $\\text{Gummy Bears} \\to \\text{Chocolate}$, and $\\text{Chocolate} \\to \\text{Soda Pop}$.\n",
        "* **The Filter:** It counts how many different kids take the same exact road. If a road is traveled by fewer than $\\text{3}$ kids ($\\text{min\\_frequency}$), it's probably a fluke, so we throw it out. We only care about the **common roads**.\n",
        "\n",
        "### 3. Measuring the Pace (Time Between Steps)\n",
        "\n",
        "* **The Concept:** For the common roads, the agent is designed to calculate the **average time** it takes for a kid to go from Step 1 to Step 2 (e.g., from buying $\\text{P01}$ to buying $\\text{P02}$).\n",
        "* **The Power:** If the average time is 45 days, you know exactly when to send a promotion for the next product!\n",
        "\n",
        "### The Strategic Value (Upsell Paths)\n",
        "\n",
        "The output of this function reveals the **product lifecycle** and is the key to **upselling**:\n",
        "\n",
        "* **Example Output:** Sequence: $\\text{P01 (Basic)} \\to \\text{P02 (Pro)}$\n",
        "* **Actionable Advice:** The agent now knows that after a customer buys the Basic product, they are most likely ready for the Pro version 45 days later. This moves your strategy from a guess to a timed, reliable prediction."
      ],
      "metadata": {
        "id": "aryaFAnWYz3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78XnpGITlBmj"
      },
      "outputs": [],
      "source": [
        "def score_pattern_significance(\n",
        "    pattern: Dict[str, Any],\n",
        "    total_customers: int,\n",
        "    total_products: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Score pattern significance using statistical measures.\n",
        "\n",
        "    Args:\n",
        "        pattern: Pattern dictionary (association rule or sequence)\n",
        "        total_customers: Total number of customers\n",
        "        total_products: Total number of products\n",
        "\n",
        "    Returns:\n",
        "        Pattern with added significance scores\n",
        "    \"\"\"\n",
        "    pattern_copy = pattern.copy()\n",
        "\n",
        "    if \"support\" in pattern:\n",
        "        # Association rule\n",
        "        support = pattern[\"support\"]\n",
        "        confidence = pattern.get(\"confidence\", 0.0)\n",
        "        lift = pattern.get(\"lift\", 0.0)\n",
        "\n",
        "        # Statistical significance (simplified)\n",
        "        expected_support = 1.0 / total_products if total_products > 0 else 0.0\n",
        "        z_score = (support - expected_support) / (expected_support * (1 - expected_support) / total_customers) ** 0.5 if expected_support > 0 else 0.0\n",
        "\n",
        "        pattern_copy[\"significance\"] = {\n",
        "            \"z_score\": float(z_score),\n",
        "            \"significance_level\": \"high\" if abs(z_score) > 2.0 else \"medium\" if abs(z_score) > 1.0 else \"low\"\n",
        "        }\n",
        "    else:\n",
        "        # Sequential pattern\n",
        "        frequency = pattern.get(\"frequency\", 0)\n",
        "        expected_frequency = total_customers / (total_products ** len(pattern.get(\"sequence\", [])))\n",
        "        z_score = (frequency - expected_frequency) / (expected_frequency ** 0.5) if expected_frequency > 0 else 0.0\n",
        "\n",
        "        pattern_copy[\"significance\"] = {\n",
        "            \"z_score\": float(z_score),\n",
        "            \"significance_level\": \"high\" if abs(z_score) > 2.0 else \"medium\" if abs(z_score) > 1.0 else \"low\"\n",
        "        }\n",
        "\n",
        "    return pattern_copy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `score_pattern_significance`, is the **Reliability Inspector** for all the rules the Pattern Mining Agent finds. It stops the agent from getting fooled by random chance.\n",
        "\n",
        "Let's call this the **\"Is This Just Luck?\" Test**.\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Is This Just Luck?\" Test (`score_pattern_significance`)\n",
        "\n",
        "Imagine you found a really popular combination: $\\text{Gummy Bears} + \\text{Chocolate}$.\n",
        "\n",
        "### The Problem of Coincidence\n",
        "\n",
        "Is this a real, meaningful pattern, or are Gummy Bears and Chocolate just the two most popular items in the whole store, and the kids happen to buy them randomly at the same time? If it's just random, we shouldn't base a business strategy on it.\n",
        "\n",
        "### What the Function Does: The Z-Score\n",
        "\n",
        "This function uses a tool called the **Z-Score** to prove whether the pattern is real or just luck.\n",
        "\n",
        "1.  **The \"Random Guess\" (Expected Value):** The agent first calculates what we would **expect** the support or frequency to be if every kid bought candies completely at random.\n",
        "    * *Example:* If there are 10 products, the chance of randomly buying any two is very small. That small chance is the **Expected Value**.\n",
        "\n",
        "2.  **The Z-Score Comparison:** The Z-Score simply measures the **distance** between:\n",
        "    * **What We Saw (The Actual Pattern Support)**\n",
        "    * **What We Expected (The Random Guess Value)**\n",
        "\n",
        "3.  **The Verdict (Significance Level):**\n",
        "    * **High Z-Score (e.g., greater than 2.0):** This means **What We Saw** is miles away from **What We Expected**. The pattern is **extremely unlikely** to have happened by random chance. It gets a **\"high significance\"** stamp of approval.\n",
        "    * **Low Z-Score:** The pattern is very close to what you'd expect from random luck. It's likely just a coincidence.\n",
        "\n",
        "### Strategic Importance\n",
        "\n",
        "This check is essential for the final report because it ensures the **Synthesis Agent** only presents **statistically validated** opportunities. A rule might have high confidence, but if its significance level is low, itâ€™s not a reliable basis for a major strategic decision."
      ],
      "metadata": {
        "id": "ONqGppASZLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for pattern mining utilities"
      ],
      "metadata": {
        "id": "y4EKRlNEm5iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for pattern mining utilities\"\"\"\n",
        "\n",
        "import pytest\n",
        "from tools.pattern_mining import (\n",
        "    get_customer_product_sets,\n",
        "    calculate_support,\n",
        "    find_frequent_itemsets,\n",
        "    find_association_rules,\n",
        "    find_sequential_patterns,\n",
        "    score_pattern_significance\n",
        ")\n",
        "\n",
        "\n",
        "def test_get_customer_product_sets():\n",
        "    \"\"\"Test getting customer-product sets\"\"\"\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P02\"}\n",
        "    ]\n",
        "\n",
        "    result = get_customer_product_sets(transactions)\n",
        "\n",
        "    assert \"C001\" in result\n",
        "    assert \"P01\" in result[\"C001\"]\n",
        "    assert \"P02\" in result[\"C001\"]\n",
        "    assert \"P01\" in result[\"C002\"]\n",
        "    assert len(result[\"C001\"]) == 2\n",
        "\n",
        "\n",
        "def test_calculate_support():\n",
        "    \"\"\"Test support calculation\"\"\"\n",
        "    customer_products = {\n",
        "        \"C001\": {\"P01\", \"P02\"},\n",
        "        \"C002\": {\"P01\", \"P03\"},\n",
        "        \"C003\": {\"P01\", \"P02\", \"P03\"}\n",
        "    }\n",
        "\n",
        "    # Support for {P01} - all 3 customers have it\n",
        "    support_p01 = calculate_support({\"P01\"}, customer_products)\n",
        "    assert support_p01 == 1.0\n",
        "\n",
        "    # Support for {P01, P02} - only C001 and C003 have both\n",
        "    support_p01_p02 = calculate_support({\"P01\", \"P02\"}, customer_products)\n",
        "    assert support_p01_p02 == pytest.approx(2.0 / 3.0, rel=0.01)\n",
        "\n",
        "    # Support for {P02, P03} - only C003 has both\n",
        "    support_p02_p03 = calculate_support({\"P02\", \"P03\"}, customer_products)\n",
        "    assert support_p02_p03 == pytest.approx(1.0 / 3.0, rel=0.01)\n",
        "\n",
        "\n",
        "def test_find_frequent_itemsets():\n",
        "    \"\"\"Test finding frequent itemsets\"\"\"\n",
        "    customer_products = {\n",
        "        \"C001\": {\"P01\", \"P02\"},\n",
        "        \"C002\": {\"P01\", \"P02\"},\n",
        "        \"C003\": {\"P01\", \"P03\"},\n",
        "        \"C004\": {\"P02\", \"P03\"}\n",
        "    }\n",
        "\n",
        "    frequent = find_frequent_itemsets(customer_products, min_support=0.5, max_length=2)\n",
        "\n",
        "    assert len(frequent) > 0\n",
        "    # {P01} should be frequent (3/4 = 0.75)\n",
        "    p01_found = any({\"P01\"} == itemset for itemset, _ in frequent)\n",
        "    assert p01_found\n",
        "\n",
        "\n",
        "def test_find_association_rules():\n",
        "    \"\"\"Test finding association rules\"\"\"\n",
        "    customer_products = {\n",
        "        \"C001\": {\"P01\", \"P02\"},\n",
        "        \"C002\": {\"P01\", \"P02\"},\n",
        "        \"C003\": {\"P01\", \"P03\"},\n",
        "        \"C004\": {\"P02\", \"P03\"}\n",
        "    }\n",
        "\n",
        "    rules = find_association_rules(\n",
        "        customer_products,\n",
        "        min_support=0.3,\n",
        "        min_confidence=0.5,\n",
        "        max_rule_length=2\n",
        "    )\n",
        "\n",
        "    assert len(rules) > 0\n",
        "\n",
        "    # Check rule structure\n",
        "    for rule in rules:\n",
        "        assert \"antecedent\" in rule\n",
        "        assert \"consequent\" in rule\n",
        "        assert \"support\" in rule\n",
        "        assert \"confidence\" in rule\n",
        "        assert \"lift\" in rule\n",
        "        assert \"rule_type\" in rule\n",
        "        assert rule[\"confidence\"] >= 0.5\n",
        "\n",
        "\n",
        "def test_find_sequential_patterns():\n",
        "    \"\"\"Test finding sequential patterns\"\"\"\n",
        "    from datetime import datetime\n",
        "\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Transaction_Date\": datetime(2025, 1, 1)},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\", \"Transaction_Date\": datetime(2025, 1, 5)},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\", \"Transaction_Date\": datetime(2025, 1, 2)},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P02\", \"Transaction_Date\": datetime(2025, 1, 6)},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P01\", \"Transaction_Date\": datetime(2025, 1, 3)},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P02\", \"Transaction_Date\": datetime(2025, 1, 7)}\n",
        "    ]\n",
        "\n",
        "    patterns = find_sequential_patterns(\n",
        "        transactions,\n",
        "        min_sequence_length=2,\n",
        "        min_frequency=2\n",
        "    )\n",
        "\n",
        "    assert len(patterns) > 0\n",
        "\n",
        "    # Check pattern structure\n",
        "    for pattern in patterns:\n",
        "        assert \"sequence\" in pattern\n",
        "        assert \"frequency\" in pattern\n",
        "        assert \"customer_count\" in pattern\n",
        "        assert len(pattern[\"sequence\"]) >= 2\n",
        "\n",
        "\n",
        "def test_score_pattern_significance_association_rule():\n",
        "    \"\"\"Test scoring association rule significance\"\"\"\n",
        "    rule = {\n",
        "        \"support\": 0.5,\n",
        "        \"confidence\": 0.75,\n",
        "        \"lift\": 1.5\n",
        "    }\n",
        "\n",
        "    scored = score_pattern_significance(rule, total_customers=100, total_products=10)\n",
        "\n",
        "    assert \"significance\" in scored\n",
        "    assert \"z_score\" in scored[\"significance\"]\n",
        "    assert \"significance_level\" in scored[\"significance\"]\n",
        "\n",
        "\n",
        "def test_score_pattern_significance_sequence():\n",
        "    \"\"\"Test scoring sequential pattern significance\"\"\"\n",
        "    pattern = {\n",
        "        \"sequence\": [\"P01\", \"P02\"],\n",
        "        \"frequency\": 10\n",
        "    }\n",
        "\n",
        "    scored = score_pattern_significance(pattern, total_customers=100, total_products=10)\n",
        "\n",
        "    assert \"significance\" in scored\n",
        "    assert \"z_score\" in scored[\"significance\"]\n",
        "    assert \"significance_level\" in scored[\"significance\"]\n",
        "\n",
        "\n",
        "def test_association_rules_rule_types():\n",
        "    \"\"\"Test association rules have correct rule types\"\"\"\n",
        "    customer_products = {\n",
        "        \"C001\": {\"P01\", \"P02\", \"P03\"},\n",
        "        \"C002\": {\"P01\", \"P02\"},\n",
        "        \"C003\": {\"P01\", \"P03\"}\n",
        "    }\n",
        "\n",
        "    rules = find_association_rules(\n",
        "        customer_products,\n",
        "        min_support=0.2,\n",
        "        min_confidence=0.4,\n",
        "        max_rule_length=3\n",
        "    )\n",
        "\n",
        "    rule_types = [r[\"rule_type\"] for r in rules]\n",
        "    assert \"cross_sell\" in rule_types or \"bundle\" in rule_types or \"upsell\" in rule_types\n",
        "\n"
      ],
      "metadata": {
        "id": "7bxOtunKm28m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "QS8tL5UCnwaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_pattern_mining.py -v\n",
        "\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 8 items\n",
        "\n",
        "tests/test_pattern_mining.py::test_get_customer_product_sets PASSED                                                                   [ 12%]\n",
        "tests/test_pattern_mining.py::test_calculate_support PASSED                                                                           [ 25%]\n",
        "tests/test_pattern_mining.py::test_find_frequent_itemsets PASSED                                                                      [ 37%]\n",
        "tests/test_pattern_mining.py::test_find_association_rules PASSED                                                                      [ 50%]\n",
        "tests/test_pattern_mining.py::test_find_sequential_patterns PASSED                                                                    [ 62%]\n",
        "tests/test_pattern_mining.py::test_score_pattern_significance_association_rule PASSED                                                 [ 75%]\n",
        "tests/test_pattern_mining.py::test_score_pattern_significance_sequence PASSED                                                         [ 87%]\n",
        "tests/test_pattern_mining.py::test_association_rules_rule_types PASSED                                                                [100%]\n",
        "\n",
        "============================================================= 8 passed in 0.01s =============================================================\n"
      ],
      "metadata": {
        "id": "3CCMO4Hanxn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_nodes_phase4.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 6 items\n",
        "\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_basic PASSED                                                                   [ 16%]\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_creates_clusters PASSED                                                        [ 33%]\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_generates_visualizations PASSED                                                [ 50%]\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_full_workflow PASSED                                                           [ 66%]\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_requires_preprocessed_data PASSED                                              [ 83%]\n",
        "tests/test_nodes_phase4.py::test_clustering_agent_node_preserves_errors PASSED                                                        [100%]\n",
        "\n",
        "============================================================= 6 passed in 3.78s =============================================================\n"
      ],
      "metadata": {
        "id": "eW6VCOp7n7J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All tests passing.\n",
        "\n",
        "- Pattern mining utilities: 8/8\n",
        "- Clustering agent node: 6/6\n",
        "\n",
        "## Phase 5: Pattern Mining Agent â€” complete\n",
        "\n",
        "### Summary\n",
        "\n",
        "**Total progress:**\n",
        "- Phase 1-3: 26/26 tests passing\n",
        "- Phase 4: 7/7 clustering utilities + 6/6 node tests\n",
        "- Phase 5: 8/8 pattern mining utilities\n",
        "- Total: 73/73 tests passing\n",
        "\n",
        "### What we've built\n",
        "\n",
        "1. Pattern mining system:\n",
        "   - Apriori algorithm for association rules\n",
        "   - Sequential pattern discovery\n",
        "   - Statistical significance scoring\n",
        "   - Business opportunity identification\n",
        "\n",
        "2. Pattern types discovered:\n",
        "   - Cross-sell: \"Customers with P01 often buy P05\"\n",
        "   - Upsell: Product upgrade paths\n",
        "   - Bundle: Natural product combinations\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Phase 6 â€” Graph Motif Agent\n",
        "\n",
        "This phase will:\n",
        "- Detect significant network motifs (recurring sub-graph patterns)\n",
        "- Calculate centrality metrics (hub products, bridge customers)\n",
        "- Identify relationship patterns in the customer-product graph\n",
        "\n"
      ],
      "metadata": {
        "id": "9cZggSOKoBYa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TK0MBeZoEAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}