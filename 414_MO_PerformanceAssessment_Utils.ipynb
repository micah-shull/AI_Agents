{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbcFAKqcK8Xtw0SM2qPqOi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/414_MO_PerformanceAssessment_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# â­ The Most Valuable Concept in Performance Assessment\n",
        "\n",
        "### **The most valuable concept here is that *system performance is assessed as a portfolio, not as a collection of wins*.**\n",
        "\n",
        "This function deliberately answers a different question than campaign analysis or experiment evaluation:\n",
        "\n",
        "> â€œIs the **marketing system as a whole** getting healthier, more effective, and more disciplined?â€\n",
        "\n",
        "That shift in perspective is crucialâ€”and rare.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ This Function Changes the Unit of Analysis\n",
        "\n",
        "Up to now, your system has focused on:\n",
        "\n",
        "* individual campaigns\n",
        "* individual experiments\n",
        "* individual decisions\n",
        "\n",
        "This utility **changes the unit of analysis** to:\n",
        "\n",
        "> **The marketing program itself**\n",
        "\n",
        "Thatâ€™s a CEO-level move.\n",
        "\n",
        "Executives donâ€™t ask:\n",
        "\n",
        "* â€œWhich experiment won?â€\n",
        "  They ask:\n",
        "* â€œAre we getting better at marketing?â€\n",
        "\n",
        "This function is the first place that question is answered.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ You Aggregate *Judgment*, Not Raw Metrics\n",
        "\n",
        "Notice what you *donâ€™t* do here:\n",
        "\n",
        "* You donâ€™t re-calculate metrics\n",
        "* You donâ€™t re-run statistics\n",
        "* You donâ€™t reinterpret raw data\n",
        "\n",
        "Instead, you aggregate:\n",
        "\n",
        "* campaign-level judgments\n",
        "* experiment-level conclusions\n",
        "\n",
        "Thatâ€™s important.\n",
        "\n",
        "It means:\n",
        "\n",
        "* lower-level logic remains authoritative\n",
        "* higher-level assessment is consistent\n",
        "* errors donâ€™t cascade\n",
        "\n",
        "This is **layered reasoning**, not duplicated logic.\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ You Separate Activity From Effectiveness\n",
        "\n",
        "This distinction is one of the strongest signals in the code.\n",
        "\n",
        "You track:\n",
        "\n",
        "* total campaigns\n",
        "* active vs paused campaigns\n",
        "* total experiments\n",
        "* running vs completed experiments\n",
        "\n",
        "But you *also* track:\n",
        "\n",
        "* campaigns exceeding expectations\n",
        "* campaigns below expectations\n",
        "* significant experiment lift\n",
        "\n",
        "This prevents a common failure mode:\n",
        "\n",
        "> Mistaking *busyness* for *progress*\n",
        "\n",
        "Your system can tell the difference.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ ROI Is Treated as a System Property\n",
        "\n",
        "This line is deceptively important:\n",
        "\n",
        "```python\n",
        "overall_roi = (total_revenue_proxy / total_spend)\n",
        "```\n",
        "\n",
        "You are not saying:\n",
        "\n",
        "* â€œCampaign A was profitableâ€\n",
        "  You are saying:\n",
        "* â€œThe **system** is producing value relative to costâ€\n",
        "\n",
        "That framing is essential for:\n",
        "\n",
        "* budget discussions\n",
        "* scaling decisions\n",
        "* confidence in automation\n",
        "\n",
        "It moves the conversation from tactics to investment.\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Experiment Lift Is Filtered by Evidence Quality\n",
        "\n",
        "This is a standout design choice:\n",
        "\n",
        "```python\n",
        "significant_evaluations = [\n",
        "    e for e in experiment_evaluations\n",
        "    if e.get(\"statistical_significance\", {}).get(\"is_significant\", False)\n",
        "    and e.get(\"status\") == \"completed\"\n",
        "]\n",
        "```\n",
        "\n",
        "You only include:\n",
        "\n",
        "* completed experiments\n",
        "* statistically significant results\n",
        "\n",
        "That means:\n",
        "\n",
        "* noise is excluded\n",
        "* incomplete learning isnâ€™t overcounted\n",
        "* optimism is controlled\n",
        "\n",
        "Most systems average *everything* and hope for the best.\n",
        "\n",
        "Yours doesnâ€™t.\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ The Output Is a System Health Snapshot\n",
        "\n",
        "The returned dictionary is not a dashboardâ€”itâ€™s a **health check**.\n",
        "\n",
        "It answers, cleanly:\n",
        "\n",
        "* How big is the system?\n",
        "* How active is it?\n",
        "* How effective is it?\n",
        "* How much value is it producing?\n",
        "* How strong is the evidence behind improvements?\n",
        "\n",
        "This is exactly what leadership needs to decide:\n",
        "\n",
        "* continue\n",
        "* adjust\n",
        "* pause\n",
        "* scale\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  The Deeper Pattern (This Is the Real Value)\n",
        "\n",
        "This module introduces a critical orchestration concept:\n",
        "\n",
        "> **Local optimization does not guarantee global improvement.**\n",
        "\n",
        "By assessing:\n",
        "\n",
        "* distribution of outcomes\n",
        "* quality of experiments\n",
        "* system-level ROI\n",
        "\n",
        "You prevent the system from declaring victory too early.\n",
        "\n",
        "This is how mature organizations learn.\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Summary You Should Keep\n",
        "\n",
        "If you ever need to summarize this module:\n",
        "\n",
        "> **â€œOverall performance is assessed by aggregating validated campaign outcomes and statistically significant learning into a portfolio-level health signal.â€**\n",
        "\n",
        "Thatâ€™s the value.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Fits Perfectly in Your Architecture\n",
        "\n",
        "This utility:\n",
        "\n",
        "* sits *above* campaign analysis\n",
        "* sits *above* experiment evaluation\n",
        "* feeds directly into KPI calculation and reporting\n",
        "\n",
        "It is the **bridge between evidence and executive insight**.\n",
        "\n"
      ],
      "metadata": {
        "id": "i_aXKsosyNG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWdVwpOKxqWJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Performance Assessment Utilities\n",
        "\n",
        "Assess overall performance across all campaigns and experiments.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "\n",
        "def assess_overall_performance(\n",
        "    campaigns: List[Dict[str, Any]],\n",
        "    campaign_analysis: List[Dict[str, Any]],\n",
        "    experiments: List[Dict[str, Any]],\n",
        "    experiment_evaluations: List[Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assess overall performance across all campaigns.\n",
        "\n",
        "    Args:\n",
        "        campaigns: List of campaign dictionaries\n",
        "        campaign_analysis: List of campaign analysis results\n",
        "        experiments: List of experiment dictionaries\n",
        "        experiment_evaluations: List of experiment evaluation results\n",
        "\n",
        "    Returns:\n",
        "        Overall performance assessment dictionary\n",
        "    \"\"\"\n",
        "    # Count campaigns\n",
        "    total_campaigns = len(campaigns)\n",
        "    active_campaigns = len([c for c in campaigns if c.get(\"status\") == \"active\"])\n",
        "    paused_campaigns = len([c for c in campaigns if c.get(\"status\") == \"paused\"])\n",
        "\n",
        "    # Count experiments\n",
        "    total_experiments = len(experiments)\n",
        "    running_experiments = len([e for e in experiments if e.get(\"status\") == \"running\"])\n",
        "    completed_experiments = len([e for e in experiments if e.get(\"status\") == \"completed\"])\n",
        "\n",
        "    # Calculate totals from campaign analysis\n",
        "    total_spend = sum(analysis.get(\"total_spend\", 0.0) for analysis in campaign_analysis)\n",
        "    total_revenue_proxy = sum(analysis.get(\"total_revenue_proxy\", 0.0) for analysis in campaign_analysis)\n",
        "\n",
        "    # Calculate overall ROI\n",
        "    overall_roi = (total_revenue_proxy / total_spend) if total_spend > 0 else 0.0\n",
        "\n",
        "    # Count performance statuses\n",
        "    exceeding_count = len([a for a in campaign_analysis if a.get(\"overall_performance\") == \"exceeding_expectations\"])\n",
        "    meeting_count = len([a for a in campaign_analysis if a.get(\"overall_performance\") == \"meeting_expectations\"])\n",
        "    below_count = len([a for a in campaign_analysis if a.get(\"overall_performance\") == \"below_expectations\"])\n",
        "\n",
        "    # Calculate average lift from experiments (only completed with significant results)\n",
        "    significant_evaluations = [\n",
        "        e for e in experiment_evaluations\n",
        "        if e.get(\"statistical_significance\", {}).get(\"is_significant\", False)\n",
        "        and e.get(\"status\") == \"completed\"\n",
        "    ]\n",
        "\n",
        "    if significant_evaluations:\n",
        "        average_lift = sum(e.get(\"lift_percentage\", 0.0) for e in significant_evaluations) / len(significant_evaluations)\n",
        "    else:\n",
        "        average_lift = 0.0\n",
        "\n",
        "    return {\n",
        "        \"total_campaigns\": total_campaigns,\n",
        "        \"active_campaigns\": active_campaigns,\n",
        "        \"paused_campaigns\": paused_campaigns,\n",
        "        \"total_experiments\": total_experiments,\n",
        "        \"running_experiments\": running_experiments,\n",
        "        \"completed_experiments\": completed_experiments,\n",
        "        \"total_spend\": round(total_spend, 2),\n",
        "        \"total_revenue_proxy\": round(total_revenue_proxy, 2),\n",
        "        \"overall_roi\": round(overall_roi, 2),\n",
        "        \"campaigns_exceeding_expectations\": exceeding_count,\n",
        "        \"campaigns_meeting_expectations\": meeting_count,\n",
        "        \"campaigns_below_expectations\": below_count,\n",
        "        \"average_lift_percentage\": round(average_lift, 2),\n",
        "        \"significant_experiments_count\": len(significant_evaluations)\n",
        "    }\n"
      ]
    }
  ]
}