{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsma8uFPzvN1mP5rnZn8QE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/038_JW_Modular_AgentDesign_II_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Building a Simple Agent Framework\n",
        "\n",
        "Now, we are going to put the components together into a **reusable agent class**. This class encapsulates the GAME components and provides a clean interface for running the agent loop. We can create different agents simply by changing the **Goals**, **Actions**, **Memory**, and **Environment** (GAME) without modifying the core loop."
      ],
      "metadata": {
        "id": "Q82Y6AK8Cm1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2spKWItNCJDf"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        Initialize an agent with its core GAME components\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"Build prompt with memory context\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        Update memory with the agent's decision and the environment's response.\n",
        "        \"\"\"\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"user\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        Execute the GAME loop for this agent with a maximum iteration limit.\n",
        "        \"\"\"\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # Construct a prompt that includes the Goals, Actions, and the current Memory\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # Generate a response from the agent\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # Determine which action the agent wants to execute\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # Execute the action in the environment\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # Update the agent's memory with information about what happened\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # Check if the agent has decided to terminate\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 🧠 Simple Agent Framework — What to Learn & Watch For\n",
        "\n",
        "## 0) Mental model (GAME)\n",
        "\n",
        "This Agent is organized around **G**oals, **A**ctions, **M**emory, and **E**nvironment:\n",
        "\n",
        "* **Goals**: what the agent is trying to achieve (instructions/constraints).\n",
        "* **Actions**: the tools the agent may call (wrapped in your `Action` objects + `ActionRegistry`).\n",
        "* **Memory**: the evolving transcript/state (what was asked, what the model decided, tool results).\n",
        "* **Environment**: the executor/sandbox that *runs* actions and returns results.\n",
        "\n",
        "The Agent orchestrates these in a loop.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Constructor (dependencies, not logic)\n",
        "\n",
        "```python\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* **Dependency injection**: you pass in everything (LLM caller, registry, environment). This makes it **testable and modular**.\n",
        "* `agent_language` is a key abstraction:\n",
        "\n",
        "  * `construct_prompt(...)`: how to turn Goals/Actions/Memory into model input.\n",
        "  * `parse_response(...)`: how to read the model’s output into `{\"tool\": name, \"args\": {...}}` (or a terminal answer).\n",
        "  * This lets you swap prompt formats (plain text vs. OpenAI tool-calling) **without changing the Agent**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Prompt construction\n",
        "\n",
        "```python\n",
        "def construct_prompt(self, goals, memory, actions) -> Prompt:\n",
        "    return self.agent_language.construct_prompt(\n",
        "        actions=actions.get_actions(),\n",
        "        environment=self.environment,\n",
        "        goals=goals,\n",
        "        memory=memory\n",
        "    )\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* Keep prompts **deterministic and short**. Only include what the model needs: goals, tool list (names + short descriptions + schemas), and the **relevant** memory.\n",
        "* For long runs, add a **memory summarizer** to avoid context blowup.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Parsing the model’s decision\n",
        "\n",
        "```python\n",
        "def get_action(self, response):\n",
        "    invocation = self.agent_language.parse_response(response)\n",
        "    action = self.actions.get_action(invocation[\"tool\"])\n",
        "    return action, invocation\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* `parse_response` should return a **normalized invocation** like:\n",
        "\n",
        "  ```python\n",
        "  {\"tool\": \"read_file\", \"args\": {\"file_name\": \"notes.txt\"}}\n",
        "  ```\n",
        "* **Validation goes here** (or right after): check tool name whitelist, schema (types/required keys), and **semantic rules** (e.g., path safety).\n",
        "* If you adopt **OpenAI tool calling**, `parse_response` should read `message.tool_calls[*]` instead of scraping text.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Termination\n",
        "\n",
        "```python\n",
        "def should_terminate(self, response: str) -> bool:\n",
        "    action_def, _ = self.get_action(response)\n",
        "    return action_def.terminal\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* You’ll typically have a special action like `final_answer` with `terminal=True`.\n",
        "* Alternatively, let `agent_language.parse_response` return a flag like `{\"tool\":\"final_answer\", \"args\":{...}, \"terminal\": True}`.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Memory updates\n",
        "\n",
        "```python\n",
        "def set_current_task(self, memory, task):\n",
        "    memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "def update_memory(self, memory, response, result):\n",
        "    new_memories = [\n",
        "        {\"type\": \"assistant\", \"content\": response},\n",
        "        {\"type\": \"user\", \"content\": json.dumps(result)}\n",
        "    ]\n",
        "    for m in new_memories:\n",
        "        memory.add_memory(m)\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* Keep roles consistent (e.g., `\"user\"`, `\"assistant\"`, `\"tool\"`). Consider storing tool results under a **tool role** or a structured entry like `{\"type\":\"tool\",\"name\":action.name,\"result\":...}`.\n",
        "* Watch **memory growth**: summarize or window past steps.\n",
        "* Store **structured** results when possible (JSON) to enable downstream reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) The run loop (the engine)\n",
        "\n",
        "```python\n",
        "def run(self, user_input, memory=None, max_iterations=50) -> Memory:\n",
        "    memory = memory or Memory()\n",
        "    self.set_current_task(memory, user_input)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "        print(\"Agent thinking...\")\n",
        "        response = self.prompt_llm_for_action(prompt)\n",
        "        print(f\"Agent Decision: {response}\")\n",
        "\n",
        "        action, invocation = self.get_action(response)\n",
        "        result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "        print(f\"Action Result: {result}\")\n",
        "\n",
        "        self.update_memory(memory, response, result)\n",
        "\n",
        "        if self.should_terminate(response):\n",
        "            break\n",
        "\n",
        "    return memory\n",
        "```\n",
        "\n",
        "### What to focus on\n",
        "\n",
        "* **Separation of concerns**:\n",
        "\n",
        "  * Agent builds prompt → LLM decides → Registry resolves tool → Environment executes → Memory records → Loop decides to continue.\n",
        "* **Where guardrails live**:\n",
        "\n",
        "  * Before `execute_action`: **validate invocation args** (schema + business rules).\n",
        "  * In `environment.execute_action`: sandboxing, path whitelists, network policy, timeouts, idempotency.\n",
        "  * Add **retry-on-parse-fail** (≤1–2) with stricter instructions/temperature 0.0.\n",
        "* **Observability**:\n",
        "\n",
        "  * Log prompts, tool calls, results, tokens/costs, iterations, errors. Add a circuit breaker if failures spike.\n",
        "* **Max iterations**: prevents infinite loops. Many agents also track a **budget** (token/cost/time).\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Key features to learn from this architecture\n",
        "\n",
        "1. **Pluggable layers**\n",
        "\n",
        "   * Swap the LLM (`generate_response`), prompt/parse logic (`agent_language`), tools (`ActionRegistry`), or execution (`Environment`) independently.\n",
        "\n",
        "2. **Action contract**\n",
        "\n",
        "   * Each tool has a **name, description, JSON Schema**, and a Python function. This mirrors provider APIs and supports validation.\n",
        "\n",
        "3. **Structured decisions**\n",
        "\n",
        "   * Model output becomes a **machine-readable invocation** (`tool` + `args`), not free-form text. That’s what makes it an *agent*, not just a chatbot.\n",
        "\n",
        "4. **Memory as context**\n",
        "\n",
        "   * The loop builds on past steps. Learn when to **summarize**, **filter**, or **pin** critical facts to keep the context small and relevant.\n",
        "\n",
        "5. **Termination & control**\n",
        "\n",
        "   * Clear end conditions (terminal action or goal satisfied). Avoid unbounded loops.\n",
        "\n",
        "6. **Safety & validation**\n",
        "\n",
        "   * Treat model output as untrusted: **schema-validate** + **semantic-validate** *before* executing anything with side effects.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Pragmatic upgrades you can add\n",
        "\n",
        "* **OpenAI tool calling**: move argument parsing from text to `message.tool_calls[*]`.\n",
        "* **Pydantic**: validate invocations (`tool_name` enums; typed arg models; custom validators).\n",
        "* **Retry-on-parse**: if `parse_response` fails, append a corrective message and try once more at `temperature=0`.\n",
        "* **Result shaping**: make tools return **structured JSON** for better downstream reasoning.\n",
        "* **Memory policy**: sliding window + periodic summary nodes (map-reduce style).\n",
        "* **Metrics**: count steps, tool-call success rate, invalid-invocation rate.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) What the grader/interviewer will care about\n",
        "\n",
        "* Clear **boundaries** between planning (LLM) and acting (Python/Environment).\n",
        "* Robust **validation** and **guardrails**.\n",
        "* Ability to **swap** prompting strategy (e.g., JSON-mode/tool-calling) without rewriting the agent.\n",
        "* Sensible **termination** and **memory management**.\n",
        "\n",
        "---\n",
        "\n",
        "### TL;DR\n",
        "\n",
        "Focus on the **interfaces** between: Prompt ↔️ LLM ↔️ Invocation ↔️ Registry ↔️ Environment ↔️ Memory.\n",
        "If each boundary is clean and validated, your agent will be reliable, extensible, and safe.\n"
      ],
      "metadata": {
        "id": "6V90I1sSPuZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧠 What Is `AgentLanguage`?\n",
        "\n",
        "The `AgentLanguage` class is a **core abstraction** responsible for:\n",
        "\n",
        "* 📝 **Formatting the prompt** that gets sent to the LLM\n",
        "* 📦 **Parsing the model’s response** into structured tool calls\n",
        "\n",
        "It acts as the “language layer” that determines how your agent **talks to** and **interprets responses from** the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ How `AgentLanguage` Is Used in the Agent Loop\n",
        "\n",
        "### 🔹 Step 1: Constructing the Prompt\n",
        "\n",
        "When the agent loop begins, it first constructs the LLM prompt like this:\n",
        "\n",
        "```python\n",
        "def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "    return self.agent_language.construct_prompt(\n",
        "        actions=actions.get_actions(),\n",
        "        environment=self.environment,\n",
        "        goals=goals,\n",
        "        memory=memory\n",
        "    )\n",
        "```\n",
        "\n",
        "This method builds a **structured input** for the model using four key components:\n",
        "\n",
        "| Component      | Description                                                             |\n",
        "| -------------- | ----------------------------------------------------------------------- |\n",
        "| 🧭 Goals       | What the agent is trying to accomplish                                  |\n",
        "| 🛠️ Actions    | The tools or functions available to the agent                           |\n",
        "| 🧠 Memory      | Prior messages, file contents, and context relevant to the current task |\n",
        "| 🌍 Environment | Constraints, settings, or metadata about where the agent is running     |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 Parsing Responses: AgentLanguage Again\n",
        "\n",
        "Later in the loop, after receiving the LLM response, `AgentLanguage` also handles **interpreting that response**:\n",
        "\n",
        "```python\n",
        "action, invocation = self.agent_language.parse_action_response(response)\n",
        "```\n",
        "\n",
        "If you're using **OpenAI function calling**, this is typically as simple as extracting the structured `tool_calls` block.\n",
        "\n",
        "But by **decoupling** the formatting/parsing logic into `AgentLanguage`, you can:\n",
        "\n",
        "* 🔁 Swap out OpenAI for another LLM format\n",
        "* 📜 Use natural language responses instead of tool calls (for simulation)\n",
        "* 🧪 Test your agent without hard-coding the LLM's output logic\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Why This Matters\n",
        "\n",
        "This modular `AgentLanguage` component lets your agent:\n",
        "\n",
        "* Speak **different languages** (OpenAI function calling, plain text, etc.)\n",
        "* Be tested or simulated in different contexts\n",
        "* Separate **communication logic** from **business logic**\n",
        "\n",
        "So even though we use OpenAI tools most of the time, this abstraction gives you a path to scale, extend, and simulate more flexibly.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 🧠 **Memory Context**\n",
        "\n",
        "**Definition**:\n",
        "Memory is a record of everything the agent has \"seen\" or \"done\" so far in the current session.\n",
        "\n",
        "### 🔍 Why it matters:\n",
        "\n",
        "* It helps the agent **maintain continuity** over multiple steps.\n",
        "* Without it, the agent would be stateless — it would forget past actions, results, or user instructions.\n",
        "\n",
        "### 🧩 What's typically stored:\n",
        "\n",
        "* The **user’s original request**\n",
        "* Past **LLM responses**\n",
        "* Previous **tool invocations**\n",
        "* Results of those tools (e.g., content from a file it read)\n",
        "\n",
        "### ✅ Example:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"type\": \"user\", \"content\": \"Refactor the data processing script.\"},\n",
        "  {\"type\": \"assistant\", \"content\": \"I'll begin by listing files in the 'scripts/' directory.\"},\n",
        "  {\"type\": \"user\", \"content\": \"['clean.py', 'process.py']\"},\n",
        "  {\"type\": \"assistant\", \"content\": \"Reading process.py now.\"},\n",
        "  {\"type\": \"user\", \"content\": \"# contents of process.py...\"}\n",
        "]\n",
        "```\n",
        "\n",
        "This history allows the agent to ask follow-up questions, avoid repeating steps, and remember relevant files or outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 **Environment Info**\n",
        "\n",
        "**Definition**:\n",
        "A description of the **context** in which the agent is operating. This is static info about the world the agent is working in.\n",
        "\n",
        "### 📦 Typical contents:\n",
        "\n",
        "* \"You are working in a local Python project folder.\"\n",
        "* \"You have access to functions like `list_files()` and `read_file()`.\"\n",
        "* \"You do *not* have internet access.\"\n",
        "\n",
        "### 🔐 Why it matters:\n",
        "\n",
        "It **guides the agent’s reasoning boundaries**:\n",
        "\n",
        "* Prevents hallucinating actions it cannot perform (like calling an API when it's not available).\n",
        "* Sets correct assumptions about what data/tools are available.\n",
        "\n",
        "### ✅ Example:\n",
        "\n",
        "```text\n",
        "You are operating in a local development environment.\n",
        "You can read and write files, but cannot access the internet.\n",
        "Use the tools provided below to complete the task.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Summary\n",
        "\n",
        "| Component        | Purpose                                           | Keeps Agent Aware Of     |\n",
        "| ---------------- | ------------------------------------------------- | ------------------------ |\n",
        "| Memory Context   | Conversation and execution history                | What’s already been done |\n",
        "| Environment Info | Operational boundaries and available capabilities | What’s *possible* to do  |\n",
        "\n"
      ],
      "metadata": {
        "id": "qn60vwmVDnHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤖 Step 2: Generating a Response\n",
        "\n",
        "After the prompt is constructed, the agent sends it to the language model:\n",
        "\n",
        "```python\n",
        "def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "    response = self.generate_response(full_prompt)\n",
        "    return response\n",
        "```\n",
        "\n",
        "### 🔧 What's Happening?\n",
        "\n",
        "* `generate_response()` is an **injected function** defined during initialization.\n",
        "* It handles the actual call to the LLM.\n",
        "* This abstraction allows the agent framework to stay **model-agnostic**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🤖 Why This Matters\n",
        "\n",
        "This design provides flexibility:\n",
        "\n",
        "* You can use **LiteLLM**, **OpenAI**, **Anthropic**, or any LLM.\n",
        "* You don’t have to change the core agent loop to switch models.\n",
        "* Great for mocking, testing, or deploying in environments with different LLMs.\n"
      ],
      "metadata": {
        "id": "0r7MRBjWE26f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Step 3: Parsing the Response\n",
        "\n",
        "```python\n",
        "action, invocation = self.get_action(response)\n",
        "```\n",
        "\n",
        "* `AgentLanguage` parses the model’s reply.\n",
        "* `ActionRegistry` retrieves the action definition.\n",
        "* `invocation` contains the tool name and argument values.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ What Is `AgentLanguage`?\n",
        "\n",
        "`AgentLanguage` is a **custom abstraction** used in this lecture's framework to:\n",
        "\n",
        "> Define how the LLM communicates actions to the agent.\n",
        "\n",
        "It **controls how the LLM should express tool calls** in text, and how we **parse** those calls from its output.\n",
        "\n",
        "### Think of `AgentLanguage` as:\n",
        "\n",
        "* A bridge between **natural language** (from the LLM) and **structured commands** (the agent executes).\n",
        "* A reusable module that **parses the LLM's output** into something your code can understand and act on.\n",
        "\n",
        "### It likely provides:\n",
        "\n",
        "* `format_action(action_name, args)` → Returns a prompt string to tell the LLM how to call the tool.\n",
        "* `parse_action_response(response_text)` → Extracts `action_name` and `args` from the LLM’s text.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 So What Is `get_action()` Doing?\n",
        "\n",
        "In the agent loop:\n",
        "\n",
        "```python\n",
        "action, invocation = self.get_action(response)\n",
        "```\n",
        "\n",
        "Here’s what’s happening:\n",
        "\n",
        "1. **`response`** = the raw LLM output (text).\n",
        "2. `self.get_action()` internally calls something like:\n",
        "\n",
        "   ```python\n",
        "   return self.language.parse_action_response(response)\n",
        "   ```\n",
        "3. That parsing breaks the response into:\n",
        "\n",
        "   * `action`: the name of the tool (e.g., `\"read_python_file\"`)\n",
        "   * `invocation`: the actual argument dictionary (e.g., `{\"file_name\": \"main.py\"}`)\n",
        "\n",
        "So **you don’t see `parse_response()` defined**, because it's likely wrapped inside this `AgentLanguage` object’s method.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Where Does This Fit in the Loop?\n",
        "\n",
        "The structure is roughly:\n",
        "\n",
        "```python\n",
        "prompt = self.construct_prompt(...)\n",
        "response = llm_call(prompt)\n",
        "action, invocation = agent_language.parse_action_response(response)\n",
        "tool_fn = registry[action]\n",
        "tool_fn(**invocation)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Summary\n",
        "\n",
        "| Term            | Role                                                    |\n",
        "| --------------- | ------------------------------------------------------- |\n",
        "| `AgentLanguage` | Class that defines LLM input/output formatting          |\n",
        "| `get_action()`  | Uses `AgentLanguage` to extract tool call from response |\n",
        "| `invocation`    | Parsed tool arguments ready to call in Python           |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IqSSAFSSE9Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 🔧 Step 4: Executing the Action\n",
        "\n",
        "Once the tool and arguments are known, the agent performs the action:\n",
        "\n",
        "```python\n",
        "result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "```\n",
        "\n",
        "### 🛠️ Execution Happens in the Environment\n",
        "\n",
        "* The `Environment` class is responsible for **carrying out the action**.\n",
        "* This might involve:\n",
        "\n",
        "  * Calling APIs\n",
        "  * Accessing files\n",
        "  * Running computations\n",
        "  * Querying databases\n",
        "\n",
        "### 🧩 Separation of Concerns\n",
        "\n",
        "* The `ActionRegistry` knows **what** can be done.\n",
        "* The `Environment` knows **how** to do it in the current context.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UN6BAWzUJTV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### 🧠 Step 5: Updating Memory\n",
        "\n",
        "After the agent executes an action, it needs to **record what happened**—both the decision it made and the result of that action.\n",
        "\n",
        "```python\n",
        "def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "    \"\"\"\n",
        "    Update memory with the agent's decision and the environment's response.\n",
        "    \"\"\"\n",
        "    new_memories = [\n",
        "        {\"type\": \"assistant\", \"content\": response},             # What the LLM said\n",
        "        {\"type\": \"user\", \"content\": json.dumps(result)}         # What the environment returned\n",
        "    ]\n",
        "    for m in new_memories:\n",
        "        memory.add_memory(m)\n",
        "```\n",
        "\n",
        "### 💡 Why Update Memory?\n",
        "\n",
        "* Keeps a **chronological record** of what the agent did and why.\n",
        "* Memory becomes **part of the next prompt**, allowing the agent to reason across multiple steps.\n",
        "* Helps the agent **avoid repetition**, reuse past insights, and improve coherence.\n",
        "\n",
        "### 🧠 What Gets Stored?\n",
        "\n",
        "1. The **LLM’s response** (usually a tool call or text-based reasoning).\n",
        "2. The **environment’s result** (i.e., output from the executed action).\n",
        "\n",
        "Together, this builds a conversational history that gets looped back into the LLM’s input, making the agent smarter over time.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Here's what happens regarding token usage and cost:\n",
        "\n",
        "#### 💾 1. **Memory is added to the prompt**\n",
        "\n",
        "Each time the agent loops:\n",
        "\n",
        "* It **constructs a new prompt**.\n",
        "* This prompt includes:\n",
        "\n",
        "  * The agent’s goals.\n",
        "  * Available tool definitions (tool schemas).\n",
        "  * The full **memory history** so far (unless truncated).\n",
        "  * Possibly some environmental context.\n",
        "\n",
        "#### 🔄 2. **All of that is sent to the LLM**\n",
        "\n",
        "* OpenAI (and most LLM APIs) **charge based on token input and output**.\n",
        "* So every piece of memory stored — whether it’s the user's request, the agent's tool call, or the tool's result — will be **converted into tokens** and **counted toward the prompt token budget**.\n",
        "\n",
        "#### 💸 3. **You pay for it**\n",
        "\n",
        "* You're billed for:\n",
        "\n",
        "  * All **tokens in the request** (the full prompt).\n",
        "  * All **tokens in the response** (e.g., a tool call or text reply).\n",
        "* In long-running loops, the **memory can grow large**, increasing prompt size and cost quickly.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 How to Manage This\n",
        "\n",
        "* 🔄 **Summarize or compress memory**: Store only the essential decisions and results.\n",
        "* 📉 **Truncate old context**: Only keep the last N exchanges.\n",
        "* 🧱 **Structured memory**: Instead of dumping raw results, store simplified records (e.g., `\"Searched files A, B, found match in B\"`).\n",
        "* 🧠 **Hybrid memory**: Use a vector store or database to retrieve memory selectively, instead of including all memory in every call.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HyrXBAa1J4Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⛔ Step 6: Termination Check\n",
        "\n",
        "```python\n",
        "if self.should_terminate(response): break\n",
        "```\n",
        "\n",
        "* Uses `action_def.terminal` to see if the agent should stop.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why We Need a Termination Check\n",
        "\n",
        "Agents usually run in loops:\n",
        "\n",
        "1. Construct a prompt.\n",
        "2. Call the LLM.\n",
        "3. Decide what to do.\n",
        "4. Execute a tool.\n",
        "5. Update memory.\n",
        "6. Repeat...\n",
        "\n",
        "But this can’t go on forever. At some point, the agent must decide:\n",
        "\n",
        "> “I’ve done everything I needed to do. Time to stop.”\n",
        "\n",
        "That’s where the **`should_terminate()`** check comes in.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What the Code Does\n",
        "\n",
        "```python\n",
        "def should_terminate(self, response: str) -> bool:\n",
        "    action_def, _ = self.get_action(response)\n",
        "    return action_def.terminal\n",
        "```\n",
        "\n",
        "Here’s what this means, step by step:\n",
        "\n",
        "1. **`get_action(response)`**\n",
        "   → This parses the model’s response and returns:\n",
        "\n",
        "   * `action_def`: the tool definition (from the registry)\n",
        "   * `invocation`: the tool name + arguments\n",
        "\n",
        "2. **`action_def.terminal`**\n",
        "   → This is a Boolean property on the tool definition:\n",
        "\n",
        "   * If `True`, this tool is a **terminal action**, i.e. “I'm done.”\n",
        "   * If `False`, the loop should continue.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Example: “terminate\\_agent” Tool\n",
        "\n",
        "You might define a tool like this:\n",
        "\n",
        "```python\n",
        "Action(\n",
        "    name=\"terminate_agent\",\n",
        "    description=\"Indicates that the task is complete and the agent should shut down.\",\n",
        "    parameters={},\n",
        "    terminal=True  # <--- THIS is what causes the loop to end\n",
        ")\n",
        "```\n",
        "\n",
        "When the model selects this action, `should_terminate()` will return `True`, and the loop exits.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why This Design Is Powerful\n",
        "\n",
        "* ✅ It gives the **LLM control** over when it's done — like finishing a conversation.\n",
        "* ✅ It keeps the agent loop generic — you don’t hardcode “when” to stop.\n",
        "* ✅ You can have **multiple terminal tools**, like `terminate`, `cancel`, or `complete_task`.\n",
        "\n"
      ],
      "metadata": {
        "id": "lh8XTfqaLOoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 🔁 Information Flow in One Loop Iteration\n",
        "\n",
        "1. **Memory**: Supplies past conversations and results.\n",
        "2. **Goals**: Define what the agent is trying to achieve.\n",
        "3. **ActionRegistry**: Tells what the agent *can* do.\n",
        "4. **AgentLanguage**: Converts everything into a structured prompt.\n",
        "5. **LLM**: Chooses an action and returns tool + args.\n",
        "6. **Environment**: Executes the selected action.\n",
        "7. **Memory**: Gets updated with the result and decision.\n",
        "8. **Loop**: Repeats or ends.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Creating Specialized Agents (Examples)\n",
        "\n",
        "### 📚 Research Agent\n",
        "\n",
        "```python\n",
        "research_agent = Agent(\n",
        "    goals=[Goal(\"Find and summarize information on topic X\")],\n",
        "    agent_language=ResearchLanguage(),\n",
        "    action_registry=ActionRegistry([SearchAction(), SummarizeAction(), ...]),\n",
        "    generate_response=openai_call,\n",
        "    environment=WebEnvironment()\n",
        ")\n",
        "```\n",
        "\n",
        "### 🧑‍💻 Coding Agent\n",
        "\n",
        "```python\n",
        "coding_agent = Agent(\n",
        "    goals=[Goal(\"Write and debug Python code for task Y\")],\n",
        "    agent_language=CodingLanguage(),\n",
        "    action_registry=ActionRegistry([WriteCodeAction(), TestCodeAction(), ...]),\n",
        "    generate_response=anthropic_call,\n",
        "    environment=DevEnvironment()\n",
        ")\n",
        "```\n",
        "\n",
        "Each agent uses the **same loop** but behaves entirely differently due to their unique GAME components.\n",
        "\n"
      ],
      "metadata": {
        "id": "F3Lc4bRHCba-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is “Environment?\" I\n",
        "\n",
        "Here’s a gentle way to think about an **Environment** in an AI agent.\n",
        "\n",
        "---\n",
        "\n",
        "## the simple story\n",
        "\n",
        "* The **LLM** is the **brain**. It plans what to do: “read this file,” “search for a word,” “summarize.”\n",
        "* The **Environment** is the **hands and the room** around the brain. It actually **does** the thing (opens files, runs tools) and makes sure it’s **safe**.\n",
        "\n",
        "Think of a classroom robot:\n",
        "\n",
        "* The robot’s brain says: “Open the math book to page 12.”\n",
        "* The robot’s hands (the Environment) check: “Is this really the math book? Is it allowed to open? Okay, open it.”\n",
        "  If something’s wrong, the hands say: “Nope! Not allowed,” and report back.\n",
        "\n",
        "---\n",
        "\n",
        "## why we need an Environment\n",
        "\n",
        "1. **Safety** – like rules on a playground\n",
        "\n",
        "   * “You can only play in this fenced area.”\n",
        "   * For agents: “You can only read files in this folder,” “No deleting stuff,” “Only visit safe websites.”\n",
        "\n",
        "2. **Fairness & limits** – like a timer on a game\n",
        "\n",
        "   * “You have 10 minutes.”\n",
        "   * For agents: “Stop if a tool takes too long,” “Only make a few web calls.”\n",
        "\n",
        "3. **Organization** – like a hall monitor’s checklist\n",
        "\n",
        "   * Keep track of what was done, how long it took, and any errors.\n",
        "\n",
        "4. **Consistency** – like using the same whistle for every game\n",
        "\n",
        "   * All tools return results in the **same shape** (e.g., `{ok: true, data: ...}`), so the brain can understand what happened next.\n",
        "\n",
        "---\n",
        "\n",
        "## what does an Environment actually do?\n",
        "\n",
        "When the brain says:\n",
        "\n",
        "> “Use the **read\\_file** tool with `{\"file_name\": \"notes.txt\"}`”\n",
        "\n",
        "The Environment:\n",
        "\n",
        "1. Checks the request is safe (is “notes.txt” inside our allowed folder?).\n",
        "2. Runs the tool (opens the file).\n",
        "3. Wraps the result nicely (success or error).\n",
        "4. Gives that back to the brain so it can plan the next step.\n",
        "\n",
        "---\n",
        "\n",
        "## a tiny example (very simple)\n",
        "\n",
        "```python\n",
        "class Environment:\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir  # the \"fenced playground\"\n",
        "\n",
        "    def _safe_path(self, file_name):\n",
        "        import os\n",
        "        full = os.path.abspath(os.path.join(self.base_dir, file_name))\n",
        "        if not full.startswith(os.path.abspath(self.base_dir) + os.sep):\n",
        "            raise PermissionError(\"Not allowed outside the folder!\")\n",
        "        return full\n",
        "\n",
        "    def execute_action(self, action, args):\n",
        "        try:\n",
        "            # simple safety rule for file tools\n",
        "            if action.name in {\"read_file\", \"search_in_file\"} and \"file_name\" in args:\n",
        "                args[\"file_name\"] = self._safe_path(args[\"file_name\"])\n",
        "\n",
        "            data = action.execute(**args)  # run the tool\n",
        "            return {\"ok\": True, \"action\": action.name, \"data\": data}\n",
        "        except Exception as e:\n",
        "            return {\"ok\": False, \"action\": action.name, \"error\": str(e)}\n",
        "```\n",
        "\n",
        "* **You can start even simpler** (just call `action.execute(**args)`), then add safety checks later.\n",
        "\n",
        "---\n",
        "\n",
        "## different “rooms” (environments) you might build\n",
        "\n",
        "* **FileEnvironment**: read/search files in one safe folder.\n",
        "* **WebEnvironment**: only allow certain websites; limit requests.\n",
        "* **DataEnvironment**: talk to a database safely with timeouts.\n",
        "* **MixedEnvironment**: combine several, each with its own rules.\n",
        "\n",
        "---\n",
        "\n",
        "## how it fits in your agent loop\n",
        "\n",
        "1. Brain (LLM): “I want to run `read_file` with `{file_name: \"notes.txt\"}`.”\n",
        "2. Agent finds that tool in your **ActionRegistry**.\n",
        "3. **Environment** runs it safely and returns `{ok: true, data: ...}` or `{ok: false, error: ...}`.\n",
        "4. Brain uses that result to decide the next step… or finish.\n",
        "\n",
        "---\n",
        "\n",
        "## key takeaways (sticky notes)\n",
        "\n",
        "* **Brain plans; Environment does.**\n",
        "* Environment is where you put **rules, safety, and logs**.\n",
        "* Start with a **pass-through** version, then add seatbelts: safe paths, timeouts, and consistent results.\n",
        "* Keeping this boundary clear makes your agent easier to **trust**, **debug**, and **grow**.\n"
      ],
      "metadata": {
        "id": "exa2pY-nTDC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Environment” is one of the trickiest ideas until it clicks. Think of it as **the boundary between the agent’s plan and the real world**.\n",
        "\n",
        "# What is “Environment?\" II\n",
        "\n",
        "* A **runtime sandbox** that **executes actions** (tools) on behalf of the agent.\n",
        "* It owns the **resources** (file system, network, DBs, APIs), **policies** (what’s allowed), and **guardrails** (validation, timeouts, rate limits).\n",
        "* The LLM plans; **Environment does**—safely and predictably.\n",
        "\n",
        "# Why not just call `action.execute(**args)` directly?\n",
        "\n",
        "Because you want a single, central place to handle:\n",
        "\n",
        "* **Safety**: path whitelists, URL allowlists, read-only vs write, argument sanitization.\n",
        "* **Reliability**: retries, timeouts, idempotency keys, circuit breakers.\n",
        "* **Observability**: logging, metrics, traces.\n",
        "* **Context**: shared handles (DB connections, base folders, API clients).\n",
        "* **Policy**: auth/ACLs, quotas, cost budgets.\n",
        "* **Normalization**: consistent result and error shapes for the agent’s memory.\n",
        "\n",
        "# Minimal interface (fits your loop)\n",
        "\n",
        "Your agent calls:\n",
        "\n",
        "```python\n",
        "result = environment.execute_action(action, invocation[\"args\"])\n",
        "```\n",
        "\n",
        "So implement `Environment` as the **execution gateway**:\n",
        "\n",
        "```python\n",
        "import json, time, os\n",
        "from typing import Any, Dict\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self,\n",
        "                 base_dir: str | None = None,\n",
        "                 allow_net: bool = False,\n",
        "                 allowed_exts: set[str] = {\".txt\", \".md\"},\n",
        "                 timeout_s: float = 10.0):\n",
        "        self.base_dir = os.path.abspath(base_dir) if base_dir else None\n",
        "        self.allow_net = allow_net\n",
        "        self.allowed_exts = allowed_exts\n",
        "        self.timeout_s = timeout_s\n",
        "\n",
        "    # ---- guardrails -------------------------------------------------\n",
        "    def _assert_safe_path(self, filename: str):\n",
        "        if not self.base_dir:\n",
        "            return\n",
        "        full = os.path.abspath(os.path.join(self.base_dir, filename))\n",
        "        if not full.startswith(self.base_dir + os.sep):\n",
        "            raise PermissionError(\"Path traversal blocked.\")\n",
        "        _, ext = os.path.splitext(full)\n",
        "        if self.allowed_exts and ext not in self.allowed_exts:\n",
        "            raise PermissionError(f\"Extension {ext} not allowed.\")\n",
        "        return full\n",
        "\n",
        "    # ---- execution --------------------------------------------------\n",
        "    def execute_action(self, action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Run the tool with safety, timing, and normalized output.\"\"\"\n",
        "        start = time.time()\n",
        "        try:\n",
        "            # Basic semantic validation examples\n",
        "            if action.name in {\"read_file\", \"search_in_file\"} and \"file_name\" in args:\n",
        "                args[\"file_name\"] = self._assert_safe_path(args[\"file_name\"])\n",
        "\n",
        "            # TODO: attach timeouts/retries around the call if needed\n",
        "            out = action.execute(**args)\n",
        "\n",
        "            elapsed = time.time() - start\n",
        "            return {\"ok\": True, \"action\": action.name, \"args\": args, \"data\": out, \"ms\": int(elapsed * 1000)}\n",
        "        except Exception as e:\n",
        "            elapsed = time.time() - start\n",
        "            return {\"ok\": False, \"action\": action.name, \"args\": args, \"error\": str(e), \"ms\": int(elapsed * 1000)}\n",
        "```\n",
        "\n",
        "### Notes on this pattern\n",
        "\n",
        "* **Guardrails before execution** (path checks, allowed extensions).\n",
        "* **Normalized result shape** (`ok`, `data` or `error`, `ms`). That makes your `update_memory` simple and structured.\n",
        "* You can add: **retry on transient errors**, **timeout wrappers**, **rate limits**, **network allowlist checks**, **audit logging**, etc.\n",
        "\n",
        "# Concrete examples of “Environment”\n",
        "\n",
        "* **FileSystemEnvironment** (like above): read-only base directory, only `.txt/.md`, deny traversal.\n",
        "* **WebEnvironment**: HTTP client with domain allowlist, per-host rate limits, JSON-only responses.\n",
        "* **DataEnvironment**: DB connections, transaction boundaries, query timeouts.\n",
        "* **Sandboxed Python Environment**: executes small computations with resource caps (CPU, mem).\n",
        "* **Orchestrated Environment**: coordinates multiple sub-environments (files + web + vector DB).\n",
        "\n",
        "# How it works with Actions and the Registry\n",
        "\n",
        "* **Action** = capability definition (name, description, JSON Schema, `function`).\n",
        "* **Registry** = catalog to find actions by name.\n",
        "* **Environment** = *executes* them with safety & policy.\n",
        "\n",
        "Flow:\n",
        "\n",
        "```\n",
        "LLM → {tool: \"read_file\", args: {\"file_name\":\"notes.txt\"}}\n",
        "Agent → registry.get_action(\"read_file\")\n",
        "Agent → environment.execute_action(action, args)\n",
        "Environment → validates + runs action.function(**args) → returns {ok:..., data/error}\n",
        "Agent → update memory → maybe next step or terminate\n",
        "```\n",
        "\n",
        "# What to focus on (key learning goals)\n",
        "\n",
        "1. **Separation of concerns**\n",
        "   Planning (LLM) vs Doing (Environment). The agent stays simple; the environment is where real-world complexity lives.\n",
        "\n",
        "2. **Guardrails & policy**\n",
        "   Treat LLM output as *untrusted*. Enforce allowlists, resource limits, and semantics (e.g., deny write ops in a read-only lecture).\n",
        "\n",
        "3. **Deterministic outputs**\n",
        "   Return structured results from tools; the environment wraps them in a consistent envelope (`ok/data/error`).\n",
        "\n",
        "4. **Observability**\n",
        "   Time each call, log args (after redaction), track failures. This is how you debug agents.\n",
        "\n",
        "5. **Testability**\n",
        "   You can swap in a **FakeEnvironment** for unit tests. The same agent loop runs, but tools return canned results—no real side effects.\n",
        "\n",
        "# Common pitfalls\n",
        "\n",
        "* **Letting the LLM choose any path/URL** without checks.\n",
        "* **No timeouts** → stuck or expensive calls.\n",
        "* **Unbounded memory** growth (environment can help by summarizing bulky results).\n",
        "* **Inconsistent tool return shapes** → hard for the LLM to reason about next steps.\n",
        "\n",
        "---\n",
        "\n",
        "**TL;DR:**\n",
        "The **Environment** is your agent’s execution sandbox. It centralizes **safety, policy, and reliability** for all tool calls. Keep it strict, observable, and consistent—and your agent loop will be far easier to reason about, debug, and scale.\n"
      ],
      "metadata": {
        "id": "TExzrwMnQnXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier snippets you shared didn’t need a separate `Environment` because they executed tools **directly**. Think of `Environment` as an optional abstraction:\n",
        "\n",
        "* In small demos: you can just call the Python function (tool) yourself. No `Environment` needed.\n",
        "* In a more complete agent (like your `Agent.run` loop): `Environment` is the **gateway** that runs tools and enforces safety/policy/logging. Same effect, cleaner and safer place to put guardrails.\n",
        "\n",
        "### Why Environment wasn’t defined earlier\n",
        "\n",
        "Those early lectures focused on parsing and tool selection, not execution policy. They likely omitted `Environment` for brevity and had the loop do something like:\n",
        "\n",
        "```python\n",
        "result = TOOLS[tool_name](**args)\n",
        "```\n",
        "\n",
        "All “environment” concerns (paths, timeouts, error shaping) were implicitly inside tool functions or not handled at all.\n",
        "\n",
        "### Can the agent run without `Environment`?\n",
        "\n",
        "Yes. Replace the call:\n",
        "\n",
        "```python\n",
        "result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "```\n",
        "\n",
        "with:\n",
        "\n",
        "```python\n",
        "result = action.execute(**invocation[\"args\"])\n",
        "```\n",
        "\n",
        "and it will still work. You just lose a central place for:\n",
        "\n",
        "* Safety (path/url allowlists, read-only rules)\n",
        "* Reliability (timeouts, retries)\n",
        "* Observability (uniform result shape, timing, logging)\n",
        "* Policy (quotas, permissions, budgets)\n",
        "\n",
        "### Minimal “pass-through” Environment (drop-in)\n",
        "\n",
        "If your current Agent requires an `environment`, use a no-op version and evolve later:\n",
        "\n",
        "```python\n",
        "class Environment:\n",
        "    def execute_action(self, action, args: dict) -> dict:\n",
        "        try:\n",
        "            data = action.execute(**args)\n",
        "            return {\"ok\": True, \"action\": action.name, \"args\": args, \"data\": data}\n",
        "        except Exception as e:\n",
        "            return {\"ok\": False, \"action\": action.name, \"args\": args, \"error\": str(e)}\n",
        "```\n",
        "\n",
        "Wire it up:\n",
        "\n",
        "```python\n",
        "env = Environment()\n",
        "agent = Agent(goals, agent_language, action_registry, generate_response, env)\n",
        "```\n",
        "\n",
        "### A slightly safer file-focused Environment (example)\n",
        "\n",
        "```python\n",
        "import os, time\n",
        "\n",
        "class FileEnvironment(Environment):\n",
        "    def __init__(self, base_dir: str, allowed_exts={\".txt\", \".md\"}, timeout_s=10):\n",
        "        self.base_dir = os.path.abspath(base_dir)\n",
        "        self.allowed_exts = allowed_exts\n",
        "        self.timeout_s = timeout_s\n",
        "\n",
        "    def _safe_path(self, filename: str) -> str:\n",
        "        full = os.path.abspath(os.path.join(self.base_dir, filename))\n",
        "        if not full.startswith(self.base_dir + os.sep):\n",
        "            raise PermissionError(\"Path traversal blocked.\")\n",
        "        _, ext = os.path.splitext(full)\n",
        "        if self.allowed_exts and ext.lower() not in self.allowed_exts:\n",
        "            raise PermissionError(f\"Extension {ext} not allowed.\")\n",
        "        return full\n",
        "\n",
        "    def execute_action(self, action, args: dict) -> dict:\n",
        "        start = time.time()\n",
        "        # Example semantic guardrails\n",
        "        if action.name in {\"read_file\", \"search_in_file\"} and \"file_name\" in args:\n",
        "            args[\"file_name\"] = self._safe_path(args[\"file_name\"])\n",
        "\n",
        "        try:\n",
        "            data = action.execute(**args)\n",
        "            return {\"ok\": True, \"action\": action.name, \"args\": args, \"data\": data, \"ms\": int((time.time()-start)*1000)}\n",
        "        except Exception as e:\n",
        "            return {\"ok\": False, \"action\": action.name, \"args\": args, \"error\": str(e), \"ms\": int((time.time()-start)*1000)}\n",
        "```\n",
        "\n",
        "### Takeaways for your notes\n",
        "\n",
        "* `Environment` isn’t an OpenAI thing; it’s your **application boundary** for executing tools.\n",
        "* You can **omit it** in simple prototypes; your agent will still run.\n",
        "* As soon as you care about **safety, reliability, and clean logs**, introduce `Environment` and route **all** tool calls through it.\n",
        "* Keep tool functions simple; put policy/guardrails **in the environment**, not scattered across tools.\n"
      ],
      "metadata": {
        "id": "6-ouY0z9SWzs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdmUlQhTCl5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}