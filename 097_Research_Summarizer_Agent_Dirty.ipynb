{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLFDfbW2n66HjVYn+fqdfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/097_Research_Summarizer_Agent_Dirty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a **Research Summarizer Agent** that’s focused on summarizing key points of text docs, we’ll want at least these capabilities in the **Environment** so the Agent can fully accomplish the goal:\n",
        "\n",
        "---\n",
        "\n",
        "### **Actions / Tools Needed**\n",
        "\n",
        "1. **`list_txt_files`** – List all `.txt` documents in `/content/files`\n",
        "2. **`read_txt_file`** – Read the content of a specified file\n",
        "3. **`summarize_text`** – Summarize a given chunk of text (this will call the LLM)\n",
        "4. **`write_summary_file`** – Save the generated summary into `/content/summaries`\n",
        "5. *(Optional)* **`search_text_in_file`** – If you want the Agent to pull specific sections before summarizing\n",
        "\n",
        "---\n",
        "\n",
        "### **Proposed GOAL Object**\n",
        "\n",
        "```python\n",
        "file_summary_goal = Goal(\n",
        "    priority=1,\n",
        "    name=\"file_summary\",\n",
        "    description=\"\"\"\n",
        "    Summarize the key points of text documents in /content/files.\n",
        "    Steps:\n",
        "    1. List all available text files using list_txt_files.\n",
        "    2. Read the content of each relevant file with read_txt_file.\n",
        "    3. Create a concise, clear summary highlighting the most important points.\n",
        "    4. Save the summary to /content/summaries using write_summary_file.\n",
        "    \"\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Workflow in English**\n",
        "\n",
        "* The **Agent** starts with the **Goal** of summarizing all text files in the `/content/files` folder.\n",
        "* It calls `list_txt_files` to see what’s available.\n",
        "* For each file, it calls `read_txt_file` to fetch the contents.\n",
        "* It sends the content to `summarize_text` (LLM-based) to condense into a summary.\n",
        "* Finally, it calls `write_summary_file` to save the result in `/content/summaries`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gmOFBGLbGbSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Summarizer — Environment & Actions\n",
        "# - Local text file environment (list/read/write)\n",
        "# - LLM-powered summarize_text action (uses OpenAI via a small helper)\n",
        "# - ActionRegistry wiring with JSON-schema-like parameter specs\n",
        "\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import os, io, time, traceback, re\n",
        "\n",
        "# --- Base Environment from your template (kept for consistency) ---\n",
        "class Environment:\n",
        "    def execute_action(self, action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result, \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")}\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc(),\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "            }\n",
        "\n",
        "# --- Actions & Registry (same interface as your template) ---\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn, self.description = name, fn, description\n",
        "        self.parameters, self.terminal = parameters, terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "\n",
        "# --- Research Summarizer Environment ---\n",
        "class ResearchEnvironment(Environment):\n",
        "    base_dir = \"/content/files\"\n",
        "    out_dir  = \"/content/summaries\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_join(base: str, name: str) -> str:\n",
        "        path = os.path.abspath(os.path.join(base, name))\n",
        "        if not path.startswith(os.path.abspath(base)):\n",
        "            raise ValueError(\"Invalid path\")\n",
        "        return path\n",
        "\n",
        "    @staticmethod\n",
        "    def _sanitize(name: str) -> str:\n",
        "        stem = os.path.splitext(os.path.basename(name))[0]\n",
        "        safe = re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", stem)\n",
        "        return safe + \".summary.txt\"\n",
        "\n",
        "    def list_txt_files(self) -> List[str]:\n",
        "        if not os.path.exists(self.base_dir):\n",
        "            return []\n",
        "        return sorted([f for f in os.listdir(self.base_dir) if f.lower().endswith('.txt')])\n",
        "\n",
        "    def read_txt_file(self, file_name: str) -> Dict[str, Any]:\n",
        "        path = self._safe_join(self.base_dir, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            return {\"error\": f\"File not found: {file_name}\", \"hint\": \"Call list_txt_files first\"}\n",
        "        with io.open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            text = f.read()\n",
        "        # Truncate very large documents for safety; agent can iterate in chunks if needed\n",
        "        max_chars = 12000\n",
        "        truncated = len(text) > max_chars\n",
        "        if truncated:\n",
        "            text = text[:max_chars] + \"\\n... [truncated]\"\n",
        "        return {\"file_name\": file_name, \"content\": text, \"truncated\": truncated}\n",
        "\n",
        "    def write_summary_file(self, source_file: str, content: str) -> str:\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        out_name = self._sanitize(source_file)\n",
        "        out_path = self._safe_join(self.out_dir, out_name)\n",
        "        with io.open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        return out_path"
      ],
      "metadata": {
        "id": "4UNYTXuqUFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What’s the “base Environment”?\n",
        "\n",
        "Think of it as the **body’s nervous system**: a tiny, reusable wrapper that **executes any Action safely** and **returns a uniform result envelope**.\n",
        "\n",
        "```python\n",
        "class Environment:\n",
        "    def execute_action(self, action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result, \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e), \"traceback\": traceback.format_exc(),\n",
        "                    \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")}\n",
        "```\n",
        "\n",
        "#### Why this exists\n",
        "\n",
        "* **Uniform envelope**: The agent never has to guess what a tool returns.\n",
        "* **Centralized safety**: Errors are caught and converted into structured feedback the LLM can recover from.\n",
        "* **Reusability**: Works for any domain—files, APIs, DBs, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### What’s the “ResearchEnvironment” then?\n",
        "\n",
        "That’s a **specialized Environment** with **domain-specific operations** for your use case (local text summarization). It provides concrete, safe methods like:\n",
        "\n",
        "* `list_txt_files()` — list sources\n",
        "* `read_txt_file(file_name)` — read a doc with path checks + truncation guard\n",
        "* `write_summary_file(source_file, content)` — save a summary with safe filenames\n",
        "\n",
        "It still **inherits the base behavior** (uniform envelopes via `execute_action`) but adds the **actual tools** this agent needs.\n",
        "\n",
        "A sketch:\n",
        "\n",
        "```python\n",
        "class ResearchEnvironment(Environment):\n",
        "    base_dir = \"/content/files\"\n",
        "    out_dir  = \"/content/summaries\"\n",
        "\n",
        "    def list_txt_files(self):\n",
        "        return sorted(f for f in os.listdir(self.base_dir) if f.lower().endswith(\".txt\"))\n",
        "\n",
        "    def read_txt_file(self, file_name: str):\n",
        "        path = os.path.abspath(os.path.join(self.base_dir, file_name))\n",
        "        if not path.startswith(os.path.abspath(self.base_dir)) or not os.path.exists(path):\n",
        "            return {\"error\": f\"File not found: {file_name}\", \"hint\": \"Call list_txt_files first\"}\n",
        "        with io.open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "            text = f.read()\n",
        "        if len(text) > 12000:\n",
        "            text = text[:12000] + \"\\n... [truncated]\"\n",
        "        return {\"file_name\": file_name, \"content\": text}\n",
        "\n",
        "    def write_summary_file(self, source_file: str, content: str):\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        safe = re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", os.path.splitext(os.path.basename(source_file))[0])\n",
        "        out_path = os.path.join(self.out_dir, f\"{safe}.summary.txt\")\n",
        "        with io.open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "        return out_path\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Why have both layers?\n",
        "\n",
        "* **Base Environment = generic executor**\n",
        "  Keeps the orchestrator simple: it always calls `environment.execute_action(action, args)` and gets a structured reply.\n",
        "* **Domain Environment = concrete tools**\n",
        "  Encapsulates *how* to interact with your world (files, paths, truncation, naming). If you later switch from local files to, say, S3 or Github, you **swap this class**, not the orchestrator or actions logic.\n",
        "\n",
        "---\n",
        "\n",
        "### Where do `Action` and `ActionRegistry` fit?\n",
        "\n",
        "* `Action` wraps a callable (e.g., `env.read_txt_file`) with metadata (description, JSON-like schema).\n",
        "* `ActionRegistry` is the **tool shed**—you **register** these actions so the agent can discover and invoke them by name.\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  registry.register(Action(\n",
        "      name=\"read_txt_file\",\n",
        "      fn=lambda file_name: env.read_txt_file(file_name),\n",
        "      description=\"Read a text file from /content/files\",\n",
        "      parameters={\"type\":\"object\",\"properties\":{\"file_name\":{\"type\":\"string\"}}, \"required\":[\"file_name\"]}\n",
        "  ))\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### TL;DR\n",
        "\n",
        "* Keep the **Base Environment**: generic, safe execution + uniform envelopes.\n",
        "* Add a **ResearchEnvironment**: your domain-specific “body” with file/list/read/write logic.\n",
        "* Register env methods as **Actions** in the **ActionRegistry** so the orchestrator can call them via tool selection.\n",
        "\n",
        "If you’re ready, the next step is: **define `ResearchEnvironment`**, then **register its methods as actions**. After that we’ll plug them into the orchestrator.\n"
      ],
      "metadata": {
        "id": "0tYQW699WT9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LLM Summarization Helper (uses your OpenAI client outside the loop) ---\n",
        "def make_summarizer(openai_chat_fn: Callable[[List[Dict[str, str]]], str]):\n",
        "    \"\"\"Return a summarize_text(text, max_points, style) function using provided LLM call.\n",
        "    openai_chat_fn: function that takes messages=[...] and returns string content.\n",
        "    \"\"\"\n",
        "    def summarize_text(text: str, max_points: int = 5, style: str = \"bullet\") -> str:\n",
        "        system = (\n",
        "            \"You are a precise technical summarizer. Extract key points, preserve facts, \"\n",
        "            \"and avoid speculation. Keep it concise.\"\n",
        "        )\n",
        "        user = (\n",
        "            f\"Summarize the following text into at most {max_points} key points. \"\n",
        "            f\"Format: {'bullets' if style=='bullet' else 'short paragraphs'}.\\n\\n\" + text\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ]\n",
        "        return openai_chat_fn(messages)\n",
        "    return summarize_text\n",
        "\n",
        "# --- Wiring helper to build the registry for this agent ---\n",
        "def build_research_actions(env: ResearchEnvironment, summarizer_fn: Callable[[str, int, str], str]) -> ActionRegistry:\n",
        "    registry = ActionRegistry()\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"list_txt_files\",\n",
        "        fn=lambda: env.list_txt_files(),\n",
        "        description=\"Return .txt file names from /content/files\",\n",
        "        parameters={\"type\":\"object\",\"properties\":{},\"required\":[]},\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"read_txt_file\",\n",
        "        fn=lambda file_name: env.read_txt_file(file_name),\n",
        "        description=\"Read a text file from /content/files\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"file_name\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"summarize_text\",\n",
        "        fn=lambda text, max_points=5, style=\"bullet\": summarizer_fn(text, max_points, style),\n",
        "        description=\"Summarize raw text into key points using the LLM\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"text\": {\"type\": \"string\"},\n",
        "                \"max_points\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 12},\n",
        "                \"style\": {\"type\": \"string\", \"enum\": [\"bullet\", \"paragraph\"]},\n",
        "            },\n",
        "            \"required\": [\"text\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"write_summary_file\",\n",
        "        fn=lambda source_file, content: env.write_summary_file(source_file, content),\n",
        "        description=\"Write summary text to /content/summaries (auto-named from source)\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"source_file\": {\"type\": \"string\"},\n",
        "                \"content\": {\"type\": \"string\"},\n",
        "            },\n",
        "            \"required\": [\"source_file\", \"content\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    return registry"
      ],
      "metadata": {
        "id": "uuGzXAKoWwFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Injection\n",
        "\n",
        "Instead of your code **creating its own dependencies inside** (hard-coding them),\n",
        "you **pass them in from the outside** so they can be swapped, mocked, or upgraded without touching the main logic.\n",
        "\n",
        "---\n",
        "\n",
        "### Example without dependency injection (hard-coded dependency)\n",
        "\n",
        "```python\n",
        "def summarize_text(text):\n",
        "    # Directly calls OpenAI here\n",
        "    return openai_client.chat(messages=[{\"role\": \"user\", \"content\": text}])\n",
        "```\n",
        "\n",
        "**Problems:**\n",
        "\n",
        "* Can’t test without hitting the API.\n",
        "* Stuck with `openai_client` — can’t swap to Anthropic or local model without editing this function.\n",
        "* Harder to reuse in other projects.\n",
        "\n",
        "---\n",
        "\n",
        "### Example **with** dependency injection\n",
        "\n",
        "```python\n",
        "def make_summarizer(chat_fn):\n",
        "    def summarize_text(text):\n",
        "        return chat_fn([{\"role\": \"user\", \"content\": text}])\n",
        "    return summarize_text\n",
        "\n",
        "# Pass in whatever LLM you want\n",
        "summarizer = make_summarizer(openai_client.chat)\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "* In tests, you can do:\n",
        "\n",
        "  ```python\n",
        "  fake_summarizer = make_summarizer(lambda msgs: \"fake summary\")\n",
        "  ```\n",
        "* In prod, you can do:\n",
        "\n",
        "  ```python\n",
        "  real_summarizer = make_summarizer(openai_client.chat)\n",
        "  ```\n",
        "* Main summarizer logic doesn’t care *which* LLM it’s using — it just calls the function it was given.\n",
        "\n",
        "---\n",
        "\n",
        "💡 In your code,\n",
        "`make_summarizer(openai_chat_fn)` is doing **dependency injection**:\n",
        "the *dependency* = the OpenAI chat function,\n",
        "and you’re *injecting* it into the summarizer logic instead of hard-coding it inside.\n",
        "\n",
        "---\n",
        "\n",
        "## What to focus on\n",
        "\n",
        "### 1) `make_summarizer(...)` = dependency injection\n",
        "\n",
        "* You pass in `openai_chat_fn(messages) -> str`, and it returns a ready-to-call `summarize_text(text, max_points, style)`.\n",
        "* This **decouples** your environment/actions from any specific LLM client. In tests you can pass a mock; in prod you pass your real OpenAI caller.\n",
        "* The inner function is **pure** (no state), so it’s easy to reason about and reuse.\n",
        "\n",
        "### 2) Strong, simple system prompt\n",
        "\n",
        "* The `system` text sets behavior tightly: “precise technical summarizer… preserve facts…”\n",
        "* This reduces model wandering and keeps summaries consistent.\n",
        "\n",
        "### 3) Explicit knobs for the LLM (`max_points`, `style`)\n",
        "\n",
        "* Clear, bounded control surface for the model output.\n",
        "* `style` acts like a formatting contract (bullets vs short paragraphs).\n",
        "* These are easy to expose as tool args later if you want the agent to pick them.\n",
        "\n",
        "### 4) ActionRegistry wiring = clean “tool shed”\n",
        "\n",
        "Each action entry has:\n",
        "\n",
        "* **`name`**: specific and human-readable.\n",
        "* **`fn`**: a tiny lambda that calls the real implementation (`env.*` or `summarizer_fn`).\n",
        "* **`description`**: tells the LLM when to use it.\n",
        "* **`parameters` (JSON-schema-like)**: defines what args are allowed/required.\n",
        "\n",
        "  * This is huge for reliability; your orchestrator can validate before execution.\n",
        "\n",
        "### 5) Constrained schemas to reduce errors\n",
        "\n",
        "* `read_txt_file` requires `\"file_name\"`.\n",
        "* `summarize_text` constrains `max_points` (1–12) and `style` (enum).\n",
        "* Tight schemas = fewer tool-call mistakes and better LLM focus.\n",
        "\n",
        "### 6) No hidden side effects in the tools\n",
        "\n",
        "* `list_txt_files` and `read_txt_file` are **read-only**.\n",
        "* `write_summary_file` is the only writer, and it names outputs safely in one place.\n",
        "* This makes behavior predictable and easier to audit.\n",
        "\n",
        "### 7) Small lambdas, big clarity\n",
        "\n",
        "* The lambdas keep the registry lightweight; the real logic lives in `env` or `summarizer_fn`.\n",
        "* You can swap `env` (local → S3 → GitHub) without changing the registry interface.\n",
        "\n",
        "# Nice upgrades (if/when you need them)\n",
        "\n",
        "* **Chunking for long docs:** add a `read_txt_file_chunked(file_name, start, n_chars)` tool so the agent can iterate large texts.\n",
        "* **Deterministic truncation signal:** your `read_txt_file` already returns `truncated`; good. The agent can branch on it.\n",
        "* **Terminate tool:** register a `terminate` action (`terminal=True`) so the agent can end cleanly when summaries are saved.\n",
        "* **Return schema** (optional): some teams add a `return_schema` in `parameters` or Action metadata to validate outputs post-exec.\n",
        "\n",
        "# Quick checklist for this block\n",
        "\n",
        "* [x] Summarizer is **injected** via `make_summarizer` (mockable).\n",
        "* [x] Actions have **clear names**, **good descriptions**, **tight schemas**.\n",
        "* [x] Only one tool writes to disk; all others are read-only.\n",
        "* [x] Parameters constrain LLM behavior (`enum`, `min/max`).\n",
        "\n",
        "If you’re happy with this, next step is to define `ResearchEnvironment` (you likely already have it) and then wire these actions + summarizer into your orchestrator. After that we’ll run an end-to-end pass.\n"
      ],
      "metadata": {
        "id": "EErSMkbcXbev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWjM0iBxFh3W"
      },
      "outputs": [],
      "source": [
        "# STEP 2 — Language & Prereqs (clean)\n",
        "# Put this ABOVE the wiring cell. Defines: Goal, Memory, AgentLanguage, SummarizerLanguage.\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Minimal prereqs --------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: int | None = None):\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# --- AgentLanguage base + concrete SummarizerLanguage ----------------------\n",
        "class AgentLanguage:\n",
        "    \"\"\"Build prompt for the LLM; parse the LLM's response (usually handled by generate_response).\"\"\"\n",
        "    def construct_prompt(self, actions: List[Any], environment: Any, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Default: response already structured as {\"tool\": ..., \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the summarizer agent. Tool-call parsing is done in generate_response().\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        goals_text = (\n",
        "            \"You are a file summarizer. Follow these goals in order of priority:\\n\" +\n",
        "            \"\\n\".join(f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority))\n",
        "        )\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai python-dotenv"
      ],
      "metadata": {
        "id": "acD5yXJKOP2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That block is the **wiring layer** that lets the LLM pick tools via **function calling**. It’s good—and it belongs **after** you’ve defined:\n",
        "\n",
        "* `Agent` (orchestrator), `Goal`, `Action`, `ActionRegistry`, `Memory`, `Environment`\n",
        "* `ResearchEnvironment`, `make_summarizer`, `build_research_actions`\n",
        "* Base `AgentLanguage` (your `SummarizerLanguage` subclasses it)\n",
        "\n",
        "### What it does\n",
        "\n",
        "* Exports your registry to OpenAI’s `tools` format (`registry_to_openai_tools`)\n",
        "* Builds a compact prompt (`SummarizerLanguage.construct_prompt`)\n",
        "* Calls `gpt-4o-mini` with `tools=...` and parses the **tool call** into `{ \"tool\": ..., \"args\": ... }`\n",
        "* If no tool call is returned, it safely defaults to `list_txt_files`\n",
        "\n",
        "### Things to double-check\n",
        "\n",
        "* Make sure **base `AgentLanguage`** is defined earlier (or include a small base class).\n",
        "* Ensure `ResearchEnvironment`, `make_summarizer`, and `build_research_actions` are already loaded.\n",
        "* Your `.env` path and key name: `'/content/API_KEYS.env'` with `OPENAI_API_KEY`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l_uNCNYXbU5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Research Summarizer — Orchestrator Wiring (function calling)\n",
        "# REQUIREMENTS (already defined earlier in your notebook):\n",
        "# - Agent (orchestrator template)\n",
        "# - Goal, Action, ActionRegistry\n",
        "# - ResearchEnvironment, make_summarizer, build_research_actions (from the previous cell)\n",
        "# - Memory class from your template\n",
        "#\n",
        "# This cell wires those pieces together, adds an AgentLanguage\n",
        "# and a generate_response() that uses OpenAI function calling.\n",
        "\n",
        "import os, json\n",
        "from typing import Dict, Any, List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------------- Load API key & client ----------------\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# ---------------- Tools export helper ------------------\n",
        "def registry_to_openai_tools(registry: ActionRegistry) -> List[Dict[str, Any]]:\n",
        "    tools = []\n",
        "    for a in registry.get_actions():\n",
        "        tools.append({\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": a.name,\n",
        "                \"description\": a.description,\n",
        "                \"parameters\": a.parameters or {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
        "            },\n",
        "        })\n",
        "    return tools\n",
        "\n",
        "# ---------------- AgentLanguage ------------------------\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the LLM; parse is handled in generate_response.\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals, memory):\n",
        "        goals_text = \"You are a file summarizer. Follow these goals in order of priority:\\n\" + \"\\n\".join(\n",
        "            f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority)\n",
        "        )\n",
        "        # Keep a tight memory window\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n",
        "# ------------- generate_response (OpenAI call) ---------\n",
        "# NOTE: This returns a structured dict {\"tool\": name, \"args\": {...}} for the orchestrator.\n",
        "\n",
        "def make_generate_response(registry: ActionRegistry):\n",
        "    tools_spec = registry_to_openai_tools(registry)\n",
        "\n",
        "    def build_messages(prompt_dict: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "        system = (\n",
        "            prompt_dict[\"goals_text\"]\n",
        "            + \"\\n\\nYou must use tools via function calling to make progress. \"\n",
        "            + \"Choose exactly one next tool per step. If you have saved all summaries, call a terminate tool if available; otherwise indicate completion.\"\n",
        "        )\n",
        "        # Replay memory if you'd like the model to see prior context (optional here)\n",
        "        memory_msgs = []\n",
        "        for m in prompt_dict[\"memory\"]:\n",
        "            role = m.get(\"role\") or m.get(\"type\") or \"user\"\n",
        "            content = m.get(\"content\")\n",
        "            # Coerce non-strings for safety\n",
        "            if not isinstance(content, str):\n",
        "                content = json.dumps(content)\n",
        "            memory_msgs.append({\"role\": role if role in (\"system\",\"user\",\"assistant\") else \"user\", \"content\": content})\n",
        "\n",
        "        # Nudge the model with a fresh user instruction\n",
        "        user_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Pick the best next tool from the available functions to progress toward summarizing the files. \"\n",
        "                \"Return a function call, not prose.\"\n",
        "            ),\n",
        "        }\n",
        "        return [{\"role\": \"system\", \"content\": system}] + memory_msgs + [user_msg]\n",
        "\n",
        "    def _generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        messages = build_messages(prompt_dict)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=tools_spec,\n",
        "            tool_choice=\"auto\",\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        msg = resp.choices[0].message\n",
        "        # If the model chose a tool, parse it\n",
        "        if msg.tool_calls:\n",
        "            call = msg.tool_calls[0]\n",
        "            name = call.function.name\n",
        "            try:\n",
        "                args = json.loads(call.function.arguments or \"{}\")\n",
        "            except json.JSONDecodeError:\n",
        "                args = {}\n",
        "            return {\"tool\": name, \"args\": args}\n",
        "        # Fallback if no tool was called; gently kick off with list_txt_files\n",
        "        return {\"tool\": \"list_txt_files\", \"args\": {}}\n",
        "\n",
        "    return _generate_response\n",
        "\n",
        "# ---------------- Build environment & tools -------------\n",
        "env = ResearchEnvironment()\n",
        "\n",
        "# Summarizer uses your OpenAI client under the hood\n",
        "\n",
        "def openai_chat_fn(messages):\n",
        "    resp = client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "summarizer = make_summarizer(openai_chat_fn)\n",
        "registry = build_research_actions(env, summarizer)\n",
        "\n",
        "# ---------------- Goals --------------------------------\n",
        "file_summary_goal = Goal(\n",
        "    priority=1,\n",
        "    name=\"file_summary\",\n",
        "    description=(\n",
        "        \"Summarize key points of text documents in /content/files.\\n\"\n",
        "        \"Steps: 1) list files, 2) read each file, 3) summarize to ≤5 bullets, 4) write to /content/summaries.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ---------------- Orchestrator instance -----------------\n",
        "language = SummarizerLanguage()\n",
        "generate_response = make_generate_response(registry)\n",
        "\n",
        "agent = Agent(\n",
        "    goals=[file_summary_goal],\n",
        "    agent_language=language,\n",
        "    action_registry=registry,\n",
        "    generate_response=generate_response,\n",
        "    environment=env,\n",
        ")"
      ],
      "metadata": {
        "id": "yTTvwRB4a8Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This “STEP 1 — Base Orchestrator (GAME skeleton)” block is exactly the **foundational layer** you should run first. Here’s how it fits and what to watch for:\n",
        "\n",
        "# Where this block fits\n",
        "\n",
        "* It defines your **core GAME building blocks**:\n",
        "\n",
        "  * **G**: `Goal`\n",
        "  * **A**: `Action`, `ActionRegistry` (+ lightweight `validate_args`)\n",
        "  * **M**: `Memory`\n",
        "  * **E**: `Environment` (base, uniform result envelope)\n",
        "* Everything else (ResearchEnvironment, summarizer tool, wiring, function-calling, orchestrator loop) sits **on top** of this.\n",
        "\n",
        "# Important notes for this block\n",
        "\n",
        "* Keep this block **once** in your notebook (avoid duplicates later).\n",
        "* The base `Environment.execute_action` returns a **uniform envelope**:\n",
        "\n",
        "  * success → `{\"tool_executed\": True, \"result\": ...}`\n",
        "  * failure → `{\"tool_executed\": False, \"error\": \"...\"}`\n",
        "    Your ResearchEnvironment will subclass/compose this behavior.\n",
        "* `ActionRegistry.validate_args` checks **required** params. That’s plenty for now; you can add type checks later if needed.\n",
        "\n",
        "# What still needs to be defined elsewhere\n",
        "\n",
        "* **AgentLanguage** (base) and your concrete `SummarizerLanguage`\n",
        "* **Agent** (the orchestrator loop)\n",
        "* **ResearchEnvironment** + `make_summarizer` + `build_research_actions`\n",
        "* The **wiring** for function-calling (`make_generate_response`, goals, agent instantiation)\n",
        "\n",
        "# Recommended run order (clean, error-free)\n",
        "\n",
        "1. **STEP 1 — Base Orchestrator (GAME skeleton)** ← this block\n",
        "2. **Language & Prereqs** (base `AgentLanguage` + `SummarizerLanguage`)\n",
        "3. **Research Summarizer — Environment & Actions** (ResearchEnvironment, summarizer DI, action registry)\n",
        "4. **Orchestrator Wiring (function calling)** (OpenAI tools export + `make_generate_response`)\n",
        "5. **Final wiring** (env, registry, goal, language, agent) + `agent.run(...)`\n",
        "\n",
        "# One small suggestion\n",
        "\n",
        "* For max portability (older Python kernels), if you used `int | None` elsewhere, prefer `Optional[int]`.\n",
        "\n",
        "If you’ve already run this block, you’re set to proceed to Step 2 (Language) and Step 3 (Environment & Actions).\n"
      ],
      "metadata": {
        "id": "A_0ZGEIRcYJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 — Base Orchestrator (GAME skeleton)\n",
        "# Run this cell first. It defines the core classes we'll reuse.\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ---- G: Goals --------------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "# ---- A: Actions + Registry -------------------------------------------------\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn = name, fn\n",
        "        self.description, self.parameters = description, parameters\n",
        "        self.terminal = terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "# ---- M: Memory -------------------------------------------------------------\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []  # each item: {role, content}\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: Optional[int]=None) -> List[Dict[str, Any]]:\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# ---- E: Environment --------------------------------------------------------\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e)}"
      ],
      "metadata": {
        "id": "3lApB0_4NWVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is your **orchestrator + base AgentLanguage + smoke test**. It’s correct and useful. A few quick notes so it plays nicely with the Summarizer agent you’re building:\n",
        "\n",
        "# What this block is for\n",
        "\n",
        "* Defines a minimal **AgentLanguage** and the **Agent** loop (orchestrator).\n",
        "* Includes a **hello\\_tool** smoke test to prove the loop works without OpenAI/files.\n",
        "\n",
        "# Keep it — but watch these gotchas\n",
        "\n",
        "1. **Don’t redefine classes**\n",
        "   If you already defined `AgentLanguage`, `Agent`, `Action`, `ActionRegistry`, `Memory`, `Environment` earlier, avoid duplicate definitions. Keep *one* canonical version.\n",
        "\n",
        "2. **Notebook guard**\n",
        "   In Colab/Jupyter, `if __name__ == \"__main__\":` won’t fire. If you want the smoke test to run, call it directly (or remove the guard). Not necessary once you move to the summarizer.\n",
        "\n",
        "3. **Roles & envelopes**\n",
        "   You’re already writing tool results back to memory as `{\"role\":\"tool\", \"content\": ...}` and using the uniform `{\"tool_executed\": True|False, ...}` envelope—great. Keep that invariant.\n",
        "\n",
        "4. **Validation path**\n",
        "   You validate required args before executing—perfect. If you later add type checks, do it in `ActionRegistry.validate_args`.\n",
        "\n",
        "# How to switch from smoke test → Summarizer\n",
        "\n",
        "Leave the orchestrator as-is. Replace the smoke test section with your Research Summarizer wiring (the blocks you have already):\n",
        "\n",
        "1. Build env + actions:\n",
        "\n",
        "```python\n",
        "env = ResearchEnvironment()\n",
        "\n",
        "def openai_chat_fn(messages):\n",
        "    resp = client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "summarizer = make_summarizer(openai_chat_fn)\n",
        "registry = build_research_actions(env, summarizer)\n",
        "```\n",
        "\n",
        "2. Language + generate\\_response (function calling):\n",
        "\n",
        "```python\n",
        "language = SummarizerLanguage()\n",
        "generate_response = make_generate_response(registry)\n",
        "```\n",
        "\n",
        "3. Goal + agent + run:\n",
        "\n",
        "```python\n",
        "file_summary_goal = Goal(\n",
        "    priority=1,\n",
        "    name=\"file_summary\",\n",
        "    description=(\n",
        "        \"Summarize key points of text documents in /content/files.\\n\"\n",
        "        \"Steps: 1) list files, 2) read each file, 3) summarize to ≤5 bullets, 4) write to /content/summaries.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    goals=[file_summary_goal],\n",
        "    agent_language=language,\n",
        "    action_registry=registry,\n",
        "    generate_response=generate_response,\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "memory = agent.run(\"Please summarize all .txt files in /content/files\", max_iterations=8, verbose=True)\n",
        "print(\"\\nTail of memory:\", memory.get_memories(4))\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99YvurtFc4A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---- AgentLanguage (prompt builder + parser) ------------------------------\n",
        "class AgentLanguage:\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"goals\": [g.description for g in sorted(goals, key=lambda g: g.priority)],\n",
        "            \"tools\": [a.name for a in actions],\n",
        "            \"memory\": memory.get_memories(6),\n",
        "        }\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Expect a structured dict: {\"tool\": name, \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "# ---- Orchestrator (Agent) -------------------------------------------------\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.generate_response = generate_response  # Callable[prompt_dict] -> {tool,args}\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals, memory, actions):\n",
        "        return self.agent_language.construct_prompt(actions=actions.get_actions(),\n",
        "                                                    environment=self.environment,\n",
        "                                                    goals=goals,\n",
        "                                                    memory=memory)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt):\n",
        "        return self.generate_response(full_prompt)\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation.get(\"tool\"))\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response):\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return bool(action_def and action_def.terminal)\n",
        "\n",
        "    def run(self, user_input: str, memory: Optional[Memory]=None, max_iterations: int=3, verbose: bool=True) -> Memory:\n",
        "        memory = memory or Memory()\n",
        "        memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "        for _ in range(max_iterations):\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "            if verbose:\n",
        "                print(\"Prompt →\", prompt)\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            if verbose:\n",
        "                print(\"Decision ←\", response)\n",
        "            action, invocation = self.get_action(response)\n",
        "            if not action:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Unknown action: {invocation.get('tool')}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                break\n",
        "            ok, msg = self.actions.validate_args(action, invocation.get(\"args\", {}))\n",
        "            if not ok:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Invalid args: {msg}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                continue\n",
        "            result = self.environment.execute_action(action, invocation.get(\"args\", {}))\n",
        "            if verbose:\n",
        "                print(\"Result ←\", result)\n",
        "            memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "            if not result.get(\"tool_executed\", False):\n",
        "                memory.add_memory({\"role\": \"assistant\", \"content\": \"Got an error; choosing another action next.\"})\n",
        "                continue\n",
        "            if self.should_terminate(response):\n",
        "                if verbose:\n",
        "                    print(\"Terminate signal: stopping loop.\")\n",
        "                break\n",
        "        return memory\n",
        "\n",
        "# ---- Smoke test (no OpenAI, no files) -------------------------------------\n",
        "# Define a tiny tool and a mock \"LLM\" that always selects it\n",
        "\n",
        "def hello_tool(name: str = \"world\"):\n",
        "    return f\"hello, {name}!\"\n",
        "\n",
        "reg = ActionRegistry()\n",
        "reg.register(Action(\n",
        "    name=\"hello_tool\",\n",
        "    fn=hello_tool,\n",
        "    description=\"Say hello\",\n",
        "    parameters={\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"}},\"required\":[]}\n",
        "))\n",
        "\n",
        "lang = AgentLanguage()\n",
        "\n",
        "def mock_generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Always choose hello_tool with no args\n",
        "    return {\"tool\": \"hello_tool\", \"args\": {}}\n",
        "\n",
        "env = Environment()\n",
        "goals = [Goal(1, \"demo\", \"Run a single tool to confirm wiring works.\")]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent(goals, lang, reg, mock_generate_response, env)\n",
        "    _ = agent.run(\"Say hi\", verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay1-fSikTyXZ",
        "outputId": "7e047891-217a-4a00-e0db-f54ec59809e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt → {'goals': ['Run a single tool to confirm wiring works.'], 'tools': ['hello_tool'], 'memory': [{'role': 'user', 'content': 'Say hi'}]}\n",
            "Decision ← {'tool': 'hello_tool', 'args': {}}\n",
            "Result ← {'tool_executed': True, 'result': 'hello, world!'}\n",
            "Prompt → {'goals': ['Run a single tool to confirm wiring works.'], 'tools': ['hello_tool'], 'memory': [{'role': 'user', 'content': 'Say hi'}, {'role': 'tool', 'content': {'tool_executed': True, 'result': 'hello, world!'}}]}\n",
            "Decision ← {'tool': 'hello_tool', 'args': {}}\n",
            "Result ← {'tool_executed': True, 'result': 'hello, world!'}\n",
            "Prompt → {'goals': ['Run a single tool to confirm wiring works.'], 'tools': ['hello_tool'], 'memory': [{'role': 'user', 'content': 'Say hi'}, {'role': 'tool', 'content': {'tool_executed': True, 'result': 'hello, world!'}}, {'role': 'tool', 'content': {'tool_executed': True, 'result': 'hello, world!'}}]}\n",
            "Decision ← {'tool': 'hello_tool', 'args': {}}\n",
            "Result ← {'tool_executed': True, 'result': 'hello, world!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code Dirty\n",
        "\n",
        "You’re very close—this is a solid build. I’d make a few housekeeping tweaks so it’s rock-steady and easy to reuse.\n",
        "\n",
        "# What’s great\n",
        "\n",
        "* Clean, safe **Environment** with a uniform result envelope.\n",
        "* Clear **ResearchEnvironment** with path safety + truncation flag.\n",
        "* **ActionRegistry** + JSON-schema-like parameters and basic validation.\n",
        "* DI via `make_summarizer(...)` (nice!).\n",
        "* A compact `SummarizerLanguage` and a function-calling `make_generate_response(...)`.\n",
        "\n",
        "# Changes I recommend\n",
        "\n",
        "1. **Deduplicate class definitions (keep one canonical copy).**\n",
        "   In your file, `Goal`, `Action`, `ActionRegistry`, `Memory`, `Environment`, `AgentLanguage`, and `SummarizerLanguage` appear more than once. In a notebook, re-defining can cause subtle mismatches. Keep one version of each.\n",
        "\n",
        "2. **Order of cells (avoid NameError):**\n",
        "\n",
        "   * Step 1: GAME skeleton — `Goal`, `Action`, `ActionRegistry`, `Memory`, `Environment`, `AgentLanguage`, `Agent`\n",
        "   * Step 2: Research layer — `ResearchEnvironment`, `make_summarizer`, `build_research_actions`\n",
        "   * Step 3: Wiring — `registry_to_openai_tools`, `SummarizerLanguage` (only once), `make_generate_response`, OpenAI client, env, registry, goal, agent, `agent.run(...)`\n",
        "\n",
        "3. **Use one `SummarizerLanguage`.**\n",
        "   You define it twice. Keep the more complete one and delete the duplicate.\n",
        "\n",
        "4. **Standardize on `role` (not `type`) in memory.**\n",
        "   You sometimes use `{\"type\": ...}`. Pick `{\"role\": ...}` everywhere to keep prompts consistent (your wiring already assumes `role` and falls back to `type`).\n",
        "\n",
        "   **Tiny patch (where you add to memory):**\n",
        "\n",
        "   ```python\n",
        "   memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "   # later\n",
        "   memory.add_memory({\"role\": \"assistant\", \"content\": response})\n",
        "   memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "   ```\n",
        "\n",
        "5. **(Optional, nice) Add a `terminate` tool.**\n",
        "   Lets the model end cleanly when summaries are saved.\n",
        "\n",
        "   ```python\n",
        "   registry.register(Action(\n",
        "       name=\"terminate\",\n",
        "       fn=lambda message=\"All summaries written.\": {\"message\": message},\n",
        "       description=\"Stop the loop when work is complete.\",\n",
        "       parameters={\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"}}, \"required\":[]},\n",
        "       terminal=True,\n",
        "   ))\n",
        "   ```\n",
        "\n",
        "   Then add a gentle hint in your system text (already there) that after saving all summaries, call `terminate`.\n",
        "\n",
        "6. **Portability nit:** If you ever run on older Python, change `int | None` to `Optional[int]`.\n",
        "\n",
        "7. **Avoid redefining `AgentLanguage` in the wiring cell.**\n",
        "   You already defined a base earlier—no need to redeclare. Keep the one base and one `SummarizerLanguage`.\n",
        "\n",
        "# Minimal “diff” style patches\n",
        "\n",
        "**A. Use only one `SummarizerLanguage` (keep this one):**\n",
        "\n",
        "```python\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    def construct_prompt(self, actions, environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        goals_text = (\n",
        "            \"You are a file summarizer. Follow these goals in order of priority:\\n\" +\n",
        "            \"\\n\".join(f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority))\n",
        "        )\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "```\n",
        "\n",
        "**B. Standardize memory writes (replace any `type` with `role`):**\n",
        "\n",
        "```python\n",
        "memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "# ...\n",
        "memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "# (and if you log the agent’s decision)\n",
        "memory.add_memory({\"role\": \"assistant\", \"content\": response})\n",
        "```\n",
        "\n",
        "**C. Add a terminate action (optional, but recommended):**\n",
        "\n",
        "```python\n",
        "registry.register(Action(\n",
        "    name=\"terminate\",\n",
        "    fn=lambda message=\"All summaries written.\": {\"message\": message},\n",
        "    description=\"Signal that the task is complete.\",\n",
        "    parameters={\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"}}, \"required\":[]},\n",
        "    terminal=True\n",
        "))\n",
        "```\n",
        "\n",
        "# Quick sanity checklist (before running)\n",
        "\n",
        "* [ ] Only one copy of each base class exists in the session.\n",
        "* [ ] The GAME skeleton cell ran **before** research + wiring cells.\n",
        "* [ ] `OPENAI_API_KEY` is loaded from `/content/API_KEYS.env`.\n",
        "* [ ] `/content/files` has `.txt` docs, and `/content/summaries` is writeable.\n",
        "* [ ] (Optional) `terminate` tool is in the registry.\n",
        "\n",
        "If you want, I can paste a **compact “final order” notebook index** tailored to what you already have, so you can re-run cleanly from top to bottom without duplicate defs.\n",
        "\n"
      ],
      "metadata": {
        "id": "L6o5oA7TdMKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Research Summarizer — Environment & Actions\n",
        "# - Local text file environment (list/read/write)\n",
        "# - LLM-powered summarize_text action (uses OpenAI via a small helper)\n",
        "# - ActionRegistry wiring with JSON-schema-like parameter specs\n",
        "\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import os, io, time, traceback, re\n",
        "\n",
        "# --- Base Environment from your template (kept for consistency) ---\n",
        "class Environment:\n",
        "    def execute_action(self, action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result, \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")}\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc(),\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "            }\n",
        "\n",
        "# --- Actions & Registry (same interface as your template) ---\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn, self.description = name, fn, description\n",
        "        self.parameters, self.terminal = parameters, terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "\n",
        "# --- Research Summarizer Environment ---\n",
        "class ResearchEnvironment(Environment):\n",
        "    base_dir = \"/content/files\"\n",
        "    out_dir  = \"/content/summaries\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_join(base: str, name: str) -> str:\n",
        "        path = os.path.abspath(os.path.join(base, name))\n",
        "        if not path.startswith(os.path.abspath(base)):\n",
        "            raise ValueError(\"Invalid path\")\n",
        "        return path\n",
        "\n",
        "    @staticmethod\n",
        "    def _sanitize(name: str) -> str:\n",
        "        stem = os.path.splitext(os.path.basename(name))[0]\n",
        "        safe = re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", stem)\n",
        "        return safe + \".summary.txt\"\n",
        "\n",
        "    def list_txt_files(self) -> List[str]:\n",
        "        if not os.path.exists(self.base_dir):\n",
        "            return []\n",
        "        return sorted([f for f in os.listdir(self.base_dir) if f.lower().endswith('.txt')])\n",
        "\n",
        "    def read_txt_file(self, file_name: str) -> Dict[str, Any]:\n",
        "        path = self._safe_join(self.base_dir, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            return {\"error\": f\"File not found: {file_name}\", \"hint\": \"Call list_txt_files first\"}\n",
        "        with io.open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            text = f.read()\n",
        "        # Truncate very large documents for safety; agent can iterate in chunks if needed\n",
        "        max_chars = 12000\n",
        "        truncated = len(text) > max_chars\n",
        "        if truncated:\n",
        "            text = text[:max_chars] + \"\\n... [truncated]\"\n",
        "        return {\"file_name\": file_name, \"content\": text, \"truncated\": truncated}\n",
        "\n",
        "    def write_summary_file(self, source_file: str, content: str) -> str:\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        out_name = self._sanitize(source_file)\n",
        "        out_path = self._safe_join(self.out_dir, out_name)\n",
        "        with io.open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        return out_path\n",
        "\n",
        "\n",
        "\n",
        "# --- LLM Summarization Helper (uses your OpenAI client outside the loop) ---\n",
        "def make_summarizer(openai_chat_fn: Callable[[List[Dict[str, str]]], str]):\n",
        "    \"\"\"Return a summarize_text(text, max_points, style) function using provided LLM call.\n",
        "    openai_chat_fn: function that takes messages=[...] and returns string content.\n",
        "    \"\"\"\n",
        "    def summarize_text(text: str, max_points: int = 5, style: str = \"bullet\") -> str:\n",
        "        system = (\n",
        "            \"You are a precise technical summarizer. Extract key points, preserve facts, \"\n",
        "            \"and avoid speculation. Keep it concise.\"\n",
        "        )\n",
        "        user = (\n",
        "            f\"Summarize the following text into at most {max_points} key points. \"\n",
        "            f\"Format: {'bullets' if style=='bullet' else 'short paragraphs'}.\\n\\n\" + text\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ]\n",
        "        return openai_chat_fn(messages)\n",
        "    return summarize_text\n",
        "\n",
        "# --- Wiring helper to build the registry for this agent ---\n",
        "def build_research_actions(env: ResearchEnvironment, summarizer_fn: Callable[[str, int, str], str]) -> ActionRegistry:\n",
        "    registry = ActionRegistry()\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"list_txt_files\",\n",
        "        fn=lambda: env.list_txt_files(),\n",
        "        description=\"Return .txt file names from /content/files\",\n",
        "        parameters={\"type\":\"object\",\"properties\":{},\"required\":[]},\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"read_txt_file\",\n",
        "        fn=lambda file_name: env.read_txt_file(file_name),\n",
        "        description=\"Read a text file from /content/files\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"file_name\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"summarize_text\",\n",
        "        fn=lambda text, max_points=5, style=\"bullet\": summarizer_fn(text, max_points, style),\n",
        "        description=\"Summarize raw text into key points using the LLM\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"text\": {\"type\": \"string\"},\n",
        "                \"max_points\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 12},\n",
        "                \"style\": {\"type\": \"string\", \"enum\": [\"bullet\", \"paragraph\"]},\n",
        "            },\n",
        "            \"required\": [\"text\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"write_summary_file\",\n",
        "        fn=lambda source_file, content: env.write_summary_file(source_file, content),\n",
        "        description=\"Write summary text to /content/summaries (auto-named from source)\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"source_file\": {\"type\": \"string\"},\n",
        "                \"content\": {\"type\": \"string\"},\n",
        "            },\n",
        "            \"required\": [\"source_file\", \"content\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    return registry\n",
        "\n",
        "\n",
        "# STEP 2 — Language & Prereqs (clean)\n",
        "# Put this ABOVE the wiring cell. Defines: Goal, Memory, AgentLanguage, SummarizerLanguage.\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Minimal prereqs --------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: int | None = None):\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# --- AgentLanguage base + concrete SummarizerLanguage ----------------------\n",
        "class AgentLanguage:\n",
        "    \"\"\"Build prompt for the LLM; parse the LLM's response (usually handled by generate_response).\"\"\"\n",
        "    def construct_prompt(self, actions: List[Any], environment: Any, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Default: response already structured as {\"tool\": ..., \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the summarizer agent. Tool-call parsing is done in generate_response().\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        goals_text = (\n",
        "            \"You are a file summarizer. Follow these goals in order of priority:\\n\" +\n",
        "            \"\\n\".join(f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority))\n",
        "        )\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n",
        "# Research Summarizer — Orchestrator Wiring (function calling)\n",
        "# REQUIREMENTS (already defined earlier in your notebook):\n",
        "# - Agent (orchestrator template)\n",
        "# - Goal, Action, ActionRegistry\n",
        "# - ResearchEnvironment, make_summarizer, build_research_actions (from the previous cell)\n",
        "# - Memory class from your template\n",
        "#\n",
        "# This cell wires those pieces together, adds an AgentLanguage\n",
        "# and a generate_response() that uses OpenAI function calling.\n",
        "\n",
        "import os, json\n",
        "from typing import Dict, Any, List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------------- Load API key & client ----------------\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# ---------------- Tools export helper ------------------\n",
        "def registry_to_openai_tools(registry: ActionRegistry) -> List[Dict[str, Any]]:\n",
        "    tools = []\n",
        "    for a in registry.get_actions():\n",
        "        tools.append({\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": a.name,\n",
        "                \"description\": a.description,\n",
        "                \"parameters\": a.parameters or {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
        "            },\n",
        "        })\n",
        "    return tools\n",
        "\n",
        "# ---------------- AgentLanguage ------------------------\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the LLM; parse is handled in generate_response.\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals, memory):\n",
        "        goals_text = \"You are a file summarizer. Follow these goals in order of priority:\\n\" + \"\\n\".join(\n",
        "            f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority)\n",
        "        )\n",
        "        # Keep a tight memory window\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n",
        "# ------------- generate_response (OpenAI call) ---------\n",
        "# NOTE: This returns a structured dict {\"tool\": name, \"args\": {...}} for the orchestrator.\n",
        "\n",
        "def make_generate_response(registry: ActionRegistry):\n",
        "    tools_spec = registry_to_openai_tools(registry)\n",
        "\n",
        "    def build_messages(prompt_dict: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "        system = (\n",
        "            prompt_dict[\"goals_text\"]\n",
        "            + \"\\n\\nYou must use tools via function calling to make progress. \"\n",
        "            + \"Choose exactly one next tool per step. If you have saved all summaries, call a terminate tool if available; otherwise indicate completion.\"\n",
        "        )\n",
        "        # Replay memory if you'd like the model to see prior context (optional here)\n",
        "        memory_msgs = []\n",
        "        for m in prompt_dict[\"memory\"]:\n",
        "            role = m.get(\"role\") or m.get(\"type\") or \"user\"\n",
        "            content = m.get(\"content\")\n",
        "            # Coerce non-strings for safety\n",
        "            if not isinstance(content, str):\n",
        "                content = json.dumps(content)\n",
        "            memory_msgs.append({\"role\": role if role in (\"system\",\"user\",\"assistant\") else \"user\", \"content\": content})\n",
        "\n",
        "        # Nudge the model with a fresh user instruction\n",
        "        user_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Pick the best next tool from the available functions to progress toward summarizing the files. \"\n",
        "                \"Return a function call, not prose.\"\n",
        "            ),\n",
        "        }\n",
        "        return [{\"role\": \"system\", \"content\": system}] + memory_msgs + [user_msg]\n",
        "\n",
        "    def _generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        messages = build_messages(prompt_dict)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=tools_spec,\n",
        "            tool_choice=\"auto\",\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        msg = resp.choices[0].message\n",
        "        # If the model chose a tool, parse it\n",
        "        if msg.tool_calls:\n",
        "            call = msg.tool_calls[0]\n",
        "            name = call.function.name\n",
        "            try:\n",
        "                args = json.loads(call.function.arguments or \"{}\")\n",
        "            except json.JSONDecodeError:\n",
        "                args = {}\n",
        "            return {\"tool\": name, \"args\": args}\n",
        "        # Fallback if no tool was called; gently kick off with list_txt_files\n",
        "        return {\"tool\": \"list_txt_files\", \"args\": {}}\n",
        "\n",
        "    return _generate_response\n",
        "\n",
        "# ---------------- Build environment & tools -------------\n",
        "env = ResearchEnvironment()\n",
        "\n",
        "# Summarizer uses your OpenAI client under the hood\n",
        "\n",
        "def openai_chat_fn(messages):\n",
        "    resp = client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "summarizer = make_summarizer(openai_chat_fn)\n",
        "registry = build_research_actions(env, summarizer)\n",
        "\n",
        "# ---------------- Goals --------------------------------\n",
        "file_summary_goal = Goal(\n",
        "    priority=1,\n",
        "    name=\"file_summary\",\n",
        "    description=(\n",
        "        \"Summarize key points of text documents in /content/files.\\n\"\n",
        "        \"Steps: 1) list files, 2) read each file, 3) summarize to ≤5 bullets, 4) write to /content/summaries.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ---------------- Orchestrator instance -----------------\n",
        "language = SummarizerLanguage()\n",
        "generate_response = make_generate_response(registry)\n",
        "\n",
        "agent = Agent(\n",
        "    goals=[file_summary_goal],\n",
        "    agent_language=language,\n",
        "    action_registry=registry,\n",
        "    generate_response=generate_response,\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "\n",
        "# STEP 1 — Base Orchestrator (GAME skeleton)\n",
        "# Run this cell first. It defines the core classes we'll reuse.\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ---- G: Goals --------------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "# ---- A: Actions + Registry -------------------------------------------------\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn = name, fn\n",
        "        self.description, self.parameters = description, parameters\n",
        "        self.terminal = terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "# ---- M: Memory -------------------------------------------------------------\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []  # each item: {role, content}\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: Optional[int]=None) -> List[Dict[str, Any]]:\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# ---- E: Environment --------------------------------------------------------\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e)}\n",
        "\n",
        "\n",
        "\n",
        "# ---- AgentLanguage (prompt builder + parser) ------------------------------\n",
        "class AgentLanguage:\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"goals\": [g.description for g in sorted(goals, key=lambda g: g.priority)],\n",
        "            \"tools\": [a.name for a in actions],\n",
        "            \"memory\": memory.get_memories(6),\n",
        "        }\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Expect a structured dict: {\"tool\": name, \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "# ---- Orchestrator (Agent) -------------------------------------------------\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.generate_response = generate_response  # Callable[prompt_dict] -> {tool,args}\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals, memory, actions):\n",
        "        return self.agent_language.construct_prompt(actions=actions.get_actions(),\n",
        "                                                    environment=self.environment,\n",
        "                                                    goals=goals,\n",
        "                                                    memory=memory)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt):\n",
        "        return self.generate_response(full_prompt)\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation.get(\"tool\"))\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response):\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return bool(action_def and action_def.terminal)\n",
        "\n",
        "    def run(self, user_input: str, memory: Optional[Memory]=None, max_iterations: int=3, verbose: bool=True) -> Memory:\n",
        "        memory = memory or Memory()\n",
        "        memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "        for _ in range(max_iterations):\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "            if verbose:\n",
        "                print(\"Prompt →\", prompt)\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            if verbose:\n",
        "                print(\"Decision ←\", response)\n",
        "            action, invocation = self.get_action(response)\n",
        "            if not action:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Unknown action: {invocation.get('tool')}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                break\n",
        "            ok, msg = self.actions.validate_args(action, invocation.get(\"args\", {}))\n",
        "            if not ok:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Invalid args: {msg}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                continue\n",
        "            result = self.environment.execute_action(action, invocation.get(\"args\", {}))\n",
        "            if verbose:\n",
        "                print(\"Result ←\", result)\n",
        "            memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "            if not result.get(\"tool_executed\", False):\n",
        "                memory.add_memory({\"role\": \"assistant\", \"content\": \"Got an error; choosing another action next.\"})\n",
        "                continue\n",
        "            if self.should_terminate(response):\n",
        "                if verbose:\n",
        "                    print(\"Terminate signal: stopping loop.\")\n",
        "                break\n",
        "        return memory\n",
        "\n",
        "# ---- Smoke test (no OpenAI, no files) -------------------------------------\n",
        "# Define a tiny tool and a mock \"LLM\" that always selects it\n",
        "\n",
        "def hello_tool(name: str = \"world\"):\n",
        "    return f\"hello, {name}!\"\n",
        "\n",
        "reg = ActionRegistry()\n",
        "reg.register(Action(\n",
        "    name=\"hello_tool\",\n",
        "    fn=hello_tool,\n",
        "    description=\"Say hello\",\n",
        "    parameters={\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"}},\"required\":[]}\n",
        "))\n",
        "\n",
        "lang = AgentLanguage()\n",
        "\n",
        "def mock_generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Always choose hello_tool with no args\n",
        "    return {\"tool\": \"hello_tool\", \"args\": {}}\n",
        "\n",
        "env = Environment()\n",
        "goals = [Goal(1, \"demo\", \"Run a single tool to confirm wiring works.\")]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent(goals, lang, reg, mock_generate_response, env)\n",
        "    _ = agent.run(\"Say hi\", verbose=True)"
      ],
      "metadata": {
        "id": "Q9WuZJpGdNyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code Clean"
      ],
      "metadata": {
        "id": "YWt6YrtCdzqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Research Summarizer — Environment & Actions\n",
        "# - Local text file environment (list/read/write)\n",
        "# - LLM-powered summarize_text action (uses OpenAI via a small helper)\n",
        "# - ActionRegistry wiring with JSON-schema-like parameter specs\n",
        "\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import os, io, time, traceback, re\n",
        "\n",
        "# --- Base Environment from your template (kept for consistency) ---\n",
        "class Environment:\n",
        "    def execute_action(self, action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result, \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")}\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc(),\n",
        "                \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "            }\n",
        "\n",
        "# --- Actions & Registry (same interface as your template) ---\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn, self.description = name, fn, description\n",
        "        self.parameters, self.terminal = parameters, terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "\n",
        "# --- Research Summarizer Environment ---\n",
        "class ResearchEnvironment(Environment):\n",
        "    base_dir = \"/content/files\"\n",
        "    out_dir  = \"/content/summaries\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_join(base: str, name: str) -> str:\n",
        "        path = os.path.abspath(os.path.join(base, name))\n",
        "        if not path.startswith(os.path.abspath(base)):\n",
        "            raise ValueError(\"Invalid path\")\n",
        "        return path\n",
        "\n",
        "    @staticmethod\n",
        "    def _sanitize(name: str) -> str:\n",
        "        stem = os.path.splitext(os.path.basename(name))[0]\n",
        "        safe = re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", stem)\n",
        "        return safe + \".summary.txt\"\n",
        "\n",
        "    def list_txt_files(self) -> List[str]:\n",
        "        if not os.path.exists(self.base_dir):\n",
        "            return []\n",
        "        return sorted([f for f in os.listdir(self.base_dir) if f.lower().endswith('.txt')])\n",
        "\n",
        "    def read_txt_file(self, file_name: str) -> Dict[str, Any]:\n",
        "        path = self._safe_join(self.base_dir, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            return {\"error\": f\"File not found: {file_name}\", \"hint\": \"Call list_txt_files first\"}\n",
        "        with io.open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            text = f.read()\n",
        "        # Truncate very large documents for safety; agent can iterate in chunks if needed\n",
        "        max_chars = 12000\n",
        "        truncated = len(text) > max_chars\n",
        "        if truncated:\n",
        "            text = text[:max_chars] + \"\\n... [truncated]\"\n",
        "        return {\"file_name\": file_name, \"content\": text, \"truncated\": truncated}\n",
        "\n",
        "    def write_summary_file(self, source_file: str, content: str) -> str:\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        out_name = self._sanitize(source_file)\n",
        "        out_path = self._safe_join(self.out_dir, out_name)\n",
        "        with io.open(out_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        return out_path\n"
      ],
      "metadata": {
        "id": "ekDu2wt-eLgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- LLM Summarization Helper (uses your OpenAI client outside the loop) ---\n",
        "def make_summarizer(openai_chat_fn: Callable[[List[Dict[str, str]]], str]):\n",
        "    \"\"\"Return a summarize_text(text, max_points, style) function using provided LLM call.\n",
        "    openai_chat_fn: function that takes messages=[...] and returns string content.\n",
        "    \"\"\"\n",
        "    def summarize_text(text: str, max_points: int = 5, style: str = \"bullet\") -> str:\n",
        "        system = (\n",
        "            \"You are a precise technical summarizer. Extract key points, preserve facts, \"\n",
        "            \"and avoid speculation. Keep it concise.\"\n",
        "        )\n",
        "        user = (\n",
        "            f\"Summarize the following text into at most {max_points} key points. \"\n",
        "            f\"Format: {'bullets' if style=='bullet' else 'short paragraphs'}.\\n\\n\" + text\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ]\n",
        "        return openai_chat_fn(messages)\n",
        "    return summarize_text\n",
        "\n",
        "# --- Wiring helper to build the registry for this agent ---\n",
        "def build_research_actions(env: ResearchEnvironment, summarizer_fn: Callable[[str, int, str], str]) -> ActionRegistry:\n",
        "    registry = ActionRegistry()\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"list_txt_files\",\n",
        "        fn=lambda: env.list_txt_files(),\n",
        "        description=\"Return .txt file names from /content/files\",\n",
        "        parameters={\"type\":\"object\",\"properties\":{},\"required\":[]},\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"read_txt_file\",\n",
        "        fn=lambda file_name: env.read_txt_file(file_name),\n",
        "        description=\"Read a text file from /content/files\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"file_name\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"summarize_text\",\n",
        "        fn=lambda text, max_points=5, style=\"bullet\": summarizer_fn(text, max_points, style),\n",
        "        description=\"Summarize raw text into key points using the LLM\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"text\": {\"type\": \"string\"},\n",
        "                \"max_points\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 12},\n",
        "                \"style\": {\"type\": \"string\", \"enum\": [\"bullet\", \"paragraph\"]},\n",
        "            },\n",
        "            \"required\": [\"text\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    registry.register(Action(\n",
        "        name=\"write_summary_file\",\n",
        "        fn=lambda source_file, content: env.write_summary_file(source_file, content),\n",
        "        description=\"Write summary text to /content/summaries (auto-named from source)\",\n",
        "        parameters={\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"source_file\": {\"type\": \"string\"},\n",
        "                \"content\": {\"type\": \"string\"},\n",
        "            },\n",
        "            \"required\": [\"source_file\", \"content\"],\n",
        "        },\n",
        "    ))\n",
        "\n",
        "    return registry\n",
        "\n",
        "\n",
        "# STEP 2 — Language & Prereqs (clean)\n",
        "# Put this ABOVE the wiring cell. Defines: Goal, Memory, AgentLanguage, SummarizerLanguage.\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Minimal prereqs --------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: int | None = None):\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# --- AgentLanguage base + concrete SummarizerLanguage ----------------------\n",
        "class AgentLanguage:\n",
        "    \"\"\"Build prompt for the LLM; parse the LLM's response (usually handled by generate_response).\"\"\"\n",
        "    def construct_prompt(self, actions: List[Any], environment: Any, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Default: response already structured as {\"tool\": ..., \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the summarizer agent. Tool-call parsing is done in generate_response().\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        goals_text = (\n",
        "            \"You are a file summarizer. Follow these goals in order of priority:\\n\" +\n",
        "            \"\\n\".join(f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority))\n",
        "        )\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n",
        "# Research Summarizer — Orchestrator Wiring (function calling)\n",
        "# REQUIREMENTS (already defined earlier in your notebook):\n",
        "# - Agent (orchestrator template)\n",
        "# - Goal, Action, ActionRegistry\n",
        "# - ResearchEnvironment, make_summarizer, build_research_actions (from the previous cell)\n",
        "# - Memory class from your template\n",
        "#\n",
        "# This cell wires those pieces together, adds an AgentLanguage\n",
        "# and a generate_response() that uses OpenAI function calling.\n",
        "\n",
        "import os, json\n",
        "from typing import Dict, Any, List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------------- Load API key & client ----------------\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# ---------------- Tools export helper ------------------\n",
        "def registry_to_openai_tools(registry: ActionRegistry) -> List[Dict[str, Any]]:\n",
        "    tools = []\n",
        "    for a in registry.get_actions():\n",
        "        tools.append({\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": a.name,\n",
        "                \"description\": a.description,\n",
        "                \"parameters\": a.parameters or {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
        "            },\n",
        "        })\n",
        "    return tools\n",
        "\n",
        "# ---------------- AgentLanguage ------------------------\n",
        "class SummarizerLanguage(AgentLanguage):\n",
        "    \"\"\"Formats goals/memory for the LLM; parse is handled in generate_response.\"\"\"\n",
        "    def construct_prompt(self, actions, environment, goals, memory):\n",
        "        goals_text = \"You are a file summarizer. Follow these goals in order of priority:\\n\" + \"\\n\".join(\n",
        "            f\"- ({g.priority}) {g.name}: {g.description.strip()}\" for g in sorted(goals, key=lambda g: g.priority)\n",
        "        )\n",
        "        # Keep a tight memory window\n",
        "        mem = memory.get_memories(8)\n",
        "        return {\"goals_text\": goals_text, \"memory\": mem, \"actions\": actions}\n",
        "\n",
        "# ------------- generate_response (OpenAI call) ---------\n",
        "# NOTE: This returns a structured dict {\"tool\": name, \"args\": {...}} for the orchestrator.\n",
        "\n",
        "def make_generate_response(registry: ActionRegistry):\n",
        "    tools_spec = registry_to_openai_tools(registry)\n",
        "\n",
        "    def build_messages(prompt_dict: Dict[str, Any]) -> List[Dict[str, str]]:\n",
        "        system = (\n",
        "            prompt_dict[\"goals_text\"]\n",
        "            + \"\\n\\nYou must use tools via function calling to make progress. \"\n",
        "            + \"Choose exactly one next tool per step. If you have saved all summaries, call a terminate tool if available; otherwise indicate completion.\"\n",
        "        )\n",
        "        # Replay memory if you'd like the model to see prior context (optional here)\n",
        "        memory_msgs = []\n",
        "        for m in prompt_dict[\"memory\"]:\n",
        "            role = m.get(\"role\") or m.get(\"type\") or \"user\"\n",
        "            content = m.get(\"content\")\n",
        "            # Coerce non-strings for safety\n",
        "            if not isinstance(content, str):\n",
        "                content = json.dumps(content)\n",
        "            memory_msgs.append({\"role\": role if role in (\"system\",\"user\",\"assistant\") else \"user\", \"content\": content})\n",
        "\n",
        "        # Nudge the model with a fresh user instruction\n",
        "        user_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Pick the best next tool from the available functions to progress toward summarizing the files. \"\n",
        "                \"Return a function call, not prose.\"\n",
        "            ),\n",
        "        }\n",
        "        return [{\"role\": \"system\", \"content\": system}] + memory_msgs + [user_msg]\n",
        "\n",
        "    def _generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        messages = build_messages(prompt_dict)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=tools_spec,\n",
        "            tool_choice=\"auto\",\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        msg = resp.choices[0].message\n",
        "        # If the model chose a tool, parse it\n",
        "        if msg.tool_calls:\n",
        "            call = msg.tool_calls[0]\n",
        "            name = call.function.name\n",
        "            try:\n",
        "                args = json.loads(call.function.arguments or \"{}\")\n",
        "            except json.JSONDecodeError:\n",
        "                args = {}\n",
        "            return {\"tool\": name, \"args\": args}\n",
        "        # Fallback if no tool was called; gently kick off with list_txt_files\n",
        "        return {\"tool\": \"list_txt_files\", \"args\": {}}\n",
        "\n",
        "    return _generate_response\n",
        "\n",
        "# ---------------- Build environment & tools -------------\n",
        "env = ResearchEnvironment()\n",
        "\n",
        "# Summarizer uses your OpenAI client under the hood\n",
        "\n",
        "def openai_chat_fn(messages):\n",
        "    resp = client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "summarizer = make_summarizer(openai_chat_fn)\n",
        "registry = build_research_actions(env, summarizer)\n",
        "\n",
        "# ---------------- Goals --------------------------------\n",
        "file_summary_goal = Goal(\n",
        "    priority=1,\n",
        "    name=\"file_summary\",\n",
        "    description=(\n",
        "        \"Summarize key points of text documents in /content/files.\\n\"\n",
        "        \"Steps: 1) list files, 2) read each file, 3) summarize to ≤5 bullets, 4) write to /content/summaries.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ---------------- Orchestrator instance -----------------\n",
        "language = SummarizerLanguage()\n",
        "generate_response = make_generate_response(registry)\n",
        "\n",
        "agent = Agent(\n",
        "    goals=[file_summary_goal],\n",
        "    agent_language=language,\n",
        "    action_registry=registry,\n",
        "    generate_response=generate_response,\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "\n",
        "# STEP 1 — Base Orchestrator (GAME skeleton)\n",
        "# Run this cell first. It defines the core classes we'll reuse.\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# ---- G: Goals --------------------------------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "# ---- A: Actions + Registry -------------------------------------------------\n",
        "class Action:\n",
        "    def __init__(self, name: str, fn: Callable, description: str, parameters: Dict, terminal: bool=False):\n",
        "        self.name, self.fn = name, fn\n",
        "        self.description, self.parameters = description, parameters\n",
        "        self.terminal = terminal\n",
        "    def execute(self, **kwargs):\n",
        "        return self.fn(**kwargs)\n",
        "\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self._actions: Dict[str, Action] = {}\n",
        "    def register(self, action: Action):\n",
        "        if action.name in self._actions:\n",
        "            raise ValueError(f\"Action already registered: {action.name}\")\n",
        "        self._actions[action.name] = action\n",
        "    def get_action(self, name: str) -> Optional[Action]:\n",
        "        return self._actions.get(name)\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        return list(self._actions.values())\n",
        "    def validate_args(self, action: Action, args: Dict[str, Any]) -> (bool, str):\n",
        "        schema = action.parameters or {\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "        for key in schema.get(\"required\", []):\n",
        "            if key not in args:\n",
        "                return False, f\"Missing required arg: {key}\"\n",
        "        return True, \"ok\"\n",
        "\n",
        "# ---- M: Memory -------------------------------------------------------------\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items: List[Dict[str, Any]] = []  # each item: {role, content}\n",
        "    def add_memory(self, m: Dict[str, Any]):\n",
        "        self.items.append(m)\n",
        "    def get_memories(self, limit: Optional[int]=None) -> List[Dict[str, Any]]:\n",
        "        return self.items[-limit:] if limit else self.items\n",
        "\n",
        "# ---- E: Environment --------------------------------------------------------\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return {\"tool_executed\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"tool_executed\": False, \"error\": str(e)}\n",
        "\n",
        "\n",
        "\n",
        "# ---- AgentLanguage (prompt builder + parser) ------------------------------\n",
        "class AgentLanguage:\n",
        "    def construct_prompt(self, actions: List[Action], environment: Environment, goals: List[Goal], memory: Memory) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"goals\": [g.description for g in sorted(goals, key=lambda g: g.priority)],\n",
        "            \"tools\": [a.name for a in actions],\n",
        "            \"memory\": memory.get_memories(6),\n",
        "        }\n",
        "    def parse_response(self, response: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        # Expect a structured dict: {\"tool\": name, \"args\": {...}}\n",
        "        return response\n",
        "\n",
        "# ---- Orchestrator (Agent) -------------------------------------------------\n",
        "class Agent:\n",
        "    def __init__(self, goals, agent_language, action_registry, generate_response, environment):\n",
        "        self.goals = goals\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.generate_response = generate_response  # Callable[prompt_dict] -> {tool,args}\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals, memory, actions):\n",
        "        return self.agent_language.construct_prompt(actions=actions.get_actions(),\n",
        "                                                    environment=self.environment,\n",
        "                                                    goals=goals,\n",
        "                                                    memory=memory)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt):\n",
        "        return self.generate_response(full_prompt)\n",
        "\n",
        "    def get_action(self, response):\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation.get(\"tool\"))\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response):\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return bool(action_def and action_def.terminal)\n",
        "\n",
        "    def run(self, user_input: str, memory: Optional[Memory]=None, max_iterations: int=3, verbose: bool=True) -> Memory:\n",
        "        memory = memory or Memory()\n",
        "        memory.add_memory({\"role\": \"user\", \"content\": user_input})\n",
        "        for _ in range(max_iterations):\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "            if verbose:\n",
        "                print(\"Prompt →\", prompt)\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            if verbose:\n",
        "                print(\"Decision ←\", response)\n",
        "            action, invocation = self.get_action(response)\n",
        "            if not action:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Unknown action: {invocation.get('tool')}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                break\n",
        "            ok, msg = self.actions.validate_args(action, invocation.get(\"args\", {}))\n",
        "            if not ok:\n",
        "                err = {\"tool_executed\": False, \"error\": f\"Invalid args: {msg}\"}\n",
        "                memory.add_memory({\"role\": \"tool\", \"content\": err})\n",
        "                continue\n",
        "            result = self.environment.execute_action(action, invocation.get(\"args\", {}))\n",
        "            if verbose:\n",
        "                print(\"Result ←\", result)\n",
        "            memory.add_memory({\"role\": \"tool\", \"content\": result})\n",
        "            if not result.get(\"tool_executed\", False):\n",
        "                memory.add_memory({\"role\": \"assistant\", \"content\": \"Got an error; choosing another action next.\"})\n",
        "                continue\n",
        "            if self.should_terminate(response):\n",
        "                if verbose:\n",
        "                    print(\"Terminate signal: stopping loop.\")\n",
        "                break\n",
        "        return memory\n",
        "\n",
        "# ---- Smoke test (no OpenAI, no files) -------------------------------------\n",
        "# Define a tiny tool and a mock \"LLM\" that always selects it\n",
        "\n",
        "def hello_tool(name: str = \"world\"):\n",
        "    return f\"hello, {name}!\"\n",
        "\n",
        "reg = ActionRegistry()\n",
        "reg.register(Action(\n",
        "    name=\"hello_tool\",\n",
        "    fn=hello_tool,\n",
        "    description=\"Say hello\",\n",
        "    parameters={\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"}},\"required\":[]}\n",
        "))\n",
        "\n",
        "lang = AgentLanguage()\n",
        "\n",
        "def mock_generate_response(prompt_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Always choose hello_tool with no args\n",
        "    return {\"tool\": \"hello_tool\", \"args\": {}}\n",
        "\n",
        "env = Environment()\n",
        "goals = [Goal(1, \"demo\", \"Run a single tool to confirm wiring works.\")]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent(goals, lang, reg, mock_generate_response, env)\n",
        "    _ = agent.run(\"Say hi\", verbose=True)"
      ],
      "metadata": {
        "id": "07uoJMgMd04P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}