{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi/r7FS79HXANVbtr3SJke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/540_EaaS_v2_gaolPlanning_nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Goal & Planning Nodes — Why This Matters\n",
        "\n",
        "These two nodes define the **governance layer** of the EaaS Orchestrator.\n",
        "\n",
        "Before a single scenario is executed or a single score is computed, the system explicitly answers two questions:\n",
        "\n",
        "1. **What are we trying to evaluate, and why?**\n",
        "2. **How will we go about doing it, step by step?**\n",
        "\n",
        "That ordering is not accidental — and it’s one of the biggest differences between this system and most AI agents in production today.\n",
        "\n",
        "---\n",
        "\n",
        "## The Goal Node: Making Evaluation Intent Explicit\n",
        "\n",
        "### What this node does in real-world terms\n",
        "\n",
        "The `goal_node` turns a loosely defined idea like *“let’s test the agent”* into a **formal evaluation contract**.\n",
        "\n",
        "It explicitly defines:\n",
        "\n",
        "* the scope of the evaluation\n",
        "* the metrics that matter\n",
        "* what success looks like\n",
        "* and what thresholds determine pass or fail\n",
        "\n",
        "This means the system never evaluates “just because.”\n",
        "It evaluates **against a declared objective**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why this matters to executives and managers\n",
        "\n",
        "Executives are often uncomfortable with AI systems because they can’t tell:\n",
        "\n",
        "* what the system is optimizing for\n",
        "* whether success criteria changed silently\n",
        "* or why a result was considered acceptable\n",
        "\n",
        "This node eliminates that ambiguity.\n",
        "\n",
        "By encoding:\n",
        "\n",
        "* evaluation type\n",
        "* metric definitions\n",
        "* success thresholds\n",
        "\n",
        "leaders can see — in plain data — **what standards are being applied**.\n",
        "\n",
        "That’s deeply reassuring, especially in regulated or customer-facing environments.\n",
        "\n",
        "---\n",
        "\n",
        "### Why this is different from most agents today\n",
        "\n",
        "Most agents:\n",
        "\n",
        "* implicitly define goals inside prompts\n",
        "* mix execution logic with evaluation logic\n",
        "* change behavior without changing declared objectives\n",
        "\n",
        "Here, the goal is:\n",
        "\n",
        "* structured\n",
        "* inspectable\n",
        "* reproducible\n",
        "* versionable\n",
        "\n",
        "That’s governance, not guesswork.\n",
        "\n",
        "---\n",
        "\n",
        "## The Planning Node: Turning Intent Into a Controlled Workflow\n",
        "\n",
        "### What this node does in practical terms\n",
        "\n",
        "The `planning_node` takes the declared goal and converts it into a **deterministic execution plan**.\n",
        "\n",
        "This plan answers:\n",
        "\n",
        "* what steps will run\n",
        "* in what order\n",
        "* with what dependencies\n",
        "* and what artifacts each step produces\n",
        "\n",
        "This is the difference between:\n",
        "\n",
        "> “The agent ran and something happened”\n",
        "\n",
        "and:\n",
        "\n",
        "> “The system executed a known workflow and produced known outputs.”\n",
        "\n",
        "---\n",
        "\n",
        "### Why leaders are relieved to see this\n",
        "\n",
        "From a business perspective, this plan:\n",
        "\n",
        "* reduces operational risk\n",
        "* prevents hidden behavior\n",
        "* supports auditability\n",
        "\n",
        "If something goes wrong, teams can trace:\n",
        "\n",
        "* which step ran\n",
        "* what inputs it used\n",
        "* what outputs it produced\n",
        "\n",
        "This is how **enterprise systems** are designed — not experimental demos.\n",
        "\n",
        "---\n",
        "\n",
        "### Why this is rare in agentic systems\n",
        "\n",
        "Most AI agents today:\n",
        "\n",
        "* act in an event-driven or reactive way\n",
        "* do not expose a formal execution plan\n",
        "* rely on implicit sequencing inside code or prompts\n",
        "\n",
        "This makes failures:\n",
        "\n",
        "* hard to debug\n",
        "* hard to explain\n",
        "* hard to trust\n",
        "\n",
        "Your approach makes the workflow **explicit and reviewable**, which is exactly what stakeholders want when AI starts influencing real decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## Rule-Based Planning: A Strategic Choice\n",
        "\n",
        "One of the most important design decisions here is what you *didn’t* do.\n",
        "\n",
        "You deliberately avoided:\n",
        "\n",
        "* LLM-based planning\n",
        "* dynamic goal reinterpretation\n",
        "* opaque reasoning chains\n",
        "\n",
        "Instead, you chose:\n",
        "\n",
        "* fixed steps\n",
        "* clear dependencies\n",
        "* predictable outputs\n",
        "\n",
        "This shows maturity.\n",
        "\n",
        "It signals:\n",
        "\n",
        "> “We optimize for reliability first, intelligence second.”\n",
        "\n",
        "That’s how trust is earned.\n",
        "\n",
        "---\n",
        "\n",
        "## How These Nodes Support ROI and Accountability\n",
        "\n",
        "Together, these nodes ensure that:\n",
        "\n",
        "* Every evaluation has a **declared purpose**\n",
        "* Every workflow follows a **known structure**\n",
        "* Every output can be tied back to a **specific step**\n",
        "* Every success or failure can be explained\n",
        "\n",
        "This directly supports:\n",
        "\n",
        "* release gating\n",
        "* regression detection\n",
        "* performance reporting\n",
        "* executive oversight\n",
        "\n",
        "In short, they make AI behavior **manageable at scale**.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Architectural Pattern\n",
        "\n",
        "These nodes reinforce a broader philosophy that runs through your entire system:\n",
        "\n",
        "> **AI should execute within boundaries that humans define — not define its own boundaries.**\n",
        "\n",
        "The goal defines *what matters*.\n",
        "The plan defines *how it will be measured*.\n",
        "Everything else follows.\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Takeaway\n",
        "\n",
        "What leaders see here is not “AI automation.”\n",
        "\n",
        "They see:\n",
        "\n",
        "* intent before action\n",
        "* structure before execution\n",
        "* policy before intelligence\n",
        "\n",
        "That’s why this design feels safe, professional, and deployable — and why it stands apart from most agent systems in production today.\n",
        "\n"
      ],
      "metadata": {
        "id": "yg-RXqP25z3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnync2CO05Ro"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "EaaS Orchestrator Nodes\n",
        "\n",
        "Nodes orchestrate the evaluation workflow. Utilities handle the actual work.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "from config import EvalAsServiceOrchestratorState\n",
        "\n",
        "\n",
        "def goal_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Goal Node: Define the evaluation goal.\n",
        "\n",
        "    Sets up the framework for evaluating AI agents by defining:\n",
        "    - What we're evaluating (scenarios, agents)\n",
        "    - What metrics we're tracking\n",
        "    - What success looks like\n",
        "    \"\"\"\n",
        "    scenario_id = state.get(\"scenario_id\")\n",
        "    target_agent_id = state.get(\"target_agent_id\")\n",
        "    errors = state.get(\"errors\", [])\n",
        "\n",
        "    # Build goal definition\n",
        "    goal = {\n",
        "        \"objective\": \"Evaluate AI agent performance using test scenarios\",\n",
        "        \"evaluation_type\": \"comprehensive\",  # or \"targeted\" if scenario_id/agent_id specified\n",
        "        \"scope\": {\n",
        "            \"scenario_id\": scenario_id,  # None = evaluate all scenarios\n",
        "            \"target_agent_id\": target_agent_id,  # None = evaluate all agents\n",
        "        },\n",
        "        \"metrics\": [\n",
        "            \"correctness_score\",  # Does output match expected?\n",
        "            \"response_time_score\",  # Is response time acceptable?\n",
        "            \"output_quality_score\",  # Is structure/format correct?\n",
        "            \"overall_score\"  # Weighted combination\n",
        "        ],\n",
        "        \"success_criteria\": {\n",
        "            \"pass_threshold\": 0.80,  # Minimum score to pass (from config)\n",
        "            \"target_pass_rate\": 0.90  # Target overall pass rate\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"goal\": goal,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "\n",
        "\n",
        "def planning_node(state: EvalAsServiceOrchestratorState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Planning Node: Create execution plan based on goal.\n",
        "\n",
        "    Creates a step-by-step plan for the evaluation workflow.\n",
        "    Rule-based, no LLM needed for MVP.\n",
        "    \"\"\"\n",
        "    goal = state.get(\"goal\")\n",
        "    errors = state.get(\"errors\", [])\n",
        "\n",
        "    if not goal:\n",
        "        return {\n",
        "            \"errors\": errors + [\"planning_node: goal is required\"]\n",
        "        }\n",
        "\n",
        "    # Create execution plan\n",
        "    plan = [\n",
        "        {\n",
        "            \"step\": 1,\n",
        "            \"name\": \"data_loading\",\n",
        "            \"description\": \"Load test scenarios, specialist agents, and supporting data\",\n",
        "            \"dependencies\": [],\n",
        "            \"outputs\": [\n",
        "                \"journey_scenarios\",\n",
        "                \"specialist_agents\",\n",
        "                \"supporting_data\",\n",
        "                \"decision_rules\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"step\": 2,\n",
        "            \"name\": \"evaluation_execution\",\n",
        "            \"description\": \"Execute test scenarios through target agents\",\n",
        "            \"dependencies\": [\"data_loading\"],\n",
        "            \"outputs\": [\n",
        "                \"executed_evaluations\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"step\": 3,\n",
        "            \"name\": \"scoring_analysis\",\n",
        "            \"description\": \"Score evaluations and analyze performance\",\n",
        "            \"dependencies\": [\"evaluation_execution\"],\n",
        "            \"outputs\": [\n",
        "                \"evaluation_scores\",\n",
        "                \"agent_performance_summary\",\n",
        "                \"evaluation_summary\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"step\": 4,\n",
        "            \"name\": \"report_generation\",\n",
        "            \"description\": \"Generate comprehensive evaluation report\",\n",
        "            \"dependencies\": [\"scoring_analysis\"],\n",
        "            \"outputs\": [\n",
        "                \"evaluation_report\",\n",
        "                \"report_file_path\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"plan\": plan,\n",
        "        \"errors\": errors\n",
        "    }\n"
      ]
    }
  ]
}