{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlXg4kU2nWstcxZygtJF0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/001_Prompt_Response.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 AI Agents 101 – Foundational Concepts & Architecture\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 What is an AI Agent?\n",
        "\n",
        "An **AI agent** is a system that:\n",
        "1. **Receives a user input (goal or question)**\n",
        "2. **Thinks about what to do** (using a language model)\n",
        "3. **Takes an action** using tools like APIs, web access, or databases\n",
        "4. **Observes the result**\n",
        "5. **Uses that observation to produce an answer or plan the next step**\n",
        "\n",
        "This loop is often described as:\n",
        "\n",
        "```\n",
        "Thought → Action → PAUSE → Observation → Answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 Core Components of a Simple AI Agent\n",
        "\n",
        "| Component           | Purpose                                                                 |\n",
        "|---------------------|-------------------------------------------------------------------------|\n",
        "| 🧠 Language Model   | Thinks and decides what to do (e.g. GPT-4 or open-source LLMs)          |\n",
        "| 📜 System Prompt     | Tells the LLM how to behave (e.g. think in steps, use tools)           |\n",
        "| 🧰 Tools / Functions | The agent's hands — lets it do useful things like web search, scraping |\n",
        "| 🧩 JSON Parsing      | Helps extract structured actions from the LLM’s response                |\n",
        "| 🔁 Control Loop      | Coordinates each step: calls LLM, parses response, runs tools, repeats  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Tools – What Can the Agent Do?\n",
        "\n",
        "The agent doesn’t know everything — so we give it **tools**:\n",
        "- `search_wikipedia(query)` → Calls Wikipedia API\n",
        "- `load_content_from_url(url)` → Fetches content from a page\n",
        "- `seo_audit_web_page(url)` → Sends site to an SEO tool\n",
        "\n",
        "These tools are just **Python functions** that return some output.\n",
        "\n",
        "---\n",
        "\n",
        "## 📜 Prompt – How Does It Know What to Do?\n",
        "\n",
        "The prompt gives the LLM instructions like:\n",
        "\n",
        "```plaintext\n",
        "Use this format:\n",
        "Thought: What you're thinking\n",
        "Action: {\n",
        "   \"function_name\": \"search_wikipedia\",\n",
        "   \"function_parms\": {\"query\": \"Albert Einstein\"}\n",
        "}\n",
        "PAUSE\n",
        "```\n",
        "\n",
        "Then we call the tool, get an Observation, and repeat.\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 Message History\n",
        "\n",
        "The agent uses **message history** to talk to the LLM:\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"When was Albert Einstein born?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"... LLM response ...\"},\n",
        "    {\"role\": \"user\", \"content\": \"Observation: Einstein was born in 1879\"}\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Agent Loop Summary (like in `agent.py`)\n",
        "\n",
        "1. Build messages (`system`, `user`, etc.)\n",
        "2. Ask the LLM for a response\n",
        "3. Extract JSON from the LLM’s response\n",
        "4. Run the function using the extracted info\n",
        "5. Add the result to the conversation as an “Observation”\n",
        "6. Repeat until the LLM gives a final answer\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Key Skills to Practice\n",
        "\n",
        "| Skill                        | What You’ll Learn                                                   |\n",
        "|------------------------------|---------------------------------------------------------------------|\n",
        "| Writing system prompts       | Teach the LLM how to behave like an agent                          |\n",
        "| Creating function/tool APIs  | Give your agent useful capabilities                                |\n",
        "| Parsing LLM output           | Safely extract JSON or instructions from natural language responses |\n",
        "| Looping through steps        | Manage flow of messages, actions, and observations                 |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 Let’s Build It Step-by-Step\n",
        "\n",
        "We’ll now go through **small hands-on exercises** to help you:\n",
        "1. Simulate how the LLM “thinks” using JSON-style responses\n",
        "2. Write a simple tool and connect it\n",
        "3. Build a single-step agent that calls a tool based on a user question\n",
        "\n"
      ],
      "metadata": {
        "id": "yZxlJQM-4aDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Exercise 1: Simulate Agent Thinking (No LLM Yet)\n",
        "\n",
        "### 🎯 Goal:\n",
        "Manually simulate how the LLM thinks, chooses a tool, and outputs a JSON-style action. This will teach you the **structure of agent reasoning**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Step-by-Step\n",
        "\n",
        "### 📌 Step 1: Define Available Tools\n",
        "\n",
        "```python\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Step 2: Simulate a User Question\n",
        "\n",
        "```python\n",
        "user_question = \"What's the weather like in Paris?\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Step 3: Simulate the Agent's Thought Process\n",
        "\n",
        "We’ll write it manually like the LLM would respond:\n",
        "\n",
        "```python\n",
        "thought = \"I need to find out the current weather for Paris, so I should use the weather tool.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\n",
        "        \"city\": \"Paris\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Step 4: Simulate the Observation\n",
        "\n",
        "```python\n",
        "observation = \"It's currently 18°C and sunny in Paris.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Step 5: Generate the Final Answer\n",
        "\n",
        "```python\n",
        "answer = \"The weather in Paris is currently 18°C and sunny.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Output It All Together\n",
        "\n",
        "```python\n",
        "print(\"🧠 Thought:\", thought)\n",
        "print(\"🔧 Action:\", action)\n",
        "print(\"👁️ Observation:\", observation)\n",
        "print(\"✅ Final Answer:\", answer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 Simulated AI Agent!\n",
        "\n",
        "You:\n",
        "- Identified a goal\n",
        "- Chose the right tool\n",
        "- Formed the right arguments\n",
        "- Waited for an observation\n",
        "- Used it to form an answer\n",
        "\n",
        "This is *exactly* what the LLM will do once we hook it up.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6rnj63-DSP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}\n",
        "\n",
        "user_question = \"What's the weather like in Paris?\"\n",
        "\n",
        "thought = \"I need to find out the current weather for Paris, so I should use the weather tool.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\n",
        "        \"city\": \"Paris\"\n",
        "    }\n",
        "}\n",
        "\n",
        "observation = \"It's currently 18°C and sunny in Paris.\"\n",
        "answer = \"The weather in Paris is currently 18°C and sunny.\"\n",
        "\n",
        "print(\"🧠 Thought:\", thought)\n",
        "print(\"🔧 Action:\", action)\n",
        "print(\"👁️ Observation:\", observation)\n",
        "print(\"✅ Final Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1TH53fYDn6s",
        "outputId": "1b17ae08-27ec-4cba-8f0b-ce0b41754b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Thought: I need to find out the current weather for Paris, so I should use the weather tool.\n",
            "🔧 Action: {'function_name': 'get_weather', 'function_parms': {'city': 'Paris'}}\n",
            "👁️ Observation: It's currently 18°C and sunny in Paris.\n",
            "✅ Final Answer: The weather in Paris is currently 18°C and sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###practice"
      ],
      "metadata": {
        "id": "FtWhxOnEF69L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}\n",
        "\n",
        "user_question = \"How old was Picasso when he died?\"\n",
        "\n",
        "thought = \"I need to find out how old Picasso was when he died, so I should search Wikipedia for the answer.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Picasso\"\n",
        "    }\n",
        "}\n",
        "\n",
        "observation = \"Pablo Picasso was born on 25 October 1881 and died on 8 April 1973, at the age of 91.\"\n",
        "\n",
        "answer = \"Picasso was 91 years old when he died.\"\n",
        "\n",
        "print(\"🧠 Thought:\", thought)\n",
        "print(\"🔧 Action:\", action)\n",
        "print(\"👁️ Observation:\", observation)\n",
        "print(\"✅ Final Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlcR97Y_DpNT",
        "outputId": "d6acd0cc-8cf3-4f87-cb74-5125e4cd416e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Thought: I need to find out how old Picasso was when he died, so I should search Wikipedia for the answer.\n",
            "🔧 Action: {'function_name': 'search_wikipedia', 'function_parms': {'query': 'Picasso'}}\n",
            "👁️ Observation: Pablo Picasso was born on 25 October 1881 and died on 8 April 1973, at the age of 91.\n",
            "✅ Final Answer: Picasso was 91 years old when he died.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ✅ **Exercise 2: Build Real Tools in Python**\n",
        "\n",
        "> You already defined your tools — now we’ll implement one: `search_wikipedia(query)`.\n",
        "\n",
        "This makes your agent **actually capable** of doing something useful.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Exercise 3: Connect the Tool to Simulated Agent Logic**\n",
        "\n",
        "You’ll feed in the `\"function_name\"` and `\"function_parms\"` (like you just did), then:\n",
        "- Run the tool in real Python\n",
        "- Print the result\n",
        "- Build the full loop manually\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Exercise 4: Bring in the LLM (Hugging Face)**\n",
        "\n",
        "Now that your agent can think and act:\n",
        "- We’ll load a lightweight open-source LLM (e.g., `tiiuae/falcon-rw-1b`)\n",
        "- Feed it your system prompt and user input\n",
        "- Let the LLM generate `Thought`, `Action`, and `Answer` itself\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Exercise 5: Add Observations and Loop**\n",
        "\n",
        "Once you have the LLM deciding actions, we:\n",
        "- Parse the response\n",
        "- Call tools based on what it says\n",
        "- Feed the result back into the next prompt\n",
        "- Let it loop\n",
        "\n",
        "---\n",
        "\n",
        "# 🔨 Exercise 2: Write a Real Tool\n"
      ],
      "metadata": {
        "id": "Jqcsx_UtF4RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    \"\"\"Searches Wikipedia and returns a snippet of the first result.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "print(search_wikipedia(\"Picasso\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djQaGAOwFzyG",
        "outputId": "29a49595-9765-48ec-b79b-6183a910efa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pablo Ruiz <span class=\"searchmatch\">Picasso</span> (25 October 1881 – 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###API Requests\n",
        "\n",
        "Every API you interact with will have its **own format, rules, and structure**, defined in its **API documentation**. While there are common patterns (especially with REST APIs), each one will have:\n",
        "\n",
        "### 🔑 Unique things you'll need to look up:\n",
        "| What | Why It's Important |\n",
        "|------|--------------------|\n",
        "| `base URL` | Where to send requests (e.g., `https://api.openai.com/v1`) |\n",
        "| `endpoints` | The different functions it offers (`/search`, `/users`, `/weather`) |\n",
        "| `HTTP method` | Whether to `GET`, `POST`, `PUT`, or `DELETE` data |\n",
        "| `query/body parameters` | What inputs are required (like `\"srsearch\"` for Wikipedia) |\n",
        "| `headers` | For things like authentication tokens (e.g., `\"Authorization: Bearer <token>\"`) |\n",
        "| `response format` | Usually JSON, but the structure is always unique |\n",
        "| `rate limits` | So you don’t overload the server and get blocked |\n",
        "| `auth method` | Some need API keys, some use OAuth, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 For example:\n",
        "\n",
        "### 🔍 Wikipedia API\n",
        "- Requires: `\"action\"`, `\"list\"`, `\"srsearch\"` as parameters\n",
        "- No auth needed\n",
        "\n",
        "### ☁️ OpenWeatherMap API\n",
        "- Requires: `\"q\"` (city name), `\"appid\"` (API key), etc.\n",
        "- Returns: current weather data\n",
        "\n",
        "### 🤖 OpenAI API\n",
        "- Endpoint: `/v1/chat/completions`\n",
        "- Requires: `model`, `messages`, `temperature`, etc.\n",
        "- Needs: Authorization header with API key\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 How to Research an API Before Using It\n",
        "\n",
        "1. **Find the official API docs** (search “XYZ API docs”)\n",
        "2. Look for:\n",
        "   - Authentication\n",
        "   - Endpoints and example requests\n",
        "   - Required parameters\n",
        "   - Example responses\n",
        "3. Test it using:\n",
        "   - Postman (GUI)\n",
        "   - Curl (command line)\n",
        "   - Python requests\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Good News for AI Agents\n",
        "\n",
        "Once you write a tool function that knows how to “talk” to the API:\n",
        "- Your LLM-powered agent doesn’t have to worry about the weird formatting\n",
        "- You can expose it like:\n",
        "  ```json\n",
        "  {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\"city\": \"Tokyo\"}\n",
        "  }\n",
        "  ```\n",
        "- Behind the scenes, your Python function handles all the complexity\n",
        "\n"
      ],
      "metadata": {
        "id": "y6Y4_0NMGzTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Exercise 3: Connect Agent Output to Real Function\n",
        "\n",
        "> You’ll simulate how an LLM might return an action block (as JSON), and your Python code will call the right function based on it.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 Step-by-Step Overview\n",
        "\n",
        "We’ll do four things:\n",
        "\n",
        "1. Define a real tool (`search_wikipedia`)\n",
        "2. Simulate an LLM output (with `function_name` and `function_parms`)\n",
        "3. Check that the function exists\n",
        "4. Call it with the correct parameters and print the result\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Step 1: Real Tool Function (from before)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "    \n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Step 2: Define Available Tools\n",
        "\n",
        "```python\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🎭 Step 3: Simulated Agent Output (Like LLM Would Return)\n",
        "\n",
        "```python\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Marie Curie\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Step 4: Check and Run the Tool\n",
        "\n",
        "```python\n",
        "# Extract the function name and parameters\n",
        "fn_name = action[\"function_name\"]\n",
        "fn_params = action[\"function_parms\"]\n",
        "\n",
        "# Check if the tool is available\n",
        "if fn_name not in available_tools:\n",
        "    raise ValueError(f\"Unknown tool: {fn_name}\")\n",
        "\n",
        "# Get the function reference\n",
        "tool_fn = available_tools[fn_name]\n",
        "\n",
        "# Call the function with unpacked parameters\n",
        "result = tool_fn(**fn_params)\n",
        "\n",
        "# Print the result (the \"Observation\")\n",
        "print(\"👁️ Observation:\", result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Output Example\n",
        "\n",
        "If all goes well, you should see something like:\n",
        "\n",
        "```\n",
        "👁️ Observation: Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 You Just Simulated the Agent Control Loop!\n",
        "\n",
        "You now have:\n",
        "- A real tool that works\n",
        "- A structured action block\n",
        "- A way to route actions to the right function\n",
        "- A way to handle the result and pass it to the agent\n",
        "\n"
      ],
      "metadata": {
        "id": "nm0eApnGHT2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Marie Curie\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Extract the function name and parameters\n",
        "fn_name = action[\"function_name\"]\n",
        "fn_params = action[\"function_parms\"]\n",
        "\n",
        "# Check if the tool is available\n",
        "if fn_name not in available_tools:\n",
        "    raise ValueError(f\"Unknown tool: {fn_name}\")\n",
        "\n",
        "# Get the function reference\n",
        "tool_fn = available_tools[fn_name]\n",
        "\n",
        "# Call the function with unpacked parameters\n",
        "result = tool_fn(**fn_params)\n",
        "\n",
        "# Print the result (the \"Observation\")\n",
        "print(\"👁️ Observation:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEeChBawG4Ef",
        "outputId": "0bfb7e7b-1770-4dff-c958-6f9ec0f8facb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👁️ Observation: Skłodowska-<span class=\"searchmatch\">Curie</span> (Polish: [ˈmarja salɔˈmɛa skwɔˈdɔfska kʲiˈri] ; née Skłodowska; 7 November 1867 – 4 July 1934), known simply as <span class=\"searchmatch\">Marie</span> <span class=\"searchmatch\">Curie</span> (/ˈkjʊəri/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the tool\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "# Define available tools\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "# Loop to try different queries\n",
        "while True:\n",
        "    user_query = input(\"\\n🧠 Enter a Wikipedia search topic (or 'exit' to quit): \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        print(\"👋 Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Simulate agent output\n",
        "    action = {\n",
        "        \"function_name\": \"search_wikipedia\",\n",
        "        \"function_parms\": {\n",
        "            \"query\": user_query\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Extract and run\n",
        "    fn_name = action[\"function_name\"]\n",
        "    fn_params = action[\"function_parms\"]\n",
        "\n",
        "    if fn_name not in available_tools:\n",
        "        print(f\"🚫 Unknown tool: {fn_name}\")\n",
        "        continue\n",
        "\n",
        "    tool_fn = available_tools[fn_name]\n",
        "    result = tool_fn(**fn_params)\n",
        "\n",
        "    print(\"👁️ Observation:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRbQUs8NIACP",
        "outputId": "3ea0b574-2b55-4581-d5e9-9415b9f08a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Enter a Wikipedia search topic (or 'exit' to quit): Picasso\n",
            "👁️ Observation: Pablo Ruiz <span class=\"searchmatch\">Picasso</span> (25 October 1881 – 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his\n",
            "\n",
            "🧠 Enter a Wikipedia search topic (or 'exit' to quit): Tasmania\n",
            "👁️ Observation: <span class=\"searchmatch\">Tasmania</span> (/tæzˈmeɪniə/; palawa kani: Lutruwita) is an island state of Australia. It is located 240 kilometres (150 miles) to the south of the Australian\n",
            "\n",
            "🧠 Enter a Wikipedia search topic (or 'exit' to quit): exit\n",
            "👋 Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 🤯 Big Idea: Tools Extend the Power of LLMs\n",
        "\n",
        "Language models like GPT-4, Mistral, or Falcon are:\n",
        "- **Very smart**\n",
        "- **Very good at reasoning, writing, and understanding**\n",
        "- But they are also:\n",
        "  - ❌ Blind to the real world\n",
        "  - ❌ Unable to access live data (like websites, databases, files)\n",
        "  - ❌ Not great at math or APIs by themselves\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ Tools Fix That\n",
        "\n",
        "By giving an LLM access to external tools like:\n",
        "\n",
        "- 🌐 Web search\n",
        "- 📄 File readers (PDF, CSV)\n",
        "- 📊 Data analysis\n",
        "- 📆 Calendars\n",
        "- 📡 APIs (weather, finance, YouTube, etc.)\n",
        "\n",
        "...you’re turning it from a **passive text generator** into an **active, goal-oriented agent** that can take real-world actions.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 LLM + Tools = AI Agent\n",
        "\n",
        "| Part            | Example                                          |\n",
        "|------------------|--------------------------------------------------|\n",
        "| LLM             | \"To answer this, I need to check Wikipedia...\"   |\n",
        "| Tool Call       | `search_wikipedia(\"Marie Curie\")`               |\n",
        "| Observation     | \"She was born in 1867 and died in 1934...\"       |\n",
        "| Final Answer    | \"Marie Curie was 66 when she died.\"              |\n",
        "\n",
        "You’re not replacing the LLM — you’re **enhancing it** with a toolkit, like giving a smart person access to the internet and a calculator.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Why This Is So Important\n",
        "\n",
        "Agents are the **bridge between LLM intelligence and real-world usefulness**.\n",
        "\n",
        "This is how:\n",
        "- 🧑‍💼 Virtual assistants book meetings\n",
        "- 🔍 Research bots look up info\n",
        "- 📊 Analyst agents run data analysis\n",
        "- 🧾 Legal/medical/chatbots reference documents\n",
        "\n"
      ],
      "metadata": {
        "id": "yLqfSu_5ItCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ✅ Exercise: Use a Hugging Face LLM to Think Like an Agent\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Goal\n",
        "\n",
        "We want the LLM to:\n",
        "1. Receive a user question\n",
        "2. Think through the problem\n",
        "3. Output a structured action block like:\n",
        "```json\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Marie Curie\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This is the moment your **LLM starts acting like an agent**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ What We’ll Use\n",
        "\n",
        "- Model: `tiiuae/falcon-rw-1b` (small, no restrictions, good for testing)\n",
        "- Library: `transformers`\n",
        "- Interface: `pipeline(\"text-generation\")`\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Step-by-Step Setup\n",
        "\n",
        "### 1. Install Dependencies (if needed)\n",
        "\n",
        "```bash\n",
        "pip install transformers huggingface_hub\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Load the Model from Hugging Face\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"tiiuae/falcon-rw-1b\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Define the System Prompt\n",
        "\n",
        "We’ll guide the LLM to act like an agent by showing it an **example**:\n",
        "\n",
        "```python\n",
        "system_prompt = \"\"\"\n",
        "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
        "\n",
        "Use this format exactly:\n",
        "{\n",
        "  \"function_name\": \"...\",\n",
        "  \"function_parms\": {\n",
        "    ...\n",
        "  }\n",
        "}\n",
        "\n",
        "Available tools:\n",
        "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
        "\n",
        "Examples:\n",
        "\n",
        "User: When was Albert Einstein born?\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Albert Einstein\"\n",
        "  }\n",
        "}\n",
        "\n",
        "User: {}\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "We’ll insert the user’s question into `{}`.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Ask the LLM a Question\n",
        "\n",
        "```python\n",
        "user_question = \"How old was Marie Curie when she died?\"\n",
        "\n",
        "# Insert the question into the prompt\n",
        "full_prompt = system_prompt.format(user_question)\n",
        "\n",
        "# Generate a response\n",
        "output = generator(full_prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']\n",
        "\n",
        "# Print it\n",
        "print(output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What to Look For\n",
        "\n",
        "You're hoping the LLM returns something like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Marie Curie\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "You can then:\n",
        "- Extract that JSON\n",
        "- Run the corresponding function (from earlier)\n",
        "- Return the observation\n",
        "- Send it back to the model to continue\n",
        "\n"
      ],
      "metadata": {
        "id": "uccgHXd6JKkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"tiiuae/falcon-rw-1b\" # continues to run after completion of task\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "C861bVpAfoHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let’s switch to a **lighter, faster model** that still works well for prototyping agents.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Recommended Small Models (Open Source, Hugging Face)\n",
        "\n",
        "Here are some great lightweight options:\n",
        "\n",
        "| Model ID                          | Size  | Notes |\n",
        "|-----------------------------------|-------|-------|\n",
        "| `tiiuae/falcon-rw-1b`             | 1B    | Very small and fast, decent reasoning |\n",
        "| `microsoft/DialoGPT-medium`       | 345M  | Chat-focused, very lightweight        |\n",
        "| `google/flan-t5-base`             | 250M | Instruction-tuned, good reasoning    |\n",
        "| `facebook/blenderbot-3B`          | 3B    | Trained for dialogue                 |\n",
        "\n",
        "All the models we’ve talked about (like `falcon-rw-1b`, `flan-t5-base`, `DialoGPT`, `blenderbot`, etc.) are **pretrained LLMs**. Let’s break that down a bit so you understand exactly what you’re working with:\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What Is a Pretrained LLM?\n",
        "\n",
        "A **pretrained Large Language Model (LLM)** is a model that has already been:\n",
        "1. **Trained on a large dataset** (e.g., books, websites, conversations)\n",
        "2. **Taught the structure of language** (grammar, reasoning, logic)\n",
        "3. **Saved and shared** so you can use it out of the box\n",
        "\n",
        "You don’t need to retrain or fine-tune it to start using it.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏗️ Common Types of Pretrained Models\n",
        "\n",
        "### 1. **Base LLMs**\n",
        "- Just predict text or fill in blanks\n",
        "- No special tuning for tasks or instruction-following\n",
        "\n",
        "🔹 Example: `tiiuae/falcon-rw-1b`\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Instruction-Tuned LLMs**\n",
        "- Trained to follow commands like \"Summarize this\" or \"Answer this question\"\n",
        "- Usually work better for agents and tools\n",
        "\n",
        "🔹 Example: `google/flan-t5-base`, `HuggingFaceH4/zephyr-7b-beta`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Chat-Tuned Models**\n",
        "- Specifically trained for back-and-forth dialogue\n",
        "- Often trained on user/assistant style prompts\n",
        "\n",
        "🔹 Example: `microsoft/DialoGPT-medium`, `facebook/blenderbot-3B`\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 Pretrained Models Are Perfect for Agents\n",
        "\n",
        "Why?\n",
        "- They already \"know things\"\n",
        "- They can reason and make decisions\n",
        "- You can guide them with a well-written prompt\n",
        "\n",
        "And by adding **tools**, you give them superpowers — the ability to:\n",
        "- Get live data\n",
        "- Read your documents\n",
        "- Query APIs\n",
        "- Control apps\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Later, You Can Fine-Tune\n",
        "\n",
        "Once you're comfortable with agents and want them to behave in a more custom way, you can:\n",
        "- Fine-tune a model on your own business data\n",
        "- Or use techniques like RAG (Retrieval-Augmented Generation) for domain knowledge\n",
        "\n",
        "But for now — **pretrained models are exactly what you want** for building AI agents and learning the ropes.\n",
        "\n"
      ],
      "metadata": {
        "id": "wrRiC4WCSscv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "#model_id = \"tiiuae/falcon-rw-1b\" # continues to run after completion of task\n",
        "model_id = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Imw9KwYFfwHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt template with JSON action format\n",
        "system_prompt_template = \"\"\"\n",
        "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
        "\n",
        "Use this format exactly:\n",
        "{{\n",
        "  \"function_name\": \"...\",\n",
        "  \"function_parms\": {{\n",
        "    ...\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Available tools:\n",
        "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
        "\n",
        "Examples:\n",
        "\n",
        "User: When was Albert Einstein born?\n",
        "{{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {{\n",
        "    \"query\": \"Albert Einstein\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "User: {}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Choose a test question\n",
        "user_question = \"How old was Marie Curie when she died?\"\n",
        "\n",
        "# Format the full prompt\n",
        "full_prompt = system_prompt_template.format(user_question)\n",
        "\n",
        "# ✅ Print the prompt sent to the LLM\n",
        "print(\"📤 Full Prompt Sent to LLM:\\n\")\n",
        "print(full_prompt)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Generate model output\n",
        "output = generator(\n",
        "    full_prompt,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.3,  # Less randomness\n",
        "    do_sample=False,  # Deterministic generation\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")[0]['generated_text']\n",
        "\n",
        "\n",
        "# ✅ Print the output from the model\n",
        "print(\"🤖 Model Output:\\n\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8KxqyqGH_85",
        "outputId": "a4e2890f-10d1-4adb-d29e-57ea42ddf3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📤 Full Prompt Sent to LLM:\n",
            "\n",
            "\n",
            "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
            "\n",
            "Use this format exactly:\n",
            "{\n",
            "  \"function_name\": \"...\",\n",
            "  \"function_parms\": {\n",
            "    ...\n",
            "  }\n",
            "}\n",
            "\n",
            "Available tools:\n",
            "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
            "\n",
            "Examples:\n",
            "\n",
            "User: When was Albert Einstein born?\n",
            "{\n",
            "  \"function_name\": \"search_wikipedia\",\n",
            "  \"function_parms\": {\n",
            "    \"query\": \"Albert Einstein\"\n",
            "  }\n",
            "}\n",
            "\n",
            "User: How old was Marie Curie when she died?\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "🤖 Model Output:\n",
            "\n",
            "\n",
            "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
            "\n",
            "Use this format exactly:\n",
            "{\n",
            "  \"function_name\": \"...\",\n",
            "  \"function_parms\": {\n",
            "    ...\n",
            "  }\n",
            "}\n",
            "\n",
            "Available tools:\n",
            "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
            "\n",
            "Examples:\n",
            "\n",
            "User: When was Albert Einstein born?\n",
            "{\n",
            "  \"function_name\": \"search_wikipedia\",\n",
            "  \"function_parms\": {\n",
            "    \"query\": \"Albert Einstein\"\n",
            "  }\n",
            "}\n",
            "\n",
            "User: How old was Marie Curie when she died?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! You got it running 🎉 — and this is an **important milestone**. You’ve:\n",
        "\n",
        "- Loaded a real Hugging Face model (`DialoGPT`)\n",
        "- Sent it a properly structured system prompt\n",
        "- Got a response back\n",
        "\n",
        "Now you're ready for the **next critical step** in building an agent:\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Next Step: Parse the Model Output to Extract the Action\n",
        "\n",
        "Right now, the model is **just echoing the prompt** — which means it's not reasoning or generating its own action block yet. That’s totally normal for `DialoGPT`, because it's **chat-optimized** but not **instruction-tuned**.\n",
        "\n",
        "We’ll move forward by:\n",
        "1. Testing a model that better follows instructions (like `flan-t5-base`)\n",
        "2. Extracting a real JSON-style action from the model\n",
        "3. Running the tool based on that action\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Step 1: Switch to a Model That Can Generate Actions\n",
        "\n",
        "Let's use:\n",
        "```python\n",
        "model_id = \"google/flan-t5-base\"\n",
        "```\n",
        "\n",
        "Because:\n",
        "- It's **instruction-tuned** — perfect for this kind of task\n",
        "- It uses the `\"text2text-generation\"` pipeline, which expects prompts like:\n",
        "  ```\n",
        "  Question: ...\n",
        "  Answer: ...\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 Updated Code for `flan-t5-base`\n",
        "\n",
        "### 🔧 Load the model\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Simplified Prompt (no JSON escaping needed here)\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Generate the action\n",
        "\n",
        "```python\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 What’s Next?\n",
        "\n",
        "If this gives you a real function name and parameters, we can:\n",
        "1. Extract that output using regex or simple parsing\n",
        "2. Look it up in your `available_tools`\n",
        "3. Run the tool and display the final answer (like an actual agent!)\n",
        "\n"
      ],
      "metadata": {
        "id": "O4F8tF81Uk1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "D9z9oLWuf174"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"🤖 Model Output:\\n\", output)\n"
      ],
      "metadata": {
        "id": "9lkOfa8rLmWz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}