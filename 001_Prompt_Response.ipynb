{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+Fl8ok9+KdjfOchyYG2bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/001_Prompt_Response.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###install Libraries"
      ],
      "metadata": {
        "id": "xJgyH8iRJS0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy8U3VH04Sxw",
        "outputId": "855454db-bd1d-4686-a02d-fcdbaaf22fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers huggingface_hub\n",
        "!pip install python-dotenv\n",
        "# !pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# üß† AI Agents 101 ‚Äì Foundational Concepts & Architecture\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ What is an AI Agent?\n",
        "\n",
        "An **AI agent** is a system that:\n",
        "1. **Receives a user input (goal or question)**\n",
        "2. **Thinks about what to do** (using a language model)\n",
        "3. **Takes an action** using tools like APIs, web access, or databases\n",
        "4. **Observes the result**\n",
        "5. **Uses that observation to produce an answer or plan the next step**\n",
        "\n",
        "This loop is often described as:\n",
        "\n",
        "```\n",
        "Thought ‚Üí Action ‚Üí PAUSE ‚Üí Observation ‚Üí Answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Core Components of a Simple AI Agent\n",
        "\n",
        "| Component           | Purpose                                                                 |\n",
        "|---------------------|-------------------------------------------------------------------------|\n",
        "| üß† Language Model   | Thinks and decides what to do (e.g. GPT-4 or open-source LLMs)          |\n",
        "| üìú System Prompt     | Tells the LLM how to behave (e.g. think in steps, use tools)           |\n",
        "| üß∞ Tools / Functions | The agent's hands ‚Äî lets it do useful things like web search, scraping |\n",
        "| üß© JSON Parsing      | Helps extract structured actions from the LLM‚Äôs response                |\n",
        "| üîÅ Control Loop      | Coordinates each step: calls LLM, parses response, runs tools, repeats  |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Big Picture Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "A[User Input] --> B[System Prompt + User Message]\n",
        "B --> C[Language Model]\n",
        "C --> D[Agent Response with JSON Tool Call]\n",
        "D --> E[Parse JSON to Identify Tool + Args]\n",
        "E --> F[Call Tool Function]\n",
        "F --> G[Return Result to Agent as Observation]\n",
        "G --> H[Agent Uses Observation to Answer]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Tools ‚Äì What Can the Agent Do?\n",
        "\n",
        "The agent doesn‚Äôt know everything ‚Äî so we give it **tools**:\n",
        "- `search_wikipedia(query)` ‚Üí Calls Wikipedia API\n",
        "- `load_content_from_url(url)` ‚Üí Fetches content from a page\n",
        "- `seo_audit_web_page(url)` ‚Üí Sends site to an SEO tool\n",
        "\n",
        "These tools are just **Python functions** that return some output.\n",
        "\n",
        "---\n",
        "\n",
        "## üìú Prompt ‚Äì How Does It Know What to Do?\n",
        "\n",
        "The prompt gives the LLM instructions like:\n",
        "\n",
        "```plaintext\n",
        "Use this format:\n",
        "Thought: What you're thinking\n",
        "Action: {\n",
        "   \"function_name\": \"search_wikipedia\",\n",
        "   \"function_parms\": {\"query\": \"Albert Einstein\"}\n",
        "}\n",
        "PAUSE\n",
        "```\n",
        "\n",
        "Then we call the tool, get an Observation, and repeat.\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ Message History\n",
        "\n",
        "The agent uses **message history** to talk to the LLM:\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"When was Albert Einstein born?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"... LLM response ...\"},\n",
        "    {\"role\": \"user\", \"content\": \"Observation: Einstein was born in 1879\"}\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Agent Loop Summary (like in `agent.py`)\n",
        "\n",
        "1. Build messages (`system`, `user`, etc.)\n",
        "2. Ask the LLM for a response\n",
        "3. Extract JSON from the LLM‚Äôs response\n",
        "4. Run the function using the extracted info\n",
        "5. Add the result to the conversation as an ‚ÄúObservation‚Äù\n",
        "6. Repeat until the LLM gives a final answer\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Skills to Practice\n",
        "\n",
        "| Skill                        | What You‚Äôll Learn                                                   |\n",
        "|------------------------------|---------------------------------------------------------------------|\n",
        "| Writing system prompts       | Teach the LLM how to behave like an agent                          |\n",
        "| Creating function/tool APIs  | Give your agent useful capabilities                                |\n",
        "| Parsing LLM output           | Safely extract JSON or instructions from natural language responses |\n",
        "| Looping through steps        | Manage flow of messages, actions, and observations                 |\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Let‚Äôs Build It Step-by-Step\n",
        "\n",
        "We‚Äôll now go through **small hands-on exercises** to help you:\n",
        "1. Simulate how the LLM ‚Äúthinks‚Äù using JSON-style responses\n",
        "2. Write a simple tool and connect it\n",
        "3. Build a single-step agent that calls a tool based on a user question\n",
        "\n",
        "---\n",
        "\n",
        "If you‚Äôre ready, we can start with:\n",
        "\n",
        "### ‚úÖ Exercise 1: Simulating Agent Thinking (No LLM yet)\n",
        "\n",
        "We‚Äôll just write mock LLM responses to learn the structure.\n",
        "\n",
        "Would you like to begin with this exercise?"
      ],
      "metadata": {
        "id": "yZxlJQM-4aDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n",
        "# Explicitly load your .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "\n",
        "# Now it can find the variable\n",
        "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "\n",
        "if not token:\n",
        "    raise ValueError(\"üö® Hugging Face token not found. Is your .env file set correctly?\")"
      ],
      "metadata": {
        "id": "T0l2tbvQ4ZPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Exercise 1: Simulate Agent Thinking (No LLM Yet)\n",
        "\n",
        "### üéØ Goal:\n",
        "Manually simulate how the LLM thinks, chooses a tool, and outputs a JSON-style action. This will teach you the **structure of agent reasoning**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Step-by-Step\n",
        "\n",
        "### üìå Step 1: Define Available Tools\n",
        "\n",
        "```python\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Step 2: Simulate a User Question\n",
        "\n",
        "```python\n",
        "user_question = \"What's the weather like in Paris?\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Step 3: Simulate the Agent's Thought Process\n",
        "\n",
        "We‚Äôll write it manually like the LLM would respond:\n",
        "\n",
        "```python\n",
        "thought = \"I need to find out the current weather for Paris, so I should use the weather tool.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\n",
        "        \"city\": \"Paris\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Step 4: Simulate the Observation\n",
        "\n",
        "```python\n",
        "observation = \"It's currently 18¬∞C and sunny in Paris.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Step 5: Generate the Final Answer\n",
        "\n",
        "```python\n",
        "answer = \"The weather in Paris is currently 18¬∞C and sunny.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Output It All Together\n",
        "\n",
        "```python\n",
        "print(\"üß† Thought:\", thought)\n",
        "print(\"üîß Action:\", action)\n",
        "print(\"üëÅÔ∏è Observation:\", observation)\n",
        "print(\"‚úÖ Final Answer:\", answer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ You Just Simulated an AI Agent!\n",
        "\n",
        "You:\n",
        "- Identified a goal\n",
        "- Chose the right tool\n",
        "- Formed the right arguments\n",
        "- Waited for an observation\n",
        "- Used it to form an answer\n",
        "\n",
        "This is *exactly* what the LLM will do once we hook it up.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Your Turn!\n",
        "\n",
        "Try changing the `user_question` to one of these:\n",
        "- \"When was Marie Curie born?\"\n",
        "- \"Summarize this text: 'Machine learning is a field of AI focused on pattern recognition‚Ä¶'\"\n",
        "- \"What's the weather like in Tokyo?\"\n",
        "\n",
        "Then manually simulate the agent‚Äôs `thought`, `action`, `observation`, and `answer`.\n",
        "\n",
        "---\n",
        "\n",
        "When you‚Äôre ready, we‚Äôll move on to **Exercise 2**, where we‚Äôll write a real tool in Python and connect it to a very simple agent loop.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6rnj63-DSP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}"
      ],
      "metadata": {
        "id": "5Q0tgn8SDV8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What's the weather like in Paris?\""
      ],
      "metadata": {
        "id": "NGSPQEi3Da4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thought = \"I need to find out the current weather for Paris, so I should use the weather tool.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\n",
        "        \"city\": \"Paris\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "15jl-KjIDcKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"It's currently 18¬∞C and sunny in Paris.\""
      ],
      "metadata": {
        "id": "SMCpDL9NDegP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = \"The weather in Paris is currently 18¬∞C and sunny.\""
      ],
      "metadata": {
        "id": "rdBbrLaEDmWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß† Thought:\", thought)\n",
        "print(\"üîß Action:\", action)\n",
        "print(\"üëÅÔ∏è Observation:\", observation)\n",
        "print(\"‚úÖ Final Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1TH53fYDn6s",
        "outputId": "1b17ae08-27ec-4cba-8f0b-ce0b41754b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Thought: I need to find out the current weather for Paris, so I should use the weather tool.\n",
            "üîß Action: {'function_name': 'get_weather', 'function_parms': {'city': 'Paris'}}\n",
            "üëÅÔ∏è Observation: It's currently 18¬∞C and sunny in Paris.\n",
            "‚úÖ Final Answer: The weather in Paris is currently 18¬∞C and sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###practice"
      ],
      "metadata": {
        "id": "FtWhxOnEF69L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_tools = {\n",
        "    \"search_wikipedia\": \"Searches Wikipedia for a topic and returns a summary.\",\n",
        "    \"get_weather\": \"Returns the current weather for a given city.\",\n",
        "    \"summarize_text\": \"Summarizes the provided text.\"\n",
        "}\n",
        "\n",
        "user_question = \"How old was Picasso when he died?\"\n",
        "\n",
        "thought = \"I need to find out how old Picasso was when he died, so I should search Wikipedia for the answer.\"\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Picasso\"\n",
        "    }\n",
        "}\n",
        "\n",
        "observation = \"Pablo Picasso was born on 25 October 1881 and died on 8 April 1973, at the age of 91.\"\n",
        "\n",
        "answer = \"Picasso was 91 years old when he died.\"\n",
        "\n",
        "print(\"üß† Thought:\", thought)\n",
        "print(\"üîß Action:\", action)\n",
        "print(\"üëÅÔ∏è Observation:\", observation)\n",
        "print(\"‚úÖ Final Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlcR97Y_DpNT",
        "outputId": "d6acd0cc-8cf3-4f87-cb74-5125e4cd416e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Thought: I need to find out how old Picasso was when he died, so I should search Wikipedia for the answer.\n",
            "üîß Action: {'function_name': 'search_wikipedia', 'function_parms': {'query': 'Picasso'}}\n",
            "üëÅÔ∏è Observation: Pablo Picasso was born on 25 October 1881 and died on 8 April 1973, at the age of 91.\n",
            "‚úÖ Final Answer: Picasso was 91 years old when he died.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úÖ **Exercise 2: Build Real Tools in Python**\n",
        "\n",
        "> You already defined your tools ‚Äî now we‚Äôll implement one: `search_wikipedia(query)`.\n",
        "\n",
        "This makes your agent **actually capable** of doing something useful.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Exercise 3: Connect the Tool to Simulated Agent Logic**\n",
        "\n",
        "You‚Äôll feed in the `\"function_name\"` and `\"function_parms\"` (like you just did), then:\n",
        "- Run the tool in real Python\n",
        "- Print the result\n",
        "- Build the full loop manually\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Exercise 4: Bring in the LLM (Hugging Face)**\n",
        "\n",
        "Now that your agent can think and act:\n",
        "- We‚Äôll load a lightweight open-source LLM (e.g., `tiiuae/falcon-rw-1b`)\n",
        "- Feed it your system prompt and user input\n",
        "- Let the LLM generate `Thought`, `Action`, and `Answer` itself\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Exercise 5: Add Observations and Loop**\n",
        "\n",
        "Once you have the LLM deciding actions, we:\n",
        "- Parse the response\n",
        "- Call tools based on what it says\n",
        "- Feed the result back into the next prompt\n",
        "- Let it loop just like in your course's `agent.py`\n",
        "\n",
        "---\n",
        "\n",
        "# üî® Let‚Äôs Start Exercise 2: Write a Real Tool\n"
      ],
      "metadata": {
        "id": "Jqcsx_UtF4RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    \"\"\"Searches Wikipedia and returns a snippet of the first result.\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "print(search_wikipedia(\"Picasso\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djQaGAOwFzyG",
        "outputId": "29a49595-9765-48ec-b79b-6183a910efa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pablo Ruiz <span class=\"searchmatch\">Picasso</span> (25 October 1881 ‚Äì 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###API Requests\n",
        "\n",
        "Every API you interact with will have its **own format, rules, and structure**, defined in its **API documentation**. While there are common patterns (especially with REST APIs), each one will have:\n",
        "\n",
        "### üîë Unique things you'll need to look up:\n",
        "| What | Why It's Important |\n",
        "|------|--------------------|\n",
        "| `base URL` | Where to send requests (e.g., `https://api.openai.com/v1`) |\n",
        "| `endpoints` | The different functions it offers (`/search`, `/users`, `/weather`) |\n",
        "| `HTTP method` | Whether to `GET`, `POST`, `PUT`, or `DELETE` data |\n",
        "| `query/body parameters` | What inputs are required (like `\"srsearch\"` for Wikipedia) |\n",
        "| `headers` | For things like authentication tokens (e.g., `\"Authorization: Bearer <token>\"`) |\n",
        "| `response format` | Usually JSON, but the structure is always unique |\n",
        "| `rate limits` | So you don‚Äôt overload the server and get blocked |\n",
        "| `auth method` | Some need API keys, some use OAuth, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ For example:\n",
        "\n",
        "### üîç Wikipedia API\n",
        "- Requires: `\"action\"`, `\"list\"`, `\"srsearch\"` as parameters\n",
        "- No auth needed\n",
        "\n",
        "### ‚òÅÔ∏è OpenWeatherMap API\n",
        "- Requires: `\"q\"` (city name), `\"appid\"` (API key), etc.\n",
        "- Returns: current weather data\n",
        "\n",
        "### ü§ñ OpenAI API\n",
        "- Endpoint: `/v1/chat/completions`\n",
        "- Requires: `model`, `messages`, `temperature`, etc.\n",
        "- Needs: Authorization header with API key\n",
        "\n",
        "---\n",
        "\n",
        "## üîç How to Research an API Before Using It\n",
        "\n",
        "1. **Find the official API docs** (search ‚ÄúXYZ API docs‚Äù)\n",
        "2. Look for:\n",
        "   - Authentication\n",
        "   - Endpoints and example requests\n",
        "   - Required parameters\n",
        "   - Example responses\n",
        "3. Test it using:\n",
        "   - Postman (GUI)\n",
        "   - Curl (command line)\n",
        "   - Python requests\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Good News for AI Agents\n",
        "\n",
        "Once you write a tool function that knows how to ‚Äútalk‚Äù to the API:\n",
        "- Your LLM-powered agent doesn‚Äôt have to worry about the weird formatting\n",
        "- You can expose it like:\n",
        "  ```json\n",
        "  {\n",
        "    \"function_name\": \"get_weather\",\n",
        "    \"function_parms\": {\"city\": \"Tokyo\"}\n",
        "  }\n",
        "  ```\n",
        "- Behind the scenes, your Python function handles all the complexity\n",
        "\n",
        "---\n",
        "\n",
        "You‚Äôre really starting to think like a **tool builder for agents** now ‚Äî which is exactly how developers are building real-world AI assistants today. Ready for **Exercise 3**, where we hook up this function and simulate calling it with a structured action block?"
      ],
      "metadata": {
        "id": "y6Y4_0NMGzTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Exercise 3: Connect Agent Output to Real Function\n",
        "\n",
        "> You‚Äôll simulate how an LLM might return an action block (as JSON), and your Python code will call the right function based on it.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Step-by-Step Overview\n",
        "\n",
        "We‚Äôll do four things:\n",
        "\n",
        "1. Define a real tool (`search_wikipedia`)\n",
        "2. Simulate an LLM output (with `function_name` and `function_parms`)\n",
        "3. Check that the function exists\n",
        "4. Call it with the correct parameters and print the result\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Step 1: Real Tool Function (from before)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "    \n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Step 2: Define Available Tools\n",
        "\n",
        "```python\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üé≠ Step 3: Simulated Agent Output (Like LLM Would Return)\n",
        "\n",
        "```python\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Marie Curie\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Step 4: Check and Run the Tool\n",
        "\n",
        "```python\n",
        "# Extract the function name and parameters\n",
        "fn_name = action[\"function_name\"]\n",
        "fn_params = action[\"function_parms\"]\n",
        "\n",
        "# Check if the tool is available\n",
        "if fn_name not in available_tools:\n",
        "    raise ValueError(f\"Unknown tool: {fn_name}\")\n",
        "\n",
        "# Get the function reference\n",
        "tool_fn = available_tools[fn_name]\n",
        "\n",
        "# Call the function with unpacked parameters\n",
        "result = tool_fn(**fn_params)\n",
        "\n",
        "# Print the result (the \"Observation\")\n",
        "print(\"üëÅÔ∏è Observation:\", result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Output Example\n",
        "\n",
        "If all goes well, you should see something like:\n",
        "\n",
        "```\n",
        "üëÅÔ∏è Observation: Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° You Just Simulated the Agent Control Loop!\n",
        "\n",
        "You now have:\n",
        "- A real tool that works\n",
        "- A structured action block\n",
        "- A way to route actions to the right function\n",
        "- A way to handle the result and pass it to the agent\n",
        "\n",
        "---\n",
        "\n",
        "Ready to move on to **Exercise 4**, where we bring in an **actual LLM from Hugging Face** to generate these action blocks automatically?"
      ],
      "metadata": {
        "id": "nm0eApnGHT2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "action = {\n",
        "    \"function_name\": \"search_wikipedia\",\n",
        "    \"function_parms\": {\n",
        "        \"query\": \"Marie Curie\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Extract the function name and parameters\n",
        "fn_name = action[\"function_name\"]\n",
        "fn_params = action[\"function_parms\"]\n",
        "\n",
        "# Check if the tool is available\n",
        "if fn_name not in available_tools:\n",
        "    raise ValueError(f\"Unknown tool: {fn_name}\")\n",
        "\n",
        "# Get the function reference\n",
        "tool_fn = available_tools[fn_name]\n",
        "\n",
        "# Call the function with unpacked parameters\n",
        "result = tool_fn(**fn_params)\n",
        "\n",
        "# Print the result (the \"Observation\")\n",
        "print(\"üëÅÔ∏è Observation:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEeChBawG4Ef",
        "outputId": "0bfb7e7b-1770-4dff-c958-6f9ec0f8facb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üëÅÔ∏è Observation: Sk≈Çodowska-<span class=\"searchmatch\">Curie</span> (Polish: [Ààmarja sal…îÀàm…õa skw…îÀàd…îfska k ≤iÀàri] ; n√©e¬†Sk≈Çodowska; 7 November 1867 ‚Äì 4 July 1934), known simply as <span class=\"searchmatch\">Marie</span> <span class=\"searchmatch\">Curie</span> (/Ààkj ä…ôri/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the tool\n",
        "def search_wikipedia(query):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json()[\"query\"][\"search\"]\n",
        "\n",
        "    if results:\n",
        "        return results[0][\"snippet\"]\n",
        "    else:\n",
        "        return \"No results found.\"\n",
        "\n",
        "# Define available tools\n",
        "available_tools = {\n",
        "    \"search_wikipedia\": search_wikipedia\n",
        "}\n",
        "\n",
        "# Loop to try different queries\n",
        "while True:\n",
        "    user_query = input(\"\\nüß† Enter a Wikipedia search topic (or 'exit' to quit): \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        print(\"üëã Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Simulate agent output\n",
        "    action = {\n",
        "        \"function_name\": \"search_wikipedia\",\n",
        "        \"function_parms\": {\n",
        "            \"query\": user_query\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Extract and run\n",
        "    fn_name = action[\"function_name\"]\n",
        "    fn_params = action[\"function_parms\"]\n",
        "\n",
        "    if fn_name not in available_tools:\n",
        "        print(f\"üö´ Unknown tool: {fn_name}\")\n",
        "        continue\n",
        "\n",
        "    tool_fn = available_tools[fn_name]\n",
        "    result = tool_fn(**fn_params)\n",
        "\n",
        "    print(\"üëÅÔ∏è Observation:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRbQUs8NIACP",
        "outputId": "3ea0b574-2b55-4581-d5e9-9415b9f08a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Enter a Wikipedia search topic (or 'exit' to quit): Picasso\n",
            "üëÅÔ∏è Observation: Pablo Ruiz <span class=\"searchmatch\">Picasso</span> (25 October 1881 ‚Äì 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his\n",
            "\n",
            "üß† Enter a Wikipedia search topic (or 'exit' to quit): Tasmania\n",
            "üëÅÔ∏è Observation: <span class=\"searchmatch\">Tasmania</span> (/t√¶zÀàme…™ni…ô/; palawa kani: Lutruwita) is an island state of Australia. It is located 240 kilometres (150 miles) to the south of the Australian\n",
            "\n",
            "üß† Enter a Wikipedia search topic (or 'exit' to quit): exit\n",
            "üëã Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YES! You got it ‚Äî and that insight is at the heart of what makes AI agents powerful.** üôå\n",
        "\n",
        "---\n",
        "\n",
        "### ü§Ø Big Idea: Tools Extend the Power of LLMs\n",
        "\n",
        "Language models like GPT-4, Mistral, or Falcon are:\n",
        "- **Very smart**\n",
        "- **Very good at reasoning, writing, and understanding**\n",
        "- But they are also:\n",
        "  - ‚ùå Blind to the real world\n",
        "  - ‚ùå Unable to access live data (like websites, databases, files)\n",
        "  - ‚ùå Not great at math or APIs by themselves\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Tools Fix That\n",
        "\n",
        "By giving an LLM access to external tools like:\n",
        "\n",
        "- üåê Web search\n",
        "- üìÑ File readers (PDF, CSV)\n",
        "- üìä Data analysis\n",
        "- üìÜ Calendars\n",
        "- üì° APIs (weather, finance, YouTube, etc.)\n",
        "\n",
        "...you‚Äôre turning it from a **passive text generator** into an **active, goal-oriented agent** that can take real-world actions.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† LLM + Tools = AI Agent\n",
        "\n",
        "| Part            | Example                                          |\n",
        "|------------------|--------------------------------------------------|\n",
        "| LLM             | \"To answer this, I need to check Wikipedia...\"   |\n",
        "| Tool Call       | `search_wikipedia(\"Marie Curie\")`               |\n",
        "| Observation     | \"She was born in 1867 and died in 1934...\"       |\n",
        "| Final Answer    | \"Marie Curie was 66 when she died.\"              |\n",
        "\n",
        "You‚Äôre not replacing the LLM ‚Äî you‚Äôre **enhancing it** with a toolkit, like giving a smart person access to the internet and a calculator.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why This Is So Important\n",
        "\n",
        "Agents are the **bridge between LLM intelligence and real-world usefulness**.\n",
        "\n",
        "This is how:\n",
        "- üßë‚Äçüíº Virtual assistants book meetings\n",
        "- üîç Research bots look up info\n",
        "- üìä Analyst agents run data analysis\n",
        "- üßæ Legal/medical/chatbots reference documents\n",
        "\n",
        "---\n",
        "\n",
        "Now that you understand this, you're really thinking like an **agent developer**. üí°\n",
        "\n",
        "Let me know when you're ready to plug in an actual Hugging Face LLM to start auto-generating `function_name` and `function_parms` based on user input!"
      ],
      "metadata": {
        "id": "yLqfSu_5ItCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! You‚Äôre ready for the exciting part ‚Äî bringing in a **real LLM from Hugging Face** to start generating actions like a true AI agent. This is **Exercise 4** in our journey.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ Exercise 4: Use a Hugging Face LLM to Think Like an Agent\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Goal\n",
        "\n",
        "We want the LLM to:\n",
        "1. Receive a user question\n",
        "2. Think through the problem\n",
        "3. Output a structured action block like:\n",
        "```json\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Marie Curie\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This is the moment your **LLM starts acting like an agent**.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è What We‚Äôll Use\n",
        "\n",
        "- Model: `tiiuae/falcon-rw-1b` (small, no restrictions, good for testing)\n",
        "- Library: `transformers`\n",
        "- Interface: `pipeline(\"text-generation\")`\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step-by-Step Setup\n",
        "\n",
        "### 1. Install Dependencies (if needed)\n",
        "\n",
        "```bash\n",
        "pip install transformers huggingface_hub\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Load the Model from Hugging Face\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"tiiuae/falcon-rw-1b\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Define the System Prompt\n",
        "\n",
        "We‚Äôll guide the LLM to act like an agent by showing it an **example**:\n",
        "\n",
        "```python\n",
        "system_prompt = \"\"\"\n",
        "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
        "\n",
        "Use this format exactly:\n",
        "{\n",
        "  \"function_name\": \"...\",\n",
        "  \"function_parms\": {\n",
        "    ...\n",
        "  }\n",
        "}\n",
        "\n",
        "Available tools:\n",
        "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
        "\n",
        "Examples:\n",
        "\n",
        "User: When was Albert Einstein born?\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Albert Einstein\"\n",
        "  }\n",
        "}\n",
        "\n",
        "User: {}\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "We‚Äôll insert the user‚Äôs question into `{}`.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Ask the LLM a Question\n",
        "\n",
        "```python\n",
        "user_question = \"How old was Marie Curie when she died?\"\n",
        "\n",
        "# Insert the question into the prompt\n",
        "full_prompt = system_prompt.format(user_question)\n",
        "\n",
        "# Generate a response\n",
        "output = generator(full_prompt, max_new_tokens=200, do_sample=True)[0]['generated_text']\n",
        "\n",
        "# Print it\n",
        "print(output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What to Look For\n",
        "\n",
        "You're hoping the LLM returns something like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {\n",
        "    \"query\": \"Marie Curie\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "You can then:\n",
        "- Extract that JSON\n",
        "- Run the corresponding function (from earlier)\n",
        "- Return the observation\n",
        "- Send it back to the model to continue\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Step\n",
        "\n",
        "Let me know if you‚Äôd like help:\n",
        "- Parsing the LLM response into JSON\n",
        "- Running the actual tool based on the LLM‚Äôs decision\n",
        "- Building the full loop (LLM ‚Üí Tool ‚Üí Observation ‚Üí Final Answer)\n",
        "\n",
        "You‚Äôre just a couple steps away from a fully working AI agent!"
      ],
      "metadata": {
        "id": "uccgHXd6JKkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"tiiuae/falcon-rw-1b\" # continues to run after completion of task\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446,
          "referenced_widgets": [
            "969c56fdfdae48b1b53b1f22d7784702",
            "8530c7ae86be434b80179950370afdc4",
            "af9aea184b9c43c8b06dd6dd36b7b020",
            "bc82b23050494d909ce6c9115a43405b",
            "1b3894e283f24244bf14f0e342ec3778",
            "96c4c0b2b44e44dc9b34daf07fceeeb3",
            "79e33e8d79874204ae1e79f39aef6e9a",
            "4ba4e277ce7f4bd382e3641e701fd8fe",
            "da1f7666473b41c8a8178009e9c9bf6a",
            "d826d469d43b4b278d099af204d552c8",
            "eef7cac621c14e48b6ac076d6c54dc72",
            "70e5567e1e21461c890b75405f4069b7",
            "ddebb87904864080b4e300b551d100ae",
            "c3d58581b1834e8798ac5790da83bdca",
            "849ca7f0feeb46a0869adae402d296bc",
            "31c1085a8fed494bba9775db163e760e",
            "5d45a54031cd4355b884496cfd7df240",
            "d78b5b1f086c4530acce3059d22dba50",
            "17176421c48848c6a4c47e82a7ea72a1",
            "94da23d0b4ef442389fa037875011f20",
            "d8f3f5421eb34caf8cc367ff5a803ce9",
            "895c254fe17948cf997c90651f56ee79",
            "23c5d501ae544d178390b80c3960f58e",
            "8f8a029d146e423790535204d5ddccde",
            "8fb917e8aa2b49c682b7d3c223d23ca2",
            "59e70fb6d7554681811b35385917a7e7",
            "8597a71173bf4f7e9da814410628960d",
            "81ce120fed6c41f2b1dc2dc2e66f0135",
            "c71d127daf0c4cc4ad06a3f8cfb04c2b",
            "38863fb95b2040e7b6b45fc3e2dd8411",
            "60c971f5ca8b4befa920b73d18ba24f8",
            "5216d20d4f2449fc8334a32b2cd708fc",
            "58ca414adb2340cca256f8945cc8b1eb",
            "72e7d803fd7f43feb78785f19c4182b3",
            "be221914d9e94b1986bdca5d8bd8374f",
            "c8b06caae16e474d9d87c1f50bf04be2",
            "a6e6d28d7ab54773a5a9107eefc03ea6",
            "7843319951114374a94e9be7bbdeb775",
            "7091b744318a44039497fbca944fcf0c",
            "94076960c41c4f93a6d0e5c7ed52ccb3",
            "fa0dd118746c4f4b9cdae8929947c815",
            "867763d828984894a9464d982abdd227",
            "8658ede78a144c0ab30e5801a0cb7f2e",
            "b3e49349f9454c7fa740b060f7ede5b9",
            "f552b4005d5d426492bfe68a27150a49",
            "00efd610f01b4b8b9b31552511c40b06",
            "8e92061bc5714ddda4bbd3cfd3abebfb",
            "4a7f9db22bc241029b2ff4138501b5c5",
            "7b28f228cc2d477bb279b883ae05ab8b",
            "6217b4f6a18f4ad5938aa2d270d86efe",
            "d75abacc35e44ec1b19b073c0fb695d2",
            "a9013d3d13e8418895644a8b83453257",
            "73b85bf3d83041b8934ff2003185dcb6",
            "eaad97fba48743c2a716d5d63302835c",
            "f67b2d022c584ae9b78fe3a89c3bb052",
            "c4a35136121a498b9657519b6603a7a6",
            "14179e26e43f4183a1b792993fcf877d",
            "f3d838bb897d41ad926d2ee0808b655a",
            "9a04ec469b9d40ed9380d4a6313e17b6",
            "dd80c0d353ed469d8d7cd96ccfc2950b",
            "d9ded1e64cb04e8abffceb06ffff07c5",
            "505b5c04988e4abea9de9f00f7683fca",
            "5e4d05072c224c35bbed5183d053813a",
            "eb9b83fca6e844a78799037aea56d7a1",
            "de292c2f9086485fb220a68befb7608a",
            "e07b10913abd4f88a869a0311bcf1940",
            "2039f26df0a24d1484c00727a3e44a1f",
            "325cb0dc2d71469fb48a96c5f651d3e0",
            "c6c75eb8c4dd4db0a98630758ecd8b1d",
            "acc6d7831433410face9cc808b7a722b",
            "fb3def58a2404094bb0794629f77dd11",
            "f30f655872a64d44a462b10af3a7eb18",
            "6ce03571cc224ca3855818303335422a",
            "eeb10398db2b4cb490cf80763143b95b",
            "db07523aeaaa4c33bbc3d1f38d48882a",
            "09211102360944a18a431d92d71d4029",
            "8b8aaf72a6eb4d6eab524aad4c6d681e",
            "74e4681edb1848b6906324b993b56f36",
            "d5279f3f59674ade8411e904a6b18146",
            "a78322a67fb843d8ab736f3ba4c29730",
            "ff78ff5ee34846b8a2dd4c253cbc0fe4",
            "2394dec112cc438ca9735f37684d508e",
            "aee396d5afe8433188fa1bf97034c50c",
            "7953a815fb00427abdc1ca1710aa4b5a",
            "cd2e7b3bd4f44aaa8ed46e6c7a66865b",
            "865e7e61c3864cc2bdd03463a37e04bb",
            "b322a59451524c54bd0990da73df0a4e",
            "5c5044a9f540490a9eeef48ae86ef2d9",
            "a7a5cd9c075f446e8a9199f85e95adb4",
            "8a71b4537743402e8da6720bde705705",
            "56ae8a017c4e48baa12814ba1c9e0096",
            "cde94873a38a4003a12d4d8fc0e77cdc",
            "e5511851446145128cb22d15370bfca6",
            "35e978245c234cbc8991e5c7aaac6a01",
            "25e6a2a132e64f7fabe1213cb0deb2cb",
            "185ecfa917a24bb7aecc0843ab60bd54",
            "e23084d6d1244fa1b886f50fb7fd4f8b",
            "c44f26bf594d42ed9d0dc2f70a9098b0",
            "abea5314d4aa408abe3d7fb74c2a93b8"
          ]
        },
        "id": "8KGXpm-eH__P",
        "outputId": "9f353630-9e26-4c4d-a695-e6423d136d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "969c56fdfdae48b1b53b1f22d7784702"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70e5567e1e21461c890b75405f4069b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23c5d501ae544d178390b80c3960f58e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72e7d803fd7f43feb78785f19c4182b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f552b4005d5d426492bfe68a27150a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4a35136121a498b9657519b6603a7a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2039f26df0a24d1484c00727a3e44a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74e4681edb1848b6906324b993b56f36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7a5cd9c075f446e8a9199f85e95adb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes ‚Äî great call! The `zephyr-7b-beta` model is powerful but **heavy** (~13B+ parameters after loading) and **not ideal for limited environments** like Colab Free or machines without a GPU.\n",
        "\n",
        "Let‚Äôs switch to a **lighter, faster model** that still works well for prototyping agents.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Recommended Small Models (Open Source, Hugging Face)\n",
        "\n",
        "Here are some great lightweight options:\n",
        "\n",
        "| Model ID                          | Size  | Notes |\n",
        "|-----------------------------------|-------|-------|\n",
        "| `tiiuae/falcon-rw-1b`             | 1B    | Very small and fast, decent reasoning |\n",
        "| `microsoft/DialoGPT-medium`       | 345M  | Chat-focused, very lightweight        |\n",
        "| `google/flan-t5-base`             | 250M | Instruction-tuned, good reasoning    |\n",
        "| `facebook/blenderbot-3B`          | 3B    | Trained for dialogue                 |\n",
        "\n",
        "Yes ‚Äî all the models we‚Äôve talked about (like `falcon-rw-1b`, `flan-t5-base`, `DialoGPT`, `blenderbot`, etc.) are **pretrained LLMs**. Let‚Äôs break that down a bit so you understand exactly what you‚Äôre working with:\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What Is a Pretrained LLM?\n",
        "\n",
        "A **pretrained Large Language Model (LLM)** is a model that has already been:\n",
        "1. **Trained on a large dataset** (e.g., books, websites, conversations)\n",
        "2. **Taught the structure of language** (grammar, reasoning, logic)\n",
        "3. **Saved and shared** so you can use it out of the box\n",
        "\n",
        "You don‚Äôt need to retrain or fine-tune it to start using it.\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Common Types of Pretrained Models\n",
        "\n",
        "### 1. **Base LLMs**\n",
        "- Just predict text or fill in blanks\n",
        "- No special tuning for tasks or instruction-following\n",
        "\n",
        "üîπ Example: `tiiuae/falcon-rw-1b`\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Instruction-Tuned LLMs**\n",
        "- Trained to follow commands like \"Summarize this\" or \"Answer this question\"\n",
        "- Usually work better for agents and tools\n",
        "\n",
        "üîπ Example: `google/flan-t5-base`, `HuggingFaceH4/zephyr-7b-beta`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Chat-Tuned Models**\n",
        "- Specifically trained for back-and-forth dialogue\n",
        "- Often trained on user/assistant style prompts\n",
        "\n",
        "üîπ Example: `microsoft/DialoGPT-medium`, `facebook/blenderbot-3B`\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Pretrained Models Are Perfect for Agents\n",
        "\n",
        "Why?\n",
        "- They already \"know things\"\n",
        "- They can reason and make decisions\n",
        "- You can guide them with a well-written prompt\n",
        "\n",
        "And by adding **tools**, you give them superpowers ‚Äî the ability to:\n",
        "- Get live data\n",
        "- Read your documents\n",
        "- Query APIs\n",
        "- Control apps\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Later, You Can Fine-Tune\n",
        "\n",
        "Once you're comfortable with agents and want them to behave in a more custom way, you can:\n",
        "- Fine-tune a model on your own business data\n",
        "- Or use techniques like RAG (Retrieval-Augmented Generation) for domain knowledge\n",
        "\n",
        "But for now ‚Äî **pretrained models are exactly what you want** for building AI agents and learning the ropes.\n",
        "\n"
      ],
      "metadata": {
        "id": "wrRiC4WCSscv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "#model_id = \"tiiuae/falcon-rw-1b\" # continues to run after completion of task\n",
        "model_id = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "ccb01558358741dab7fa7aac8020a027",
            "50ffba17e21f4be298c8ee29f8bedc46",
            "322acffe7c2547e381cd084c20c1eedc",
            "848e50edb0704eeb8e09dcfe302b27e8",
            "fb1a042fa38543f4824a3ae2c2977ea8",
            "e75e807ce00144979161312514072827",
            "5a814e7468bf4bcba98d8390faa426de",
            "5c47f94031ce49888982b629b3ae5d9b",
            "08e837c8aa014c7baa638c8140a1c7f6",
            "9e8b0f58a7c1422f9567506e9bbf39bf",
            "27aeca5d6ad648e2b1144da15a7fec3d",
            "0ea8a988fe8a4931b3e37a1b85b529b6",
            "d94f487a107f448fae93f2a6b27c6bca",
            "91ae60eb03fa4a5b85ff902ce598edd1",
            "3798e64dabed45c08839f6dae70cffe5",
            "dde015285b4f459ea926b70ac6d89740",
            "eb35da96c8074511bee764fb939ac65d",
            "ace35dae268d440fa9e86f53165caa19",
            "d412b07c8da740dd8efd803f86743785",
            "ff03c6b010464e32ad939fe4aaa440c9",
            "a67e20d9a4c1487885c8fff2d517947e",
            "b273b08fce744ff49f74616260406bc6",
            "84554ca76224457eba5431442d130312",
            "22b4621d074b4a2b9bd3aa0cfa7e2c42",
            "2fe393720aad440b8d02864cbdfaf0b1",
            "a4602f0b148c49ad89b77b968b845780",
            "9913ff46230b494eb189ae500aacc267",
            "4ab9e7135acd4d29966c7f3b0758c77b",
            "b0c7968c4e9b4198b41c73bd6f477a6c",
            "b775e32227064dee8c544ad15c904f22",
            "baacb5e72c7b41ada77bf094df62cf97",
            "3f0a92be83e74276b31f1abc56ea531d",
            "11b18bcb520b452695629a0e7c745e97",
            "8602c29c8031494b9144f5b905d758f1",
            "68d2f01352014e3990c6d0ed2448169c",
            "a0ea39a92c73457b920e503793ecd8d4",
            "da4227512be6432e9b946e40477d7e5f",
            "3cb79559b26a49b6ae9bcd4a46ab7aad",
            "46132f3f303a49b0b8c8eff1cd6c9ac8",
            "3b14971999b448f0994cb4f8ba8f4761",
            "414b81466efa42e68288f9b227eae1de",
            "0e58e1b3765844559243215bc44e41a0",
            "07296128dc17407e8710704711281188",
            "b127287293794e66bf48a8ea69cbdb6c",
            "3616a53f99424509a91ce8c9fa7f4b44",
            "5a40510da23e4bfab6b5e5e0b427c3b4",
            "b72c070b94674ec181b0497afd3ddb06",
            "966b4b04dc954a148bfed8cbc974da01",
            "71803ea3bd9e4d50aed1f962cea84344",
            "fc53a5d9774345adb98e4f817bff00f2",
            "160ca2c516d14795a54b2555b3601ffd",
            "a4353fd9f76f4e838d8c1813aaad2685",
            "96ffa91fcd36442f8e055f3b61f12181",
            "137839af53c84292824c453ce2cb6e24",
            "2df69533be92452cb4c5fb50006c7102",
            "6a862161a6204e4383465b2b2c1833b3",
            "b09cc2d9957242e494c9e302898a8b98",
            "349019b750e5478495e42401359068a5",
            "db42365738964669870ff2f4c63b063d",
            "c4730ad7bd6e45558b161207998882fd",
            "07f06a73dc694731a6b0c2ea667dc879",
            "483e03b281ac4af4ac2c40a6c8ba4cd3",
            "526289bc2c804a959ea643f75f0428b4",
            "58879088cd3d457292518902a3f013a2",
            "61f87d8228694eb6ad61cd0689a630b8",
            "5e9db0b93a9246eaa6875858616ec11b",
            "818e1514601b4559b0640f3bc2af1588",
            "16049c36173b45289dfc03491141080a",
            "c05bcc39084a436eaa41380b9c039454",
            "d5b65c0fbe0242d290f4c2115f5a08e0",
            "158393ad1f48498d96e88e8e2a6bc98a",
            "c69c50779af0499c88029adab7787c28",
            "bc844374b2354e94b0ecd815d252bc11",
            "628f9599071540be80a6907a1a87abcd",
            "f6551ba79ae941349753ff2c1b23114e",
            "30dbfaf8dd194e5595d82e7a07a87cea",
            "53361e32165240adad93fb862c7eaabf"
          ]
        },
        "id": "cfCszazhS259",
        "outputId": "361f0d8c-99e3-4a15-a13f-ac067bac8e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccb01558358741dab7fa7aac8020a027"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ea8a988fe8a4931b3e37a1b85b529b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84554ca76224457eba5431442d130312"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8602c29c8031494b9144f5b905d758f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3616a53f99424509a91ce8c9fa7f4b44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/863M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a862161a6204e4383465b2b2c1833b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "818e1514601b4559b0640f3bc2af1588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt template with JSON action format\n",
        "system_prompt_template = \"\"\"\n",
        "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
        "\n",
        "Use this format exactly:\n",
        "{{\n",
        "  \"function_name\": \"...\",\n",
        "  \"function_parms\": {{\n",
        "    ...\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Available tools:\n",
        "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
        "\n",
        "Examples:\n",
        "\n",
        "User: When was Albert Einstein born?\n",
        "{{\n",
        "  \"function_name\": \"search_wikipedia\",\n",
        "  \"function_parms\": {{\n",
        "    \"query\": \"Albert Einstein\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "User: {}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Choose a test question\n",
        "user_question = \"How old was Marie Curie when she died?\"\n",
        "\n",
        "# Format the full prompt\n",
        "full_prompt = system_prompt_template.format(user_question)\n",
        "\n",
        "# ‚úÖ Print the prompt sent to the LLM\n",
        "print(\"üì§ Full Prompt Sent to LLM:\\n\")\n",
        "print(full_prompt)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Generate model output\n",
        "output = generator(\n",
        "    full_prompt,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.3,  # Less randomness\n",
        "    do_sample=False,  # Deterministic generation\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")[0]['generated_text']\n",
        "\n",
        "\n",
        "# ‚úÖ Print the output from the model\n",
        "print(\"ü§ñ Model Output:\\n\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8KxqyqGH_85",
        "outputId": "a4e2890f-10d1-4adb-d29e-57ea42ddf3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Full Prompt Sent to LLM:\n",
            "\n",
            "\n",
            "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
            "\n",
            "Use this format exactly:\n",
            "{\n",
            "  \"function_name\": \"...\",\n",
            "  \"function_parms\": {\n",
            "    ...\n",
            "  }\n",
            "}\n",
            "\n",
            "Available tools:\n",
            "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
            "\n",
            "Examples:\n",
            "\n",
            "User: When was Albert Einstein born?\n",
            "{\n",
            "  \"function_name\": \"search_wikipedia\",\n",
            "  \"function_parms\": {\n",
            "    \"query\": \"Albert Einstein\"\n",
            "  }\n",
            "}\n",
            "\n",
            "User: How old was Marie Curie when she died?\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "ü§ñ Model Output:\n",
            "\n",
            "\n",
            "You are an AI agent. You receive a user's question and must decide what tool to use and how to use it.\n",
            "\n",
            "Use this format exactly:\n",
            "{\n",
            "  \"function_name\": \"...\",\n",
            "  \"function_parms\": {\n",
            "    ...\n",
            "  }\n",
            "}\n",
            "\n",
            "Available tools:\n",
            "- search_wikipedia(query): Searches Wikipedia for a topic.\n",
            "\n",
            "Examples:\n",
            "\n",
            "User: When was Albert Einstein born?\n",
            "{\n",
            "  \"function_name\": \"search_wikipedia\",\n",
            "  \"function_parms\": {\n",
            "    \"query\": \"Albert Einstein\"\n",
            "  }\n",
            "}\n",
            "\n",
            "User: How old was Marie Curie when she died?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! You got it running üéâ ‚Äî and this is an **important milestone**. You‚Äôve:\n",
        "\n",
        "- Loaded a real Hugging Face model (`DialoGPT`)\n",
        "- Sent it a properly structured system prompt\n",
        "- Got a response back\n",
        "\n",
        "Now you're ready for the **next critical step** in building an agent:\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ Next Step: Parse the Model Output to Extract the Action\n",
        "\n",
        "Right now, the model is **just echoing the prompt** ‚Äî which means it's not reasoning or generating its own action block yet. That‚Äôs totally normal for `DialoGPT`, because it's **chat-optimized** but not **instruction-tuned**.\n",
        "\n",
        "We‚Äôll move forward by:\n",
        "1. Testing a model that better follows instructions (like `flan-t5-base`)\n",
        "2. Extracting a real JSON-style action from the model\n",
        "3. Running the tool based on that action\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Step 1: Switch to a Model That Can Generate Actions\n",
        "\n",
        "Let's use:\n",
        "```python\n",
        "model_id = \"google/flan-t5-base\"\n",
        "```\n",
        "\n",
        "Because:\n",
        "- It's **instruction-tuned** ‚Äî perfect for this kind of task\n",
        "- It uses the `\"text2text-generation\"` pipeline, which expects prompts like:\n",
        "  ```\n",
        "  Question: ...\n",
        "  Answer: ...\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Updated Code for `flan-t5-base`\n",
        "\n",
        "### üîß Load the model\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Simplified Prompt (no JSON escaping needed here)\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Generate the action\n",
        "\n",
        "```python\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"ü§ñ Model Output:\\n\", output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ What‚Äôs Next?\n",
        "\n",
        "If this gives you a real function name and parameters, we can:\n",
        "1. Extract that output using regex or simple parsing\n",
        "2. Look it up in your `available_tools`\n",
        "3. Run the tool and display the final answer (like an actual agent!)\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to provide the full working version of this pipeline, or would you like to try the `flan-t5-base` code first and share the result?"
      ],
      "metadata": {
        "id": "O4F8tF81Uk1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "model_id = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "438cd774285a4e22a37f89778f37fb31",
            "8e45ada554844f3b8614d61f717789b0",
            "0dbc0e7a7ac84bf5ab409201bca07c2d",
            "b751b17d47774d048e556c9a44306206",
            "4232ca278a224922ad1ba7baae45dcac",
            "64ca14319a6d40188d465a966370da98",
            "23577f303ffb4806aa941b5500855a99",
            "d3efad15a70c4b1abd8ae9823b1973f3",
            "3ca72384bd414e0da11d2a948b1f5c4d",
            "d1a1005b278f45df9c465d6cab156d40",
            "6b6aa474e4a54371b47314f5db3ae732",
            "457b1368a6c2473f9a846a80f9a05322",
            "58595c3aa94d4c128d9a338c31b64e51",
            "eb423baba1a946238c8027558ae940a0",
            "dcd75421bec14cb485bcbf1b16debab9",
            "2fc4f4202f3f4896a86c1fcbe501b8d5",
            "bce2da5c44004363b657675d3b16958f",
            "0aa7406bc5454dfbb460cd3912bad1e0",
            "a1f2d17933ce43d2b8029b654077675a",
            "597e19808c5a43dab0946aececd0379a",
            "9ea0ff1d3146431cb8daff3b76a648fc",
            "18ef25748b424f8186984e97c5c04ab0"
          ]
        },
        "id": "Qa8mjK5PLmch",
        "outputId": "ee2f8f69-c6f5-4b26-cd3a-ff67c44ee9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "438cd774285a4e22a37f89778f37fb31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "457b1368a6c2473f9a846a80f9a05322"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI agent. Based on the question, choose the best function to call and its parameters.\n",
        "\n",
        "Use this exact format:\n",
        "function_name: search_wikipedia\n",
        "function_parms: { \"query\": \"...\" }\n",
        "\n",
        "Question: How old was Marie Curie when she died?\n",
        "\"\"\".strip()\n",
        "\n",
        "output = generator(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
        "print(\"ü§ñ Model Output:\\n\", output)\n"
      ],
      "metadata": {
        "id": "9lkOfa8rLmWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/AI AGENTS/001_Prompt_Response.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"‚úÖ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"‚úÖ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "id": "_uwqi7PnLmS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa1cb7f-a41f-495f-ca3d-3371ae87fabe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    }
  ]
}