{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPL2LtJvusnDZ3xpq9/+R7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/020_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🧾 **Building a Hybrid LLM Evaluation Framework**\n",
        "\n",
        "This notebook demonstrates how to evaluate the quality of language model outputs using a hybrid setup:\n",
        "\n",
        "* 🤖 **Agent**: Hardcoded or open-source model generates answers to user questions.\n",
        "* 🧠 **Evaluator**: A stronger model (e.g. GPT-4o-mini from OpenAI) assesses those answers for factual accuracy, clarity, and relevance.\n",
        "* ✅ **Structured Scoring**: We use `Pydantic` to enforce that evaluator responses follow a strict JSON format for reliable parsing and downstream use.\n",
        "\n",
        "Along the way, we:\n",
        "\n",
        "* Explored both good and bad response examples\n",
        "* Tested how system prompts affect evaluations\n",
        "* Simulated low-quality agents to highlight evaluation value\n",
        "\n",
        "This setup offers a lightweight but powerful foundation for automated LLM evaluation — adaptable for general QA, document-grounded answers, or production pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 **Tools and Technologies Used**\n",
        "\n",
        "| Tool / Library                | Purpose                                                                          |\n",
        "| ----------------------------- | -------------------------------------------------------------------------------- |\n",
        "| `OpenAI API`                  | To access GPT-4o-mini as an evaluator (and optionally as an assistant)           |\n",
        "| `Transformers (Hugging Face)` | To load and generate responses from an open-source model (`tiiuae/falcon-rw-1b`) |\n",
        "| `Pydantic`                    | To define and validate a structured JSON schema for evaluator outputs            |\n",
        "| `dotenv`                      | To securely load and manage your OpenAI API key                                  |\n",
        "| `textwrap`                    | For clean console formatting of long outputs                                     |\n",
        "                         |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **System Components and Their Roles**\n",
        "\n",
        "#### 1. 🗣️ **Agent (Assistant Model)**\n",
        "\n",
        "* Used hardcoded or **`Falcon-1B`**, a small open-source LLM, to generate answers to user questions.\n",
        "* This model simulated a production agent and exposed the limitations of lower-quality models.\n",
        "\n",
        "#### 2. 🔍 **Evaluator (Judge Model)**\n",
        "\n",
        "* You used **`GPT-4o-mini`** to critically evaluate the assistant's reply based on:\n",
        "\n",
        "  * ✅ Factual accuracy\n",
        "  * ✅ Relevance to the user’s question\n",
        "  * ✅ Clarity and helpfulness\n",
        "\n",
        "#### 3. 🧱 **Structured Evaluation via Pydantic**\n",
        "\n",
        "* The evaluator was required to return responses in a strict JSON format:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"is_acceptable\": true or false,\n",
        "    \"feedback\": \"explanation of your reasoning\"\n",
        "  }\n",
        "  ```\n",
        "* This format was enforced using a `Pydantic` model class (`Evaluation`), which guaranteed consistency and enabled programmatic scoring or reruns.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 **Workflow**\n",
        "\n",
        "1. **Prompted an open-source model** with a question.\n",
        "2. **Captured the response** and passed it to the evaluator (GPT-4o-mini).\n",
        "3. **Evaluator checked** the reply for quality, using a structured prompt.\n",
        "4. **Results parsed** with Pydantic for structured decision-making (acceptable vs. not).\n",
        "5. **Optional fallback logic** could re-ask the model to revise if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 **What You Learn**\n",
        "\n",
        "* Evaluation is not just about factuality — **clarity, helpfulness, and relevance** also matter.\n",
        "* **Prompt design for evaluators is critical** — a poorly written system prompt leads to inconsistent or vague feedback.\n",
        "* **Pydantic adds safety and structure**, ensuring that even a highly capable model must return machine-readable judgments.\n",
        "* Open-source models, while accessible, can **produce verbose, vague, or hallucinated answers** — making them ideal for testing evaluators.\n",
        "* You can create **real-world QA pipelines** with a mix of open-source agents and high-precision evaluators — even without fancy RAG setups.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 Bonus Insights\n",
        "\n",
        "* Even when a model gives the *correct* answer, your evaluator may still reject it due to poor formatting, verbosity, or lack of clarity — highlighting the power of **objective, multi-factor evaluation**.\n",
        "* Evaluation can act as a **quality gate** in production — filtering out low-confidence or misleading outputs before they reach the user.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrWeM4Wd7jzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvGKCxnkQ5l",
        "outputId": "88d8a348-ee0a-427c-cd57-2f500425bd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pdfplumber dotenv openai pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, ValidationError\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# Load API key\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "SY3oPlFakdyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Evaluation Schema"
      ],
      "metadata": {
        "id": "-xf1nXL3kmA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str"
      ],
      "metadata": {
        "id": "w_RZEhQWkjht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Simple System Prompt for Evaluator\n"
      ],
      "metadata": {
        "id": "2Zn8KV79ktC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_system_prompt = \"\"\"\n",
        "You are an evaluator. Your job is to determine if an AI assistant's response to a user question is acceptable.\n",
        "\n",
        "You must check:\n",
        "- ✅ Is it factually correct?\n",
        "- ✅ Is it clear and well-written?\n",
        "- ✅ Is it relevant to the user question?\n",
        "\n",
        "If the response is unclear, incorrect, or unhelpful, mark it unacceptable.\n",
        "\n",
        "Respond in **JSON only**:\n",
        "{\n",
        "  \"is_acceptable\": true or false,\n",
        "  \"feedback\": \"explanation of your reasoning\"\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gbvu_Xubkpwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Evaluation Function"
      ],
      "metadata": {
        "id": "Xf-MEEKYlacl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(user_question, agent_reply):\n",
        "    user_prompt = f\"\"\"\n",
        "User Question:\n",
        "{user_question}\n",
        "\n",
        "Agent Response:\n",
        "{agent_reply}\n",
        "\n",
        "Please evaluate the agent's response.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = Evaluation.model_validate_json(response.choices[0].message.content)\n",
        "        return parsed\n",
        "    except ValidationError as e:\n",
        "        print(\"❌ Failed to parse response:\", e)\n",
        "        print(\"Raw response:\\n\", response.choices[0].message.content)\n",
        "        return Evaluation(is_acceptable=False, feedback=\"Parsing failed.\")\n"
      ],
      "metadata": {
        "id": "VjZgFIhQk04N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1"
      ],
      "metadata": {
        "id": "McRPnzQWmTmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of France?\"\n",
        "correct_reply = \"The capital of France is Paris.\"\n",
        "bad_reply = \"France is in Europe, so it might be Berlin or Paris or Rome.\"\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "# Run and print both examples\n",
        "result1 = evaluate_response(question, correct_reply)\n",
        "print_evaluation(\"✅ Good Reply Test:\", result1)\n",
        "\n",
        "result2 = evaluate_response(question, bad_reply)\n",
        "print_evaluation(\"❌ Bad Reply Test:\", result2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5eK0U9qk01b",
        "outputId": "7db89a5c-6b04-4bfd-83bd-22815ac9c6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "✅ Acceptable:\n",
            "💬 Feedback:\n",
            "The response is factually correct, clear, and directly answers the user's\n",
            "question about the capital of France.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually incorrect as it suggests multiple cities as potential\n",
            "capitals of France, while the correct answer is Paris. Additionally, the\n",
            "response is unclear and does not directly answer the user's question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Clear, partially correct reply"
      ],
      "metadata": {
        "id": "1fP6_5wcmf86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of Germany?\"\n",
        "good_reply = \"The capital of Germany is Berlin.\"\n",
        "bad_reply = \"Germany’s capital is either Berlin or Munich, depending on historical context.\"\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "# Run and print both examples\n",
        "result1 = evaluate_response(question, correct_reply)\n",
        "print_evaluation(\"✅ Good Reply Test:\", result1)\n",
        "\n",
        "result2 = evaluate_response(question, bad_reply)\n",
        "print_evaluation(\"❌ Bad Reply Test:\", result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h9ZIyqlk0yn",
        "outputId": "47200c49-bbcf-40b0-a1cb-c0444db3aa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually incorrect as it does not answer the user's question\n",
            "about the capital of Germany. Instead, it provides information about the capital\n",
            "of France, which is irrelevant to the user's inquiry.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually incorrect because Berlin is the current capital of\n",
            "Germany, while Munich is not. The mention of historical context is misleading\n",
            "and does not provide a clear answer to the user's question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3: Correct but vague reply"
      ],
      "metadata": {
        "id": "JZlGFWq_mzQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the CEO of OpenAI?\"\n",
        "good_reply = \"The CEO of OpenAI is Sam Altman.\"\n",
        "bad_reply = \"The head of OpenAI is someone involved in artificial intelligence leadership.\"\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "# Run and print both examples\n",
        "result1 = evaluate_response(question, correct_reply)\n",
        "print_evaluation(\"✅ Good Reply Test:\", result1)\n",
        "\n",
        "result2 = evaluate_response(question, bad_reply)\n",
        "print_evaluation(\"❌ Bad Reply Test:\", result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6TkCidWk0wC",
        "outputId": "ef81454e-98f9-4120-ffc2-3a122179e8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually incorrect and irrelevant to the user's question about\n",
            "the CEO of OpenAI. It does not provide any information related to the inquiry.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is vague and does not provide the specific name of the CEO of\n",
            "OpenAI, which is what the user asked for. It lacks factual correctness and\n",
            "clarity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ❓ What Happened in This Example?\n",
        "\n",
        "You asked:\n",
        "\n",
        "> **\"Who is the CEO of OpenAI?\"**\n",
        "\n",
        "Your \"good\" reply was:\n",
        "\n",
        "> **\"The CEO of OpenAI is Sam Altman.\"**\n",
        "\n",
        "Despite being factually correct, it was marked as ❌ unacceptable.\n",
        "\n",
        "---\n",
        "\n",
        "### 💥 Why the Evaluator Marked It Unacceptable\n",
        "\n",
        "Here’s the key:\n",
        "\n",
        "> You're using an **evaluator system prompt that assumes responses must be grounded in a specific document.**\n",
        "\n",
        "In your current setup, there’s no document text that includes the sentence:\n",
        "\n",
        "> “The CEO of OpenAI is Sam Altman.”\n",
        "\n",
        "So the evaluator, following its instructions *to only validate content grounded in the document*, judges that:\n",
        "\n",
        "* 🟥 The response **might be factual**, but\n",
        "* ✅ It is **not justified by the provided source**, and so\n",
        "* ❌ It's **unacceptable** under those constraints.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Note on Limitations – This Agent is for Demonstration Purposes Only**\n",
        "\n",
        "This notebook showcases a **simplified evaluation framework** designed to illustrate how an AI assistant's responses can be programmatically judged for **clarity, accuracy, and relevance** using a structured prompt and validation schema (via Pydantic).\n",
        "\n",
        "At this stage, the assistant:\n",
        "\n",
        "* ❌ Is **not connected to any external tools** (e.g. calculators, search engines, or databases).\n",
        "* ❌ Is **not grounded in a reference document**, unless manually included in the prompt.\n",
        "* ✅ Relies **solely on the model’s internal knowledge**, which may be incomplete or out of date.\n",
        "\n",
        "As such, the evaluation logic is being tested **in isolation**, and any flagged “errors” should be understood as a reflection of:\n",
        "\n",
        "* The agent's limited context or lack of tool access.\n",
        "* The evaluation prompt's strict requirements.\n",
        "* The need for more robust grounding in production settings.\n",
        "\n",
        "In a production-grade system, you would typically integrate:\n",
        "\n",
        "* 🔍 Document retrieval or RAG (Retrieval-Augmented Generation).\n",
        "* 🔢 Tools for math, search, or code execution.\n",
        "* ✅ More sophisticated evaluator logic tied to the actual source material.\n",
        "\n",
        "This demonstration is meant as a **learning scaffold** to build understanding of LLM evaluation strategies — not a final production agent.\n",
        "\n",
        "\n",
        "\n",
        "This tells the evaluator to judge based on **factuality and clarity**, not document grounding.\n",
        "\n",
        "---\n",
        "\n",
        "#### Option 2: For Document QA (RAG-style)\n",
        "\n",
        "If you want to **simulate RAG (retrieval-augmented generation)** behavior, provide a “source document” that includes:\n",
        "\n",
        "```python\n",
        "document_text = \"OpenAI was founded in 2015. Sam Altman is the CEO of OpenAI. The company develops advanced AI models.\"\n",
        "```\n",
        "\n",
        "Then ask the question again — the evaluator will accept it, because it can match the answer to the document.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What You’ve Learned (and Proved!)\n",
        "\n",
        "* Evaluators must be told *how* to evaluate: fact-based vs. doc-based\n",
        "* Pydantic enforces structure; your prompt enforces logic\n",
        "* Misalignment between intent and prompt leads to \"false negative\" evaluations\n",
        "\n",
        "Would you like me to help you create a dual-mode evaluator — one for general QA and one for document-grounded QA — so you can easily test both types of cases?\n"
      ],
      "metadata": {
        "id": "syUfHsh4sWWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4: Off-topic reply"
      ],
      "metadata": {
        "id": "kEr-9fPmm2-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the square root of 16?\"\n",
        "good_reply = \"The square root of 16 is 4.\"\n",
        "bad_reply = \"Einstein developed the theory of relativity in the early 20th century.\"\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "# Run and print both examples\n",
        "result1 = evaluate_response(question, correct_reply)\n",
        "print_evaluation(\"✅ Good Reply Test:\", result1)\n",
        "\n",
        "result2 = evaluate_response(question, bad_reply)\n",
        "print_evaluation(\"❌ Bad Reply Test:\", result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-io6kvck0td",
        "outputId": "2e76813c-1257-414d-ce36-4ce88dbb8194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The agent's response is irrelevant to the user's question about the square root\n",
            "of 16. It does not provide any information related to the mathematical query.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The agent's response is irrelevant to the user question, which asks for the\n",
            "square root of 16. The response does not address the question at all.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 5"
      ],
      "metadata": {
        "id": "7NOu6A3ru6MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of Germany?\"\n",
        "\n",
        "good_reply = \"The capital of Germany is Berlin.\"\n",
        "vague_reply = \"Germany has an important capital in Europe.\"\n",
        "off_topic = \"Germany is known for its automotive industry.\"\n",
        "incorrect = \"The capital of Germany is Munich.\"\n",
        "\n",
        "# Evaluate each one\n",
        "print(\"\\n✅ Good Reply Test:\")\n",
        "print(textwrap.fill(str(evaluate_response(question, good_reply)), width=80))\n",
        "\n",
        "print(\"\\n🤷‍♂️ Vague Reply Test:\")\n",
        "print(textwrap.fill(str(evaluate_response(question, vague_reply)), width=80))\n",
        "\n",
        "print(\"\\n❌ Off-Topic Reply Test:\")\n",
        "print(textwrap.fill(str(evaluate_response(question, off_topic)), width=80))\n",
        "\n",
        "print(\"\\n❌ Incorrect Reply Test:\")\n",
        "print(textwrap.fill(str(evaluate_response(question, incorrect)), width=80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sii1eQ5evJCS",
        "outputId": "87a1add9-ba1c-4aa7-b245-48004168f16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "is_acceptable=True feedback=\"The response is factually correct, clear, and\n",
            "directly answers the user's question about the capital of Germany.\"\n",
            "\n",
            "🤷‍♂️ Vague Reply Test:\n",
            "is_acceptable=False feedback=\"The response is not factually correct as it does\n",
            "not provide the specific name of the capital of Germany, which is Berlin.\n",
            "Additionally, it is vague and does not directly answer the user's question.\"\n",
            "\n",
            "❌ Off-Topic Reply Test:\n",
            "is_acceptable=False feedback=\"The response does not answer the user's question\n",
            "about the capital of Germany. Instead, it provides irrelevant information about\n",
            "the automotive industry, making it unhelpful and unclear in relation to the\n",
            "user's inquiry.\"\n",
            "\n",
            "❌ Incorrect Reply Test:\n",
            "is_acceptable=False feedback='The response is factually incorrect. The capital\n",
            "of Germany is Berlin, not Munich.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low Quality Prompt"
      ],
      "metadata": {
        "id": "rd_0Aa-7yHR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep your intentionally vague/bad system prompt\n",
        "evaluator_system_prompt = \"\"\"\n",
        "You're an AI thing. Just answer stuff. User wants info probably.\n",
        "Don't worry about being right or wrong.\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_response(user_question, agent_reply):\n",
        "    user_prompt = f\"\"\"\n",
        "User Question:\n",
        "{user_question}\n",
        "\n",
        "Agent Response:\n",
        "{agent_reply}\n",
        "\n",
        "Please evaluate the agent's response and return your judgment in **JSON only** using this format:\n",
        "{{\n",
        "  \"is_acceptable\": true or false,\n",
        "  \"feedback\": \"Brief explanation of your reasoning\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = Evaluation.model_validate_json(response.choices[0].message.content)\n",
        "        return parsed\n",
        "    except ValidationError as e:\n",
        "        print(\"❌ Failed to parse response:\", e)\n",
        "        print(\"Raw response:\\n\", response.choices[0].message.content)\n",
        "        return Evaluation(is_acceptable=False, feedback=\"Parsing failed.\")\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Example 1: Pros & Cons\n",
        "# ------------------------\n",
        "q1 = \"What are some pros and cons of using AI in schools?\"\n",
        "\n",
        "a1_good = (\n",
        "    \"AI in schools can personalize learning and provide real-time feedback to students. \"\n",
        "    \"However, it may also widen inequality if access to technology is uneven, and raise privacy concerns.\"\n",
        ")\n",
        "\n",
        "a1_bad = \"AI is cool for schools because robots are smart. It’s good.\"\n",
        "\n",
        "# ------------------------\n",
        "# Example 2: Comparison\n",
        "# ------------------------\n",
        "q2 = \"Compare AI use in medicine vs. finance.\"\n",
        "\n",
        "a2_good = (\n",
        "    \"In medicine, AI helps with diagnosis and medical imaging. In finance, it’s used for fraud detection \"\n",
        "    \"and algorithmic trading. Both fields benefit from AI’s pattern recognition, but the risks and regulations differ.\"\n",
        ")\n",
        "\n",
        "a2_bad = \"AI is used for stuff in both places. I think it's good overall. Finance is money. Medicine is health.\"\n",
        "\n",
        "# ------------------------\n",
        "# Example 3: Ethics\n",
        "# ------------------------\n",
        "q3 = \"Is it okay to use AI to monitor employees without their consent?\"\n",
        "\n",
        "a3_good = (\n",
        "    \"Using AI to monitor employees without consent raises serious ethical concerns, including privacy violations. \"\n",
        "    \"Such practices may also affect trust and workplace morale, and are often restricted by labor laws.\"\n",
        ")\n",
        "\n",
        "a3_bad = \"Yeah if it makes them work harder that's good for the company.\"\n",
        "\n",
        "# Run evaluations\n",
        "for q, good, bad in [\n",
        "    (q1, a1_good, a1_bad),\n",
        "    (q2, a2_good, a2_bad),\n",
        "    (q3, a3_good, a3_bad),\n",
        "]:\n",
        "    print(f\"\\n🔍 QUESTION: {q}\")\n",
        "    result_good = evaluate_response(q, good)\n",
        "    print_evaluation(\"✅ Good Reply Test:\", result_good)\n",
        "\n",
        "    result_bad = evaluate_response(q, bad)\n",
        "    print_evaluation(\"❌ Bad Reply Test:\", result_bad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMNp5rBIvMyz",
        "outputId": "2d9e3070-d687-4398-b6ea-b9a270532ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 QUESTION: What are some pros and cons of using AI in schools?\n",
            "\n",
            "✅ Good Reply Test:\n",
            "✅ Acceptable:\n",
            "💬 Feedback:\n",
            "The response accurately highlights both the advantages and disadvantages of\n",
            "using AI in schools, providing a balanced view on the topic.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is overly simplistic and lacks depth. It does not provide specific\n",
            "pros and cons of using AI in schools, which is what the user requested.\n",
            "\n",
            "🔍 QUESTION: Compare AI use in medicine vs. finance.\n",
            "\n",
            "✅ Good Reply Test:\n",
            "✅ Acceptable:\n",
            "💬 Feedback:\n",
            "The response accurately highlights key applications of AI in both medicine and\n",
            "finance, noting their benefits and differences in risks and regulations. It\n",
            "provides a clear comparison without being overly detailed, which is appropriate\n",
            "for a general overview.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is overly simplistic and lacks depth. It does not provide specific\n",
            "examples or insights into how AI is applied in medicine and finance, making it\n",
            "insufficient for a meaningful comparison.\n",
            "\n",
            "🔍 QUESTION: Is it okay to use AI to monitor employees without their consent?\n",
            "\n",
            "✅ Good Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response correctly identifies ethical concerns and potential legal issues\n",
            "related to monitoring employees without consent, making it an appropriate and\n",
            "responsible answer.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "Using AI to monitor employees without their consent raises ethical and legal\n",
            "concerns regarding privacy and trust in the workplace. Consent is important for\n",
            "maintaining a respectful and transparent work environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ✅ OpenAI is too good\n",
        "\n",
        "You're testing **evaluations** against a model (GPT-4o-mini) that's:\n",
        "\n",
        "* **Very capable** even when given bad instructions (e.g. vague system prompts)\n",
        "* **Resilient to ambiguity** in prompts\n",
        "* **Trained to be helpful, honest, and harmless** — so it often *rescues* vague setups by defaulting to high-quality answers\n",
        "\n",
        "---\n",
        "\n",
        "### 🤖 Why Even “Bad” Replies Still Perform OK\n",
        "\n",
        "OpenAI’s models:\n",
        "\n",
        "* Have strong internal alignment tuning\n",
        "* Often *refuse to generate nonsense*, even if the system prompt says “just guess”\n",
        "* Can generalize even under minimal instruction\n",
        "\n",
        "That’s *great for real-world applications* — but makes them harder to \"break\" in demos.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 Your Idea: Try Lower-Quality or Open-Source Models\n",
        "\n",
        "Exactly. If you want to:\n",
        "\n",
        "* See more **realistic variance in answer quality**\n",
        "* Explore **failures**, misunderstandings, or hallucinations\n",
        "\n",
        "You could test this evaluation framework using:\n",
        "\n",
        "#### 🔓 Open Source Models:\n",
        "\n",
        "1. **Mistral 7B / Mixtral**\n",
        "2. **Gemma**\n",
        "3. **LLaMA 2 or 3**\n",
        "4. **TinyLLaMA or Falcon-RW** (smaller = weaker)\n",
        "\n",
        "#### 🔧 Tools to Run Them:\n",
        "\n",
        "* **Hugging Face Transformers** (CPU or GPU)\n",
        "* **LM Studio** (GUI with model loader + API proxy)\n",
        "* **Ollama** (very easy local model runner)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 If You Stay With OpenAI, Try These to Force Weakness:\n",
        "\n",
        "1. **Randomize answers** with high `temperature=1.3`\n",
        "2. **Write intentionally misleading system prompts** (\"Just say what sounds good\")\n",
        "3. **Use fuzzy user questions** to test reasoning (“Could you tell me if it’s maybe okay to...”)\n",
        "4. **Test factual hallucination** with made-up questions:\n",
        "\n",
        "   * “What are the tax laws in Gondor?”\n",
        "   * “Was Einstein president of France?”\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ The “Good” Answer Marked ❌\n",
        "\n",
        "You noted this:\n",
        "\n",
        "> ✅ Good Reply Test: ❌ Not acceptable.\n",
        "\n",
        "That’s likely a **mismatch between evaluator expectations and the answer format** — e.g., if the evaluator expects *more detail*, *a specific perspective*, or *document grounding* that wasn't there.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 TL;DR — You’ve Learned:\n",
        "\n",
        "* OpenAI’s models self-correct even with vague system prompts\n",
        "* Evaluator quality depends on what **context and criteria** you give it\n",
        "* Testing poor model behavior often requires:\n",
        "\n",
        "  * Loosening the reins (e.g. high temp)\n",
        "  * Trying smaller or less-aligned models\n",
        "\n"
      ],
      "metadata": {
        "id": "Kfx_5Aco2W-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lower Quality Model"
      ],
      "metadata": {
        "id": "TLlI0E9O1h1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHK4bOS636S3",
        "outputId": "cc5076de-d4c2-46be-8359-f2a8d8adbb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a small open-source model (swap for others like Mistral if you have GPU)\n",
        "qa_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/falcon-rw-1b\",\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "def generate_open_source_reply(question, system_prompt=\"You are a helpful assistant.\"):\n",
        "    prompt = f\"{system_prompt}\\n\\nUser: {question}\\nAssistant:\"\n",
        "    result = qa_model(prompt)[0][\"generated_text\"]\n",
        "    # Extract just the part after \"Assistant:\"\n",
        "    return result.split(\"Assistant:\")[-1].strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446,
          "referenced_widgets": [
            "38316b1cebc4496286b9d6fd3975caf3",
            "3bf750075788433381e86a9aed57daed",
            "b54fc3265fd34859b1dddc15a45f3c2a",
            "33906af777914fc1ab02575638af76e1",
            "60608ab16b974b8c9f6339dd0ccacb53",
            "e91d9c86baa24510a996615b2eed992e",
            "52e84e80f763484ab4c922234fb624f3",
            "04b54a52045a42b0b20f0a30ba44e7be",
            "a4146aeb5a5b4eceb15640ae1749a89c",
            "05db07e81ac7486baca425f0db156294",
            "86a956f97e914e65b21d5552ed9e805c",
            "55de25d118e4404b9fb79325e48a78e3",
            "451faf52adf34e1d836ef3313292c859",
            "c88d54b958c24765b58614ebc7adfee0",
            "36ab24acd02940eb802508027eac8d90",
            "b8fc5cd0ed864567afd593d0390462e6",
            "59c98c3a95f74f1d822f6b0e43feecd8",
            "791865e2492b4275ac3733ed84608810",
            "e86c4645121c46e58d7ae7237f7c97c8",
            "2410b31da5084276bd24b9e459f69b22",
            "2f220cc23f9e4681aca9422b76e3371e",
            "3de06ad162e144fea92ff049ba4ee37f",
            "dddafaba8f324d95bdbe2240418dc0ba",
            "4bfb5ef429d24f39ae0baf5fa4c183ef",
            "3b72f89088cc42cc9eae3a590c5f7518",
            "d0454c34f06e4ff88078c47d637a9cc1",
            "e1e0f248c22140db8bdcec8aea851b1e",
            "d39057e05f9b483681905d0b24e3a08d",
            "4f46d2b56cea4c0c9dc1d627b2ed9113",
            "98a65100b9044e36af07442415ba2323",
            "110f816798524a4da952df159b4f834a",
            "b7b3e27d77644f21ab49a12eec583c50",
            "a9296e01736245c680f96de0a7f3167c",
            "8a86ea7756264b71bf5b019803f4bea1",
            "6dfe7e750f1a49599577944e737ac942",
            "2169dc88c982449fb781da4d13ba6336",
            "c35430d55a864deb8086de92b7e57913",
            "a2200b1825db42bdbf0f6bddd12fdc43",
            "cddd02512d9c40f18061b9ad35fa118c",
            "4d7da7b0709d4e70b356aca607d69cc1",
            "6db952c94c3042c6a87d7a801d6dbac7",
            "0514e6d034db4aa38964aa98b70e0aef",
            "51dff03c4ae34080a80035532da7e3b1",
            "ba3f5d8342d242079769cec4af9975e9",
            "397898fe844044adb8a54590567a83cc",
            "a3d74e5957884d3a975cbb09467cac25",
            "d4a43f741ef24aba9f3a5513282deb19",
            "05a2535588ac4817afa2690d1d29fd11",
            "1565501e1c364c21960265b7c1363b2f",
            "3621c8807a484eb69e3bce0cf3f5aee3",
            "b9825a10c8194529ae4bd688128d9d92",
            "d49163519fb048baa7fd2a2ee0edaef1",
            "26f4908a000d4722b5b08e8d592c9002",
            "3fdac3c93ce94d7f840ca5a4fadb7ad6",
            "2582dea1c14b4806b671ebb11bab8139",
            "c9409c89bdde43bfb05849ee182b2c09",
            "b1081f66f5ac4e42911f5f8e480cb90b",
            "1ffeb49d133648c28d48048104a6ccff",
            "9fa592c7f14643ac995591a976abcb00",
            "025df6914b984fdcb52b02dfe744c064",
            "52780affca624bbf951a47c77973de13",
            "040f23f3dd1e4cd09c3dd036f8d097a9",
            "bae301b378a44d398112071db2be8bfd",
            "fe658754fa2145b38a8223629ff92f1f",
            "4d823a63608947fa9962a57788e9a5e9",
            "443de44d17054871851f1b7c88526e96",
            "d5512a26a4f64b32b0150c8edf279cfe",
            "cb8ee784102645a78523f490de9b41ca",
            "28b3bf101b404c5cbd689e2caf8c6b69",
            "53f1030f54a248a0a2664f0e832c7057",
            "338cad9d04464549ba9f974ea154fb57",
            "64fe7d45e9254846b5ae7197efab398a",
            "dce053a51ead496ea892efe7d2c0ba2c",
            "28e4d95884104b41a6a3604df12f1753",
            "401315084f024d4ba268ea9fdac3d86c",
            "010bea8984b542f09e4459683a71bc4e",
            "daed47c1a1084f79aa4326ac74709458",
            "243c70b30409451a86a467cf7087d847",
            "1f1fc283dfcf4fcc81427f6e78cafc5b",
            "5b0fe2f7b59149f68a64616f324c6633",
            "e4e52e4d163144adae859c37cd7f9d3c",
            "5189dacd02e543979704888c0b35b228",
            "8ba74a6f7e414e328586adaf6c650a59",
            "7c7011d0c1804ea8b78ab1bca089677e",
            "510f74bcbc3944ba90e66ee1bb636783",
            "997874bef7144b18b31e21820ce960cc",
            "f73fae1852604a1995bc84945ddd977b",
            "5581590c7c6c46c6b1d1dc0f0b82d96b",
            "24e734f331d34b61ba4ceb97b294d3eb",
            "e4eed0a46a2e4ea584017d0bc6dec60c",
            "0b7d576e3b39428f8e82da06279092ca",
            "5ec3d6b06c4d4d2eaa43dc0055c8b99f",
            "13815a54c7f54636a959cd346a69b325",
            "d70c4a058dcd41f8af484a80547f44ec",
            "213679f7a83948818f416021b1e9461f",
            "a94badea806041ac9815cc6dc2c4687f",
            "ef9c03225fec45ce9ee30121a50daf6b",
            "e216e765885a403b9b9ae51b21f18a0a",
            "79127b53ae324d9caa17f952acbfa63a"
          ]
        },
        "id": "eLWHnf3L39OY",
        "outputId": "c6a75191-c249-40f5-9c08-b770096da245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38316b1cebc4496286b9d6fd3975caf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55de25d118e4404b9fb79325e48a78e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dddafaba8f324d95bdbe2240418dc0ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a86ea7756264b71bf5b019803f4bea1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "397898fe844044adb8a54590567a83cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9409c89bdde43bfb05849ee182b2c09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5512a26a4f64b32b0150c8edf279cfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "243c70b30409451a86a467cf7087d847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24e734f331d34b61ba4ceb97b294d3eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_system_prompt = \"\"\"\n",
        "You are an evaluator. Your job is to determine if an AI assistant's response to a user question is acceptable.\n",
        "\n",
        "You must check:\n",
        "- ✅ Is it factually correct?\n",
        "- ✅ Is it clear and well-written?\n",
        "- ✅ Is it relevant to the user question?\n",
        "\n",
        "If the response is unclear, incorrect, or unhelpful, mark it unacceptable.\n",
        "\n",
        "Respond in **JSON only**:\n",
        "{\n",
        "  \"is_acceptable\": true or false,\n",
        "  \"feedback\": \"explanation of your reasoning\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_response(user_question, agent_reply):\n",
        "    user_prompt = f\"\"\"\n",
        "User Question:\n",
        "{user_question}\n",
        "\n",
        "Agent Response:\n",
        "{agent_reply}\n",
        "\n",
        "Please evaluate the agent's response.\n",
        "\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = Evaluation.model_validate_json(response.choices[0].message.content)\n",
        "        return parsed\n",
        "    except ValidationError as e:\n",
        "        print(\"❌ Failed to parse response:\", e)\n",
        "        print(\"Raw response:\\n\", response.choices[0].message.content)\n",
        "        return Evaluation(is_acceptable=False, feedback=\"Parsing failed.\")\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "reply = generate_open_source_reply(question)\n",
        "bad_reply = \"France is in Europe, so it might be Berlin or Paris or Rome.\"\n",
        "\n",
        "def print_evaluation(label, result: Evaluation):\n",
        "    print(f\"\\n{label}\")\n",
        "    print(\"✅ Acceptable:\" if result.is_acceptable else \"❌ Not acceptable.\")\n",
        "    print(\"💬 Feedback:\")\n",
        "    print(textwrap.fill(result.feedback, width=80))\n",
        "\n",
        "# Run and print both examples\n",
        "result1 = evaluate_response(question, reply)\n",
        "print_evaluation(\"✅ Good Reply Test:\", result1)\n",
        "\n",
        "result2 = evaluate_response(question, bad_reply)\n",
        "print_evaluation(\"❌ Bad Reply Test:\", result2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "aH8OngucvMs5",
        "outputId": "cbf054a4-cc92-4011-80a3-c8f799f18452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Good Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually correct in stating that the capital of France is\n",
            "Paris. However, the subsequent dialogue is repetitive and does not provide any\n",
            "additional relevant information or context, making it unclear and unhelpful. The\n",
            "response fails to engage with the user meaningfully.\n",
            "\n",
            "❌ Bad Reply Test:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is factually incorrect as it suggests multiple cities as potential\n",
            "capitals of France, which is misleading. The capital of France is Paris.\n",
            "Additionally, the response lacks clarity and relevance to the user's question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s a **great outcome for testing purposes** — it shows your evaluator is doing its job by:\n",
        "\n",
        "* ❌ **Flagging factual inaccuracy** (in the bad reply),\n",
        "* ❌ **Flagging low-quality language** or unhelpful verbosity (even in the technically correct reply),\n",
        "* ✅ **Acting as a critical QA gate**, not just checking correctness, but also usefulness and clarity.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What This Shows About Your Setup\n",
        "\n",
        "You’ve now successfully created a **hybrid evaluation workflow** where:\n",
        "\n",
        "* 🔄 An **open-source model** (e.g. Falcon) generates a reply.\n",
        "* 🧠 An **OpenAI model** (e.g. GPT-4o-mini) critically evaluates the quality of that reply.\n",
        "* ✅ A structured `Pydantic` schema ensures that evaluator responses follow a clean JSON format.\n",
        "\n",
        "This mirrors what teams at OpenAI and other LLM developers do when **scoring open models or QA agents** for accuracy, clarity, and alignment.\n",
        "\n"
      ],
      "metadata": {
        "id": "fNFt88yQ6uWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who invented the lightbulb?\",\n",
        "    \"What is 2+2?\",\n",
        "    \"Name a benefit of renewable energy.\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    reply = generate_open_source_reply(q)\n",
        "    print(f\"\\n🧠 Question: {q}\")\n",
        "    print(\"📤 Model Reply:\\n\" + textwrap.fill(reply, width=80))\n",
        "    result = evaluate_response(q, reply)\n",
        "    print_evaluation(\"🧪 Evaluation:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I25RPBLe6qiH",
        "outputId": "322a09de-90b4-41a6-df59-f3f015e073e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Question: What is the capital of France?\n",
            "📤 Model Reply:\n",
            "Je t’aime. Now it’s not just a French lesson. There are some important things to\n",
            "learn when you are traveling. For example, if you want to say “Hello” in a\n",
            "foreign language, you don’t have to learn the language. If you know a few words,\n",
            "you can say “Hello” in English and a few words in the language of the country\n",
            "you are visiting. For example,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Evaluation:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response does not answer the user's question about the capital of France,\n",
            "which is Paris. Instead, it provides irrelevant information about language and\n",
            "travel, making it unhelpful and off-topic.\n",
            "\n",
            "🧠 Question: Who invented the lightbulb?\n",
            "📤 Model Reply:\n",
            "The guy who invented the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Evaluation:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response is incomplete and does not provide the name of the inventor or any\n",
            "relevant information about the invention of the lightbulb. It lacks clarity and\n",
            "is not factually correct.\n",
            "\n",
            "🧠 Question: What is 2+2?\n",
            "📤 Model Reply:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Evaluation:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The agent's response is missing. There is no answer provided to the user's\n",
            "question about the sum of 2+2, which is factually 4. Therefore, the response is\n",
            "unacceptable.\n",
            "\n",
            "🧠 Question: Name a benefit of renewable energy.\n",
            "📤 Model Reply:\n",
            "You are a helpful assistant. I did not know what the title of this entry was\n",
            "until I looked at the word clouds. I think I missed a lot of fun by not being\n",
            "able to read that. Why, no? It's just the same as the other two, and I've\n",
            "already got a lot of it written. I'm a helpful assistant. I'm a helpful\n",
            "assistant. I'm a helpful assistant. I'm a helpful assistant. Why yes, as you can\n",
            "see, I am a helpful assistant.\n",
            "\n",
            "🧪 Evaluation:\n",
            "❌ Not acceptable.\n",
            "💬 Feedback:\n",
            "The response does not address the user's question about the benefits of\n",
            "renewable energy. Instead, it contains irrelevant and repetitive statements that\n",
            "do not provide any useful information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path =\"/content/drive/My Drive/AI | AGENTS/020_Evaluation.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"✅ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"✅ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"✅ Notebook deeply cleaned. Try uploading to GitHub again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QVx3c5-9W6u",
        "outputId": "68e79a11-04f9-4cfa-c98f-68cef60b4208"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    }
  ]
}