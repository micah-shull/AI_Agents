{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNR5bZlXDy1w4QTOciviump",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/318_EaaS_Utilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities for EaaS Orchestrator Agent"
      ],
      "metadata": {
        "id": "B1xOKq5jpV5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Utilities for EaaS Orchestrator Agent\n",
        "\n",
        "Reusable business logic for data loading, evaluation execution, and scoring.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def load_journey_scenarios(data_dir: str, filename: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load journey scenarios from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def load_specialist_agents(data_dir: str, filename: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load specialist agents configuration from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def load_supporting_data(\n",
        "    data_dir: str,\n",
        "    customers_file: str,\n",
        "    orders_file: str,\n",
        "    logistics_file: str,\n",
        "    marketing_signals_file: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Load all supporting data files.\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # Load customers\n",
        "    customers_path = Path(data_dir) / customers_file\n",
        "    with open(customers_path, 'r') as f:\n",
        "        data['customers'] = json.load(f)\n",
        "\n",
        "    # Load orders\n",
        "    orders_path = Path(data_dir) / orders_file\n",
        "    with open(orders_path, 'r') as f:\n",
        "        data['orders'] = json.load(f)\n",
        "\n",
        "    # Load logistics\n",
        "    logistics_path = Path(data_dir) / logistics_file\n",
        "    with open(logistics_path, 'r') as f:\n",
        "        data['logistics'] = json.load(f)\n",
        "\n",
        "    # Load marketing signals\n",
        "    marketing_path = Path(data_dir) / marketing_signals_file\n",
        "    with open(marketing_path, 'r') as f:\n",
        "        data['marketing_signals'] = json.load(f)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_decision_rules(data_dir: str, filename: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load orchestrator decision rules from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "        # The file contains both JSON and Python code, extract just the JSON\n",
        "        if 'decision_rules_json =' in content:\n",
        "            # Extract the JSON dictionary\n",
        "            start = content.find('decision_rules_json = {')\n",
        "            if start != -1:\n",
        "                # Find the matching closing brace\n",
        "                brace_count = 0\n",
        "                i = start + len('decision_rules_json = ')\n",
        "                while i < len(content):\n",
        "                    if content[i] == '{':\n",
        "                        brace_count += 1\n",
        "                    elif content[i] == '}':\n",
        "                        brace_count -= 1\n",
        "                        if brace_count == 0:\n",
        "                            json_str = content[start + len('decision_rules_json = '):i+1]\n",
        "                            return eval(json_str)  # Safe eval for JSON-like dict\n",
        "        # Fallback: try to parse as JSON\n",
        "        return json.loads(content)\n",
        "\n",
        "def build_agent_lookup(agents: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Create fast lookup dictionary for agents.\"\"\"\n",
        "    return {agent_id: agent_data for agent_id, agent_data in agents.items()}\n",
        "\n",
        "\n",
        "def build_scenario_lookup(scenarios: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Create fast lookup dictionary for scenarios.\"\"\"\n",
        "    return {s['scenario_id']: s for s in scenarios}\n",
        "\n",
        "\n",
        "def get_customer_data(customer_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get customer data by ID.\"\"\"\n",
        "    customers = supporting_data.get('customers', [])\n",
        "    return next((c for c in customers if c.get('customer_id') == customer_id), None)\n",
        "\n",
        "\n",
        "def get_order_data(order_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get order data by ID.\"\"\"\n",
        "    orders = supporting_data.get('orders', [])\n",
        "    return next((o for o in orders if o.get('order_id') == order_id), None)\n",
        "\n",
        "\n",
        "def get_logistics_data(order_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get logistics data for an order.\"\"\"\n",
        "    logistics = supporting_data.get('logistics', {})\n",
        "    for carrier, orders in logistics.items():\n",
        "        if order_id in orders:\n",
        "            return orders[order_id]\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "836cfFfToSpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# EaaS Orchestrator Utilities — Data Loading and System Inputs\n",
        "\n",
        "This section contains a set of **utility functions** used by the Evaluation-as-a-Service (EaaS) Orchestrator to load data required for evaluation runs.\n",
        "\n",
        "These functions may appear simple at first glance, but they play a critical role in making the system **reliable, auditable, and scalable**. They define how evaluation inputs enter the system and ensure that every evaluation run starts from a clean, well-defined foundation.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Data Loading Deserves Its Own Layer\n",
        "\n",
        "In production systems, many failures do not come from logic errors — they come from **bad or inconsistent inputs**.\n",
        "\n",
        "By separating data loading into dedicated utilities, the orchestrator gains:\n",
        "\n",
        "* predictable inputs\n",
        "* reusable data pipelines\n",
        "* clearer debugging paths\n",
        "* stronger guarantees around reproducibility\n",
        "\n",
        "This design choice treats data as a first-class concern rather than an afterthought.\n",
        "\n",
        "---\n",
        "\n",
        "## Journey Scenarios: The Test Cases\n",
        "\n",
        "```python\n",
        "load_journey_scenarios(...)\n",
        "```\n",
        "\n",
        "Journey scenarios define the **situations being tested**. Each scenario represents a realistic business interaction, such as a customer asking about a delayed order or requesting a refund.\n",
        "\n",
        "Loading these scenarios from a structured JSON file ensures that:\n",
        "\n",
        "* evaluations are repeatable\n",
        "* scenarios can be versioned and reviewed\n",
        "* business teams can modify test cases without changing code\n",
        "\n",
        "This keeps evaluation aligned with real customer experiences instead of abstract benchmarks.\n",
        "\n",
        "---\n",
        "\n",
        "## Specialist Agents: The Subjects of Evaluation\n",
        "\n",
        "```python\n",
        "load_specialist_agents(...)\n",
        "```\n",
        "\n",
        "Specialist agents represent the AI systems being evaluated. Loading them from configuration files allows the orchestrator to:\n",
        "\n",
        "* evaluate different agents using the same framework\n",
        "* swap agents in and out without refactoring logic\n",
        "* compare performance across agent roles\n",
        "\n",
        "This makes the evaluation system adaptable as agent ecosystems evolve.\n",
        "\n",
        "---\n",
        "\n",
        "## Supporting Data: Context Matters\n",
        "\n",
        "```python\n",
        "load_supporting_data(...)\n",
        "```\n",
        "\n",
        "Supporting data provides the **context** agents need to behave realistically. This includes:\n",
        "\n",
        "* customer records\n",
        "* order information\n",
        "* logistics data\n",
        "* marketing signals\n",
        "\n",
        "Rather than hard-coding this data, it is loaded dynamically so that:\n",
        "\n",
        "* evaluations reflect real operational conditions\n",
        "* datasets can be refreshed or expanded over time\n",
        "* simulations remain grounded in business reality\n",
        "\n",
        "This is especially important for evaluating decision-making agents, not just text generation.\n",
        "\n",
        "---\n",
        "\n",
        "## Decision Rules: Making Expectations Explicit\n",
        "\n",
        "```python\n",
        "load_decision_rules(...)\n",
        "```\n",
        "\n",
        "Decision rules define how outputs should be interpreted and validated. These rules encode business logic such as:\n",
        "\n",
        "* acceptable resolution paths\n",
        "* policy constraints\n",
        "* escalation criteria\n",
        "\n",
        "By loading decision rules from an external file, the system separates:\n",
        "\n",
        "* **what the business expects**\n",
        "  from\n",
        "* **how the system evaluates those expectations**\n",
        "\n",
        "This allows governance standards to evolve independently from execution logic.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters for Scale and Governance\n",
        "\n",
        "These loading utilities may look simple, but together they establish an important principle:\n",
        "\n",
        "> **Evaluations should be deterministic, explainable, and repeatable.**\n",
        "\n",
        "From a business perspective, this enables:\n",
        "\n",
        "* consistent evaluation results across environments\n",
        "* easier audits and compliance reviews\n",
        "* faster iteration without introducing hidden variables\n",
        "* clearer ownership over evaluation inputs\n",
        "\n",
        "In short, this layer ensures that when the orchestrator evaluates an agent, everyone can agree on **what data was used and why**.\n",
        "\n",
        "---\n",
        "\n",
        "## Design Philosophy in Practice\n",
        "\n",
        "This utilities layer reflects a broader design philosophy used throughout the EaaS system:\n",
        "\n",
        "* configuration over hard-coding\n",
        "* separation of concerns\n",
        "* transparency over convenience\n",
        "\n",
        "These choices reduce long-term risk and make the system easier to trust as it grows more complex.\n",
        "\n"
      ],
      "metadata": {
        "id": "qQVFcWBPaYD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Data as a First-Class Citizen\n",
        "\n",
        "This set of utility functions reinforces an important design principle used throughout the EaaS Orchestrator:\n",
        "\n",
        "**Data is treated as a first-class citizen, not a side effect.**\n",
        "\n",
        "That idea shows up clearly in how agent information, scenario definitions, and supporting business data are handled.\n",
        "\n",
        "---\n",
        "\n",
        "## What “First-Class” Means in Practice\n",
        "\n",
        "In many AI systems, data is:\n",
        "\n",
        "* embedded directly in prompts\n",
        "* passed loosely between components\n",
        "* accessed inconsistently\n",
        "* difficult to inspect or validate\n",
        "\n",
        "In this orchestrator, data is handled differently. It is:\n",
        "\n",
        "* loaded explicitly\n",
        "* structured intentionally\n",
        "* accessed through clear interfaces\n",
        "* separated from execution logic\n",
        "\n",
        "This makes data **visible, reusable, and trustworthy**.\n",
        "\n",
        "---\n",
        "\n",
        "## Fast Lookups Without Hidden Logic\n",
        "\n",
        "```python\n",
        "build_agent_lookup(...)\n",
        "build_scenario_lookup(...)\n",
        "```\n",
        "\n",
        "These functions convert raw lists into lookup dictionaries keyed by IDs.\n",
        "\n",
        "At a glance, this looks like a simple performance optimization — but the design intent goes deeper.\n",
        "\n",
        "By normalizing how agents and scenarios are accessed:\n",
        "\n",
        "* every part of the system refers to the same source of truth\n",
        "* identifiers become stable references\n",
        "* behavior stays consistent across evaluation runs\n",
        "\n",
        "This avoids situations where different parts of the system interpret the same data differently.\n",
        "\n",
        "---\n",
        "\n",
        "## Context Is Data, Not Guesswork\n",
        "\n",
        "```python\n",
        "get_customer_data(...)\n",
        "get_order_data(...)\n",
        "get_logistics_data(...)\n",
        "```\n",
        "\n",
        "These functions retrieve supporting data that provides **context** for evaluations.\n",
        "\n",
        "Rather than baking assumptions into agent logic, the orchestrator:\n",
        "\n",
        "* looks up customer records\n",
        "* retrieves order details\n",
        "* pulls logistics status dynamically\n",
        "\n",
        "This ensures that agents are evaluated using the same information they would have access to in real operations.\n",
        "\n",
        "Context becomes something the system **knows**, not something it invents.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters for Evaluation\n",
        "\n",
        "When evaluating agent behavior, it is important to distinguish between:\n",
        "\n",
        "* agent reasoning failures\n",
        "* incomplete or incorrect data\n",
        "* mismatched expectations\n",
        "\n",
        "By treating data as a first-class component:\n",
        "\n",
        "* evaluation failures can be traced back to their source\n",
        "* incorrect behavior can be explained, not guessed at\n",
        "* scenarios remain grounded in business reality\n",
        "\n",
        "This dramatically improves the quality of evaluation results.\n",
        "\n",
        "---\n",
        "\n",
        "## First-Class Data Enables Better Accountability\n",
        "\n",
        "Because data is:\n",
        "\n",
        "* structured\n",
        "* addressable by ID\n",
        "* loaded from known sources\n",
        "\n",
        "the system can answer questions like:\n",
        "\n",
        "* What customer information was available at the time?\n",
        "* What did the logistics system report for this order?\n",
        "* Was the agent missing context, or did it misuse it?\n",
        "\n",
        "That level of clarity is essential for audits, reviews, and continuous improvement.\n",
        "\n",
        "---\n",
        "\n",
        "## Supporting Scale and Change\n",
        "\n",
        "Treating data as first-class also makes the system easier to evolve:\n",
        "\n",
        "* new data sources can be added without changing core logic\n",
        "* existing datasets can be updated independently\n",
        "* scenarios can grow more complex without breaking evaluations\n",
        "\n",
        "As agent systems scale, this separation becomes the difference between manageable growth and fragile complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## A Subtle but Important Design Choice\n",
        "\n",
        "These utilities may appear small, but they enforce a powerful pattern:\n",
        "\n",
        "> **Agents reason.\n",
        "> Data informs.\n",
        "> Evaluation measures.**\n",
        "\n",
        "Each role is clear, and none are mixed together.\n",
        "\n",
        "That clarity is what allows the orchestrator to remain explainable, testable, and trustworthy over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "ezkz-5MUb6GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Structure Is What Creates Data Integrity\n",
        "\n",
        "These utilities do more than make data easier to access. They establish **structure and consistency**, which is what ultimately creates data integrity.\n",
        "\n",
        "Every time data enters the system:\n",
        "\n",
        "* it is loaded the same way\n",
        "* accessed through the same interfaces\n",
        "* referenced using the same identifiers\n",
        "* shaped into predictable structures\n",
        "\n",
        "This removes ambiguity from the very beginning of the evaluation process.\n",
        "\n",
        "---\n",
        "\n",
        "## Consistency Upstream Enables Reliability Downstream\n",
        "\n",
        "When data is prepared consistently at the start, every downstream component benefits:\n",
        "\n",
        "* agents receive the same type of context every time\n",
        "* evaluators compare like-for-like outputs\n",
        "* scoring logic operates on stable inputs\n",
        "* reports reflect real differences, not data noise\n",
        "\n",
        "Downstream systems do not need to guess how to interpret inputs. They can rely on them.\n",
        "\n",
        "---\n",
        "\n",
        "## Standardization Reduces Hidden Failure Modes\n",
        "\n",
        "Many AI failures are subtle. They do not come from obvious bugs, but from:\n",
        "\n",
        "* missing fields\n",
        "* inconsistent identifiers\n",
        "* partial context\n",
        "* silent data mismatches\n",
        "\n",
        "By standardizing how data is loaded and accessed, these utilities eliminate entire classes of hidden failure modes before evaluation even begins.\n",
        "\n",
        "That makes results more trustworthy and easier to explain.\n",
        "\n",
        "---\n",
        "\n",
        "## Integrity Enables Meaningful Evaluation\n",
        "\n",
        "Evaluation only works when everyone agrees on what the system saw.\n",
        "\n",
        "Because these utilities enforce:\n",
        "\n",
        "* consistent data access patterns\n",
        "* clear boundaries between data and logic\n",
        "* stable representations of customers, orders, and scenarios\n",
        "\n",
        "evaluation results reflect **agent behavior**, not accidental differences in inputs.\n",
        "\n",
        "This is essential when comparing:\n",
        "\n",
        "* agents to each other\n",
        "* versions over time\n",
        "* performance across environments\n",
        "\n",
        "---\n",
        "\n",
        "## Effortless Handling Is the Goal\n",
        "\n",
        "Downstream components are simpler because of this early attention to detail.\n",
        "\n",
        "Execution logic can focus on:\n",
        "\n",
        "* running evaluations\n",
        "* measuring outcomes\n",
        "* detecting trends\n",
        "\n",
        "Scoring and reporting layers can assume:\n",
        "\n",
        "* inputs are complete\n",
        "* structures are predictable\n",
        "* identifiers are reliable\n",
        "\n",
        "That simplicity is not accidental — it is earned through careful data design upfront.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters in Business Terms\n",
        "\n",
        "From a business perspective, this approach:\n",
        "\n",
        "* reduces operational risk\n",
        "* lowers debugging and maintenance costs\n",
        "* improves confidence in metrics\n",
        "* supports scaling without fragility\n",
        "\n",
        "When leaders look at evaluation results, they can trust that the numbers reflect reality — not data inconsistencies.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Pattern at Work\n",
        "\n",
        "This is a recurring pattern throughout the EaaS Orchestrator:\n",
        "\n",
        "> **Strong foundations make higher-level insights possible.**\n",
        "\n",
        "By treating data as a first-class citizen, the system earns the right to produce reliable evaluations, meaningful trends, and defensible reports over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "oyQIl7uGctvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_agent_execution(\n",
        "    agent_id: str,\n",
        "    scenario: Dict[str, Any],\n",
        "    supporting_data: Dict[str, Any],\n",
        "    agents: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Simulate agent execution (MVP: Rule-based simulation).\n",
        "\n",
        "    In a real implementation, this would call the actual agent.\n",
        "    For MVP, we simulate based on expected behavior.\n",
        "    \"\"\"\n",
        "    agent = agents.get(agent_id)\n",
        "    if not agent:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": f\"Agent {agent_id} not found\",\n",
        "            \"output\": None\n",
        "        }\n",
        "\n",
        "    # MVP: Simple rule-based simulation\n",
        "    # In production, this would be an actual API call to the agent\n",
        "\n",
        "    # Get relevant data\n",
        "    customer_id = scenario.get('customer_id')\n",
        "    order_id = scenario.get('order_id')\n",
        "\n",
        "    customer = get_customer_data(customer_id, supporting_data)\n",
        "    order = get_order_data(order_id, supporting_data)\n",
        "    logistics = get_logistics_data(order_id, supporting_data)\n",
        "\n",
        "    # Simulate agent response based on agent type\n",
        "    if agent_id == 'shipping_update_agent':\n",
        "        output = {\n",
        "            \"status\": \"shipping_update\",\n",
        "            \"carrier\": logistics.get('carrier') if logistics else \"Unknown\",\n",
        "            \"current_status\": logistics.get('status') if logistics else \"unknown\",\n",
        "            \"estimated_delivery\": logistics.get('estimated_delivery') if logistics else \"Unknown\",\n",
        "            \"details\": logistics.get('details') if logistics else \"No tracking information available\"\n",
        "        }\n",
        "    elif agent_id == 'refund_agent':\n",
        "        # Simulate refund calculation\n",
        "        order = get_order_data(order_id, supporting_data)\n",
        "        items = order.get('items', []) if order else []\n",
        "        # Simple refund calculation (in real system, would use actual pricing)\n",
        "        refund_amount = len(items) * 25.0  # Placeholder\n",
        "        output = {\n",
        "            \"status\": \"refund_issued\",\n",
        "            \"refund_amount\": refund_amount,\n",
        "            \"refunded_at\": datetime.now().isoformat(),\n",
        "            \"notes\": \"Refund processed successfully.\"\n",
        "        }\n",
        "    elif agent_id == 'apology_message_agent':\n",
        "        output = {\n",
        "            \"status\": \"apology_message\",\n",
        "            \"message\": \"We're very sorry for the inconvenience with your order. We're taking steps to resolve this as quickly as possible.\"\n",
        "        }\n",
        "    elif agent_id == 'escalation_agent':\n",
        "        issue_type = scenario.get('expected_issue_type', 'unknown')\n",
        "        priority = \"high\" if issue_type in ['lost_package', 'item_not_received'] else \"medium\"\n",
        "        output = {\n",
        "            \"status\": \"escalated\",\n",
        "            \"priority\": priority,\n",
        "            \"assigned_to\": \"tier_2_support\",\n",
        "            \"notes\": \"Issue escalated for manual review.\"\n",
        "        }\n",
        "    else:\n",
        "        output = {\n",
        "            \"status\": \"unknown\",\n",
        "            \"message\": f\"Agent {agent_id} executed\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"status\": \"completed\",\n",
        "        \"output\": output,\n",
        "        \"execution_time_seconds\": 0.5  # Simulated\n",
        "    }"
      ],
      "metadata": {
        "id": "3Ju5lMiwoh1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Simulated Execution as a Controlled Testing Environment\n",
        "\n",
        "The `simulate_agent_execution` function represents the **execution layer** of the Evaluation-as-a-Service Orchestrator. Its role is to run an agent against a scenario and return a structured result that can be evaluated, scored, and audited.\n",
        "\n",
        "In this implementation, execution is **intentionally simulated** rather than delegated to live agent APIs.\n",
        "\n",
        "This is a deliberate design choice.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Simulation Is a Strength, Not a Shortcut\n",
        "\n",
        "In early or controlled evaluation environments, calling live agents introduces noise:\n",
        "\n",
        "* network latency\n",
        "* external service failures\n",
        "* changing models or prompts\n",
        "* inconsistent runtime conditions\n",
        "\n",
        "By simulating execution in a rule-based way, the orchestrator creates a **stable testing environment** where:\n",
        "\n",
        "* inputs are controlled\n",
        "* behavior is predictable\n",
        "* evaluation logic can be validated independently\n",
        "\n",
        "This allows the evaluation framework itself to be tested and trusted before introducing real-world variability.\n",
        "\n",
        "---\n",
        "\n",
        "## Execution Still Respects Real-World Structure\n",
        "\n",
        "Even though execution is simulated, it follows the same structure a production system would use.\n",
        "\n",
        "The function:\n",
        "\n",
        "* identifies the target agent\n",
        "* retrieves relevant customer, order, and logistics data\n",
        "* produces structured outputs aligned with each agent’s role\n",
        "* returns execution metadata such as status and response time\n",
        "\n",
        "This ensures the evaluation pipeline behaves the same way whether execution is simulated or real.\n",
        "\n",
        "---\n",
        "\n",
        "## Context Is Pulled, Not Assumed\n",
        "\n",
        "Before generating outputs, the function retrieves:\n",
        "\n",
        "* customer data\n",
        "* order data\n",
        "* logistics status\n",
        "\n",
        "This reinforces an important system principle:\n",
        "\n",
        "**Agents operate on data, not assumptions.**\n",
        "\n",
        "By explicitly pulling context, the system ensures that execution — even simulated execution — mirrors how real agents would reason in production.\n",
        "\n",
        "---\n",
        "\n",
        "## Agent-Specific Behavior Is Explicit\n",
        "\n",
        "Each agent type produces outputs appropriate to its role:\n",
        "\n",
        "* shipping updates include carrier and delivery details\n",
        "* refunds include amounts and timestamps\n",
        "* apologies return customer-facing messages\n",
        "* escalations include priority and ownership\n",
        "\n",
        "This keeps behavior:\n",
        "\n",
        "* understandable\n",
        "* inspectable\n",
        "* easy to extend\n",
        "\n",
        "It also ensures that evaluation focuses on **role-appropriate outcomes**, not generic responses.\n",
        "\n",
        "---\n",
        "\n",
        "## Structured Outputs Enable Fair Evaluation\n",
        "\n",
        "Every execution returns a consistent structure:\n",
        "\n",
        "* execution status\n",
        "* output payload\n",
        "* execution time\n",
        "\n",
        "This consistency is critical. It allows downstream components to:\n",
        "\n",
        "* score performance without special cases\n",
        "* compare agents fairly\n",
        "* aggregate metrics across runs\n",
        "\n",
        "Evaluation logic does not need to guess what an agent meant — it evaluates what the agent produced.\n",
        "\n",
        "---\n",
        "\n",
        "## Clean Failure Modes Are Part of the Design\n",
        "\n",
        "If an agent is missing or misconfigured, the function returns a clear failure state.\n",
        "\n",
        "This prevents silent errors and ensures that:\n",
        "\n",
        "* failures are explicit\n",
        "* evaluation results remain trustworthy\n",
        "* missing agents do not contaminate metrics\n",
        "\n",
        "Clear failure signaling is essential for reliable reporting and governance.\n",
        "\n",
        "---\n",
        "\n",
        "## Designed to Transition to Production Execution\n",
        "\n",
        "The simulation layer is intentionally isolated.\n",
        "\n",
        "In a production system, this function can be replaced with:\n",
        "\n",
        "* live API calls\n",
        "* async execution\n",
        "* parallel processing\n",
        "* retries and timeouts\n",
        "\n",
        "Because the interface and outputs are already defined, the rest of the evaluation system does not need to change.\n",
        "\n",
        "This makes the architecture future-proof.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters for the Bigger System\n",
        "\n",
        "This execution layer reinforces several key principles:\n",
        "\n",
        "* controlled inputs enable trustworthy evaluation\n",
        "* structured outputs enable fair scoring\n",
        "* isolation enables safe iteration\n",
        "* realism without randomness enables confidence\n",
        "\n",
        "Together, these principles allow the orchestrator to answer an important question reliably:\n",
        "\n",
        "**“How did the agent behave under known conditions?”**\n",
        "\n",
        "That clarity is what makes everything downstream — scoring, health metrics, trend analysis, and reporting — meaningful.\n",
        "\n"
      ],
      "metadata": {
        "id": "HpLtp-PIfUm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxBCeliHoPOA"
      },
      "outputs": [],
      "source": [
        "def score_evaluation(\n",
        "    evaluation: Dict[str, Any],\n",
        "    expected_outcome: str,\n",
        "    expected_resolution_path: List[str],\n",
        "    scoring_weights: Dict[str, float],\n",
        "    pass_threshold: float\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Score an evaluation by comparing actual output to expected.\n",
        "\n",
        "    MVP: Rule-based scoring. Future: LLM-as-a-judge scoring.\n",
        "    \"\"\"\n",
        "    actual_output = evaluation.get('actual_output', {})\n",
        "    execution_status = evaluation.get('status', 'failed')\n",
        "\n",
        "    if execution_status != 'completed':\n",
        "        return {\n",
        "            \"correctness_score\": 0.0,\n",
        "            \"response_time_score\": 0.0,\n",
        "            \"output_quality_score\": 0.0,\n",
        "            \"overall_score\": 0.0,\n",
        "            \"passed\": False,\n",
        "            \"issues\": [f\"Execution failed: {evaluation.get('error', 'Unknown error')}\"]\n",
        "        }\n",
        "\n",
        "    # Score correctness (matches expected outcome)\n",
        "    correctness_score = 1.0\n",
        "    issues = []\n",
        "\n",
        "    # Check if output status matches expected outcome type\n",
        "    output_status = actual_output.get('status', '')\n",
        "    if expected_outcome == 'provide_delivery_update' and output_status != 'shipping_update':\n",
        "        correctness_score -= 0.3\n",
        "        issues.append(\"Output status doesn't match expected outcome type\")\n",
        "    elif expected_outcome == 'issue_refund_and_notify_customer' and output_status != 'refund_issued':\n",
        "        correctness_score -= 0.3\n",
        "        issues.append(\"Output status doesn't match expected outcome type\")\n",
        "    elif expected_outcome == 'acknowledge_delay_and_update_eta' and output_status not in ['shipping_update', 'apology_message']:\n",
        "        correctness_score -= 0.2\n",
        "        issues.append(\"Output doesn't match expected outcome type\")\n",
        "\n",
        "    correctness_score = max(0.0, correctness_score)\n",
        "\n",
        "    # Score response time\n",
        "    execution_time = evaluation.get('execution_time_seconds', 10.0)\n",
        "    response_time_threshold = 2.0  # From config\n",
        "    if execution_time <= response_time_threshold:\n",
        "        response_time_score = 1.0\n",
        "    elif execution_time <= response_time_threshold * 2:\n",
        "        response_time_score = 0.7\n",
        "    elif execution_time <= response_time_threshold * 3:\n",
        "        response_time_score = 0.4\n",
        "    else:\n",
        "        response_time_score = 0.1\n",
        "        issues.append(f\"Response time too slow: {execution_time}s\")\n",
        "\n",
        "    # Score output quality (structure/format)\n",
        "    output_quality_score = 1.0\n",
        "    if not isinstance(actual_output, dict):\n",
        "        output_quality_score = 0.0\n",
        "        issues.append(\"Output is not a dictionary\")\n",
        "    elif 'status' not in actual_output:\n",
        "        output_quality_score = 0.5\n",
        "        issues.append(\"Output missing 'status' field\")\n",
        "    elif len(actual_output) < 2:\n",
        "        output_quality_score = 0.7\n",
        "        issues.append(\"Output has minimal fields\")\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = (\n",
        "        correctness_score * scoring_weights.get('correctness', 0.5) +\n",
        "        response_time_score * scoring_weights.get('response_time', 0.2) +\n",
        "        output_quality_score * scoring_weights.get('output_quality', 0.3)\n",
        "    )\n",
        "\n",
        "    passed = overall_score >= pass_threshold\n",
        "\n",
        "    return {\n",
        "        \"correctness_score\": correctness_score,\n",
        "        \"response_time_score\": response_time_score,\n",
        "        \"output_quality_score\": output_quality_score,\n",
        "        \"overall_score\": overall_score,\n",
        "        \"passed\": passed,\n",
        "        \"issues\": issues\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Scoring: Turning AI Behavior into Measurable Signals\n",
        "\n",
        "The `score_evaluation` function is where raw agent behavior is translated into **clear, decision-ready metrics**.\n",
        "\n",
        "Instead of treating agent outputs as subjective or “good enough,” this function enforces a structured comparison between:\n",
        "\n",
        "* what the agent actually produced\n",
        "* what was expected\n",
        "* how quickly it responded\n",
        "* how usable the output was\n",
        "\n",
        "This is the step that turns AI from an opaque system into something that can be **measured, tracked, and improved**.\n",
        "\n",
        "---\n",
        "\n",
        "## A Simple, Transparent Scoring Model\n",
        "\n",
        "Rather than using a complex or opaque scoring system, this implementation starts with **clear, rule-based logic**.\n",
        "\n",
        "That choice is intentional.\n",
        "\n",
        "Rule-based scoring:\n",
        "\n",
        "* is easy to understand\n",
        "* is easy to explain\n",
        "* is easy to audit\n",
        "* establishes trust early\n",
        "\n",
        "More advanced techniques, such as LLM-as-a-judge, can be layered on later without changing the structure of the evaluation pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Clean Failure Handling Comes First\n",
        "\n",
        "The function begins by checking whether execution completed successfully.\n",
        "\n",
        "If execution failed:\n",
        "\n",
        "* all scores are set to zero\n",
        "* the evaluation is marked as failed\n",
        "* the reason is recorded explicitly\n",
        "\n",
        "This ensures that failures are never silently ignored and that metrics remain honest.\n",
        "\n",
        "---\n",
        "\n",
        "## Correctness: Did the Agent Do the Right Thing?\n",
        "\n",
        "Correctness scoring checks whether the agent’s output aligns with the **expected outcome** for the scenario.\n",
        "\n",
        "Rather than scoring vague similarity, the system:\n",
        "\n",
        "* looks at the agent’s declared output status\n",
        "* compares it against the expected resolution type\n",
        "* deducts points when behavior deviates\n",
        "\n",
        "This makes correctness:\n",
        "\n",
        "* explicit\n",
        "* explainable\n",
        "* scenario-aware\n",
        "\n",
        "Issues are recorded alongside the score, making it easy to understand *why* points were lost.\n",
        "\n",
        "---\n",
        "\n",
        "## Response Time: Did It Perform Fast Enough?\n",
        "\n",
        "Response time is scored against defined thresholds:\n",
        "\n",
        "* fast responses receive full credit\n",
        "* slower responses receive progressively lower scores\n",
        "* excessively slow responses are flagged as issues\n",
        "\n",
        "This reflects real operational expectations, where speed often matters as much as accuracy — especially in customer-facing workflows.\n",
        "\n",
        "---\n",
        "\n",
        "## Output Quality: Is the Result Usable?\n",
        "\n",
        "Output quality evaluates whether the agent’s response is:\n",
        "\n",
        "* structured correctly\n",
        "* complete enough to act on\n",
        "* formatted in a predictable way\n",
        "\n",
        "Rather than judging content subjectively, this score focuses on **structural quality**, which is essential for downstream automation and integrations.\n",
        "\n",
        "A response that cannot be reliably parsed or consumed is treated as lower quality, even if it is technically correct.\n",
        "\n",
        "---\n",
        "\n",
        "## Weighted Scoring Reflects Business Priorities\n",
        "\n",
        "Each dimension contributes to the final score using configurable weights.\n",
        "\n",
        "This allows organizations to tune evaluation criteria based on what matters most:\n",
        "\n",
        "* accuracy-heavy workflows\n",
        "* latency-sensitive use cases\n",
        "* integration-driven systems that demand clean structure\n",
        "\n",
        "The scoring logic remains the same — only priorities change.\n",
        "\n",
        "---\n",
        "\n",
        "## Pass/Fail as a Clear Decision Boundary\n",
        "\n",
        "The final score is compared against a configurable pass threshold.\n",
        "\n",
        "This creates a simple but powerful outcome:\n",
        "\n",
        "* the evaluation either meets the standard or it doesn’t\n",
        "\n",
        "That clarity enables:\n",
        "\n",
        "* automated gating\n",
        "* trend analysis\n",
        "* escalation logic\n",
        "* executive reporting\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Layer Is So Important\n",
        "\n",
        "This function does more than score outputs — it establishes **accountability**.\n",
        "\n",
        "Because scoring is:\n",
        "\n",
        "* consistent\n",
        "* transparent\n",
        "* repeatable\n",
        "\n",
        "changes in performance over time can be trusted. When scores drop, it reflects real behavioral changes, not shifting evaluation criteria.\n",
        "\n",
        "---\n",
        "\n",
        "## A Foundation for Growth\n",
        "\n",
        "This scoring layer is designed to evolve:\n",
        "\n",
        "* rules can be refined\n",
        "* weights can be adjusted\n",
        "* LLM-based judging can be introduced\n",
        "\n",
        "But even as sophistication increases, the core structure remains the same.\n",
        "\n",
        "That stability is what allows the orchestrator to support:\n",
        "\n",
        "* longitudinal performance tracking\n",
        "* drift detection\n",
        "* SLA enforcement\n",
        "* governance workflows\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Picture\n",
        "\n",
        "At this point in the pipeline, AI behavior has been converted into numbers, thresholds, and decisions.\n",
        "\n",
        "That conversion is what allows the system to move beyond experimentation and into **real operational use** — where performance can be tracked, compared, and trusted over time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrL8AVXAgBas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_agent_performance_summary(\n",
        "    agent_id: str,\n",
        "    evaluations: List[Dict[str, Any]],\n",
        "    scores: List[Dict[str, Any]],\n",
        "    health_thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Calculate performance summary for an agent.\"\"\"\n",
        "    agent_evaluations = [e for e in evaluations if e.get('target_agent_id') == agent_id]\n",
        "    agent_scores = [s for s in scores if s.get('target_agent_id') == agent_id]\n",
        "\n",
        "    if not agent_scores:\n",
        "        return {\n",
        "            \"agent_id\": agent_id,\n",
        "            \"total_evaluations\": 0,\n",
        "            \"passed_count\": 0,\n",
        "            \"failed_count\": 0,\n",
        "            \"average_score\": 0.0,\n",
        "            \"average_response_time\": 0.0,\n",
        "            \"health_status\": \"unknown\"\n",
        "        }\n",
        "\n",
        "    total = len(agent_scores)\n",
        "    passed = sum(1 for s in agent_scores if s.get('passed', False))\n",
        "    failed = total - passed\n",
        "    avg_score = sum(s.get('overall_score', 0.0) for s in agent_scores) / total\n",
        "\n",
        "    avg_response_time = sum(\n",
        "        e.get('execution_time_seconds', 0.0) for e in agent_evaluations\n",
        "    ) / len(agent_evaluations) if agent_evaluations else 0.0\n",
        "\n",
        "    # Determine health status\n",
        "    if avg_score >= health_thresholds.get('healthy', 0.85):\n",
        "        health_status = \"healthy\"\n",
        "    elif avg_score >= health_thresholds.get('degraded', 0.70):\n",
        "        health_status = \"degraded\"\n",
        "    else:\n",
        "        health_status = \"critical\"\n",
        "\n",
        "    return {\n",
        "        \"agent_id\": agent_id,\n",
        "        \"total_evaluations\": total,\n",
        "        \"passed_count\": passed,\n",
        "        \"failed_count\": failed,\n",
        "        \"average_score\": avg_score,\n",
        "        \"average_response_time\": avg_response_time,\n",
        "        \"health_status\": health_status\n",
        "    }"
      ],
      "metadata": {
        "id": "yPxIxMAColNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# From Individual Scores to Agent Health\n",
        "\n",
        "The `calculate_agent_performance_summary` function is responsible for one of the most important transformations in the system:\n",
        "\n",
        "**It turns many individual evaluations into a clear, high-level health signal for a single agent.**\n",
        "\n",
        "This is where detailed metrics become **manageable insight**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Aggregation Matters\n",
        "\n",
        "Raw evaluation data is useful for engineers, but it quickly becomes overwhelming at scale. Business leaders don’t want to inspect dozens of scenario scores — they want to know:\n",
        "\n",
        "* Is this agent reliable?\n",
        "* Is it getting better or worse?\n",
        "* Should we trust it in production?\n",
        "* Does it need attention?\n",
        "\n",
        "This function answers those questions in a structured, repeatable way.\n",
        "\n",
        "---\n",
        "\n",
        "## A Fair and Focused View Per Agent\n",
        "\n",
        "The function begins by isolating:\n",
        "\n",
        "* evaluations associated with a specific agent\n",
        "* scores generated from those evaluations\n",
        "\n",
        "This ensures that each agent is judged **only on its own behavior**, under consistent conditions.\n",
        "\n",
        "No cross-contamination.\n",
        "No hidden averaging across unrelated agents.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Performance Metrics\n",
        "\n",
        "For each agent, the summary computes:\n",
        "\n",
        "* **Total evaluations**\n",
        "  How much evidence exists for this agent’s performance.\n",
        "\n",
        "* **Passed vs failed evaluations**\n",
        "  A simple reliability signal that’s easy to reason about.\n",
        "\n",
        "* **Average score**\n",
        "  A normalized performance measure that can be tracked over time.\n",
        "\n",
        "* **Average response time**\n",
        "  An operational metric that highlights latency trends.\n",
        "\n",
        "These metrics create a balanced view of both **quality and efficiency**.\n",
        "\n",
        "---\n",
        "\n",
        "## Health Status: Translating Metrics into Meaning\n",
        "\n",
        "The most important output of this function is the **health status**:\n",
        "\n",
        "```python\n",
        "healthy | degraded | critical\n",
        "```\n",
        "\n",
        "Rather than exposing raw numbers alone, the system classifies agent performance using explicit thresholds defined in configuration.\n",
        "\n",
        "This creates a shared language:\n",
        "\n",
        "* “Healthy” agents are safe to rely on\n",
        "* “Degraded” agents require monitoring or tuning\n",
        "* “Critical” agents need intervention\n",
        "\n",
        "Health status is not subjective — it is computed deterministically from agreed-upon standards.\n",
        "\n",
        "---\n",
        "\n",
        "## Determinism Builds Confidence\n",
        "\n",
        "Because health classification is:\n",
        "\n",
        "* rule-based\n",
        "* threshold-driven\n",
        "* reproducible\n",
        "\n",
        "the same agent, evaluated under the same conditions, will always receive the same health status.\n",
        "\n",
        "That consistency is essential for:\n",
        "\n",
        "* trend analysis\n",
        "* escalation workflows\n",
        "* performance reviews\n",
        "* executive reporting\n",
        "\n",
        "When health changes, it reflects real behavioral change — not scoring variability.\n",
        "\n",
        "---\n",
        "\n",
        "## Designed for Long-Term Tracking\n",
        "\n",
        "This summary structure is intentionally stable and compact, making it ideal for:\n",
        "\n",
        "* storing over time\n",
        "* plotting trends\n",
        "* feeding dashboards\n",
        "* triggering alerts\n",
        "\n",
        "As evaluations run repeatedly, each agent accumulates a performance history that can answer questions like:\n",
        "\n",
        "* Is this agent improving?\n",
        "* Is performance drifting?\n",
        "* Did something change after a deployment?\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Is a Natural Capstone Utility\n",
        "\n",
        "This function brings together several earlier design choices:\n",
        "\n",
        "* standardized data\n",
        "* deterministic scoring\n",
        "* explicit thresholds\n",
        "* reproducible metrics\n",
        "\n",
        "The result is a **single, trustworthy representation of agent health** that can be consumed by both technical and non-technical stakeholders.\n",
        "\n",
        "---\n",
        "\n",
        "## The Bigger Pattern\n",
        "\n",
        "This utility reflects a recurring pattern in the EaaS Orchestrator:\n",
        "\n",
        "> **Detailed signals feed deterministic aggregation,\n",
        "> which produces simple, actionable insight.**\n",
        "\n",
        "That pattern is what allows the system to scale from individual tests to organizational oversight.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Leaders Care About This Layer\n",
        "\n",
        "From a business perspective, this function enables:\n",
        "\n",
        "* clear ownership of agent performance\n",
        "* faster decision-making\n",
        "* reduced ambiguity during incidents\n",
        "* confidence in automation at scale\n",
        "\n",
        "It transforms AI agents from experimental components into **managed, measurable systems**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KkpJcomMt6HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pejzdT6dt878"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}