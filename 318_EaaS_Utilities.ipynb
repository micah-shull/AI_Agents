{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX1Mq3GKTgb5rLUUrMhYWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/318_EaaS_Utilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities for EaaS Orchestrator Agent"
      ],
      "metadata": {
        "id": "B1xOKq5jpV5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Utilities for EaaS Orchestrator Agent\n",
        "\n",
        "Reusable business logic for data loading, evaluation execution, and scoring.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def load_journey_scenarios(data_dir: str, filename: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load journey scenarios from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def load_specialist_agents(data_dir: str, filename: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load specialist agents configuration from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def load_supporting_data(\n",
        "    data_dir: str,\n",
        "    customers_file: str,\n",
        "    orders_file: str,\n",
        "    logistics_file: str,\n",
        "    marketing_signals_file: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Load all supporting data files.\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # Load customers\n",
        "    customers_path = Path(data_dir) / customers_file\n",
        "    with open(customers_path, 'r') as f:\n",
        "        data['customers'] = json.load(f)\n",
        "\n",
        "    # Load orders\n",
        "    orders_path = Path(data_dir) / orders_file\n",
        "    with open(orders_path, 'r') as f:\n",
        "        data['orders'] = json.load(f)\n",
        "\n",
        "    # Load logistics\n",
        "    logistics_path = Path(data_dir) / logistics_file\n",
        "    with open(logistics_path, 'r') as f:\n",
        "        data['logistics'] = json.load(f)\n",
        "\n",
        "    # Load marketing signals\n",
        "    marketing_path = Path(data_dir) / marketing_signals_file\n",
        "    with open(marketing_path, 'r') as f:\n",
        "        data['marketing_signals'] = json.load(f)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_decision_rules(data_dir: str, filename: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load orchestrator decision rules from JSON file.\"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "        # The file contains both JSON and Python code, extract just the JSON\n",
        "        if 'decision_rules_json =' in content:\n",
        "            # Extract the JSON dictionary\n",
        "            start = content.find('decision_rules_json = {')\n",
        "            if start != -1:\n",
        "                # Find the matching closing brace\n",
        "                brace_count = 0\n",
        "                i = start + len('decision_rules_json = ')\n",
        "                while i < len(content):\n",
        "                    if content[i] == '{':\n",
        "                        brace_count += 1\n",
        "                    elif content[i] == '}':\n",
        "                        brace_count -= 1\n",
        "                        if brace_count == 0:\n",
        "                            json_str = content[start + len('decision_rules_json = '):i+1]\n",
        "                            return eval(json_str)  # Safe eval for JSON-like dict\n",
        "        # Fallback: try to parse as JSON\n",
        "        return json.loads(content)\n"
      ],
      "metadata": {
        "id": "836cfFfToSpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent_lookup(agents: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Create fast lookup dictionary for agents.\"\"\"\n",
        "    return {agent_id: agent_data for agent_id, agent_data in agents.items()}\n",
        "\n",
        "\n",
        "def build_scenario_lookup(scenarios: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Create fast lookup dictionary for scenarios.\"\"\"\n",
        "    return {s['scenario_id']: s for s in scenarios}\n",
        "\n",
        "\n",
        "def get_customer_data(customer_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get customer data by ID.\"\"\"\n",
        "    customers = supporting_data.get('customers', [])\n",
        "    return next((c for c in customers if c.get('customer_id') == customer_id), None)\n",
        "\n",
        "\n",
        "def get_order_data(order_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get order data by ID.\"\"\"\n",
        "    orders = supporting_data.get('orders', [])\n",
        "    return next((o for o in orders if o.get('order_id') == order_id), None)\n",
        "\n",
        "\n",
        "def get_logistics_data(order_id: str, supporting_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"Get logistics data for an order.\"\"\"\n",
        "    logistics = supporting_data.get('logistics', {})\n",
        "    for carrier, orders in logistics.items():\n",
        "        if order_id in orders:\n",
        "            return orders[order_id]\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "Bz70hOxcobeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_agent_execution(\n",
        "    agent_id: str,\n",
        "    scenario: Dict[str, Any],\n",
        "    supporting_data: Dict[str, Any],\n",
        "    agents: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Simulate agent execution (MVP: Rule-based simulation).\n",
        "\n",
        "    In a real implementation, this would call the actual agent.\n",
        "    For MVP, we simulate based on expected behavior.\n",
        "    \"\"\"\n",
        "    agent = agents.get(agent_id)\n",
        "    if not agent:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": f\"Agent {agent_id} not found\",\n",
        "            \"output\": None\n",
        "        }\n",
        "\n",
        "    # MVP: Simple rule-based simulation\n",
        "    # In production, this would be an actual API call to the agent\n",
        "\n",
        "    # Get relevant data\n",
        "    customer_id = scenario.get('customer_id')\n",
        "    order_id = scenario.get('order_id')\n",
        "\n",
        "    customer = get_customer_data(customer_id, supporting_data)\n",
        "    order = get_order_data(order_id, supporting_data)\n",
        "    logistics = get_logistics_data(order_id, supporting_data)\n",
        "\n",
        "    # Simulate agent response based on agent type\n",
        "    if agent_id == 'shipping_update_agent':\n",
        "        output = {\n",
        "            \"status\": \"shipping_update\",\n",
        "            \"carrier\": logistics.get('carrier') if logistics else \"Unknown\",\n",
        "            \"current_status\": logistics.get('status') if logistics else \"unknown\",\n",
        "            \"estimated_delivery\": logistics.get('estimated_delivery') if logistics else \"Unknown\",\n",
        "            \"details\": logistics.get('details') if logistics else \"No tracking information available\"\n",
        "        }\n",
        "    elif agent_id == 'refund_agent':\n",
        "        # Simulate refund calculation\n",
        "        order = get_order_data(order_id, supporting_data)\n",
        "        items = order.get('items', []) if order else []\n",
        "        # Simple refund calculation (in real system, would use actual pricing)\n",
        "        refund_amount = len(items) * 25.0  # Placeholder\n",
        "        output = {\n",
        "            \"status\": \"refund_issued\",\n",
        "            \"refund_amount\": refund_amount,\n",
        "            \"refunded_at\": datetime.now().isoformat(),\n",
        "            \"notes\": \"Refund processed successfully.\"\n",
        "        }\n",
        "    elif agent_id == 'apology_message_agent':\n",
        "        output = {\n",
        "            \"status\": \"apology_message\",\n",
        "            \"message\": \"We're very sorry for the inconvenience with your order. We're taking steps to resolve this as quickly as possible.\"\n",
        "        }\n",
        "    elif agent_id == 'escalation_agent':\n",
        "        issue_type = scenario.get('expected_issue_type', 'unknown')\n",
        "        priority = \"high\" if issue_type in ['lost_package', 'item_not_received'] else \"medium\"\n",
        "        output = {\n",
        "            \"status\": \"escalated\",\n",
        "            \"priority\": priority,\n",
        "            \"assigned_to\": \"tier_2_support\",\n",
        "            \"notes\": \"Issue escalated for manual review.\"\n",
        "        }\n",
        "    else:\n",
        "        output = {\n",
        "            \"status\": \"unknown\",\n",
        "            \"message\": f\"Agent {agent_id} executed\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"status\": \"completed\",\n",
        "        \"output\": output,\n",
        "        \"execution_time_seconds\": 0.5  # Simulated\n",
        "    }"
      ],
      "metadata": {
        "id": "3Ju5lMiwoh1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxBCeliHoPOA"
      },
      "outputs": [],
      "source": [
        "def score_evaluation(\n",
        "    evaluation: Dict[str, Any],\n",
        "    expected_outcome: str,\n",
        "    expected_resolution_path: List[str],\n",
        "    scoring_weights: Dict[str, float],\n",
        "    pass_threshold: float\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Score an evaluation by comparing actual output to expected.\n",
        "\n",
        "    MVP: Rule-based scoring. Future: LLM-as-a-judge scoring.\n",
        "    \"\"\"\n",
        "    actual_output = evaluation.get('actual_output', {})\n",
        "    execution_status = evaluation.get('status', 'failed')\n",
        "\n",
        "    if execution_status != 'completed':\n",
        "        return {\n",
        "            \"correctness_score\": 0.0,\n",
        "            \"response_time_score\": 0.0,\n",
        "            \"output_quality_score\": 0.0,\n",
        "            \"overall_score\": 0.0,\n",
        "            \"passed\": False,\n",
        "            \"issues\": [f\"Execution failed: {evaluation.get('error', 'Unknown error')}\"]\n",
        "        }\n",
        "\n",
        "    # Score correctness (matches expected outcome)\n",
        "    correctness_score = 1.0\n",
        "    issues = []\n",
        "\n",
        "    # Check if output status matches expected outcome type\n",
        "    output_status = actual_output.get('status', '')\n",
        "    if expected_outcome == 'provide_delivery_update' and output_status != 'shipping_update':\n",
        "        correctness_score -= 0.3\n",
        "        issues.append(\"Output status doesn't match expected outcome type\")\n",
        "    elif expected_outcome == 'issue_refund_and_notify_customer' and output_status != 'refund_issued':\n",
        "        correctness_score -= 0.3\n",
        "        issues.append(\"Output status doesn't match expected outcome type\")\n",
        "    elif expected_outcome == 'acknowledge_delay_and_update_eta' and output_status not in ['shipping_update', 'apology_message']:\n",
        "        correctness_score -= 0.2\n",
        "        issues.append(\"Output doesn't match expected outcome type\")\n",
        "\n",
        "    correctness_score = max(0.0, correctness_score)\n",
        "\n",
        "    # Score response time\n",
        "    execution_time = evaluation.get('execution_time_seconds', 10.0)\n",
        "    response_time_threshold = 2.0  # From config\n",
        "    if execution_time <= response_time_threshold:\n",
        "        response_time_score = 1.0\n",
        "    elif execution_time <= response_time_threshold * 2:\n",
        "        response_time_score = 0.7\n",
        "    elif execution_time <= response_time_threshold * 3:\n",
        "        response_time_score = 0.4\n",
        "    else:\n",
        "        response_time_score = 0.1\n",
        "        issues.append(f\"Response time too slow: {execution_time}s\")\n",
        "\n",
        "    # Score output quality (structure/format)\n",
        "    output_quality_score = 1.0\n",
        "    if not isinstance(actual_output, dict):\n",
        "        output_quality_score = 0.0\n",
        "        issues.append(\"Output is not a dictionary\")\n",
        "    elif 'status' not in actual_output:\n",
        "        output_quality_score = 0.5\n",
        "        issues.append(\"Output missing 'status' field\")\n",
        "    elif len(actual_output) < 2:\n",
        "        output_quality_score = 0.7\n",
        "        issues.append(\"Output has minimal fields\")\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = (\n",
        "        correctness_score * scoring_weights.get('correctness', 0.5) +\n",
        "        response_time_score * scoring_weights.get('response_time', 0.2) +\n",
        "        output_quality_score * scoring_weights.get('output_quality', 0.3)\n",
        "    )\n",
        "\n",
        "    passed = overall_score >= pass_threshold\n",
        "\n",
        "    return {\n",
        "        \"correctness_score\": correctness_score,\n",
        "        \"response_time_score\": response_time_score,\n",
        "        \"output_quality_score\": output_quality_score,\n",
        "        \"overall_score\": overall_score,\n",
        "        \"passed\": passed,\n",
        "        \"issues\": issues\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_agent_performance_summary(\n",
        "    agent_id: str,\n",
        "    evaluations: List[Dict[str, Any]],\n",
        "    scores: List[Dict[str, Any]],\n",
        "    health_thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Calculate performance summary for an agent.\"\"\"\n",
        "    agent_evaluations = [e for e in evaluations if e.get('target_agent_id') == agent_id]\n",
        "    agent_scores = [s for s in scores if s.get('target_agent_id') == agent_id]\n",
        "\n",
        "    if not agent_scores:\n",
        "        return {\n",
        "            \"agent_id\": agent_id,\n",
        "            \"total_evaluations\": 0,\n",
        "            \"passed_count\": 0,\n",
        "            \"failed_count\": 0,\n",
        "            \"average_score\": 0.0,\n",
        "            \"average_response_time\": 0.0,\n",
        "            \"health_status\": \"unknown\"\n",
        "        }\n",
        "\n",
        "    total = len(agent_scores)\n",
        "    passed = sum(1 for s in agent_scores if s.get('passed', False))\n",
        "    failed = total - passed\n",
        "    avg_score = sum(s.get('overall_score', 0.0) for s in agent_scores) / total\n",
        "\n",
        "    avg_response_time = sum(\n",
        "        e.get('execution_time_seconds', 0.0) for e in agent_evaluations\n",
        "    ) / len(agent_evaluations) if agent_evaluations else 0.0\n",
        "\n",
        "    # Determine health status\n",
        "    if avg_score >= health_thresholds.get('healthy', 0.85):\n",
        "        health_status = \"healthy\"\n",
        "    elif avg_score >= health_thresholds.get('degraded', 0.70):\n",
        "        health_status = \"degraded\"\n",
        "    else:\n",
        "        health_status = \"critical\"\n",
        "\n",
        "    return {\n",
        "        \"agent_id\": agent_id,\n",
        "        \"total_evaluations\": total,\n",
        "        \"passed_count\": passed,\n",
        "        \"failed_count\": failed,\n",
        "        \"average_score\": avg_score,\n",
        "        \"average_response_time\": avg_response_time,\n",
        "        \"health_status\": health_status\n",
        "    }"
      ],
      "metadata": {
        "id": "yPxIxMAColNb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}