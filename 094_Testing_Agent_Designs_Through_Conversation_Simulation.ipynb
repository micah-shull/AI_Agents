{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7sxXIGOX8lltSNl9N/yPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/094_Testing_Agent_Designs_Through_Conversation_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Agent Designs Through Conversation Simulation\n",
        "\n",
        "The big focus for this lecture is **testing your GAME design through simulation before coding** ‚Äî basically a ‚Äúdress rehearsal‚Äù for your agent.\n",
        "\n",
        "Here‚Äôs what you should concentrate on:\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Concepts to Learn**\n",
        "\n",
        "1. **Simulation Before Implementation**\n",
        "\n",
        "   * You‚Äôre verifying that your *Goals, Actions, Memory, and Environment* are actually sufficient to achieve the intended results.\n",
        "   * This is the low-cost, low-risk phase to discover issues early.\n",
        "\n",
        "2. **Structured Simulation Setup**\n",
        "\n",
        "   * Start with a clear prompt that defines:\n",
        "\n",
        "     * **Goals**: What the agent is trying to achieve.\n",
        "     * **Actions**: The tools it has available.\n",
        "   * Tell the LLM to output only the next action and wait for you to simulate the result.\n",
        "\n",
        "3. **Observing Agent Reasoning**\n",
        "\n",
        "   * Watch the order in which it uses tools.\n",
        "   * Experiment with how much context (metadata, extra structure) helps it make better decisions.\n",
        "\n",
        "4. **Refining Goals and Tools**\n",
        "\n",
        "   * Use the simulation to spot vague instructions or unclear tool descriptions.\n",
        "   * Update them until the agent behaves as intended.\n",
        "\n",
        "5. **Memory in Simulation**\n",
        "\n",
        "   * The chat naturally mirrors the agent‚Äôs message list memory.\n",
        "   * You can see how well it keeps context as history grows.\n",
        "\n",
        "6. **Learning from Failures**\n",
        "\n",
        "   * Introduce errors, malformed data, or missing information to see how it recovers.\n",
        "   * This informs how you‚Äôll build **robust error handling** in the real agent.\n",
        "\n",
        "7. **Termination Testing**\n",
        "\n",
        "   * Ensure your agent knows when to stop ‚Äî simulate different stopping conditions.\n",
        "\n",
        "8. **Rapid Iteration**\n",
        "\n",
        "   * You can test dozens of scenarios in minutes without writing a line of ‚Äúreal‚Äù code.\n",
        "\n",
        "9. **Agent Reflection**\n",
        "\n",
        "   * Ask the LLM what tools, details, or rules it wishes it had ‚Äî often it will point out gaps you missed.\n",
        "\n",
        "10. **Example Library Building**\n",
        "\n",
        "    * Save both **good** and **bad** simulation runs.\n",
        "    * Later, these become *training material* for your agent prompts and test cases.\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Most Valuable Takeaway:**\n",
        "This stage isn‚Äôt about *how* to implement, it‚Äôs about *if* the design works ‚Äî and you‚Äôll catch 80% of the problems before touching your first line of Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "wCIdaTag8wsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet python-dotenv openai"
      ],
      "metadata": {
        "id": "1vXNIgrWAnZO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxdIo1h_7gEU",
        "outputId": "5aebf34a-ddd5-4fde-a5e8-61baed674582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] ACTION ‚Üí list_project_files({})\n",
            "[1] RESULT ‚Üê {'ok': True, 'files': ['main.py', 'utils.py', 'data_processor.py'], 'total': 3, 'dir': '/project'}\n",
            "\n",
            "[2] ACTION ‚Üí read_project_file({'filename': 'main.py'})\n",
            "[2] RESULT ‚Üê {'ok': True, 'filename': 'main.py', 'content': 'def main(): pass\\n# TODO: improve error handling'}\n",
            "\n",
            "[3] ACTION ‚Üí ask_user_approval({'proposal': \"Enhance error handling in 'main.py' by adding a try-except block to catch and handle exceptions.\"})\n",
            "[3] RESULT ‚Üê {'ok': True, 'approved': True, 'note': 'Approved'}\n",
            "\n",
            "[4] ACTION ‚Üí edit_project_file({'filename': 'main.py', 'changes': \"def main():\\n    try:\\n        pass\\n    except Exception as e:\\n        print(f'An error occurred: {e}')\"})\n",
            "[4] RESULT ‚Üê {'ok': True, 'result': 'Applied small edits to main.py'}\n",
            "\n",
            "[5] ACTION ‚Üí terminate({'message': 'Task completed successfully with the enhancement implemented.'})\n",
            "[5] RESULT ‚Üê {'ok': True, 'message': 'Task completed successfully with the enhancement implemented.'}\n",
            "\n",
            "Simulation ended by agent.\n"
          ]
        }
      ],
      "source": [
        "# === GAME Simulation: Proactive Coder (no real I/O, just mock results) ===\n",
        "import os, json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# 1) Model setup\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# 2) GAME: Goals & Actions (abstract)\n",
        "GOALS = [\n",
        "    \"Identify small, self-contained code enhancements\",\n",
        "    \"Keep interfaces stable\",\n",
        "    \"Only proceed with user approval\"\n",
        "]\n",
        "\n",
        "ACTIONS = [\n",
        "    # We simulate these; no real side effects here\n",
        "    {\"name\":\"list_project_files\", \"desc\":\"List project files\"},\n",
        "    {\"name\":\"read_project_file\", \"desc\":\"Read a file's content\", \"params\":{\"filename\":\"string\"}},\n",
        "    {\"name\":\"ask_user_approval\", \"desc\":\"Ask user to approve a proposal\", \"params\":{\"proposal\":\"string\"}},\n",
        "    {\"name\":\"edit_project_file\", \"desc\":\"Apply small edits to a file\", \"params\":{\"filename\":\"string\",\"changes\":\"string\"}},\n",
        "    {\"name\":\"terminate\", \"desc\":\"Finish with a summary message\", \"params\":{\"message\":\"string\"}}\n",
        "]\n",
        "\n",
        "# 3) Function-calling tool specs for structured actions\n",
        "TOOLS = [\n",
        "    {\"type\":\"function\",\"function\":{\n",
        "        \"name\":\"list_project_files\",\n",
        "        \"description\":\"Return a list of project file names.\",\n",
        "        \"parameters\":{\"type\":\"object\",\"properties\":{},\"required\":[]}\n",
        "    }},\n",
        "    {\"type\":\"function\",\"function\":{\n",
        "        \"name\":\"read_project_file\",\n",
        "        \"description\":\"Read a file's content.\",\n",
        "        \"parameters\":{\"type\":\"object\",\"properties\":{\"filename\":{\"type\":\"string\"}},\"required\":[\"filename\"]}\n",
        "    }},\n",
        "    {\"type\":\"function\",\"function\":{\n",
        "        \"name\":\"ask_user_approval\",\n",
        "        \"description\":\"Ask the user to approve a proposal.\",\n",
        "        \"parameters\":{\"type\":\"object\",\"properties\":{\"proposal\":{\"type\":\"string\"}},\"required\":[\"proposal\"]}\n",
        "    }},\n",
        "    {\"type\":\"function\",\"function\":{\n",
        "        \"name\":\"edit_project_file\",\n",
        "        \"description\":\"Apply a small, self-contained edit to a file.\",\n",
        "        \"parameters\":{\"type\":\"object\",\"properties\":{\"filename\":{\"type\":\"string\"},\"changes\":{\"type\":\"string\"}},\"required\":[\"filename\",\"changes\"]}\n",
        "    }},\n",
        "    {\"type\":\"function\",\"function\":{\n",
        "        \"name\":\"terminate\",\n",
        "        \"description\":\"Conclude the task and stop.\",\n",
        "        \"parameters\":{\"type\":\"object\",\"properties\":{\"message\":{\"type\":\"string\"}},\"required\":[\"message\"]}\n",
        "    }},\n",
        "]\n",
        "\n",
        "# 4) Minimal simulation prompt (what + how)\n",
        "SYSTEM = f\"\"\"\n",
        "You are simulating a Proactive Coder agent using the GAME framework.\n",
        "\n",
        "GOALS:\n",
        "- {GOALS[0]}\n",
        "- {GOALS[1]}\n",
        "- {GOALS[2]}\n",
        "\n",
        "ACTIONS (choose ONE per step):\n",
        "- list_project_files()\n",
        "- read_project_file(filename)\n",
        "- ask_user_approval(proposal)\n",
        "- edit_project_file(filename, changes)\n",
        "- terminate(message)\n",
        "\n",
        "Rules:\n",
        "- Always return a function call if a tool is needed; otherwise, you may terminate.\n",
        "- Keep actions small and self-contained.\n",
        "- Seek approval before edits.\n",
        "\"\"\"\n",
        "\n",
        "# 5) Mock environment: provide canned results to test reasoning\n",
        "def mock_env_dispatch(name: str, args: dict):\n",
        "    # You can mutate these to test different scenarios (errors, big projects, etc.)\n",
        "    PROJECT_FILES = [\"main.py\", \"utils.py\", \"data_processor.py\"]\n",
        "    MOCK_CONTENT = {\n",
        "        \"main.py\": \"def main(): pass\\n# TODO: improve error handling\",\n",
        "        \"utils.py\": \"def add(a,b): return a+b\\n# no input validation\",\n",
        "        \"data_processor.py\": \"def clean(df): return df.dropna()\"\n",
        "    }\n",
        "\n",
        "    if name == \"list_project_files\":\n",
        "        return {\"ok\": True, \"files\": PROJECT_FILES, \"total\": len(PROJECT_FILES), \"dir\": \"/project\"}\n",
        "\n",
        "    if name == \"read_project_file\":\n",
        "        fn = args.get(\"filename\",\"\")\n",
        "        if fn not in MOCK_CONTENT:\n",
        "            # JIT guidance to test recovery\n",
        "            return {\"ok\": False, \"error\": f\"{fn} not found\", \"hint\": \"Pick from list_project_files()\", \"retryable\": True}\n",
        "        return {\"ok\": True, \"filename\": fn, \"content\": MOCK_CONTENT[fn]}\n",
        "\n",
        "    if name == \"ask_user_approval\":\n",
        "        proposal = args.get(\"proposal\",\"\")\n",
        "        # Simulate a user approval flow (approve small changes only)\n",
        "        approved = \"validate\" in proposal or \"error handling\" in proposal or \"docstring\" in proposal\n",
        "        return {\"ok\": True, \"approved\": approved, \"note\": (\"Approved\" if approved else \"Rejected: too large\")}\n",
        "\n",
        "    if name == \"edit_project_file\":\n",
        "        fn = args.get(\"filename\",\"\")\n",
        "        changes = args.get(\"changes\",\"\")\n",
        "        if not fn or not changes:\n",
        "            return {\"ok\": False, \"error\":\"Missing filename/changes\", \"retryable\": True}\n",
        "        if len(changes) > 400:\n",
        "            return {\"ok\": False, \"error\":\"Change set too large\", \"hint\":\"Propose smaller edits\", \"retryable\": True}\n",
        "        return {\"ok\": True, \"result\": f\"Applied small edits to {fn}\"}\n",
        "\n",
        "    if name == \"terminate\":\n",
        "        return {\"ok\": True, \"message\": args.get(\"message\",\"Done.\")}\n",
        "\n",
        "    return {\"ok\": False, \"error\": f\"Unknown tool {name}\"}\n",
        "\n",
        "# 6) Conversation loop: model chooses action ‚Üí we simulate result ‚Üí feed back ‚Üí repeat\n",
        "def simulate(task: str, max_steps: int = 8, verbose: bool = True):\n",
        "    messages = [\n",
        "        {\"role\":\"system\", \"content\": SYSTEM},\n",
        "        {\"role\":\"user\", \"content\": f\"Task: {task}\"}\n",
        "    ]\n",
        "    for step in range(1, max_steps+1):\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            tools=TOOLS,\n",
        "            tool_choice=\"auto\"\n",
        "        )\n",
        "        msg = resp.choices[0].message\n",
        "\n",
        "        if msg.tool_calls:  # model decided to call a tool\n",
        "            for call in msg.tool_calls:\n",
        "                name = call.function.name\n",
        "                args = json.loads(call.function.arguments) if call.function.arguments else {}\n",
        "                if verbose:\n",
        "                    print(f\"\\n[{step}] ACTION ‚Üí {name}({args})\")\n",
        "\n",
        "                # Simulate environment result\n",
        "                result = mock_env_dispatch(name, args)\n",
        "                if verbose:\n",
        "                    print(f\"[{step}] RESULT ‚Üê {result}\")\n",
        "\n",
        "                # Feed result back (as a tool result or user feedback)\n",
        "                messages.append({\"role\":\"assistant\", \"tool_calls\":[call]})\n",
        "                messages.append({\"role\":\"tool\", \"tool_call_id\": call.id, \"content\": json.dumps(result)})\n",
        "\n",
        "                # Optional: stop if terminated\n",
        "                if name == \"terminate\":\n",
        "                    if verbose: print(\"\\nSimulation ended by agent.\")\n",
        "                    return messages\n",
        "        else:\n",
        "            # No tool call; the model might be giving a final message\n",
        "            if verbose:\n",
        "                print(f\"\\n[{step}] TEXT ‚Üí {msg.content}\")\n",
        "            messages.append({\"role\":\"assistant\", \"content\": msg.content})\n",
        "            break\n",
        "    return messages\n",
        "\n",
        "# 7) Try a few ‚Äúdress rehearsal‚Äù scenarios\n",
        "_ = simulate(\"Scan the project, propose one small improvement, get approval, apply it, then terminate.\")\n",
        "# To test failure handling, try:\n",
        "# _ = simulate(\"Read README.md then terminate.\")  # file doesn't exist ‚Üí see recovery behavior\n",
        "# _ = simulate(\"Make sweeping refactors across all files.\")  # should seek approval / keep small\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Walk Through\n",
        "\n",
        "Here‚Äôs how that simulation harness maps directly to **‚ÄúTesting Agent Designs Through Conversation Simulation‚Äù**:\n",
        "\n",
        "# 1) Define the agent up front (design-first)\n",
        "\n",
        "* **`SYSTEM` prompt** embeds the **Goals** and **Actions** exactly like the lecture suggests.\n",
        "* You tell the model: *pick ONE action per step, keep edits small, seek approval.*\n",
        "* This is your ‚Äúdress rehearsal script‚Äù‚Äîbefore any real I/O exists.\n",
        "\n",
        "# 2) Use function calling to force structured actions\n",
        "\n",
        "* **`TOOLS`** (the function-calling specs) list the actions the agent may take:\n",
        "  `list_project_files`, `read_project_file`, `ask_user_approval`, `edit_project_file`, `terminate`.\n",
        "* Because we pass `tools=...` and `tool_choice=\"auto\"`, the model returns **`tool_calls`** (name + JSON args) instead of free text.\n",
        "* This avoids the ‚Äúplease output JSON‚Äù prompt fragility and keeps the simulation crisp.\n",
        "\n",
        "# 3) Mock the Environment to iterate fast\n",
        "\n",
        "* **`mock_env_dispatch(name, args)`** is the ‚Äústage set‚Äù: canned project files, canned file contents, and **deliberately crafted errors/hints**.\n",
        "* You can flip behaviors instantly (e.g., file missing, too-large changes) to see how the agent responds.\n",
        "* No real files, no real APIs ‚Üí you can test dozens of scenarios in minutes.\n",
        "\n",
        "# 4) The conversation loop mirrors the agent loop\n",
        "\n",
        "* **`simulate(...)`** sends `{system, user}` ‚Üí gets **a tool call** ‚Üí **feeds back a result** ‚Üí repeats.\n",
        "* Tool **results** are added as **`role: \"tool\"`** messages tied to the specific tool\\_call‚Äîthis is the API-native way to give the agent its ‚Äúworld feedback.‚Äù\n",
        "* This **chat history** *is* your memory‚Äîexactly the list-of-messages memory you‚Äôll use in a real agent.\n",
        "\n",
        "# 5) Observe and refine decision quality\n",
        "\n",
        "* Because each step prints **ACTION ‚Üí RESULT**, you can watch the agent‚Äôs reasoning unfold:\n",
        "\n",
        "  * Does it **list files first** before reading?\n",
        "  * Does it **seek approval** before editing?\n",
        "  * Does it **propose small changes** (as instructed)?\n",
        "* You can change the **result format** (add `total`, `dir`, etc.) to see if richer metadata improves choices‚Äîjust like the lecture recommends.\n",
        "\n",
        "# 6) Test error handling and recovery behavior\n",
        "\n",
        "* The mock returns **structured failures** with **hints** (e.g., `\"retryable\": true` or ‚ÄúPick from `list_project_files()`‚Äù).\n",
        "* You can inject malformed/missing data to see if the agent recovers or derails.\n",
        "* This directly informs how you‚Äôll design **robust tool errors** in the real environment.\n",
        "\n",
        "# 7) Practice termination and guardrails\n",
        "\n",
        "* Include a **`terminate`** action and test that the agent knows when to stop.\n",
        "* You can also cap `max_steps` to prevent runaway loops and study whether the agent converges under your rules.\n",
        "\n",
        "# 8) Build your example library\n",
        "\n",
        "* Interesting traces (good and bad) can be copy-pasted into your notebook as **training cues** for prompts and future tests.\n",
        "* Over time, this becomes your ‚Äúplaybook‚Äù of patterns to encourage or avoid.\n",
        "\n",
        "---\n",
        "\n",
        "## What this buys you (why simulate first)\n",
        "\n",
        "* **Catches design gaps early** (unclear tools, vague goals, missing hints).\n",
        "* **De-risks implementation**‚Äîyou‚Äôre not guessing whether the plan works; you‚Äôve watched it work.\n",
        "* **Speeds iteration**‚Äîchange the mock or the rules, rerun, observe.\n",
        "* **Directly transferable**‚Äîthe chat loop you see here becomes the real agent loop once you swap the mock environment for a real one.\n",
        "\n",
        "If you want, we can try two quick runs next:\n",
        "\n",
        "* A **happy path** (list ‚Üí read ‚Üí propose ‚Üí ask approval ‚Üí edit ‚Üí terminate).\n",
        "* A **failure path** (ask to read a missing file, see the recovery).\n",
        "\n",
        "That‚Äôll make the simulation value crystal clear.\n"
      ],
      "metadata": {
        "id": "y4XgNUrgBqb6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pab3EiaB1FA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}