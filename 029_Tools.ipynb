{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcAFSCuZNz3u98SovA5oBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/029_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† Agent-Building Recipe\n",
        "\n",
        "### **Step 1: Define the Agent‚Äôs Purpose (üìã Objective)**\n",
        "\n",
        "* *‚ÄúWhat do I want this agent to be able to do?‚Äù*\n",
        "\n",
        "  * Example: List files, read content, summarize docs, search for keywords, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Implement the Python Tool Functions (üîß Actions)**\n",
        "\n",
        "* These are the actual Python functions that carry out the agent‚Äôs work.\n",
        "\n",
        "  * `list_files()`, `read_file(filename)`, `search_file_names(keyword)`, etc.\n",
        "* Test these independently first to make sure they work.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Describe the Tools with JSON Schemas (üì¶ Tool Interfaces)**\n",
        "\n",
        "* For each Python function, create a **tool schema**:\n",
        "\n",
        "  * What is it called?\n",
        "  * What does it do?\n",
        "  * What parameters does it expect?\n",
        "\n",
        "‚úÖ Must include:\n",
        "\n",
        "```json\n",
        "\"type\": \"function\"\n",
        "```\n",
        "\n",
        "And must be structured using JSON Schema (`\"parameters\"`, `\"properties\"`, `\"required\"`).\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Assemble the Tool List (üß∞ Tools Array)**\n",
        "\n",
        "* Combine the tool schemas into a `tools = [...]` list.\n",
        "* Each entry in the list wraps the schema like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"type\": \"function\",\n",
        "  \"function\": your_tool_schema\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Build the Tool Router (üö¶ Dispatcher)**\n",
        "\n",
        "* Create a function like:\n",
        "\n",
        "```python\n",
        "def tool_router(tool_name, args):\n",
        "    if tool_name == \"read_file\":\n",
        "        return read_file(args[\"filename\"])\n",
        "    ...\n",
        "```\n",
        "\n",
        "* This lets you **translate LLM tool requests into real Python function calls**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Prompt the LLM with Tool Awareness (üß† Planning)**\n",
        "\n",
        "* Set up your LLM chat:\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Find docs about memory\"}],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "* Let the LLM choose a tool and call it with arguments.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 7: Detect Tool Calls and Handle Execution (ü§ñ Run Tools)**\n",
        "\n",
        "* Look at `response.choices[0].message.tool_calls`\n",
        "* If a tool is requested, call it using `tool_router(...)`\n",
        "* Feed results back into the conversation if needed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 8 (Optional): Add Memory or Looping (üß† Persistent Context)**\n",
        "\n",
        "* If your agent needs memory of past actions, summarize or record history.\n",
        "* You can loop tool calls or chain decisions based on results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 9: Review and Iterate (üîÅ Tweak & Refine)**\n",
        "\n",
        "* Refine your tool descriptions to improve how well the LLM uses them.\n",
        "* Add examples, tweak the prompts, test for edge cases.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ End Result:\n",
        "\n",
        "You now have an LLM-powered agent that:\n",
        "\n",
        "* Understands user goals\n",
        "* Selects the right tool\n",
        "* Executes actions in Python\n",
        "* Returns results intelligently\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1-D-EDfL17Zr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQTrPSXCsy23",
        "outputId": "21b1c038-5944-4cef-9760-458faf827c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/765.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import & Environ Set Up\n"
      ],
      "metadata": {
        "id": "CEhCEiwZ4kpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# üîπ Step 1: Imports and Setup\n",
        "source_dir = \"/content/docs_folder\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    raise FileNotFoundError(f\"üìÅ Directory not found: {source_dir}\")\n",
        "\n",
        "# List and build full file paths\n",
        "file_list = [\n",
        "    os.path.join(source_dir, f)\n",
        "    for f in os.listdir(source_dir)\n",
        "    if os.path.isfile(os.path.join(source_dir, f))\n",
        "]\n",
        "\n",
        "# Display the found files\n",
        "print(\"üìÇ Files found:\")\n",
        "for file in file_list:\n",
        "    print(\"  -\", file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noifWYZOs5FV",
        "outputId": "a34d950c-31af-423a-e560-037b8f3c19d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Files found:\n",
            "  - /content/docs_folder/004_AGENT_Tools.txt\n",
            "  - /content/docs_folder/001_PArse_the Response.txt\n",
            "  - /content/docs_folder/003_gent Feedback and Memory.txt\n",
            "  - /content/docs_folder/000_Prompting for Agents -GAIL.txt\n",
            "  - /content/docs_folder/002_Execute_the_Action.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Define and Test Python Tools"
      ],
      "metadata": {
        "id": "JildAyE34jAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Tool 1: List all .txt files\n",
        "def list_files():\n",
        "    return [os.path.basename(f) for f in file_list if f.endswith(\".txt\")]\n",
        "\n",
        "# ‚úÖ Tool 2: Read a specific file\n",
        "def read_file(filename):\n",
        "    path = os.path.join(source_dir, filename)\n",
        "    if not os.path.isfile(path):\n",
        "        return f\"‚ö†Ô∏è File not found: {filename}\"\n",
        "    with open(path, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# ‚úÖ Tool 3: Search for keyword in file names\n",
        "def search_file_names(keyword, case_sensitive=False):\n",
        "    matches = []\n",
        "    for f in file_list:\n",
        "        name = os.path.basename(f)\n",
        "        haystack = name if case_sensitive else name.lower()\n",
        "        needle = keyword if case_sensitive else keyword.lower()\n",
        "        if needle in haystack:\n",
        "            matches.append(name)\n",
        "    return matches\n",
        "\n",
        "print(\"üóÇÔ∏è All .txt files:\", list_files())\n",
        "print(\"üìñ Read sample file:\", read_file(list_files()[0]))\n",
        "print(\"üîç Search 'agent':\", search_file_names(\"agent\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFrwQaje4fiR",
        "outputId": "a57b1fd5-0654-4d98-ab88-6a0af517984e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üóÇÔ∏è All .txt files: ['004_AGENT_Tools.txt', '001_PArse_the Response.txt', '003_gent Feedback and Memory.txt', '000_Prompting for Agents -GAIL.txt', '002_Execute_the_Action.txt']\n",
            "üìñ Read sample file: \n",
            "#===========AI-Agent Tool Descriptions and Naming\n",
            "\n",
            "Describing Tools to the Agent\n",
            "\n",
            "When developing an agentic AI system, one of the most critical aspects is ensuring that the agent understands the tools it has access to. In our previous tutorial, we explored how an AI agent interacts with an environment. Now, we extend that discussion to focus on tool definition, particularly the importance of naming, parameters, and structured metadata.\n",
            "\n",
            "Example: Automating Documentation for Python Code\n",
            "Imagine we are building an AI agent that scans through all Python files in a src/ directory and automatically generates corresponding documentation files in a docs/ directory. This agent will need to:\n",
            "\n",
            "List Python files in the src/ directory.\n",
            "Read the content of each Python file.\n",
            "Write documentation files in the docs/ directory.\n",
            "Since file operations are straightforward for humans but ambiguous for an AI without context, we must clearly define these tools so the agent knows how to use them effectively.\n",
            "\n",
            "#==========Step 1: Defining a Tool with Structured Metadata\n",
            "\n",
            "A basic tool definition in Python might look like this:\n",
            "\n",
            "def list_python_files():\n",
            "    \"\"\"Returns a list of all Python files in the src/ directory.\"\"\"\n",
            "    return [f for f in os.listdir(\"src\") if f.endswith(\".py\")]\n",
            "This provides a function that retrieves all Python files in the src/ directory, but for an AI system, we need a more structured way to describe it.\n",
            "\n",
            "#===========Step 2: Using JSON Schema to Define Parameters\n",
            "When developers design APIs, they use structured documentation to describe available functions, their inputs, and their outputs. JSON Schema is a well-known format for defining APIs, making it a natural choice for AI agents as well.\n",
            "\n",
            "For example, a tool that reads a file should specify that it expects a file_path parameter of type string. JSON Schema allows us to express this in a standardized way:\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"read_file\",\n",
            "  \"description\": \"Reads the content of a specified file.\",\n",
            "  \"parameters\": {\n",
            "    \"type\": \"object\",\n",
            "    \"properties\": {\n",
            "      \"file_path\": { \"type\": \"string\" }\n",
            "    },\n",
            "    \"required\": [\"file_path\"]\n",
            "  }\n",
            "}\n",
            "Similarly, a tool for writing documentation should define that it requires a file_name and content:\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"write_doc_file\",\n",
            "  \"description\": \"Writes a documentation file to the docs/ directory.\",\n",
            "  \"parameters\": {\n",
            "    \"type\": \"object\",\n",
            "    \"properties\": {\n",
            "      \"file_name\": { \"type\": \"string\" },\n",
            "      \"content\": { \"type\": \"string\" }\n",
            "    },\n",
            "    \"required\": [\"file_name\", \"content\"]\n",
            "  }\n",
            "}\n",
            "By providing a JSON Schema for each tool:\n",
            "\n",
            "The AI can Recognize the tool‚Äôs purpose.\n",
            "The AI / Environment interface can validate input parameters before execution.\n",
            "It may look strange that multiple parameters to a function are represented as an object. When we are getting the agent to output a tool / action selection, we are going to want it to output something like this:\n",
            "\n",
            "{\n",
            "  \"tool_name\": \"read_file\",\n",
            "  \"args\": {\n",
            "    \"file_path\": \"src/file.py\"\n",
            "  }\n",
            "}\n",
            "The schema describes the overall dictionary that will be used to capture the ‚Äúargs‚Äù to the function, so it is described as an object.\n",
            "üîç Search 'agent': ['004_AGENT_Tools.txt', '000_Prompting for Agents -GAIL.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Tool 1: list_files\n",
        "\n",
        "Now that we‚Äôve built and tested the Python tools, we‚Äôll define **tool schemas in JSON** so that an LLM can understand what each tool does and how to call it.\n",
        "\n",
        "These schemas follow the [OpenAI tool calling spec](https://platform.openai.com/docs/guides/function-calling), where each tool is described using:\n",
        "\n",
        "* `name`: the function name (matches the Python function name)\n",
        "* `description`: what the tool does, in natural language\n",
        "* `parameters`: an object with:\n",
        "\n",
        "  * `type`: always `\"object\"`\n",
        "  * `properties`: dictionary of input fields with their types and descriptions\n",
        "  * `required`: list of required input fields\n"
      ],
      "metadata": {
        "id": "1vb8v2Iu5Gsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"name\": \"list_files\",\n",
        "  \"description\": \"Returns a list of all .txt files in the source directory.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {},\n",
        "    \"required\": []\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk0luY5D5EQo",
        "outputId": "07daf6d5-6f92-4ba1-c8ea-d81331125b5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'list_files',\n",
              " 'description': 'Returns a list of all .txt files in the source directory.',\n",
              " 'parameters': {'type': 'object', 'properties': {}, 'required': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Tool 2: read_file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UXlN-ZOm5JWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"name\": \"read_file\",\n",
        "  \"description\": \"Reads the contents of a specified file in the source directory.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"filename\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The name of the file to read (including .txt).\"\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"filename\"]\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSt_g95A5J9N",
        "outputId": "99e09855-4c4b-45a0-a772-e83bc050beb8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'read_file',\n",
              " 'description': 'Reads the contents of a specified file in the source directory.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'filename': {'type': 'string',\n",
              "    'description': 'The name of the file to read (including .txt).'}},\n",
              "  'required': ['filename']}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Tool 3: search_file_names"
      ],
      "metadata": {
        "id": "74XtuCFR5M9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"name\": \"search_file_names\",\n",
        "  \"description\": \"Searches for files whose names include the given keyword.\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"keyword\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The keyword to search for in the file names.\"\n",
        "      },\n",
        "      \"case_sensitive\": {\n",
        "        \"type\": \"boolean\",\n",
        "        \"description\": \"Whether the search should be case sensitive.\",\n",
        "        \"default\": False\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"keyword\"]\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpv9h0IB5Mim",
        "outputId": "6febae25-d95c-4662-af69-33f9ef1351cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'search_file_names',\n",
              " 'description': 'Searches for files whose names include the given keyword.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'keyword': {'type': 'string',\n",
              "    'description': 'The keyword to search for in the file names.'},\n",
              "   'case_sensitive': {'type': 'boolean',\n",
              "    'description': 'Whether the search should be case sensitive.',\n",
              "    'default': False}},\n",
              "  'required': ['keyword']}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining tools as individual named variables and then adding them to the `tools` list is a **very good idea** for modularity, readability, and debugging. It allows you to:\n",
        "\n",
        "### ‚úÖ Benefits of Modular Tool Definitions\n",
        "\n",
        "1. **Keep tools isolated**: You can work on or test one tool at a time.\n",
        "2. **Reuse or reorganize**: Easily reuse tools across agents or notebooks.\n",
        "3. **Improved debugging**: You can `print()` or inspect a single tool JSON by name.\n",
        "4. **Simplified diffs**: If you're using version control like Git, it's easier to see what's changed per tool.\n",
        "5. **Selective loading**: You can conditionally include tools in different toolchains.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Refactored Example\n",
        "\n",
        "Here‚Äôs how you can rewrite your tool definitions using named variables:\n",
        "\n",
        "This pattern also scales beautifully as you add more tools. You could even store each in a separate `.py` file or JSON if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "49Rv_8uV641M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Define tools individually\n",
        "\n",
        "list_files_tool = {\n",
        "    \"name\": \"list_files\",\n",
        "    \"description\": \"Lists all files in the source directory.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "read_file_tool = {\n",
        "    \"name\": \"read_file\",\n",
        "    \"description\": \"Reads the content of a specified file.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"filename\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the file to read.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"filename\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "search_file_names_tool = {\n",
        "    \"name\": \"search_file_names\",\n",
        "    \"description\": \"Searches for files whose names include the given keyword.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"keyword\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The keyword to search for in the file names.\"\n",
        "            },\n",
        "            \"case_sensitive\": {\n",
        "                \"type\": \"boolean\",\n",
        "                \"description\": \"Whether the search should be case sensitive.\",\n",
        "                \"default\": False\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"keyword\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# üß∞ Combine into master tools list\n",
        "tools = [list_files_tool, read_file_tool, search_file_names_tool]"
      ],
      "metadata": {
        "id": "kmaB3MsB5Mej"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you‚Äôve:\n",
        "\n",
        "‚úÖ Built your **Python tools** (functions that actually do the work)\n",
        "‚úÖ Defined your **JSON tool schemas** (what the LLM sees and uses)\n",
        "‚úÖ Created a modular **`tools` list** for orchestration\n",
        "\n",
        "---\n",
        "\n",
        "### üîú Next Step: Connect the Tools to the Agent\n",
        "\n",
        "We now need to wire everything together so the LLM can:\n",
        "\n",
        "1. **See the available tools** via `tools=...` when making the API call\n",
        "2. **Decide** which tool to use and provide the correct inputs (`tool_calls`)\n",
        "3. **Trigger** the correct Python function to actually do the work\n",
        "4. **Return** the result back to the LLM and continue the conversation if needed\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Here‚Äôs the Plan\n",
        "\n",
        "We‚Äôll now do the following:\n",
        "\n",
        "#### 1. Create a `tool_router` function\n",
        "\n",
        "This maps each tool name (like `\"list_files\"`) to its Python function.\n",
        "\n",
        "#### 2. Write a simple LLM call that uses tools\n",
        "\n",
        "We‚Äôll pass the `tools` list into the `client.chat.completions.create(...)` call.\n",
        "\n",
        "#### 3. Parse and execute tool calls\n",
        "\n",
        "We‚Äôll check for any `tool_calls` in the LLM response and execute the appropriate Python function using the router.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yNSZOFhG7UlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 1. tool_router: Maps tool name ‚Üí Python function\n",
        "This function dispatches the correct tool using the name the LLM returns:"
      ],
      "metadata": {
        "id": "f2jatTlS7vJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tool_router(tool_name, args):\n",
        "    if tool_name == \"list_files\":\n",
        "        return list_files()\n",
        "    elif tool_name == \"read_file\":\n",
        "        return read_file(args[\"filename\"])\n",
        "    elif tool_name == \"search_file_names\":\n",
        "        return search_file_names(\n",
        "            keyword=args[\"keyword\"],\n",
        "            case_sensitive=args.get(\"case_sensitive\", False)\n",
        "        )\n",
        "    else:\n",
        "        return f\"‚ùå Unknown tool: {tool_name}\""
      ],
      "metadata": {
        "id": "mO9NZ6g37cNm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 2. generate_agent_response:\n",
        "Sends a chat completion with tool use enabled"
      ],
      "metadata": {
        "id": "63DL6NgP7x7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_agent_response(user_input):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that helps with managing files. Use tools when needed.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        tools=tools,  # ‚Üê enables tool usage\n",
        "        tool_choice=\"auto\",  # ‚Üê LLM can choose any tool\n",
        "    )\n",
        "\n",
        "    return response.choices[0]\n"
      ],
      "metadata": {
        "id": "rL1rFiLc7nav"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. handle_tool_call:\n",
        "Executes tool call and returns result"
      ],
      "metadata": {
        "id": "dBBPuDh4722C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_tool_call(choice):\n",
        "    tool_calls = choice.message.tool_calls\n",
        "\n",
        "    if not tool_calls:\n",
        "        print(\"ü§ñ LLM Response:\\n\")\n",
        "        print(choice.message.content)\n",
        "        return\n",
        "\n",
        "    for call in tool_calls:\n",
        "        tool_name = call.function.name\n",
        "        args = json.loads(call.function.arguments)\n",
        "        print(f\"\\nüõ†Ô∏è Executing tool: {tool_name} with args: {args}\")\n",
        "        result = tool_router(tool_name, args)\n",
        "        print(f\"‚úÖ Tool Result:\\n{result}\")\n"
      ],
      "metadata": {
        "id": "oMmce-l77ppw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ 4. Test It!"
      ],
      "metadata": {
        "id": "ib5aOtrq76Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "choice = generate_agent_response(\"List the files that include 'memory'\")\n",
        "handle_tool_call(choice)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9pTv-K2U7rmN",
        "outputId": "0f5c7586-a19b-4c51-ebd1-13a868dcff4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Missing required parameter: 'tools[0].type'.\", 'type': 'invalid_request_error', 'param': 'tools[0].type', 'code': 'missing_required_parameter'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1521977202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_agent_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"List the files that include 'memory'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhandle_tool_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-1895109310.py\u001b[0m in \u001b[0;36mgenerate_agent_response\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1086\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Missing required parameter: 'tools[0].type'.\", 'type': 'invalid_request_error', 'param': 'tools[0].type', 'code': 'missing_required_parameter'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type Required\n",
        "\n",
        "\n",
        "\n",
        "The `\"type\": \"function\"` field is required because **OpenAI's function calling API expects it** as part of its tool definition format.\n",
        "\n",
        "### üîç Why it matters:\n",
        "\n",
        "When you're passing tools (functions) to the model, OpenAI's API needs to clearly distinguish:\n",
        "\n",
        "* üîß *This is a callable function* (vs. other types of tools or structured data).\n",
        "* ü§ñ *This tool can be selected and executed by the model.*\n",
        "\n",
        "So in the tool schema:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"type\": \"function\",  ‚Üê ‚úÖ tells the model this is a function\n",
        "  \"name\": \"read_file\",\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "Whenever you build tools for an agent, **you must structure them to match the expected format of the model you're using.** Here's what that means in practice:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ For OpenAI models like `gpt-4-0613`, `gpt-3.5-turbo-0613`, etc.\n",
        "\n",
        "These older models expect tools in this format:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"name\": \"tool_name\",\n",
        "  \"description\": \"What this tool does\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": { ... },\n",
        "    \"required\": [ ... ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ For **latest models** like `gpt-4-1106-preview`, `gpt-4o`, `gpt-3.5-turbo-1106`, etc.\n",
        "\n",
        "These require the **new function-style schema** with `type: \"function\"`:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\n",
        "    \"name\": \"tool_name\",\n",
        "    \"description\": \"What this tool does\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": { ... },\n",
        "      \"required\": [ ... ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Why the difference?\n",
        "\n",
        "Because OpenAI is evolving the tool-calling API to make things more modular and future-proof. Tools are no longer assumed to be just \"functions\" ‚Äî future versions might include other tool types like external APIs, files, or agents.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ† Tip for working with tools\n",
        "\n",
        "When building a toolchain:\n",
        "\n",
        "1. **Write your Python tool first and test it.**\n",
        "2. **Define the tool's schema** in the format matching the model you're using.\n",
        "3. **Pass the schema to the `tools` parameter** when calling the API.\n",
        "4. ‚úÖ Then let the model decide when and how to use it (unless you're forcing `tool_choice`).\n"
      ],
      "metadata": {
        "id": "CHdMZrvg8qG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üîÅ OpenAI APIs Can and Do Change\n",
        "\n",
        "OpenAI periodically updates:\n",
        "\n",
        "* **Model interfaces** (e.g. newer `gpt-4o` models requiring different tool formats)\n",
        "* **Tool schema expectations**\n",
        "* **Response formatting and behavior**\n",
        "\n",
        "If your code is tightly coupled to one version of the API and schema, **it can break when those assumptions change.**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Solutions to Prevent Breakage\n",
        "\n",
        "1. **üì¶ Docker or Virtual Environments**\n",
        "   Use Docker or `venv` to freeze your runtime environment (Python version, library versions like `openai`, etc.).\n",
        "   This helps you reproduce behavior consistently even years later.\n",
        "\n",
        "2. **üìå Pin API Versions and Model Names**\n",
        "   Use a specific model like `\"gpt-4-1106-preview\"` instead of just `\"gpt-4\"`.\n",
        "   If you use `\"gpt-4\"` or `\"gpt-3.5-turbo\"` without a version suffix, you‚Äôre opting into **auto-updates**, which may break your tool formatting.\n",
        "\n",
        "3. **üß™ Write Version-Aware Tool Builders**\n",
        "   Create helper functions that generate the correct schema format depending on the model you're using:\n",
        "\n",
        "   ```python\n",
        "   def make_tool_schema(name, description, parameters, model_version=\"gpt-4o\"):\n",
        "       if model_version in [\"gpt-4o\", \"gpt-4-1106-preview\"]:\n",
        "           return {\n",
        "               \"type\": \"function\",\n",
        "               \"function\": {\n",
        "                   \"name\": name,\n",
        "                   \"description\": description,\n",
        "                   \"parameters\": parameters\n",
        "               }\n",
        "           }\n",
        "       else:\n",
        "           return {\n",
        "               \"name\": name,\n",
        "               \"description\": description,\n",
        "               \"parameters\": parameters\n",
        "           }\n",
        "   ```\n",
        "\n",
        "4. **üîí Version Control Your Tool Schemas**\n",
        "   Keep your tool definitions in versioned files (e.g. `tools_v1.json`, `tools_v2.json`) so you can track what changed and why.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Bonus: Use Docker for Maximum Stability\n",
        "\n",
        "A Docker setup ensures:\n",
        "\n",
        "* Python and library versions don‚Äôt drift\n",
        "* External dependencies (like API clients) don‚Äôt suddenly introduce breaking changes\n",
        "* Deployment is reproducible across machines or cloud services\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c9wr5mRnxaKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Define tools individually\n",
        "list_files_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"list_files\",\n",
        "    \"description\": \"Lists all files in the source directory.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "read_file_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"read_file\",\n",
        "    \"description\": \"Reads the content of a specified file.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"filename\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the file to read.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"filename\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "search_file_names_tool = {\n",
        "    \"type\": \"function\",  # ‚úÖ Required by OpenAI function calling\n",
        "    \"name\": \"search_file_names\",\n",
        "    \"description\": \"Searches for files whose names include the given keyword.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"keyword\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The keyword to search for in the file names.\"\n",
        "            },\n",
        "            \"case_sensitive\": {\n",
        "                \"type\": \"boolean\",\n",
        "                \"description\": \"Whether the search should be case sensitive.\",\n",
        "                \"default\": False\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"keyword\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# router\n",
        "def tool_router(tool_name, args):\n",
        "    if tool_name == \"list_files\":\n",
        "        return list_files()\n",
        "    elif tool_name == \"read_file\":\n",
        "        return read_file(args[\"filename\"])\n",
        "    elif tool_name == \"search_file_names\":\n",
        "        return search_file_names(\n",
        "            keyword=args[\"keyword\"],\n",
        "            case_sensitive=args.get(\"case_sensitive\", False)\n",
        "        )\n",
        "    else:\n",
        "        return f\"‚ùå Unknown tool: {tool_name}\"\n",
        "\n",
        "# response\n",
        "def generate_agent_response(user_input):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that helps with managing files. Use tools when needed.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "\n",
        "    choice = response.choices[0].message\n",
        "\n",
        "    if choice.tool_calls:\n",
        "        # The LLM decided to use a tool\n",
        "        tool_call = choice.tool_calls[0]\n",
        "        return {\"type\": \"tool\", \"tool_call\": tool_call}\n",
        "    else:\n",
        "        # Regular assistant reply\n",
        "        return {\"type\": \"text\", \"content\": choice.content}\n",
        "\n",
        "def handle_tool_call(choice):\n",
        "    if choice[\"type\"] == \"tool\":\n",
        "        tool_name = choice[\"tool_call\"].function.name\n",
        "        args = json.loads(choice[\"tool_call\"].function.arguments)\n",
        "        result = tool_router(tool_name, args)\n",
        "        print(f\"üõ†Ô∏è Used Tool: {tool_name}\")\n",
        "        print(f\"üì§ Args: {args}\")\n",
        "        print(f\"üì• Result:\\n{result}\")\n",
        "    elif choice[\"type\"] == \"text\":\n",
        "        print(f\"üí¨ Assistant Response:\\n{choice['content']}\")\n",
        "    else:\n",
        "        print(\"‚ùì Unrecognized response type.\")\n",
        "\n",
        "# üß∞ Combine into master tools list\n",
        "# ‚ùå Original (Old Format ‚Äì Causes API Error)\n",
        "# tools = [list_files_tool, read_file_tool, search_file_names_tool]\n",
        "'''Why it fails:\n",
        "The new OpenAI API expects a tool definition wrapped in a dictionary with\n",
        "a \"type\": \"function\" and a \"function\" key. Without this wrapper, the API throws:\n",
        "\n",
        "\"Missing required parameter: 'tools[0].function'\"'''\n",
        "\n",
        "# ‚úÖ Updated (Correct Format ‚Äì Required by OpenAI's tools schema)\n",
        "# üß∞ Combine into master tools list using the required OpenAI tools format\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",               # ‚úÖ Declares this is a function-style tool\n",
        "        \"function\": list_files_tool       # ‚úÖ Embeds the tool definition\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": read_file_tool\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": search_file_names_tool\n",
        "    }\n",
        "]\n",
        "\n",
        "user_input = \"List all the files that contain the word 'memory'\"\n",
        "choice = generate_agent_response(user_input)\n",
        "handle_tool_call(choice)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sbqk3_RO8sZc",
        "outputId": "45ef08be-ddd4-4488-c332-d747ca897e5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Used Tool: search_file_names\n",
            "üì§ Args: {'keyword': 'memory'}\n",
            "üì• Result:\n",
            "['003_gent Feedback and Memory.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß† Key Concepts and Learnings\n",
        "\n",
        "### 1. **Separation of Concerns**\n",
        "\n",
        "* You build the **tool logic in Python** ‚Äî these are the actual implementations (`list_files()`, `read_file(filename)`, etc.).\n",
        "* You then define the **tool schema in JSON format**, which describes **what the tool does** and **what inputs it expects**, not how it works.\n",
        "\n",
        "  * This schema is what the LLM sees and uses to decide whether or not to invoke the tool.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Tool Schema Structure (OpenAI Format)**\n",
        "\n",
        "Each tool **must be wrapped like this** in the `tools` list:\n",
        "\n",
        "```python\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",        # ‚úÖ Declares this is a function-style tool\n",
        "        \"function\": tool_schema    # ‚úÖ Embeds the JSON schema defined above\n",
        "    },\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "Each schema (`tool_schema`) itself must include:\n",
        "\n",
        "* `\"name\"`: Tool name (unique string).\n",
        "* `\"description\"`: What the tool does, in natural language.\n",
        "* `\"parameters\"`: A JSON Schema definition:\n",
        "\n",
        "  * `\"type\"`: Must be `\"object\"`.\n",
        "  * `\"properties\"`: Dictionary of expected inputs.\n",
        "  * `\"required\"`: List of required argument keys.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tool Router**\n",
        "\n",
        "You need a `tool_router(tool_name, args)` function that **maps the LLM‚Äôs tool selection back to your actual Python code**.\n",
        "\n",
        "This lets you separate:\n",
        "\n",
        "* LLM *decision-making* (it picks the tool),\n",
        "* From Python *execution logic* (you do the work).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Function Calling Flow**\n",
        "\n",
        "Here‚Äôs the full flow when an agent receives a user request:\n",
        "\n",
        "1. **User provides input**\n",
        "2. LLM decides: ‚ÄúDo I need a tool?‚Äù\n",
        "3. If yes, it chooses a tool from the `tools` list and passes in arguments.\n",
        "4. You parse the `tool_call` from the LLM‚Äôs response.\n",
        "5. You route that to your Python function via `tool_router()`.\n",
        "6. You return and print the result.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Why This Is Powerful**\n",
        "\n",
        "* Tools **extend** what the LLM can do ‚Äî from text generation to *real-world actions*.\n",
        "* You can teach the LLM to **reason about tasks**, then **hand off specific subtasks to tools**.\n",
        "* This is the core of **agentic workflows**.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Pitfalls to Avoid**\n",
        "\n",
        "* If you forget `type: \"function\"` or don't wrap tools properly, you'll get 400 errors.\n",
        "* Inputs to tools must exactly match their `parameters` schema.\n",
        "* `args.get()` is safer than `args[\"key\"]` to handle optional arguments.\n",
        "* The LLM doesn‚Äôt know the internal implementation of your tool ‚Äî just the interface.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary: Your Agent Toolkit\n",
        "\n",
        "* üîß Define Python tool functions (business logic).\n",
        "* üìú Describe tools using OpenAI‚Äôs JSON Schema.\n",
        "* üß† Use `generate_agent_response()` to let the LLM decide what to do.\n",
        "* üîÅ Route tool requests with `tool_router()` and print results.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2RF-WVVE1F6l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlFRq6lj9JvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9VsttkHp17AZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}