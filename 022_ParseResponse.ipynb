{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxLLK8CL2zI763paRi+uEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/022_ParseResponse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîç **Why Do We Need to Parse the LLM‚Äôs Response?**\n",
        "\n",
        "### 1Ô∏è‚É£ **LLMs speak in natural language. Tools need structured commands.**\n",
        "\n",
        "LLMs generate free-form text. But tools (like file systems, APIs, or plugins) require:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tool_name\": \"get_weather\",\n",
        "  \"args\": {\n",
        "    \"city\": \"Toronto\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Without parsing, the model might say:\n",
        "\n",
        "> ‚ÄúTo get the weather, I‚Äôll call the weather API with ‚ÄòToronto‚Äô.‚Äù\n",
        "\n",
        "‚Ä¶ but there‚Äôs **no way to extract that into an actual API call** reliably unless it‚Äôs structured.\n",
        "\n",
        "Parsing bridges the gap between:\n",
        "\n",
        "> üí¨ Language ‚Üí ‚öôÔ∏è Action\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ **Parsing adds structure = automation**\n",
        "\n",
        "Once you define a response format like:\n",
        "\n",
        "````markdown\n",
        "```action\n",
        "{ \"tool_name\": \"...\", \"args\": { ... } }\n",
        "````\n",
        "\n",
        "You can:\n",
        "\n",
        "* Detect what the agent wants to do.\n",
        "* Route to the correct function or API.\n",
        "* Automatically execute actions without human intervention.\n",
        "\n",
        "Without parsing, you're stuck trying to ‚Äúguess‚Äù what the LLM meant.\n",
        "With parsing, the LLM becomes a **reliable controller**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ **Parsing enables error recovery and debugging**\n",
        "\n",
        "If the response fails to parse (bad JSON, missing fields), you can:\n",
        "\n",
        "* Catch the error early.\n",
        "* Ask the LLM to correct itself.\n",
        "* Log the mistake or auto-correct it.\n",
        "\n",
        "Without this step, the agent could silently fail or produce junk ‚Äî and you‚Äôd have no clean way to fix it.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ **Parsing lets you enforce expectations**\n",
        "\n",
        "You're telling the LLM:\n",
        "\n",
        "> ‚ÄúYou must reply with a JSON block inside a \\`\\`\\`action markdown block.‚Äù\n",
        "\n",
        "This teaches the LLM to behave like a programmable component, not just a chat buddy.\n",
        "\n",
        "It's the first step toward **structured, reliable, safe agent behavior**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéÅ **Benefits of Parsing**\n",
        "\n",
        "| Benefit         | Why it Matters                                         |\n",
        "| --------------- | ------------------------------------------------------ |\n",
        "| ‚úÖ Automation    | You can auto-trigger tools based on structured output  |\n",
        "| ‚úÖ Safety        | You can catch and handle errors before execution       |\n",
        "| ‚úÖ Debuggability | Easy to log, test, and trace agent decisions           |\n",
        "| ‚úÖ Agent Control | You tell the LLM exactly how to respond and enforce it |\n",
        "| ‚úÖ Integration   | Parsed actions can plug into APIs, databases, etc.     |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† TL;DR\n",
        "\n",
        "> **Parsing is how you transform LLM output from ‚Äútext‚Äù into ‚Äúcommands.‚Äù**\n",
        "> It‚Äôs what turns your language model into a reliable **autonomous agent**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RtZn5iHTpCf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxMkXQrfn69m",
        "outputId": "ca3dfd42-d1e1-4caf-d98e-e113b9b063be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/765.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m757.8/765.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU dotenv openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs build a **simple agent that illustrates parsing + action execution** clearly and interactively.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© **Mini Agent: Calculator Agent**\n",
        "\n",
        "We‚Äôll build a small agent that:\n",
        "\n",
        "1. Takes a user prompt like: `\"Divide 81 by 9\"`\n",
        "2. The LLM responds with a **structured tool call** (in a code block)\n",
        "3. We **parse** the tool\\_name and args\n",
        "4. We **run the tool function** (like a real calculator!)\n",
        "5. Show the result ‚Äî or handle errors if the LLM formats it wrong\n",
        "\n"
      ],
      "metadata": {
        "id": "_04KedJ_qDkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìö Notebook 2: Parse + Act ‚Äì Calculator Agent Demo\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# ===============================\n",
        "# 1Ô∏è‚É£ Call the LLM\n",
        "# ===============================\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ===============================\n",
        "# 2Ô∏è‚É£ Extract JSON from code block\n",
        "# ===============================\n",
        "def extract_markdown_block(text: str, tag: str = \"action\") -> str:\n",
        "    pattern = rf\"```{tag}\\s*(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        raise ValueError(f\"Missing markdown block with tag '{tag}'\")\n",
        "\n",
        "# ===============================\n",
        "# 3Ô∏è‚É£ Parse the LLM response\n",
        "# ===============================\n",
        "def parse_action(response: str) -> dict:\n",
        "    try:\n",
        "        block = extract_markdown_block(response, \"action\")\n",
        "        parsed = json.loads(block)\n",
        "        if \"tool_name\" in parsed and \"args\" in parsed:\n",
        "            return parsed\n",
        "        else:\n",
        "            return {\n",
        "                \"tool_name\": \"error\",\n",
        "                \"args\": {\"message\": \"Response missing tool_name or args\"}\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"tool_name\": \"error\",\n",
        "            \"args\": {\"message\": str(e)}\n",
        "        }\n",
        "\n",
        "# ===============================\n",
        "# 4Ô∏è‚É£ Define calculator tools\n",
        "# ===============================\n",
        "def add(a, b): return a + b\n",
        "def subtract(a, b): return a - b\n",
        "def multiply(a, b): return a * b\n",
        "def divide(a, b): return a / b if b != 0 else \"Division by zero\"\n",
        "\n",
        "TOOLS = {\n",
        "    \"add\": add,\n",
        "    \"subtract\": subtract,\n",
        "    \"multiply\": multiply,\n",
        "    \"divide\": divide\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# 5Ô∏è‚É£ Agent loop\n",
        "# ===============================\n",
        "system_prompt = \"\"\"You are a calculator agent. Always respond with a JSON action block.\n",
        "Use one of the tools: add, subtract, multiply, divide.\n",
        "\n",
        "Respond in this format:\n",
        "\n",
        "```action\n",
        "{\n",
        "  \"tool_name\": \"add\",\n",
        "  \"args\": { \"a\": 5, \"b\": 3 }\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt}\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nüßë‚Äçüíª You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "    llm_reply = generate_response(conversation)\n",
        "    print(\"\\nü§ñ Raw LLM Reply:\\n\", llm_reply)\n",
        "\n",
        "    action = parse_action(llm_reply)\n",
        "    print(\"\\nüß† Parsed Action:\", action)\n",
        "\n",
        "    tool_name = action.get(\"tool_name\")\n",
        "    args = action.get(\"args\", {})\n",
        "\n",
        "    if tool_name in TOOLS:\n",
        "        result = TOOLS[tool_name](**args)\n",
        "        print(f\"\\n‚öôÔ∏è Tool Result: {result}\")\n",
        "    else:\n",
        "        print(f\"\\nüö´ Invalid or missing tool: {tool_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhO18IGwqDIq",
        "outputId": "2a7a1a0b-5b11-4712-82a8-53c7ba3c37c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßë‚Äçüíª You: what is 15 minus 3?\n",
            "\n",
            "ü§ñ Raw LLM Reply:\n",
            " ```action\n",
            "{\n",
            "  \"tool_name\": \"subtract\",\n",
            "  \"args\": { \"a\": 15, \"b\": 3 }\n",
            "}\n",
            "```\n",
            "\n",
            "üß† Parsed Action: {'tool_name': 'subtract', 'args': {'a': 15, 'b': 3}}\n",
            "\n",
            "‚öôÔ∏è Tool Result: 12\n",
            "\n",
            "üßë‚Äçüíª You: exit\n",
            "üëã Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Repsonse\n",
        "\n",
        "Seeing the full raw `response` object from the OpenAI API will give you a **clear picture of what‚Äôs returned**, so you know how we‚Äôre getting to `response.choices[0].message.content`.\n",
        "\n",
        "Let‚Äôs walk through it step-by-step:\n",
        "\n",
        "\n",
        "## üîç Breaking it down\n",
        "\n",
        "| Field                                 | What it contains                                       |\n",
        "| ------------------------------------- | ------------------------------------------------------ |\n",
        "| `response`                            | A `ChatCompletion` object ‚Äî wraps the whole response   |\n",
        "| `response.choices`                    | A list of response options (usually just 1)            |\n",
        "| `response.choices[0].message`         | A `ChatMessage` with `role` and `content`              |\n",
        "| `response.choices[0].message.content` | ‚úÖ The actual text that the LLM wrote (our tool action) |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why this matters\n",
        "\n",
        "* The `choices` array exists because OpenAI supports **multiple completions** (e.g., top 3 ideas).\n",
        "* But we usually just grab the first one: `choices[0]`.\n",
        "* That‚Äôs where the LLM writes its natural language (or structured) response.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7d_v3P6-svF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    # üîç Print the entire response object\n",
        "    print(\"\\nüßæ Full raw response:\")\n",
        "    print(response)  # This is a full OpenAI object\n",
        "\n",
        "    # üîç Also print just the choices list\n",
        "    print(\"\\nüì¶ response.choices:\")\n",
        "    print(response.choices)\n",
        "\n",
        "    # üîç Then print the specific message object\n",
        "    print(\"\\nüí¨ response.choices[0].message:\")\n",
        "    print(response.choices[0].message)\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nüßë‚Äçüíª You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "    llm_reply = generate_response(conversation)\n",
        "    print(\"\\nü§ñ Raw LLM Reply:\\n\", llm_reply)\n",
        "\n",
        "    action = parse_action(llm_reply)\n",
        "    print(\"\\nüß† Parsed Action:\", action)\n",
        "\n",
        "    tool_name = action.get(\"tool_name\")\n",
        "    args = action.get(\"args\", {})\n",
        "\n",
        "    if tool_name in TOOLS:\n",
        "        result = TOOLS[tool_name](**args)\n",
        "        print(f\"\\n‚öôÔ∏è Tool Result: {result}\")\n",
        "    else:\n",
        "        print(f\"\\nüö´ Invalid or missing tool: {tool_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdnDfoHWqDGF",
        "outputId": "1a44f54d-810a-40ba-c6b6-0495e1eda94f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßë‚Äçüíª You: What is 7 times 7?\n",
            "\n",
            "üßæ Full raw response:\n",
            "ChatCompletion(id='chatcmpl-Bu25g4hJcaIEXkOuioyP4JxGZCzKx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```action\\n{\\n  \"tool_name\": \"multiply\",\\n  \"args\": { \"a\": 7, \"b\": 7 }\\n}\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752694104, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=108, total_tokens=139, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
            "\n",
            "üì¶ response.choices:\n",
            "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```action\\n{\\n  \"tool_name\": \"multiply\",\\n  \"args\": { \"a\": 7, \"b\": 7 }\\n}\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]\n",
            "\n",
            "üí¨ response.choices[0].message:\n",
            "ChatCompletionMessage(content='```action\\n{\\n  \"tool_name\": \"multiply\",\\n  \"args\": { \"a\": 7, \"b\": 7 }\\n}\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n",
            "\n",
            "ü§ñ Raw LLM Reply:\n",
            " ```action\n",
            "{\n",
            "  \"tool_name\": \"multiply\",\n",
            "  \"args\": { \"a\": 7, \"b\": 7 }\n",
            "}\n",
            "```\n",
            "\n",
            "üß† Parsed Action: {'tool_name': 'multiply', 'args': {'a': 7, 'b': 7}}\n",
            "\n",
            "‚öôÔ∏è Tool Result: 49\n",
            "\n",
            "üßë‚Äçüíª You: exit\n",
            "üëã Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs refactor our calculator agent to:\n",
        "\n",
        "‚úÖ Use the **`parse_action()`** with `\"tool_name\": \"error\"`\n",
        "‚úÖ Let the agent **detect invalid responses**\n",
        "‚úÖ Send a follow-up message to the LLM to correct its output\n",
        "‚úÖ Handle errors safely and clearly\n",
        "\n",
        "---\n",
        "\n",
        "# üîß Step-by-step Refactor Plan\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Step 1: Add a robust `parse_action()` function**\n",
        "\n",
        "This replaces our original, simpler version:\n",
        "\n",
        "```python\n",
        "def parse_action(response: str) -> dict:\n",
        "    try:\n",
        "        response = extract_markdown_block(response, \"action\")\n",
        "        response_json = json.loads(response)\n",
        "        if \"tool_name\" in response_json and \"args\" in response_json:\n",
        "            return response_json\n",
        "        else:\n",
        "            return {\n",
        "                \"tool_name\": \"error\",\n",
        "                \"args\": {\"message\": \"Missing 'tool_name' or 'args' keys.\"}\n",
        "            }\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"tool_name\": \"error\",\n",
        "            \"args\": {\"message\": \"Invalid JSON. You must respond with a valid JSON tool call inside a markdown block.\"}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"tool_name\": \"error\",\n",
        "            \"args\": {\"message\": str(e)}\n",
        "        }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Step 2: Handle `tool_name == \"error\"` in the agent loop**\n",
        "\n",
        "When the parser returns an error, we‚Äôll:\n",
        "\n",
        "* Log the error\n",
        "* Feed it back to the LLM as a `\"user\"` message\n",
        "* Try again once (for now, to keep it simple)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Step 3: Revised Agent Loop with Self-Correction**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ogN7br3u3Nla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def generate_response(messages):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def extract_markdown_block(text: str, tag: str = \"action\") -> str:\n",
        "    pattern = rf\"```{tag}\\s*(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    else:\n",
        "        raise ValueError(f\"Missing markdown block with tag '{tag}'\")\n",
        "\n",
        "def parse_action(response: str) -> dict:\n",
        "    try:\n",
        "        block = extract_markdown_block(response, \"action\")\n",
        "        parsed = json.loads(block)\n",
        "        if \"tool_name\" in parsed and \"args\" in parsed:\n",
        "            return parsed\n",
        "        else:\n",
        "            return {\n",
        "                \"tool_name\": \"error\",\n",
        "                \"args\": {\"message\": \"Missing 'tool_name' or 'args' in response.\"}\n",
        "            }\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"tool_name\": \"error\",\n",
        "            \"args\": {\"message\": \"Invalid JSON. Expected JSON inside a markdown block labeled ```action```.\"}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"tool_name\": \"error\",\n",
        "            \"args\": {\"message\": str(e)}\n",
        "        }\n",
        "\n",
        "# Tools\n",
        "def add(a, b): return a + b\n",
        "def subtract(a, b): return a - b\n",
        "def multiply(a, b): return a * b\n",
        "def divide(a, b): return a / b if b != 0 else \"Division by zero\"\n",
        "\n",
        "TOOLS = {\n",
        "    \"add\": add,\n",
        "    \"subtract\": subtract,\n",
        "    \"multiply\": multiply,\n",
        "    \"divide\": divide\n",
        "}\n",
        "\n",
        "# Prompt\n",
        "system_prompt = \"\"\"You are a calculator agent. Always respond with a JSON action block.\n",
        "Use one of the tools: add, subtract, multiply, divide.\n",
        "\n",
        "Respond in this format:\n",
        "```action\n",
        "{\n",
        "  \"tool_name\": \"add\",\n",
        "  \"args\": { \"a\": 5, \"b\": 3 }\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt}\n",
        "]\n",
        "\n",
        "# üîÅ Agent loop\n",
        "while True:\n",
        "    user_input = input(\"\\nüßë‚Äçüíª You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "    llm_response = generate_response(conversation)\n",
        "    print(\"\\nü§ñ Raw LLM Output:\\n\", llm_response)\n",
        "\n",
        "    action = parse_action(llm_response)\n",
        "\n",
        "    if action[\"tool_name\"] == \"error\":\n",
        "        print(\"\\nüö´ Parse Error:\", action[\"args\"][\"message\"])\n",
        "        # Feed the error back into the conversation for a retry\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": llm_response})\n",
        "        conversation.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"That was not a valid action block. Error: {action['args']['message']}. Please respond with a correct JSON tool call inside ```action```.\"\n",
        "        })\n",
        "\n",
        "        # Retry once\n",
        "        llm_response = generate_response(conversation)\n",
        "        print(\"\\nüîÅ Retry Output:\\n\", llm_response)\n",
        "        action = parse_action(llm_response)\n",
        "\n",
        "    tool_name = action.get(\"tool_name\")\n",
        "    args = action.get(\"args\", {})\n",
        "\n",
        "    if tool_name in TOOLS:\n",
        "        result = TOOLS[tool_name](**args)\n",
        "        print(f\"\\n‚öôÔ∏è Tool Result: {result}\")\n",
        "    else:\n",
        "        print(f\"\\nüö´ Unknown or still invalid tool: {tool_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dI4X42cqDD-",
        "outputId": "9becea6e-71a0-4bcc-d5e4-962a1d23ad78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßë‚Äçüíª You: What is 15 times 4?\n",
            "\n",
            "ü§ñ Raw LLM Output:\n",
            " ```action\n",
            "{\n",
            "  \"tool_name\": \"multiply\",\n",
            "  \"args\": { \"a\": 15, \"b\": 4 }\n",
            "}\n",
            "```\n",
            "\n",
            "‚öôÔ∏è Tool Result: 60\n",
            "\n",
            "üßë‚Äçüíª You: What is 75 divide by 5?\n",
            "\n",
            "ü§ñ Raw LLM Output:\n",
            " ```action\n",
            "{\n",
            "  \"tool_name\": \"divide\",\n",
            "  \"args\": { \"a\": 75, \"b\": 5 }\n",
            "}\n",
            "```\n",
            "\n",
            "‚öôÔ∏è Tool Result: 15.0\n",
            "\n",
            "üßë‚Äçüíª You: exit\n",
            "üëã Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grpVmaqo7LHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}