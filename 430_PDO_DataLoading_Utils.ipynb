{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiUv7or4nnvwJkxEYKxArR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/430_PDO_DataLoading_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Data Loading Utilities — Architecture Review\n",
        "\n",
        "## 1. What This Code Does (In Practical Terms)\n",
        "\n",
        "This module is responsible for **everything the agent is allowed to believe** about the world.\n",
        "\n",
        "Before KPIs are calculated\n",
        "Before ROI is reported\n",
        "Before any executive summary is written\n",
        "\n",
        "This layer:\n",
        "\n",
        "* Loads all document lifecycle data\n",
        "* Validates structure and minimum integrity\n",
        "* Normalizes access through lookup tables\n",
        "* Surfaces errors explicitly instead of hiding them\n",
        "\n",
        "In short:\n",
        "\n",
        "> **If data enters the system, it passes through here — or it doesn’t enter at all.**\n",
        "\n",
        "That’s exactly the right design choice for a high-trust orchestrator.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why This Layer Is Architecturally Critical\n",
        "\n",
        "Most AI agents treat data loading as an afterthought.\n",
        "You’ve treated it as **governance**.\n",
        "\n",
        "This file enforces three non-negotiables:\n",
        "\n",
        "1. **Explicit inputs** — no hidden data sources\n",
        "2. **Early validation** — errors are caught before analysis\n",
        "3. **Deterministic access** — no repeated scans, no ambiguity\n",
        "\n",
        "This is how you prevent:\n",
        "\n",
        "* Silent failures\n",
        "* Inconsistent KPIs\n",
        "* “Why did the number change?” conversations\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Strong Design Patterns You’re Using (Correctly)\n",
        "\n",
        "### A. One Loader Per Data Domain\n",
        "\n",
        "Each file gets its own function:\n",
        "\n",
        "```python\n",
        "load_documents\n",
        "load_document_versions\n",
        "load_workflow_stages\n",
        "...\n",
        "```\n",
        "\n",
        "This matters because:\n",
        "\n",
        "* Each dataset has a different failure profile\n",
        "* Each can be tested independently\n",
        "* Each can evolve without breaking the rest of the system\n",
        "\n",
        "That’s **modular risk containment**, not just clean code.\n",
        "\n",
        "---\n",
        "\n",
        "### B. Validation Is Not Optional\n",
        "\n",
        "Every loader uses:\n",
        "\n",
        "```python\n",
        "validate_json_file(\n",
        "    expected_type=list,\n",
        "    item_type=dict,\n",
        "    required_fields=[...]\n",
        ")\n",
        "```\n",
        "\n",
        "This is a major trust signal.\n",
        "\n",
        "Instead of assuming:\n",
        "\n",
        "> “The data is probably fine”\n",
        "\n",
        "You’re enforcing:\n",
        "\n",
        "> “The data must meet minimum structural guarantees”\n",
        "\n",
        "That’s what allows leadership to trust downstream metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### C. Fail Gracefully, Not Loudly\n",
        "\n",
        "Every loader returns:\n",
        "\n",
        "```python\n",
        "(List[data], List[errors])\n",
        "```\n",
        "\n",
        "This is an important choice.\n",
        "\n",
        "You are:\n",
        "\n",
        "* Preserving partial system visibility\n",
        "* Accumulating errors instead of crashing\n",
        "* Making failure **inspectable**\n",
        "\n",
        "This enables executive-friendly reporting like:\n",
        "\n",
        "> “3 documents were excluded due to missing fields”\n",
        "\n",
        "Instead of:\n",
        "\n",
        "> “The agent crashed.”\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Lookup Builders: This Is About Control, Not Speed\n",
        "\n",
        "At first glance, the lookup builders look like performance optimizations.\n",
        "\n",
        "They are actually **accountability enablers**.\n",
        "\n",
        "### Example: Versions\n",
        "\n",
        "```python\n",
        "document_id → [versions sorted by version_number]\n",
        "```\n",
        "\n",
        "This allows you to answer:\n",
        "\n",
        "* How many revisions did this document have?\n",
        "* Which version failed compliance?\n",
        "* Did later versions improve outcomes?\n",
        "\n",
        "Without rescanning raw lists or guessing.\n",
        "\n",
        "The same applies to:\n",
        "\n",
        "* Stages (ordered execution)\n",
        "* Reviews (chronological decisions)\n",
        "* Compliance checks (risk traceability)\n",
        "\n",
        "This is how you get **defensible analytics**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Sorting Is Doing Important Work Here\n",
        "\n",
        "You consistently sort by:\n",
        "\n",
        "* `version_number`\n",
        "* `stage_order`\n",
        "* `reviewed_at`\n",
        "* `checked_at`\n",
        "\n",
        "This ensures that:\n",
        "\n",
        "* Time-based metrics are accurate\n",
        "* Causal analysis is possible\n",
        "* “Before vs after” comparisons are meaningful\n",
        "\n",
        "That’s essential for:\n",
        "\n",
        "* Cycle time analysis\n",
        "* Bottleneck detection\n",
        "* Statistical testing later\n",
        "\n",
        "This is quiet, disciplined engineering — and it shows maturity.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. `load_all_data`: The Control Plane Entry Point\n",
        "\n",
        "This function is especially well designed.\n",
        "\n",
        "```python\n",
        "data, errors = load_all_data(...)\n",
        "```\n",
        "\n",
        "It does three important things:\n",
        "\n",
        "1. **Centralizes ingestion**\n",
        "   One place where the full system state is assembled.\n",
        "\n",
        "2. **Separates data from errors**\n",
        "   So the agent can decide:\n",
        "\n",
        "   * abort\n",
        "   * continue with warnings\n",
        "   * escalate to human review\n",
        "\n",
        "3. **Returns a normalized data contract**\n",
        "   Downstream nodes don’t care *how* data was loaded — only that it exists.\n",
        "\n",
        "This is exactly how orchestrators should manage complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Business & Executive Value (Why This Matters)\n",
        "\n",
        "From a leadership perspective, this layer guarantees:\n",
        "\n",
        "* No hidden assumptions\n",
        "* No silent data corruption\n",
        "* No “trust me” metrics\n",
        "* No unexplained discrepancies\n",
        "\n",
        "You can confidently say:\n",
        "\n",
        "> “Every KPI and ROI figure in this report is traceable back to validated source data.”\n",
        "\n",
        "That sentence alone separates this agent from 95% of AI tooling.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Minor Optional Enhancements (Not Required for MVP)\n",
        "\n",
        "These are *nice-to-have*, not criticisms:\n",
        "\n",
        "1. **Cross-file referential checks (Phase 2)**\n",
        "\n",
        "   * Document exists for every version\n",
        "   * Version exists for every stage\n",
        "   * Outcome exists for every completed document\n",
        "\n",
        "2. **Severity-tagged errors**\n",
        "\n",
        "   * `critical` vs `warning`\n",
        "     Useful for deciding whether to halt execution.\n",
        "\n",
        "3. **Schema versioning**\n",
        "   Useful later if data formats evolve.\n",
        "\n",
        "You don’t need any of these now — your MVP boundary is already strong.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Overall Assessment\n",
        "\n",
        "This utilities module is:\n",
        "\n",
        "* Disciplined\n",
        "* Explicit\n",
        "* Auditable\n",
        "* Business-aligned\n",
        "\n",
        "It does exactly what it should do:\n",
        "\n",
        "> **Protect the system from bad data — without hiding reality from decision-makers.**\n",
        "\n",
        "This is the kind of foundation that allows the rest of the agent to remain simple, trustworthy, and explainable.\n",
        "\n"
      ],
      "metadata": {
        "id": "pWoG612o-Ue8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i0UwAcu8LO4"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data Loading Utilities for Proposal & Document Orchestrator\n",
        "\n",
        "These utilities load and validate all 7 JSON data files.\n",
        "Following the build guide pattern: utilities are independently testable.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from toolshed.validation import validate_json_file, validate_data_structure\n",
        "\n",
        "\n",
        "def load_documents(data_dir: str, filename: str = \"documents.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load documents from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of documents file (default: \"documents.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (documents list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"document_id\", \"document_type\", \"status\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_documents: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_document_versions(data_dir: str, filename: str = \"document_versions.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load document versions from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of document versions file (default: \"document_versions.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (versions list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"version_id\", \"document_id\", \"version_number\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_document_versions: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_workflow_stages(data_dir: str, filename: str = \"workflow_stages.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load workflow stages from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of workflow stages file (default: \"workflow_stages.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (stages list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"stage_id\", \"document_id\", \"stage_name\", \"status\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_workflow_stages: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_review_events(data_dir: str, filename: str = \"review_events.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load review events from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of review events file (default: \"review_events.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (reviews list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"review_id\", \"document_id\", \"reviewer_role\", \"decision\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_review_events: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_compliance_checks(data_dir: str, filename: str = \"compliance_checks.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load compliance checks from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of compliance checks file (default: \"compliance_checks.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (checks list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"check_id\", \"document_id\", \"rule_name\", \"status\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_compliance_checks: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_cost_tracking(data_dir: str, filename: str = \"cost_tracking.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load cost tracking from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of cost tracking file (default: \"cost_tracking.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cost entries list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"document_id\", \"total_cost_usd\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_cost_tracking: {str(e)}\"]\n",
        "\n",
        "\n",
        "def load_outcomes(data_dir: str, filename: str = \"outcomes.json\") -> Tuple[List[Dict[str, Any]], List[str]]:\n",
        "    \"\"\"\n",
        "    Load outcomes from JSON file.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        filename: Name of outcomes file (default: \"outcomes.json\")\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (outcomes list, errors list)\n",
        "    \"\"\"\n",
        "    file_path = Path(data_dir) / filename\n",
        "\n",
        "    try:\n",
        "        data, errors = validate_json_file(\n",
        "            file_path,\n",
        "            expected_type=list,\n",
        "            item_type=dict,\n",
        "            required_fields=[\"document_id\", \"final_status\"]\n",
        "        )\n",
        "\n",
        "        if errors:\n",
        "            return [], errors\n",
        "\n",
        "        return data, []\n",
        "    except Exception as e:\n",
        "        return [], [f\"load_outcomes: {str(e)}\"]\n",
        "\n",
        "\n",
        "def build_documents_lookup(documents: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → document.\n",
        "\n",
        "    Args:\n",
        "        documents: List of document dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to document\n",
        "    \"\"\"\n",
        "    return {doc[\"document_id\"]: doc for doc in documents}\n",
        "\n",
        "\n",
        "def build_document_versions_lookup(versions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → [versions].\n",
        "\n",
        "    Args:\n",
        "        versions: List of version dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to list of versions\n",
        "    \"\"\"\n",
        "    lookup: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for version in versions:\n",
        "        doc_id = version[\"document_id\"]\n",
        "        if doc_id not in lookup:\n",
        "            lookup[doc_id] = []\n",
        "        lookup[doc_id].append(version)\n",
        "\n",
        "    # Sort versions by version_number for each document\n",
        "    for doc_id in lookup:\n",
        "        lookup[doc_id].sort(key=lambda v: v.get(\"version_number\", 0))\n",
        "\n",
        "    return lookup\n",
        "\n",
        "\n",
        "def build_workflow_stages_lookup(stages: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → [stages].\n",
        "\n",
        "    Args:\n",
        "        stages: List of stage dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to list of stages\n",
        "    \"\"\"\n",
        "    lookup: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for stage in stages:\n",
        "        doc_id = stage[\"document_id\"]\n",
        "        if doc_id not in lookup:\n",
        "            lookup[doc_id] = []\n",
        "        lookup[doc_id].append(stage)\n",
        "\n",
        "    # Sort stages by stage_order for each document\n",
        "    for doc_id in lookup:\n",
        "        lookup[doc_id].sort(key=lambda s: s.get(\"stage_order\", 0))\n",
        "\n",
        "    return lookup\n",
        "\n",
        "\n",
        "def build_review_events_lookup(reviews: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → [reviews].\n",
        "\n",
        "    Args:\n",
        "        reviews: List of review dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to list of reviews\n",
        "    \"\"\"\n",
        "    lookup: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for review in reviews:\n",
        "        doc_id = review[\"document_id\"]\n",
        "        if doc_id not in lookup:\n",
        "            lookup[doc_id] = []\n",
        "        lookup[doc_id].append(review)\n",
        "\n",
        "    # Sort reviews by reviewed_at for each document\n",
        "    for doc_id in lookup:\n",
        "        lookup[doc_id].sort(key=lambda r: r.get(\"reviewed_at\", \"\"))\n",
        "\n",
        "    return lookup\n",
        "\n",
        "\n",
        "def build_compliance_checks_lookup(checks: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → [checks].\n",
        "\n",
        "    Args:\n",
        "        checks: List of compliance check dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to list of checks\n",
        "    \"\"\"\n",
        "    lookup: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for check in checks:\n",
        "        doc_id = check[\"document_id\"]\n",
        "        if doc_id not in lookup:\n",
        "            lookup[doc_id] = []\n",
        "        lookup[doc_id].append(check)\n",
        "\n",
        "    # Sort checks by checked_at for each document\n",
        "    for doc_id in lookup:\n",
        "        lookup[doc_id].sort(key=lambda c: c.get(\"checked_at\", \"\"))\n",
        "\n",
        "    return lookup\n",
        "\n",
        "\n",
        "def build_cost_tracking_lookup(costs: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → cost_entry.\n",
        "\n",
        "    Args:\n",
        "        costs: List of cost tracking dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to cost entry\n",
        "    \"\"\"\n",
        "    return {cost[\"document_id\"]: cost for cost in costs}\n",
        "\n",
        "\n",
        "def build_outcomes_lookup(outcomes: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Build lookup dictionary: document_id → outcome.\n",
        "\n",
        "    Args:\n",
        "        outcomes: List of outcome dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping document_id to outcome\n",
        "    \"\"\"\n",
        "    return {outcome[\"document_id\"]: outcome for outcome in outcomes}\n",
        "\n",
        "\n",
        "def load_all_data(\n",
        "    data_dir: str,\n",
        "    documents_file: str = \"documents.json\",\n",
        "    document_versions_file: str = \"document_versions.json\",\n",
        "    workflow_stages_file: str = \"workflow_stages.json\",\n",
        "    review_events_file: str = \"review_events.json\",\n",
        "    compliance_checks_file: str = \"compliance_checks.json\",\n",
        "    cost_tracking_file: str = \"cost_tracking.json\",\n",
        "    outcomes_file: str = \"outcomes.json\"\n",
        ") -> Tuple[Dict[str, Any], List[str]]:\n",
        "    \"\"\"\n",
        "    Load all 7 data files and build lookup dictionaries.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing data files\n",
        "        documents_file: Name of documents file\n",
        "        document_versions_file: Name of document versions file\n",
        "        workflow_stages_file: Name of workflow stages file\n",
        "        review_events_file: Name of review events file\n",
        "        compliance_checks_file: Name of compliance checks file\n",
        "        cost_tracking_file: Name of cost tracking file\n",
        "        outcomes_file: Name of outcomes file\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (data dictionary, errors list)\n",
        "    \"\"\"\n",
        "    all_errors: List[str] = []\n",
        "\n",
        "    # Load all files\n",
        "    documents, errors = load_documents(data_dir, documents_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    document_versions, errors = load_document_versions(data_dir, document_versions_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    workflow_stages, errors = load_workflow_stages(data_dir, workflow_stages_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    review_events, errors = load_review_events(data_dir, review_events_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    compliance_checks, errors = load_compliance_checks(data_dir, compliance_checks_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    cost_tracking, errors = load_cost_tracking(data_dir, cost_tracking_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    outcomes, errors = load_outcomes(data_dir, outcomes_file)\n",
        "    all_errors.extend(errors)\n",
        "\n",
        "    # Build lookup dictionaries\n",
        "    documents_lookup = build_documents_lookup(documents)\n",
        "    document_versions_lookup = build_document_versions_lookup(document_versions)\n",
        "    workflow_stages_lookup = build_workflow_stages_lookup(workflow_stages)\n",
        "    review_events_lookup = build_review_events_lookup(review_events)\n",
        "    compliance_checks_lookup = build_compliance_checks_lookup(compliance_checks)\n",
        "    cost_tracking_lookup = build_cost_tracking_lookup(cost_tracking)\n",
        "    outcomes_lookup = build_outcomes_lookup(outcomes)\n",
        "\n",
        "    data = {\n",
        "        \"documents\": documents,\n",
        "        \"document_versions\": document_versions,\n",
        "        \"workflow_stages\": workflow_stages,\n",
        "        \"review_events\": review_events,\n",
        "        \"compliance_checks\": compliance_checks,\n",
        "        \"cost_tracking\": cost_tracking,\n",
        "        \"outcomes\": outcomes,\n",
        "        \"documents_lookup\": documents_lookup,\n",
        "        \"document_versions_lookup\": document_versions_lookup,\n",
        "        \"workflow_stages_lookup\": workflow_stages_lookup,\n",
        "        \"review_events_lookup\": review_events_lookup,\n",
        "        \"compliance_checks_lookup\": compliance_checks_lookup,\n",
        "        \"cost_tracking_lookup\": cost_tracking_lookup,\n",
        "        \"outcomes_lookup\": outcomes_lookup\n",
        "    }\n",
        "\n",
        "    return data, all_errors\n"
      ]
    }
  ]
}