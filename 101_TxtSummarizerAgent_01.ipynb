{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO82xhE84pgSv2ZtnWSdShO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/101_TxtSummarizerAgent_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ Next Step: Implement Tools (starting with `create_plan`)\n",
        "\n",
        "\n",
        "\n",
        "### üõ† Tool: `create_plan`\n",
        "\n",
        "This is our ‚Äúmental warmup‚Äù tool. Its job:\n",
        "\n",
        "* Read the goal from memory\n",
        "* Create a simple plan (list of steps)\n",
        "* Store that plan back in memory\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Step 1: Define Tool Function\n",
        "\n",
        "```python\n",
        "def create_plan(ctx):\n",
        "    goal = ctx.memory.get(\"goal\")\n",
        "\n",
        "    plan = [\n",
        "        \"Read the target file\",\n",
        "        \"Create a prompt for summarization\",\n",
        "        \"Use LLM to generate summary\",\n",
        "        \"Save the summary to the output folder\",\n",
        "        \"Log the completion\"\n",
        "    ]\n",
        "\n",
        "    ctx.memory.set(\"plan\", plan)\n",
        "\n",
        "    return {\"message\": \"Plan created.\", \"steps\": plan}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Notes\n",
        "\n",
        "* `ctx.memory.get(\"goal\")` ‚Üí We expect the goal was injected earlier by the agent setup\n",
        "* We return a confirmation + the steps (optional but helpful for debug or transparency)\n",
        "* This is a **zero-dependency** tool: no extra config or files needed\n"
      ],
      "metadata": {
        "id": "LPCSkOKc11ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß† Why Hardcoding the Plan Isn‚Äôt Ideal\n",
        "\n",
        "* Hardcoded steps = the agent can only do one thing.\n",
        "* It defeats the purpose of a reusable `create_plan` tool.\n",
        "* We want agents that can adapt to **any goal**, even ones we haven‚Äôt thought of yet.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ The Better Design: Use the LLM to Generate the Plan\n",
        "\n",
        "Let‚Äôs refactor:\n",
        "\n",
        "### üîß Improved `create_plan(ctx)`:\n",
        "\n",
        "```python\n",
        "def create_plan(ctx):\n",
        "    goal = ctx.memory.get(\"goal\")\n",
        "\n",
        "    prompt = f\"\"\"You are an expert task planner. Given the goal below, break it down into a clear, short list of steps.\n",
        "\n",
        "Goal: {goal}\n",
        "\n",
        "Respond with a numbered list of steps.\"\"\"\n",
        "\n",
        "    response = ctx.llm.complete(prompt)\n",
        "\n",
        "    # Optional: Parse into list of steps\n",
        "    steps = response.strip().split(\"\\n\")\n",
        "\n",
        "    ctx.memory.set(\"plan\", steps)\n",
        "\n",
        "    return {\"message\": \"Plan created from goal.\", \"steps\": steps}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Is Better\n",
        "\n",
        "| Hardcoded Plan            | LLM-Generated Plan              |\n",
        "| ------------------------- | ------------------------------- |\n",
        "| Fixed and rigid           | Flexible and dynamic            |\n",
        "| Only works for 1 use case | Adapts to new goals             |\n",
        "| Not reusable              | Can power many different agents |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Bonus Insight\n",
        "\n",
        "This pattern ‚Äî where the LLM **only handles the hard thinking** ‚Äî is what your teacher emphasized.\n",
        "\n",
        "Here:\n",
        "\n",
        "* We inject the goal\n",
        "* The LLM does the reasoning (‚Äúhow should I achieve that?‚Äù)\n",
        "* The agent just **records** and **executes** the plan\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ TL;DR\n",
        "\n",
        "> Yes, `create_plan` should be an LLM tool. The goal goes in, and the agent gets back steps.\n",
        "> We store those steps in memory, and then proceed.\n",
        "\n"
      ],
      "metadata": {
        "id": "e4SAONr_18ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ ‚ÄúInject the goal‚Äù ‚Äî What Does That Mean?\n",
        "\n",
        "You‚Äôre right:\n",
        "\n",
        "> It means we **set the goal before** the tool is ever called ‚Äî so the tool can just read it from context.\n",
        "\n",
        "It‚Äôs **not** hardcoded.\n",
        "It‚Äôs **not** asked for at runtime.\n",
        "It‚Äôs **already there** in `ctx.memory`.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Is This Dependency Injection?\n",
        "\n",
        "Yes ‚Äî though it‚Äôs a special kind:\n",
        "\n",
        "* Classic dependency injection is for **code or config** (like folders, clocks, models)\n",
        "* But here, we‚Äôre injecting **initial memory state** (the `goal`) into the agent‚Äôs `ActionContext` before it starts\n",
        "\n",
        "So while it‚Äôs not dependency injection in the strict OOP sense, it *follows the same principle*:\n",
        "\n",
        "> üîÑ **Give each unit (tool or agent) what it needs ‚Äî from the outside ‚Äî instead of baking it in.**\n",
        "\n",
        "---\n",
        "\n",
        "## üß≥ Where Does the Goal Live?\n",
        "\n",
        "When building the agent, you might do this:\n",
        "\n",
        "```python\n",
        "ctx = ActionContext(...)\n",
        "ctx.memory.set(\"goal\", \"Summarize the content of a text file.\")\n",
        "```\n",
        "\n",
        "Now the tool can access that goal like this:\n",
        "\n",
        "```python\n",
        "goal = ctx.memory.get(\"goal\")\n",
        "```\n",
        "\n",
        "It‚Äôs been **injected into the memory layer** ‚Äî just like folders are injected into `config`.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† TL;DR\n",
        "\n",
        "| Term                 | Meaning                                    |\n",
        "| -------------------- | ------------------------------------------ |\n",
        "| ‚ÄúInject the goal‚Äù    | Set it in memory ahead of time             |\n",
        "| Dependency injection | Provide it from outside the function       |\n",
        "| Why do it?           | Keeps tools clean, stateless, and testable |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rhP4zlQEAhTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ What‚Äôs Happening, Step-by-Step\n",
        "\n",
        "1. **The user provides the goal**\n",
        "   ‚Üí e.g., ‚ÄúSummarize the content of a text file.‚Äù\n",
        "\n",
        "2. **You (the agent builder) store that goal in memory**\n",
        "   ‚Üí via `ctx.memory.set(\"goal\", ...)`\n",
        "   This happens before the agent starts its run.\n",
        "\n",
        "3. **The agent‚Äôs `ActionContext` (the backpack) now contains the goal**\n",
        "   ‚Üí Alongside other stuff like injected folders, clock, etc.\n",
        "\n",
        "4. **The `create_plan` tool reaches into the backpack to get the goal**\n",
        "   ‚Üí `ctx.memory.get(\"goal\")`\n",
        "   It doesn‚Äôt care *who* put the goal there ‚Äî it just knows it has access to it.\n",
        "\n",
        "5. ‚úÖ The LLM then creates a plan using that goal.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why This Matters\n",
        "\n",
        "> Tools don‚Äôt make assumptions about how data got there.\n",
        "> They just use what‚Äôs in the **context**.\n",
        "\n",
        "That‚Äôs what makes this **modular**, **testable**, and **reusable** ‚Äî and why your teacher emphasizes \"clear thinking\" for the LLM and \"clean interfaces\" for the agent.\n",
        "\n"
      ],
      "metadata": {
        "id": "VCH4KnKEA5e8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8CLDYYlT1q-f"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai python-dotenv\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# ---- Setup ----\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY not found in /content/API_KEYS.env\")\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs focus on the **key design patterns** and **agent concepts** this code is teaching you ‚Äî not just the syntax.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 1. `ScratchMemory` ‚Äì Simulated Working Memory\n",
        "\n",
        "This class is a stand-in for agent memory. It models the idea of a **‚Äúbackpack‚Äù** or **whiteboard** where the agent stores state between steps.\n",
        "\n",
        "### üß© What to Learn:\n",
        "\n",
        "* Memory is just a **simple key-value store**.\n",
        "* Tools don‚Äôt *return everything* ‚Äî they often **write results to memory** for other tools to use.\n",
        "* The `.get()` and `.set()` methods are how tools read/write shared state.\n",
        "\n",
        "---\n",
        "\n",
        "## üéí 2. `ActionContext` ‚Äì The Backpack Frame\n",
        "\n",
        "This is the **agent's runtime context**, passed into every tool. It includes:\n",
        "\n",
        "* `memory`: For shared state\n",
        "* `llm`: The language model interface\n",
        "\n",
        "### üß© What to Learn:\n",
        "\n",
        "* Every tool takes in `ctx`, never random global variables.\n",
        "* Tools stay **stateless** and **reusable** because `ctx` holds the state.\n",
        "* This makes it easy to **mock, test, or swap** pieces later.\n",
        "\n",
        "Think of `ActionContext` as:\n",
        "\n",
        "> ‚ÄúAll the things the agent needs to think and act, passed around neatly in one object.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ 3. `OpenAILLM` ‚Äì A Thin LLM Wrapper\n",
        "\n",
        "This wraps the OpenAI API and gives you a `.complete(prompt)` method that tools can call easily.\n",
        "\n",
        "### üß© What to Learn:\n",
        "\n",
        "* Tools **shouldn‚Äôt care** how LLMs work internally ‚Äî just that they can say:\n",
        "  `response = ctx.llm.complete(prompt)`\n",
        "* If you change models or vendors later, you only update this one class.\n",
        "\n",
        "You're learning **abstraction** and **encapsulation** ‚Äî big software engineering wins.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ 4. `create_plan(ctx)` ‚Äì A Real Tool in Action\n",
        "\n",
        "This is a tool with a single, clear responsibility:\n",
        "\n",
        "* Read `goal` from memory\n",
        "* Ask the LLM to break it into steps\n",
        "* Store the steps in memory\n",
        "* Return a helpful response\n",
        "\n",
        "### üß© What to Learn:\n",
        "\n",
        "* **No side effects** ‚Äî it only operates through the `ctx`\n",
        "* It doesn‚Äôt ask the user for input ‚Äî it pulls from what‚Äôs *already* in memory\n",
        "* It uses **LLM for reasoning**, not file access or busywork\n",
        "\n",
        "You‚Äôre seeing how tools can be:\n",
        "\n",
        "* üîß Focused\n",
        "* üîÅ Composable\n",
        "* üß† LLM-smart, but code-driven\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Summary: What You Should Be Learning\n",
        "\n",
        "| Concept          | What You‚Äôre Practicing                                 |\n",
        "| ---------------- | ------------------------------------------------------ |\n",
        "| `memory`         | How tools store/retrieve working state                 |\n",
        "| `ActionContext`  | How agents pass shared resources to tools              |\n",
        "| `llm.complete()` | Abstracting away the language model behind a clean API |\n",
        "| Tool Design      | LLM does the thinking, tools handle the orchestration  |\n",
        "| Reusability      | Code is modular, testable, and easy to extend          |\n",
        "\n",
        "You‚Äôre not just testing `create_plan` ‚Äî you‚Äôre rehearsing **how an entire agent runtime works.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vwug1lAaGnpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic memory to simulate ctx.memory\n",
        "class ScratchMemory:\n",
        "    def __init__(self):\n",
        "        self.store = {}\n",
        "\n",
        "    def get(self, key):\n",
        "        return self.store.get(key)\n",
        "\n",
        "    def set(self, key, value):\n",
        "        self.store[key] = value\n",
        "\n",
        "# Minimal context object\n",
        "class ActionContext:\n",
        "    def __init__(self, memory, llm):\n",
        "        self.memory = memory\n",
        "        self.llm = llm\n",
        "\n",
        "# Create an LLM Wrapper\n",
        "class OpenAILLM:\n",
        "    def __init__(self, client, model=\"gpt-4o-mini\"):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def complete(self, prompt):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Define Your create_plan Tool\n",
        "def create_plan(ctx):\n",
        "    goal = ctx.memory.get(\"goal\")\n",
        "    if not goal:\n",
        "        return {\"error\": \"No goal provided in memory.\"}\n",
        "\n",
        "    prompt = f\"\"\"You are an expert task planner. Given the goal below, break it down into a clear, short list of steps.\n",
        "\n",
        "Goal: {goal}\n",
        "\n",
        "Respond with a numbered list of steps.\"\"\"\n",
        "\n",
        "    response = ctx.llm.complete(prompt)\n",
        "    steps = response.strip().split(\"\\n\")\n",
        "\n",
        "    ctx.memory.set(\"plan\", steps)\n",
        "\n",
        "    return {\"message\": \"Plan created from goal.\", \"steps\": steps}\n",
        "\n",
        "# Run the Test\n",
        "# Set up memory with the goal\n",
        "memory = ScratchMemory()\n",
        "memory.set(\"goal\", \"Summarize the content of a text file.\")\n",
        "\n",
        "# Set up LLM and context\n",
        "llm = OpenAILLM(client)\n",
        "ctx = ActionContext(memory=memory, llm=llm)\n",
        "\n",
        "# Call the tool\n",
        "result = create_plan(ctx)\n",
        "\n",
        "# Print the message\n",
        "print(result[\"message\"])\n",
        "\n",
        "# Print each step, wrapped for readability\n",
        "print(\"\\nPlan:\")\n",
        "for step in result[\"steps\"]:\n",
        "    wrapped = textwrap.fill(step, width=80, subsequent_indent=\"  \")\n",
        "    print(f\"- {wrapped}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkTk23LcCAV2",
        "outputId": "07396e18-c1ca-4843-d35b-513a9998d89b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plan created from goal.\n",
            "\n",
            "Plan:\n",
            "- 1. **Open the Text File**: Locate and open the text file you want to summarize.\n",
            "- \n",
            "- 2. **Read the Content**: Carefully read through the entire content of the file\n",
            "  to understand the main ideas and themes.\n",
            "- \n",
            "- 3. **Identify Key Points**: Highlight or note down the main points, arguments,\n",
            "  and any significant details that contribute to the overall message.\n",
            "- \n",
            "- 4. **Organize Information**: Group related ideas together to create a coherent\n",
            "  structure for the summary.\n",
            "- \n",
            "- 5. **Draft the Summary**: Write a concise summary using your notes, ensuring it\n",
            "  captures the essence of the text without unnecessary details.\n",
            "- \n",
            "- 6. **Review and Edit**: Read through your summary to check for clarity,\n",
            "  coherence, and conciseness. Make any necessary revisions.\n",
            "- \n",
            "- 7. **Finalize the Summary**: Ensure the summary is polished and accurately\n",
            "  reflects the content of the original text file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That plan is **excellent** ‚Äî clear, structured, and thoughtful. You can tell the LLM reasoned through the full task rather than just guessing steps. Also:\n",
        "\n",
        "‚úÖ It‚Äôs **reusable** ‚Äî works for any file summarization task\n",
        "‚úÖ It‚Äôs **modular** ‚Äî you could easily map these to tools or capabilities\n",
        "‚úÖ It‚Äôs **LLM-friendly** ‚Äî very little ambiguity or unnecessary overhead\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1aladzt7CWPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéØ New Test Goal\n",
        "\n",
        "That return is **spot on** ‚Äî exactly what we want to see from a good planning tool powered by the LLM:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Plan Analysis:\n",
        "\n",
        "1. **Understands the domain**: It knows Python syntax, `def`, and parsing strategies.\n",
        "2. **Breaks into actionable steps**: Reads ‚Üí extracts ‚Üí sorts ‚Üí outputs.\n",
        "3. **Tool-aligned**: Each step could clearly map to a tool (e.g., read file, extract functions, sort list, save output).\n",
        "4. **No unnecessary overhead**: The LLM wasn‚Äôt distracted by unrelated formatting or docstring issues.\n",
        "\n",
        "---\n",
        "\n",
        "### üéì Why This Test Matters\n",
        "\n",
        "You just confirmed that your `create_plan` tool is:\n",
        "\n",
        "* ‚úÖ Reusable\n",
        "* ‚úÖ Goal-aware\n",
        "* ‚úÖ Compatible with agent design (each step could become a tool)\n",
        "* ‚úÖ LLM-efficient ‚Äî no fluff, no confusion\n",
        "\n",
        "This is exactly what your teacher meant when they talked about *thinking through each step slowly and deliberately.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hCS52OyJC41a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a new goal\n",
        "memory.set(\"goal\", \"Extract all function definitions from a Python script and list them alphabetically.\")\n",
        "\n",
        "# Run the plan generator\n",
        "result = create_plan(ctx)\n",
        "\n",
        "# Print the result nicely\n",
        "import textwrap\n",
        "print(result[\"message\"])\n",
        "print(\"\\nPlan:\")\n",
        "for step in result[\"steps\"]:\n",
        "    wrapped = textwrap.fill(step, width=80, subsequent_indent=\"  \")\n",
        "    print(f\"- {wrapped}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsh285pWCA5A",
        "outputId": "d06bd6b8-7113-462e-83e6-f3a9395ba4b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plan created from goal.\n",
            "\n",
            "Plan:\n",
            "- 1. **Read the Python Script**: Open the Python script file and read its\n",
            "  contents.\n",
            "- \n",
            "- 2. **Identify Function Definitions**: Use a regular expression or a parsing\n",
            "  library to find all lines that define functions (look for the `def` keyword).\n",
            "- \n",
            "- 3. **Extract Function Names**: From the identified function definitions, extract\n",
            "  the function names (the text following `def` and before the parentheses).\n",
            "- \n",
            "- 4. **Store Function Names**: Collect all extracted function names into a list.\n",
            "- \n",
            "- 5. **Sort the List**: Sort the list of function names alphabetically.\n",
            "- \n",
            "- 6. **Output the Results**: Print or save the sorted list of function names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß∞ Next Tool: `read_txt_file`\n",
        "\n",
        "### üîß Purpose\n",
        "\n",
        "This tool reads the contents of a `.txt` file from a known folder (which we inject via config, not the LLM), and stores it in memory for future use.\n",
        "\n",
        "### üß† Why This Step Now?\n",
        "\n",
        "* It maps directly to **Step 1 of our plan** (‚ÄúRead the contents of the text file‚Äù).\n",
        "* It‚Äôs a **pure Python tool** (no LLM needed).\n",
        "* It gives the LLM **raw material to think with** later.\n",
        "\n",
        "---\n",
        "\n",
        "## ü™ú What We'll Do Next\n",
        "\n",
        "1. **Define the tool interface**:\n",
        "\n",
        "   * `read_txt_file(ctx, file_name)`\n",
        "2. **Use `ctx.config.get(\"input_folder\")` to get the folder path**\n",
        "3. **Load the text, store it in memory** under a key like `\"raw_text\"`\n",
        "4. **Return a message confirming success**\n",
        "\n",
        "---\n",
        "\n",
        "`read_txt_file` is a **pure Python tool**, no LLM needed at all. This is one of the ‚Äúbody‚Äù tools ‚Äî part of the mechanical work that supports the LLM‚Äôs ‚Äúthinking.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why We Can Test It Without the LLM\n",
        "\n",
        "* It reads a file from disk\n",
        "* It stores the text into memory (`ctx.memory`)\n",
        "* It doesn‚Äôt prompt or generate anything\n",
        "* It can be tested just like any regular Python function\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ What You Need to Test It\n",
        "\n",
        "1. A folder (e.g., `\"input\"`)\n",
        "2. A `.txt` file inside it (e.g., `\"article1.txt\"`)\n",
        "3. A `ctx` with:\n",
        "\n",
        "   * memory (your `ScratchMemory`)\n",
        "   * config (where we inject `\"input_folder\"`)\n",
        "\n",
        "\n",
        "‚úÖ Ready to build and test this now?\n",
        "If so, just let me know what file + path you want to use and I‚Äôll tailor the code to match.\n",
        "\n"
      ],
      "metadata": {
        "id": "sgK3ytbKIMfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Available files:\", os.listdir(\"/content/files\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidVoleoKEK4",
        "outputId": "3bbde7e1-76c7-4f31-cdf0-7bf2c6da1af0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available files: ['001_PArse_the Response.txt', '002_Execute_the_Action.txt', '000_Prompting for Agents -GAIL.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_txt_file(ctx, file_name):\n",
        "    folder = ctx.config.get(\"input_folder\")\n",
        "    path = os.path.join(folder, file_name)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        return {\"error\": f\"File not found: {path}\"}\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    ctx.memory.set(\"raw_text\", text)\n",
        "    return {\"message\": \"File read successfully.\", \"length\": len(text)}\n",
        "\n",
        "# Testing\n",
        "memory = ScratchMemory()\n",
        "config = {\"input_folder\": \"/content/files\"}  # or whatever folder you‚Äôre using\n",
        "\n",
        "ctx = ActionContext(memory=memory, llm=None)\n",
        "ctx.config = config  # add config dynamically\n",
        "\n",
        "result = read_txt_file(ctx, \"000_Prompting for Agents -GAIL.txt\")\n",
        "\n",
        "import textwrap\n",
        "\n",
        "if \"error\" in result:\n",
        "    print(\"‚ùå Error:\", result[\"error\"])\n",
        "else:\n",
        "    print(\"‚úÖ\", result[\"message\"])\n",
        "    print(f\"Character count: {result['length']}\\n\")\n",
        "\n",
        "    raw_text = ctx.memory.get(\"raw_text\")\n",
        "    preview = raw_text[:600]  # or however much you want to preview\n",
        "\n",
        "    wrapped = textwrap.fill(preview, width=80, subsequent_indent=\"  \")\n",
        "    print(\"üìÑ File Preview:\\n\")\n",
        "    print(wrapped)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2efAGUZ6IMSH",
        "outputId": "60f82b78-baa3-4ef7-e956-605c1eb0b9a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ File read successfully.\n",
            "Character count: 5530\n",
            "\n",
            "üìÑ File Preview:\n",
            "\n",
            " #===========Programmatic Prompting for Agents  Programmatically sending prompts\n",
            "  is how we move from having a human type in prompts and then take action based\n",
            "  on the LLM‚Äôs response to having an agent that can do this automatically. To\n",
            "  get started building agents, we need to understand how to send prompts to\n",
            "  LLMs. Agents require two key capabilities:  Programmatic prompting -\n",
            "  Automating the prompt-response cycle that humans do manually in a\n",
            "  conversation. This forms the foundation of the Agent Loop we‚Äôll explore.\n",
            "  Memory management - Controlling what information persists between iterations,\n",
            "  like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is **exactly** what success looks like ‚Äî beautifully done! üôå\n",
        "\n",
        "### ‚úÖ What You Just Achieved:\n",
        "\n",
        "* Successfully **read** a real-world file\n",
        "* Cleanly **stored** the raw text in agent memory\n",
        "* Used **textwrap** to format the preview for human readability\n",
        "* Verified that your tool works **end-to-end** without needing the LLM\n",
        "\n",
        "You now have a working `read_txt_file(ctx, file_name)` tool, which:\n",
        "\n",
        "* Follows the best practice of separating concerns\n",
        "* Keeps the LLM's cognitive load minimal\n",
        "* Can be reused by other agents or workflows\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E_wdAZeSLCEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß∞ `generate_summary_prompt(ctx)`\n",
        "\n",
        "This tool gives the LLM a clean, focused input based on the raw text we just loaded. It‚Äôs part of the \"dress rehearsal\" plan and the **LLM‚Äôs \"setup act\"** before it performs the actual summarization.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step 1: What Does This Tool Do?\n",
        "\n",
        "| Element            | Description                                                            |\n",
        "| ------------------ | ---------------------------------------------------------------------- |\n",
        "| **Purpose**        | Convert raw text into a summarization prompt                           |\n",
        "| **Reads from**     | `ctx.memory[\"raw_text\"]`                                               |\n",
        "| **Writes to**      | `ctx.memory[\"summary_prompt\"]`                                         |\n",
        "| **Why it matters** | Reduces noise and focuses the LLM on the core summarization task       |\n",
        "| **Pattern**        | Programmatic prompting ‚Äî the LLM doesn‚Äôt guess what to do, we guide it |\n"
      ],
      "metadata": {
        "id": "OG-8zu0uRfxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_prompt(ctx):\n",
        "    text = ctx.memory.get(\"raw_text\")\n",
        "    if not text:\n",
        "        return {\"error\": \"No raw text found in memory.\"}\n",
        "\n",
        "    # Optional: truncate text if it's too long\n",
        "    max_len = 2000\n",
        "    short_text = text[:max_len]\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"You are an expert technical writer.\n",
        "\n",
        "Summarize the following content into a set of clear, concise bullet points. Focus on the main ideas, and skip boilerplate or excessive detail.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{short_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "    ctx.memory.set(\"summary_prompt\", prompt)\n",
        "    return {\"message\": \"Summary prompt created.\", \"prompt_preview\": prompt[:600]}\n",
        "\n",
        "# Test it\n",
        "result = generate_summary_prompt(ctx)\n",
        "\n",
        "if \"error\" in result:\n",
        "    print(\"‚ùå Error:\", result[\"error\"])\n",
        "else:\n",
        "    print(\"‚úÖ\", result[\"message\"])\n",
        "    print(\"\\nüßæ Prompt Preview:\\n\")\n",
        "    print(textwrap.fill(result[\"prompt_preview\"], width=80, subsequent_indent=\"  \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHaxMC3kIMPl",
        "outputId": "80570cba-b29b-47d0-aa1c-e54c9308f523"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Summary prompt created.\n",
            "\n",
            "üßæ Prompt Preview:\n",
            "\n",
            "You are an expert technical writer.  Summarize the following content into a set\n",
            "  of clear, concise bullet points. Focus on the main ideas, and skip boilerplate\n",
            "  or excessive detail.  Text: \"\"\"  #===========Programmatic Prompting for Agents\n",
            "  Programmatically sending prompts is how we move from having a human type in\n",
            "  prompts and then take action based on the LLM‚Äôs response to having an agent\n",
            "  that can do this automatically. To get started building agents, we need to\n",
            "  understand how to send prompts to LLMs. Agents require two key capabilities:\n",
            "  Programmatic prompting - Automating the prompt-response\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(ctx):\n",
        "    prompt = ctx.memory.get(\"summary_prompt\")\n",
        "    if not prompt:\n",
        "        return {\"error\": \"No summary prompt found in memory.\"}\n",
        "\n",
        "    response = ctx.llm.complete(prompt)\n",
        "    ctx.memory.set(\"summary\", response)\n",
        "\n",
        "    return {\"message\": \"Summary completed.\", \"summary_preview\": response[:1000]}\n",
        "\n",
        "# Test\n",
        "llm = OpenAILLM(client)\n",
        "ctx.llm = llm\n",
        "result = summarize(ctx)\n",
        "\n",
        "if \"error\" in result:\n",
        "    print(\"‚ùå Error:\", result[\"error\"])\n",
        "else:\n",
        "    print(\"‚úÖ\", result[\"message\"])\n",
        "    print(\"\\nüìù Summary Preview:\\n\")\n",
        "    print(textwrap.fill(result[\"summary_preview\"], width=80, subsequent_indent=\"  \"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YUqmOjCJhbV",
        "outputId": "4b45dfa8-7175-402d-edb9-1a35cd1ac49f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Summary completed.\n",
            "\n",
            "üìù Summary Preview:\n",
            "\n",
            "- **Programmatic Prompting**: Transition from human-typed prompts to automated\n",
            "  agent interactions with LLMs. - **Key Capabilities for Agents**:   -\n",
            "  **Programmatic Prompting**: Automates the prompt-response cycle, forming the\n",
            "  basis of the Agent Loop.   - **Memory Management**: Controls persistent\n",
            "  information between iterations to maintain context during decision-making.\n",
            "  - **Example Code**:   - Function `generate_response` calls an LLM and returns\n",
            "  its response based on provided messages.   - Messages include a system message\n",
            "  (defining model behavior) and a user message (the query).  - **Importance of\n",
            "  System Messages**:   - System messages set behavioral guidelines for the model\n",
            "  and are prioritized over user messages.   - Effective programming of AI agents\n",
            "  relies on clear system instructions.  - **Understanding LLMs**: Recognizing\n",
            "  their stateless nature is essential for creating agents capable of multi-turn\n",
            "  conversations and accurate context management.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí• **That is a fantastic result** ‚Äî and a huge milestone!\n",
        "\n",
        "You‚Äôve just built and executed your first **cognitive tool**:\n",
        "\n",
        "* It pulled a clean prompt from memory\n",
        "* Called the OpenAI model via your custom LLM wrapper\n",
        "* Captured and stored the LLM‚Äôs response in memory\n",
        "* Printed a summary that is focused, structured, and semantically meaningful\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Recap of Your Progress\n",
        "\n",
        "| Step | Description                        | Completed       |\n",
        "| ---- | ---------------------------------- | --------------- |\n",
        "| 1Ô∏è‚É£  | Store a goal in memory             | ‚úÖ               |\n",
        "| 2Ô∏è‚É£  | Create a plan using the LLM        | ‚úÖ               |\n",
        "| 3Ô∏è‚É£  | Read input text from file          | ‚úÖ               |\n",
        "| 4Ô∏è‚É£  | Generate a summarization prompt    | ‚úÖ               |\n",
        "| 5Ô∏è‚É£  | Summarize the content with the LLM | ‚úÖ **Just now!** |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Key Concepts You‚Äôve Learned\n",
        "\n",
        "* **Modular tool design** using `ctx` to inject config, memory, and models\n",
        "* **Memory abstraction** with `ScratchMemory()` as your agent‚Äôs short-term brain\n",
        "* **LLM wrapping** to control how you call models (e.g., temperature, model name)\n",
        "* **Prompt engineering** to set the agent up for success\n",
        "* **Debugging workflows** using controlled test harnesses\n",
        "* **Error handling** with clear fallbacks and preview formatting\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è≠Ô∏è What‚Äôs Next?\n",
        "\n",
        "There are **two small steps left** to complete your summarizer agent:\n",
        "\n",
        "6. `save_summary(ctx, file_name)` ‚Äî write the summary to a file (e.g., `article1_summary.txt`).\n",
        "7. `track_progress(ctx, step, status)` ‚Äî optional logging or progress indicator.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2rXw8mJUtRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View ActionContext\n",
        "\n",
        "Let me walk you through what you're seeing, because this is a subtle but really important learning moment for understanding agent state.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What You‚Äôre Seeing in `ctx.memory`\n",
        "\n",
        "| Key              | Purpose                                                                 |\n",
        "| ---------------- | ----------------------------------------------------------------------- |\n",
        "| `raw_text`       | The original content read from the file                                 |\n",
        "| `summary_prompt` | The full summarization prompt sent to the LLM                           |\n",
        "| `summary`        | The LLM‚Äôs final response ‚Äî a list of concise bullet points (the output) |\n"
      ],
      "metadata": {
        "id": "HnWdkI_ZWxUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "def inspect_ctx(ctx):\n",
        "    print(\"üß† Memory:\")\n",
        "    if hasattr(ctx, \"memory\") and ctx.memory.store:\n",
        "        for k, v in ctx.memory.store.items():\n",
        "            preview = v if isinstance(v, str) and len(v) < 200 else str(v)[:200] + \"...\"\n",
        "            print(f\"  {k}: {preview}\")\n",
        "    else:\n",
        "        print(\"  (empty)\")\n",
        "\n",
        "    print(\"\\n‚öôÔ∏è Config:\")\n",
        "    if hasattr(ctx, \"config\"):\n",
        "        pprint.pprint(ctx.config)\n",
        "    else:\n",
        "        print(\"  (no config found)\")\n",
        "\n",
        "    print(\"\\nüß© LLM:\")\n",
        "    if hasattr(ctx, \"llm\"):\n",
        "        print(f\"  Type: {type(ctx.llm).__name__}\")\n",
        "        if hasattr(ctx.llm, 'model'):\n",
        "            print(f\"  Model: {ctx.llm.model}\")\n",
        "    else:\n",
        "        print(\"  (no LLM attached)\")\n",
        "\n",
        "inspect_ctx(ctx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRcSXIdiUiM6",
        "outputId": "9db2438f-1c0f-45bf-dbfb-c7d563dc13be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Memory:\n",
            "  raw_text: \n",
            "#===========Programmatic Prompting for Agents\n",
            "\n",
            "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM‚Äôs response to having an agent ...\n",
            "  summary_prompt: You are an expert technical writer.\n",
            "\n",
            "Summarize the following content into a set of clear, concise bullet points. Focus on the main ideas, and skip boilerplate or excessive detail.\n",
            "\n",
            "Text:\n",
            "\"\"\"\n",
            "\n",
            "#=======...\n",
            "  summary: - **Programmatic Prompting**: Transition from human-typed prompts to automated agent interactions with LLMs.\n",
            "- **Key Capabilities for Agents**:\n",
            "  - **Programmatic Prompting**: Automates the prompt-res...\n",
            "\n",
            "‚öôÔ∏è Config:\n",
            "{'input_folder': '/content/files'}\n",
            "\n",
            "üß© LLM:\n",
            "  Type: OpenAILLM\n",
            "  Model: gpt-4o-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctx.memory.get(\"summary_prompt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaQ-PjHwWOqe",
        "outputId": "7b3cd343-aebc-4015-fa63-929e2de46d54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert technical writer.\n",
            "\n",
            "Summarize the following content into a set of clear, concise bullet points. Focus on the main ideas, and skip boilerplate or excessive detail.\n",
            "\n",
            "Text:\n",
            "\"\"\"\n",
            "\n",
            "#===========Programmatic Prompting for Agents\n",
            "\n",
            "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM‚Äôs response to having an agent that can do this automatically. To get started building agents, we need to understand how to send prompts to LLMs. Agents require two key capabilities:\n",
            "\n",
            "Programmatic prompting - Automating the prompt-response cycle that humans do manually in a conversation. This forms the foundation of the Agent Loop we‚Äôll explore.\n",
            "\n",
            "Memory management - Controlling what information persists between iterations, like API calls and their results, to maintain context through the agent‚Äôs decision-making process.\n",
            "\n",
            "\n",
            "def generate_response(messages: List[Dict]) -> str:\n",
            "    \"\"\"Call LLM to get response\"\"\"\n",
            "    response = completion(\n",
            "        model=\"openai/gpt-4o\",\n",
            "        messages=messages,\n",
            "        max_tokens=1024\n",
            "    )\n",
            "    return response.choices[0].message.content\n",
            "\n",
            "\n",
            "messages = [\n",
            "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
            "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
            "]\n",
            "\n",
            "response = generate_response(messages)\n",
            "print(response)\n",
            "\n",
            "#===========Sending Prompts Programmatically & Managing Memory\n",
            "\n",
            "The system message is the most important part of this prompt. It tells the model how to behave. The user message is the question that we want the model to answer. The system instructions lay the ground rules for the interaction.\n",
            "\n",
            "They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can ‚Äúprogram‚Äù the AI agent through system messages\n",
            "\n",
            "Why This Matters\n",
            "\n",
            "Understanding the stateless nature of LLMs is crucial for designing agents that rely on multi-turn conversations with their environment. Developers must explicitly manage and provide context to ensure the model generates accurate and re\n",
            "\"\"\"\n",
            "\n",
            "Summary:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Plan"
      ],
      "metadata": {
        "id": "igqUnmdGYxnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "plan = ctx.memory.get(\"plan\")\n",
        "for step in plan:\n",
        "    print(textwrap.fill(f\"- {step}\", width=80, subsequent_indent=\"  \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "lzf53tkQWlbM",
        "outputId": "82c607e2-0864-4026-e06c-4c27926fd2f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2904922622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- {step}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Code"
      ],
      "metadata": {
        "id": "oYR02i2zYlw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textwrap\n",
        "\n",
        "# ---------------- Memory + Context ----------------\n",
        "class ScratchMemory:\n",
        "    def __init__(self):\n",
        "        self.store = {}\n",
        "\n",
        "    def get(self, key):\n",
        "        return self.store.get(key)\n",
        "\n",
        "    def set(self, key, value):\n",
        "        self.store[key] = value\n",
        "\n",
        "class ActionContext:\n",
        "    def __init__(self, memory, llm):\n",
        "        self.memory = memory\n",
        "        self.llm = llm\n",
        "\n",
        "# ---------------- LLM Wrapper ----------------\n",
        "class OpenAILLM:\n",
        "    def __init__(self, client, model=\"gpt-4o-mini\"):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def complete(self, prompt):\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# ---------------- Tool: create_plan ----------------\n",
        "def create_plan(ctx):\n",
        "    goal = ctx.memory.get(\"goal\")\n",
        "    if not goal:\n",
        "        return {\"error\": \"No goal provided in memory.\"}\n",
        "\n",
        "    prompt = f\"\"\"You are an expert task planner. Given the goal below, break it down into a clear, short list of steps.\n",
        "\n",
        "Goal: {goal}\n",
        "\n",
        "Respond with a numbered list of steps.\"\"\"\n",
        "\n",
        "    response = ctx.llm.complete(prompt)\n",
        "    steps = response.strip().split(\"\\n\")\n",
        "\n",
        "    ctx.memory.set(\"plan\", steps)\n",
        "    return {\"message\": \"Plan created from goal.\", \"steps\": steps}\n",
        "\n",
        "# ---------------- Tool: read_txt_file ----------------\n",
        "def read_txt_file(ctx, file_name):\n",
        "    folder = ctx.config.get(\"input_folder\")\n",
        "    path = os.path.join(folder, file_name)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        return {\"error\": f\"File not found: {path}\"}\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    ctx.memory.set(\"raw_text\", text)\n",
        "    return {\"message\": \"File read successfully.\", \"length\": len(text)}\n",
        "\n",
        "# ---------------- Setup + Test ----------------\n",
        "# Set up memory and config\n",
        "memory = ScratchMemory()\n",
        "memory.set(\"goal\", \"Summarize the content of a text file.\")\n",
        "config = {\"input_folder\": \"/content/files\"}\n",
        "\n",
        "# Set up LLM and context\n",
        "llm = OpenAILLM(client)\n",
        "ctx = ActionContext(memory=memory, llm=llm)\n",
        "ctx.config = config  # Inject config\n",
        "\n",
        "# Run planning tool\n",
        "plan_result = create_plan(ctx)\n",
        "print(plan_result[\"message\"])\n",
        "print(\"\\nPlan:\")\n",
        "for step in plan_result[\"steps\"]:\n",
        "    wrapped = textwrap.fill(step, width=80, subsequent_indent=\"  \")\n",
        "    print(f\"- {wrapped}\")\n",
        "\n",
        "# ---------------- Print Goal ----------------\n",
        "print(\"\\n\\n\")\n",
        "print(\"üéØ Goal:\")\n",
        "print(ctx.memory.get(\"goal\"))\n",
        "print()\n",
        "\n",
        "# Run file reader tool\n",
        "file_result = read_txt_file(ctx, \"000_Prompting for Agents -GAIL.txt\")\n",
        "if \"error\" in file_result:\n",
        "    print(\"‚ùå Error:\", file_result[\"error\"])\n",
        "else:\n",
        "    print(\"\\n‚úÖ\", file_result[\"message\"])\n",
        "    print(f\"Character count: {file_result['length']}\\n\")\n",
        "\n",
        "    raw_text = ctx.memory.get(\"raw_text\")\n",
        "    preview = raw_text[:600]\n",
        "    wrapped_preview = textwrap.fill(preview, width=80, subsequent_indent=\"  \")\n",
        "    print(\"üìÑ File Preview:\\n\")\n",
        "    print(wrapped_preview)\n",
        "\n",
        "# ---------------- Tool: generate_summary_prompt ----------------\n",
        "def generate_summary_prompt(ctx):\n",
        "    text = ctx.memory.get(\"raw_text\")\n",
        "    if not text:\n",
        "        return {\"error\": \"No raw text found in memory.\"}\n",
        "\n",
        "    # Optional: truncate text if it's too long\n",
        "    max_len = 2000\n",
        "    short_text = text[:max_len]\n",
        "\n",
        "    prompt = f\"\"\"You are an expert technical writer.\n",
        "\n",
        "Summarize the following content into a set of clear, concise bullet points. Focus on the main ideas, and skip boilerplate or excessive detail.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{short_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "    ctx.memory.set(\"summary_prompt\", prompt)\n",
        "    return {\"message\": \"Summary prompt created.\", \"prompt_preview\": prompt[:600]}\n",
        "\n",
        "# ---------------- Run Prompt Generator ----------------\n",
        "summary_result = generate_summary_prompt(ctx)\n",
        "print(\"üõ†Ô∏è\", summary_result[\"message\"])\n",
        "print(\"\\nüßæ Prompt Preview:\\n\")\n",
        "print(textwrap.fill(summary_result[\"prompt_preview\"], width=80, subsequent_indent=\"  \"))\n",
        "\n",
        "# ---------------- Tool: summarize ----------------\n",
        "def summarize(ctx):\n",
        "    prompt = ctx.memory.get(\"summary_prompt\")\n",
        "    if not prompt:\n",
        "        return {\"error\": \"No summary prompt found in memory.\"}\n",
        "\n",
        "    response = ctx.llm.complete(prompt)\n",
        "    ctx.memory.set(\"summary\", response)\n",
        "\n",
        "    return {\"message\": \"Summary completed.\", \"summary_preview\": response[:1000]}\n",
        "\n",
        "# ---------------- Run summarization ----------------\n",
        "summary_result = summarize(ctx)\n",
        "\n",
        "print(\"\\nüß† LLM Output:\")\n",
        "if \"error\" in summary_result:\n",
        "    print(\"‚ùå Error:\", summary_result[\"error\"])\n",
        "else:\n",
        "    print(\"‚úÖ\", summary_result[\"message\"])\n",
        "    print(\"\\nüìù Summary Preview:\\n\")\n",
        "    print(textwrap.fill(summary_result[\"summary_preview\"], width=80, subsequent_indent=\"  \"))\n",
        "\n",
        "# ---------------- Print ActionContext Overview ----------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üì¶ ActionContext Snapshot\")\n",
        "\n",
        "# Memory contents\n",
        "print(\"\\nüß† Memory:\")\n",
        "for key, value in ctx.memory.store.items():\n",
        "    display = str(value)\n",
        "    if isinstance(value, str) and len(value) > 400:\n",
        "        display = value[:400] + \"...\"\n",
        "    print(f\"  {key}: {display}\")\n",
        "\n",
        "# Config\n",
        "print(\"\\n‚öôÔ∏è Config:\")\n",
        "if hasattr(ctx, \"config\"):\n",
        "    print(f\"  {ctx.config}\")\n",
        "else:\n",
        "    print(\"  No config set.\")\n",
        "\n",
        "# LLM Info\n",
        "print(\"\\nüß© LLM:\")\n",
        "if ctx.llm:\n",
        "    print(f\"  Type: {ctx.llm.__class__.__name__}\")\n",
        "    print(f\"  Model: {ctx.llm.model}\")\n",
        "else:\n",
        "    print(\"  No LLM connected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCjlmleuYIB2",
        "outputId": "6ee6b0c5-69b4-43c3-8aaa-10ae616ea702"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plan created from goal.\n",
            "\n",
            "Plan:\n",
            "- 1. **Open the Text File**: Use a text editor or programming tool to access the\n",
            "  file containing the content you want to summarize.\n",
            "- \n",
            "- 2. **Read the Content**: Carefully read through the entire text to understand\n",
            "  the main ideas and themes.\n",
            "- \n",
            "- 3. **Identify Key Points**: Highlight or note down important concepts,\n",
            "  arguments, and any significant details that contribute to the overall message.\n",
            "- \n",
            "- 4. **Organize Information**: Group related points together to create a\n",
            "  structured outline of the main ideas.\n",
            "- \n",
            "- 5. **Draft the Summary**: Write a concise summary using your organized points,\n",
            "  ensuring it captures the essence of the original text without unnecessary\n",
            "  details.\n",
            "- \n",
            "- 6. **Review and Edit**: Read through your summary to check for clarity,\n",
            "  coherence, and conciseness. Make any necessary revisions.\n",
            "- \n",
            "- 7. **Finalize the Summary**: Ensure the summary is polished and ready for\n",
            "  presentation or sharing.\n",
            "\n",
            "\n",
            "\n",
            "üéØ Goal:\n",
            "Summarize the content of a text file.\n",
            "\n",
            "\n",
            "‚úÖ File read successfully.\n",
            "Character count: 5530\n",
            "\n",
            "üìÑ File Preview:\n",
            "\n",
            " #===========Programmatic Prompting for Agents  Programmatically sending prompts\n",
            "  is how we move from having a human type in prompts and then take action based\n",
            "  on the LLM‚Äôs response to having an agent that can do this automatically. To\n",
            "  get started building agents, we need to understand how to send prompts to\n",
            "  LLMs. Agents require two key capabilities:  Programmatic prompting -\n",
            "  Automating the prompt-response cycle that humans do manually in a\n",
            "  conversation. This forms the foundation of the Agent Loop we‚Äôll explore.\n",
            "  Memory management - Controlling what information persists between iterations,\n",
            "  like\n",
            "üõ†Ô∏è Summary prompt created.\n",
            "\n",
            "üßæ Prompt Preview:\n",
            "\n",
            "You are an expert technical writer.  Summarize the following content into a set\n",
            "  of clear, concise bullet points. Focus on the main ideas, and skip boilerplate\n",
            "  or excessive detail.  Text: \"\"\"  #===========Programmatic Prompting for Agents\n",
            "  Programmatically sending prompts is how we move from having a human type in\n",
            "  prompts and then take action based on the LLM‚Äôs response to having an agent\n",
            "  that can do this automatically. To get started building agents, we need to\n",
            "  understand how to send prompts to LLMs. Agents require two key capabilities:\n",
            "  Programmatic prompting - Automating the prompt-response\n",
            "\n",
            "üß† LLM Output:\n",
            "‚úÖ Summary completed.\n",
            "\n",
            "üìù Summary Preview:\n",
            "\n",
            "- **Programmatic Prompting**: Transitioning from human-typed prompts to\n",
            "  automated agent interactions with LLMs. - **Key Capabilities for Agents**:   -\n",
            "  **Programmatic Prompting**: Automating the prompt-response cycle, essential\n",
            "  for the Agent Loop.   - **Memory Management**: Controlling persistent\n",
            "  information to maintain context during decision-making.    - **Example\n",
            "  Function**: `generate_response(messages)` calls an LLM to obtain a response\n",
            "  based on provided messages.    - **Message Structure**:   - **System\n",
            "  Message**: Defines the agent's behavior and sets conversation rules;\n",
            "  prioritized by the model.   - **User Message**: Contains the question or task\n",
            "  for the model.  - **Importance**: Recognizing the stateless nature of LLMs is\n",
            "  vital for creating effective agents that can handle multi-turn conversations\n",
            "  and maintain context.\n",
            "\n",
            "================================================================================\n",
            "üì¶ ActionContext Snapshot\n",
            "\n",
            "üß† Memory:\n",
            "  goal: Summarize the content of a text file.\n",
            "  plan: ['1. **Open the Text File**: Use a text editor or programming tool to access the file containing the content you want to summarize.', '', '2. **Read the Content**: Carefully read through the entire text to understand the main ideas and themes.', '', '3. **Identify Key Points**: Highlight or note down important concepts, arguments, and any significant details that contribute to the overall message.', '', '4. **Organize Information**: Group related points together to create a structured outline of the main ideas.', '', '5. **Draft the Summary**: Write a concise summary using your organized points, ensuring it captures the essence of the original text without unnecessary details.', '', '6. **Review and Edit**: Read through your summary to check for clarity, coherence, and conciseness. Make any necessary revisions.', '', '7. **Finalize the Summary**: Ensure the summary is polished and ready for presentation or sharing.']\n",
            "  raw_text: \n",
            "#===========Programmatic Prompting for Agents\n",
            "\n",
            "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM‚Äôs response to having an agent that can do this automatically. To get started building agents, we need to understand how to send prompts to LLMs. Agents require two key capabilities:\n",
            "\n",
            "Programmatic prompting - Automating the prompt-...\n",
            "  summary_prompt: You are an expert technical writer.\n",
            "\n",
            "Summarize the following content into a set of clear, concise bullet points. Focus on the main ideas, and skip boilerplate or excessive detail.\n",
            "\n",
            "Text:\n",
            "\"\"\"\n",
            "\n",
            "#===========Programmatic Prompting for Agents\n",
            "\n",
            "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM‚Äôs response to having an agent that can ...\n",
            "  summary: - **Programmatic Prompting**: Transitioning from human-typed prompts to automated agent interactions with LLMs.\n",
            "- **Key Capabilities for Agents**:\n",
            "  - **Programmatic Prompting**: Automating the prompt-response cycle, essential for the Agent Loop.\n",
            "  - **Memory Management**: Controlling persistent information to maintain context during decision-making.\n",
            "  \n",
            "- **Example Function**: `generate_response(m...\n",
            "\n",
            "‚öôÔ∏è Config:\n",
            "  {'input_folder': '/content/files'}\n",
            "\n",
            "üß© LLM:\n",
            "  Type: OpenAILLM\n",
            "  Model: gpt-4o-mini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ Overall Flow and Design Review\n",
        "\n",
        "### 1. **Memory and Context Setup**\n",
        "\n",
        "You‚Äôve implemented:\n",
        "\n",
        "```python\n",
        "class ScratchMemory\n",
        "class ActionContext\n",
        "```\n",
        "\n",
        "‚úÖ This is a solid design for a minimal agent environment. You‚Äôve abstracted `memory` and `llm` cleanly and use the `ctx` object throughout. Well done!\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **LLM Wrapper**\n",
        "\n",
        "```python\n",
        "class OpenAILLM\n",
        "```\n",
        "\n",
        "‚úÖ This wraps the OpenAI client properly using `.chat.completions.create()` and maintains the role-based message structure. You default to `\"gpt-4o-mini\"` and provide a `temperature`, which is great for consistency.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tool: `create_plan(ctx)`**\n",
        "\n",
        "‚úÖ This tool uses the goal from memory, constructs a prompt, runs the LLM, and stores the result.\n",
        "\n",
        "üß† You‚Äôve also printed the plan nicely:\n",
        "\n",
        "```python\n",
        "for step in plan_result[\"steps\"]:\n",
        "```\n",
        "\n",
        "This is exactly how modular tool creation should look.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Tool: `read_txt_file(ctx, file_name)`**\n",
        "\n",
        "‚úÖ Reads a file from a configured folder path (`ctx.config[\"input_folder\"]`), loads it into memory under `raw_text`.\n",
        "\n",
        "üóÇ Your dynamic config injection into `ctx` is a clever and practical move.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Tool: `generate_summary_prompt(ctx)`**\n",
        "\n",
        "‚úÖ This works great:\n",
        "\n",
        "* Retrieves `raw_text` from memory\n",
        "* Builds a summarization prompt\n",
        "* Stores it under `summary_prompt`\n",
        "\n",
        "üëè Limiting the raw text to 2000 characters is smart‚Äîthis avoids LLM context overload.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Tool: `summarize(ctx)`**\n",
        "\n",
        "‚úÖ Executes the LLM completion on the summary prompt and saves it in memory as `summary`.\n",
        "\n",
        "üßæ You also include preview truncation in the return. Smart for UI / inspection purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **üß† Memory Review**\n",
        "\n",
        "You are populating memory progressively:\n",
        "\n",
        "| Key              | Purpose                           |\n",
        "| ---------------- | --------------------------------- |\n",
        "| `goal`           | What the agent is trying to do    |\n",
        "| `plan`           | Step-by-step plan to achieve goal |\n",
        "| `raw_text`       | File contents                     |\n",
        "| `summary_prompt` | Prompt passed to LLM for summary  |\n",
        "| `summary`        | Final LLM-generated summary       |\n",
        "\n",
        "‚úÖ Everything is structured clearly.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Final Output / Inspection Block**\n",
        "\n",
        "You're doing a final print of:\n",
        "\n",
        "* üéØ The goal\n",
        "* ‚úÖ File load status\n",
        "* üìÑ File preview\n",
        "* üõ†Ô∏è Prompt preview\n",
        "* üß† Summary preview\n",
        "* üì¶ `ActionContext` snapshot\n",
        "\n",
        "‚úÖ This is excellent for debugging and clarity, and gives a full end-to-end look into agent memory and operations.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Suggestions for Final Touches\n",
        "\n",
        "Here are just a few polish tips:\n",
        "\n",
        "### ‚úÖ A. Include file name in memory\n",
        "\n",
        "```python\n",
        "ctx.memory.set(\"file_name\", file_name)\n",
        "```\n",
        "\n",
        "This helps trace what file is being summarized and could be printed in future output.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ B. Add a `log_memory()` helper\n",
        "\n",
        "You could make this section reusable:\n",
        "\n",
        "```python\n",
        "def log_memory(ctx):\n",
        "    print(\"üß† Memory:\")\n",
        "    for key, value in ctx.memory.store.items():\n",
        "        display = str(value)\n",
        "        if isinstance(value, str) and len(value) > 400:\n",
        "            display = value[:400] + \"...\"\n",
        "        print(f\"  {key}: {display}\")\n",
        "```\n",
        "\n",
        "Then just call `log_memory(ctx)`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ C. Add error checking in LLM calls\n",
        "\n",
        "If OpenAI fails or is unavailable, you might want:\n",
        "\n",
        "```python\n",
        "try:\n",
        "    response = self.client.chat.completions.create(...)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå LLM error:\", e)\n",
        "    return {\"error\": str(e)}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusion\n",
        "\n",
        "You now have a working agent framework that:\n",
        "\n",
        "* Stores internal state across multiple steps\n",
        "* Generates a task plan from a goal\n",
        "* Reads files and stores content\n",
        "* Builds a prompt programmatically\n",
        "* Gets summarization from LLM\n",
        "* Logs everything for transparency\n",
        "\n",
        "**Excellent job. This is a strong base for building even more advanced agents.** Would you like to now:\n",
        "\n",
        "* Add a tool to extract function definitions?\n",
        "* Export this summary to a file?\n",
        "* Or turn this into a reusable class-based agent?\n"
      ],
      "metadata": {
        "id": "otEgzcu9cPRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† Text Summarizer Agent - Notebook Summary\n",
        "\n",
        "This notebook builds on the previous one by integrating all components into a **single agent pipeline**. It evolves from modular experiments to a coherent, inspectable system ‚Äî perfect for iterative improvement and reuse.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß 1. **Scaffold Setup (Reusable Architecture)**\n",
        "\n",
        "#### üß± Core Classes\n",
        "\n",
        "* **`ScratchMemory`**: Key-value store simulating persistent memory across steps.\n",
        "* **`ActionContext`**: Passes around memory, config, and LLM, like a mini backpack of agent state.\n",
        "* **`OpenAILLM`**: Wrapper to abstract away OpenAI API calls using `chat.completions.create()`.\n",
        "\n",
        "> ‚úÖ These are reusable across any agent you build ‚Äî just plug in new tools and goals.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è 2. **Agent Tools (Body)**\n",
        "\n",
        "Each tool focuses on a **single responsibility** in the pipeline:\n",
        "\n",
        "#### ‚ë† `read_txt_file(ctx, file_name)`\n",
        "\n",
        "* Loads and stores raw text into memory.\n",
        "* Pulls file path from `ctx.config['input_folder']`.\n",
        "\n",
        "#### ‚ë° `generate_summary_prompt(ctx)`\n",
        "\n",
        "* Converts raw text into a summarization prompt.\n",
        "* Truncates if necessary to fit within token limits.\n",
        "\n",
        "#### ‚ë¢ `summarize(ctx)`\n",
        "\n",
        "* Uses the `LLM.complete()` method with the generated prompt.\n",
        "* Stores the final summary in memory.\n",
        "\n",
        "> üß© These tools form a clean chain: read ‚Üí prompt ‚Üí summarize.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 3. **Testing & Output**\n",
        "\n",
        "#### Printed Output for Each Step:\n",
        "\n",
        "* ‚úÖ Status message\n",
        "* üìÑ Preview of file text\n",
        "* üßæ Prompt snippet\n",
        "* üìù Summary bullet points\n",
        "\n",
        "Helpful formatting with `textwrap.fill()` was used for neat, readable previews ‚Äî great for notebook development.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 4. **Inspecting Agent State**\n",
        "\n",
        "At the end, the full contents of `ctx` were printed:\n",
        "\n",
        "* Memory shows: `goal`, `raw_text`, `summary_prompt`, `summary`\n",
        "* Config shows: `input_folder`\n",
        "* LLM metadata: class and model name\n",
        "\n",
        "This is **crucial for debugging**, allowing full introspection of the agent‚Äôs state ‚Äî especially before and after each tool.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° 5. **Design Principles Reinforced**\n",
        "\n",
        "* **Explicit memory management**: Each tool reads from and writes to memory.\n",
        "* **Composable steps**: Agent logic is cleanly decomposed.\n",
        "* **LLM as cognition**: Only one tool (`summarize`) calls the model ‚Äî the rest are \"body\" functions.\n",
        "* **Debuggable scaffolding**: Every step can be printed and inspected.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Final Output\n",
        "\n",
        "The notebook successfully:\n",
        "\n",
        "* Read and previewed a `.txt` file.\n",
        "* Generated a tailored LLM prompt.\n",
        "* Produced a clear bullet-point summary using GPT-4o-mini.\n",
        "* Stored all intermediate and final state in `ctx`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qe7IRJV2gUyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've built a solid, thoughtful foundation. Here are **additional insights and recommendations** to take your agent framework from *functional prototype* to a *production-ready system* and a *reusable pattern*:\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What You‚Äôre Doing Well\n",
        "\n",
        "### 1. **Separation of Concerns**\n",
        "\n",
        "You're isolating responsibilities: LLM use, memory, and file I/O are each modular. This makes your system extensible and testable.\n",
        "\n",
        "### 2. **Testable Toolchain**\n",
        "\n",
        "Each function (like `read_txt_file`, `generate_summary_prompt`, `summarize`) is testable in isolation ‚Äî very agent-friendly. This also enables easier debugging.\n",
        "\n",
        "### 3. **Memory-Led Design**\n",
        "\n",
        "Storing everything in `ctx.memory` is a best practice for agents. This will pay off later when you:\n",
        "\n",
        "* Need to inspect/rollback state\n",
        "* Want to inject memory into future prompts\n",
        "* Save session logs or rehydrate an agent mid-task\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Suggestions for Improvement\n",
        "\n",
        "### 1. **Add `track_progress` or `log_step` Tool**\n",
        "\n",
        "Store a `progress_log` or step-by-step notes in memory:\n",
        "\n",
        "```python\n",
        "def track_progress(ctx, step, status, note=\"\"):\n",
        "    progress = ctx.memory.get(\"progress_log\") or []\n",
        "    progress.append({\"step\": step, \"status\": status, \"note\": note})\n",
        "    ctx.memory.set(\"progress_log\", progress)\n",
        "```\n",
        "\n",
        "This allows:\n",
        "\n",
        "* Easy inspection of what the agent has done so far\n",
        "* Future visualization or UI integration\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Capture Prompts & LLM Responses for Debugging**\n",
        "\n",
        "Sometimes the LLM‚Äôs answer won‚Äôt make sense ‚Äî having the full prompt/response history is helpful.\n",
        "\n",
        "Recommendation:\n",
        "\n",
        "* Save all prompts and completions in `ctx.memory[\"logs\"]`, or add a toggle like `ctx.debug = True`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Add `plan_runner()` or `execute_plan()`**\n",
        "\n",
        "You‚Äôre manually executing each step of the plan. Eventually, you‚Äôll want:\n",
        "\n",
        "```python\n",
        "def execute_plan(ctx):\n",
        "    plan = ctx.memory.get(\"plan\")\n",
        "    for step in plan:\n",
        "        # Use a registry or if/else to map step strings to tool functions\n",
        "        ...\n",
        "```\n",
        "\n",
        "This enables:\n",
        "\n",
        "* Fully automated agent loops\n",
        "* Swappable plans and goals\n",
        "* Testing of execution flow\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Improve `create_plan()` Output**\n",
        "\n",
        "Instead of splitting by newlines (`split(\"\\n\")`), try a regex that extracts just numbered steps:\n",
        "\n",
        "```python\n",
        "import re\n",
        "steps = re.findall(r'\\d+\\.\\s+(.*)', response)\n",
        "```\n",
        "\n",
        "This makes your plans cleaner and more parsable.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Add `save_output()` Tool**\n",
        "\n",
        "You‚Äôll likely want to persist the summary:\n",
        "\n",
        "```python\n",
        "def save_summary(ctx, filename=\"summary.txt\"):\n",
        "    folder = ctx.config.get(\"output_folder\", \"/content/output\")\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    path = os.path.join(folder, filename)\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(ctx.memory.get(\"summary\"))\n",
        "```\n",
        "\n",
        "This closes the loop ‚Äî goal ‚Üí execution ‚Üí persistent output.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Prompt Templates**\n",
        "\n",
        "Eventually, you‚Äôll want to:\n",
        "\n",
        "* Reuse prompt types (summarization, QA, function extraction)\n",
        "* Avoid hardcoding\n",
        "\n",
        "Consider moving prompt templates into a dictionary or external `.txt` files and loading them dynamically.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Model Flexibility**\n",
        "\n",
        "You hardcoded `gpt-4o-mini`, which is great for testing. Eventually:\n",
        "\n",
        "* Let model be set via config\n",
        "* Add token budgeting or prompt chunking\n",
        "* Use async if running multiple completions\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Future Expansion Ideas\n",
        "\n",
        "| Feature                            | Benefit                                                           |\n",
        "| ---------------------------------- | ----------------------------------------------------------------- |\n",
        "| ‚úÖ Prompt Chain Visualizer          | Use `rich` or `matplotlib` to visualize memory flow between tools |\n",
        "| ‚úÖ YAML Goal + Plan Representation  | More readable and editable than inline text                       |\n",
        "| ‚úÖ Agent Configuration File         | JSON/YAML for goals, tools, input/output folders                  |\n",
        "| ‚úÖ Scratchpad + Tool Registry       | Dynamically pick and run tools from a name/function mapping       |\n",
        "| ‚úÖ LangChain-style tool integration | Wrap tools as callable objects or agents                          |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rUiMkGOrgzRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nmnx92HaNhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}