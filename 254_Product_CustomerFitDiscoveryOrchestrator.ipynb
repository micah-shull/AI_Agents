{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMO1/TJhaNLP+me1zjj/B9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/254_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering utilities for Product-Customer Fit Discovery Orchestrator\n",
        "\n",
        "This set of utilities represents the **Clustering Agent** itselfâ€”**Step 3** in your DAGâ€”which is dedicated to performing **unsupervised machine learning** to achieve the \"customer\\_segmentation\" and \"product\\_bundling\" goals.\n",
        "\n",
        "This section confirms your architectureâ€™s reliance on **specialized ML libraries** for tasks where numerical precision is paramount.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Autonomous Machine Learning\n",
        "\n",
        "The primary focus here is on **feature engineering for algorithms**, **autonomous execution**, and **ML quality assurance**.\n",
        "\n",
        "### ðŸŽ¯ What to Focus On: Feature Preparation and Scaling\n",
        "\n",
        "The most important step for reliable clustering is preparing the feature matrix, covered by `prepare_customer_features` and `prepare_product_features`.\n",
        "\n",
        "1.  **Homogenization of Features (One-Hot Encoding):**\n",
        "    * **Focus:** Clustering algorithms like K-means rely on calculating the distance between points. Categorical text data (like `Age_Group` or `Product_Type`) must be converted into a numerical format using **One-Hot Encoding** (a binary vector where $\\text{1.0}$ means \"this feature is present\"). This is a fundamental requirement for distance-based ML.\n",
        "2.  **Standardization vs. Normalization:**\n",
        "    * **Standard Scaling (`StandardScaler`):** This is applied to the final feature matrix. It transforms data to have a mean of $\\text{0}$ and a standard deviation of $\\text{1}$.\n",
        "    * **Why it Matters:** Without scaling, features with naturally large ranges (like $\\text{transaction\\_count}$) would completely dominate the distance calculation, making the segmentation meaningless. Scaling ensures that the $\\text{Engagement Score}$ has the same impact on the clusters as the $\\text{Transaction Count}$.\n",
        "3.  **Feature Blend:** You are clustering based on a blend of **demographics** (age, location) and **behavioral/derived metrics** (diversity, engagement score). This creates segments that are both *describable* (demographics) and *predictive* (behavior).\n",
        "\n",
        "***\n",
        "\n",
        "### âœ¨ Differentiation: Autonomous and Validated Clustering\n",
        "\n",
        "Your agent is more powerful than a simple ML script because it automates complex decisions and validates its own work:\n",
        "\n",
        "1.  **Automated Cluster Selection (`find_optimal_clusters`):**\n",
        "    * **The Power:** This function provides **Autonomy**. It uses the **Silhouette Score**, a quantitative metric for measuring how similar an object is to its own cluster compared to other clusters (score closer to $\\text{+1}$ is better).\n",
        "    * By iterating from $\\text{k=2}$ to $\\text{max\\_clusters}$ and choosing the $\\text{k}$ with the best Silhouette Score, the agent eliminates the need for a human data scientist to manually determine the optimal number of segments, making the whole workflow faster and more objective.\n",
        "\n",
        "2.  **The Final Translation (`analyze_cluster_characteristics`):**\n",
        "    * **The Power:** This function acts as the **Analytic Interpreter**. The raw output of K-means is just a list of numbers (labels $\\text{0, 1, 2, ...}$). This function takes those labels and translates them back into a strategic summary by calculating things like the **most common age group**, **top products**, and **average usage** for each cluster.\n",
        "    * This **Strategic Summarization** provides the Synthesis Agent with the human-readable insights it needs to name the segments (e.g., \"The Low-Engagement Seniors\") and formulate the final business opportunities.\n",
        "\n",
        "This robust framework ensures your agent's conclusions are not just LLM-generated guesses, but **data-validated, rigorously calculated machine learning results.**"
      ],
      "metadata": {
        "id": "sZEflFi-gvW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMA7nK_QfNfG"
      },
      "outputs": [],
      "source": [
        "\"\"\"Clustering utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "def prepare_customer_features(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for customer clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Demographics (age group, location tier, acquisition channel) - one-hot encoded\n",
        "    - Behavioral (transaction count, product diversity, engagement score)\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, customer_ids)\n",
        "    \"\"\"\n",
        "    customer_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get engagement metrics\n",
        "    engagement = derived_features.get(\"customer_engagement\", {})\n",
        "\n",
        "    # Group transactions by customer\n",
        "    customer_txn_count = defaultdict(int)\n",
        "    customer_products = defaultdict(set)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id:\n",
        "            customer_txn_count[customer_id] += 1\n",
        "            if product_id:\n",
        "                customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Age group mapping\n",
        "    age_groups = [\"18-24\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    location_tiers = [\"Tier 1 (High)\", \"Tier 2 (Medium)\", \"Tier 3 (Low)\"]\n",
        "    acquisition_channels = [\"Email\", \"Referral\", \"Search\", \"Social\"]\n",
        "\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if not customer_id:\n",
        "            continue\n",
        "\n",
        "        customer_ids.append(customer_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Age group (one-hot)\n",
        "        age_group = customer.get(\"Age_Group\", \"\")\n",
        "        for age in age_groups:\n",
        "            feature_vector.append(1.0 if age_group == age else 0.0)\n",
        "\n",
        "        # Location tier (one-hot)\n",
        "        location = customer.get(\"Location_Tier\", \"\")\n",
        "        for tier in location_tiers:\n",
        "            feature_vector.append(1.0 if location == tier else 0.0)\n",
        "\n",
        "        # Acquisition channel (one-hot)\n",
        "        channel = customer.get(\"Acquisition_Channel\", \"\")\n",
        "        for acq in acquisition_channels:\n",
        "            feature_vector.append(1.0 if channel == acq else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        txn_count = customer_txn_count.get(customer_id, 0)\n",
        "        product_diversity = len(customer_products.get(customer_id, set()))\n",
        "\n",
        "        # Engagement score\n",
        "        eng_data = engagement.get(customer_id, {})\n",
        "        engagement_score = eng_data.get(\"engagement_score\", 0.0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            txn_count / 100.0,  # Normalize\n",
        "            product_diversity / 20.0,  # Normalize\n",
        "            engagement_score\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), customer_ids\n",
        "\n",
        "\n",
        "def prepare_product_features(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for product clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Product attributes (type, monetization model) - one-hot encoded\n",
        "    - Feature set (which features A, B, C, D) - one-hot encoded\n",
        "    - Behavioral (popularity score, customer count, transaction count)\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, product_ids)\n",
        "    \"\"\"\n",
        "    product_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get popularity metrics\n",
        "    popularity = derived_features.get(\"product_popularity\", {})\n",
        "\n",
        "    # Product type and monetization model mappings\n",
        "    product_types = [\"Hardware\", \"Software\", \"Service\"]\n",
        "    monetization_models = [\"One-Time Purchase\", \"Freemium\", \"Subscription\"]\n",
        "    feature_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if not product_id:\n",
        "            continue\n",
        "\n",
        "        product_ids.append(product_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Product type (one-hot)\n",
        "        ptype = product.get(\"Product_Type\", \"\")\n",
        "        for pt in product_types:\n",
        "            feature_vector.append(1.0 if ptype == pt else 0.0)\n",
        "\n",
        "        # Monetization model (one-hot)\n",
        "        monet = product.get(\"Monetization_Model\", \"\")\n",
        "        for mm in monetization_models:\n",
        "            feature_vector.append(1.0 if monet == mm else 0.0)\n",
        "\n",
        "        # Feature set (one-hot for A, B, C, D)\n",
        "        feature_list = product.get(\"feature_list\", [])\n",
        "        for feat in feature_letters:\n",
        "            feature_vector.append(1.0 if feat in feature_list else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        pop_data = popularity.get(product_id, {})\n",
        "        popularity_score = pop_data.get(\"popularity_score\", 0.0)\n",
        "        customer_count = pop_data.get(\"customer_count\", 0)\n",
        "        txn_count = pop_data.get(\"transaction_count\", 0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            popularity_score,\n",
        "            customer_count / 200.0,  # Normalize\n",
        "            txn_count / 1000.0  # Normalize\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), product_ids\n",
        "\n",
        "\n",
        "def find_optimal_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    max_clusters: int = 10,\n",
        "    min_clusters: int = 2\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find optimal number of clusters using elbow method and silhouette score.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix for clustering\n",
        "        max_clusters: Maximum clusters to consider\n",
        "        min_clusters: Minimum clusters to consider\n",
        "\n",
        "    Returns:\n",
        "        Optimal number of clusters\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) < min_clusters:\n",
        "        return len(feature_matrix)\n",
        "\n",
        "    max_clusters = min(max_clusters, len(feature_matrix) - 1)\n",
        "\n",
        "    if max_clusters < min_clusters:\n",
        "        return min_clusters\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    best_k = min_clusters\n",
        "    best_silhouette = -1\n",
        "\n",
        "    for k in range(min_clusters, max_clusters + 1):\n",
        "        try:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "            if len(set(labels)) > 1:  # Need at least 2 clusters\n",
        "                silhouette = silhouette_score(scaled_features, labels)\n",
        "                if silhouette > best_silhouette:\n",
        "                    best_silhouette = silhouette\n",
        "                    best_k = k\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return best_k\n",
        "\n",
        "\n",
        "def cluster_customers(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster customers using K-means.\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, customer_ids = prepare_customer_features(\n",
        "        customers, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"customer_ids\": customer_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "\n",
        "def cluster_products(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster products using K-means.\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, product_ids = prepare_product_features(\n",
        "        products, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"product_ids\": product_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "\n",
        "def analyze_cluster_characteristics(\n",
        "    cluster_labels: List[int],\n",
        "    entity_ids: List[str],\n",
        "    entities: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    entity_type: str = \"customer\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyze characteristics of each cluster.\n",
        "\n",
        "    Args:\n",
        "        cluster_labels: Cluster assignment for each entity\n",
        "        entity_ids: List of entity IDs\n",
        "        entities: List of entity dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        entity_type: \"customer\" or \"product\"\n",
        "\n",
        "    Returns:\n",
        "        List of cluster analysis dictionaries\n",
        "    \"\"\"\n",
        "    # Group entities by cluster\n",
        "    clusters = defaultdict(list)\n",
        "    for idx, (entity_id, label) in enumerate(zip(entity_ids, cluster_labels)):\n",
        "        clusters[label].append((entity_id, idx))\n",
        "\n",
        "    cluster_analyses = []\n",
        "\n",
        "    # Create entity lookup\n",
        "    entity_lookup = {e.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\"): e\n",
        "                     for e in entities}\n",
        "\n",
        "    # Group transactions by entity\n",
        "    entity_transactions = defaultdict(list)\n",
        "    for txn in transactions:\n",
        "        entity_id = txn.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\")\n",
        "        if entity_id:\n",
        "            entity_transactions[entity_id].append(txn)\n",
        "\n",
        "    for cluster_id, members in sorted(clusters.items()):\n",
        "        member_ids = [m[0] for m in members]\n",
        "        member_entities = [entity_lookup.get(eid) for eid in member_ids if eid in entity_lookup]\n",
        "\n",
        "        if not member_entities:\n",
        "            continue\n",
        "\n",
        "        # Analyze characteristics\n",
        "        if entity_type == \"customer\":\n",
        "            # Customer cluster analysis\n",
        "            age_groups = [e.get(\"Age_Group\", \"\") for e in member_entities if e]\n",
        "            location_tiers = [e.get(\"Location_Tier\", \"\") for e in member_entities if e]\n",
        "            channels = [e.get(\"Acquisition_Channel\", \"\") for e in member_entities if e]\n",
        "\n",
        "            # Product usage\n",
        "            products_used = set()\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    products_used.add(txn.get(\"Product_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Customer Segment {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"avg_age_group\": Counter(age_groups).most_common(1)[0][0] if age_groups else \"\",\n",
        "                    \"common_location_tiers\": [tier for tier, count in Counter(location_tiers).most_common(2)],\n",
        "                    \"common_acquisition_channels\": [ch for ch, count in Counter(channels).most_common(2)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"top_products\": list(products_used)[:5],\n",
        "                    \"product_diversity\": float(len(products_used))\n",
        "                },\n",
        "                \"underserved_products\": [],  # Will be filled by synthesis\n",
        "                \"business_value\": float(len(member_ids) * avg_usage)  # Simple estimate\n",
        "            }\n",
        "        else:\n",
        "            # Product cluster analysis\n",
        "            product_types = [e.get(\"Product_Type\", \"\") for e in member_entities if e]\n",
        "            monetization_models = [e.get(\"Monetization_Model\", \"\") for e in member_entities if e]\n",
        "            feature_sets = [e.get(\"feature_list\", []) for e in member_entities if e]\n",
        "\n",
        "            # Flatten feature sets\n",
        "            all_features = []\n",
        "            for fs in feature_sets:\n",
        "                all_features.extend(fs)\n",
        "\n",
        "            # Usage metrics\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "            customers_using = set()\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    customers_using.add(txn.get(\"Customer_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Product Bundle {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"common_features\": list(set(all_features)),\n",
        "                    \"monetization_models\": [mm for mm, count in Counter(monetization_models).most_common(3)],\n",
        "                    \"product_types\": [pt for pt, count in Counter(product_types).most_common(3)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"customer_count\": len(customers_using)\n",
        "                },\n",
        "                \"bundle_potential\": float(len(customers_using) / max(len(member_ids), 1))  # Simple estimate\n",
        "            }\n",
        "\n",
        "        cluster_analyses.append(cluster_analysis)\n",
        "\n",
        "    return cluster_analyses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Customer Features\n",
        "```\n",
        "def prepare_customer_features(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for customer clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Demographics (age group, location tier, acquisition channel) - one-hot encoded\n",
        "    - Behavioral (transaction count, product diversity, engagement score)\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, customer_ids)\n",
        "    \"\"\"\n",
        "    customer_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get engagement metrics\n",
        "    engagement = derived_features.get(\"customer_engagement\", {})\n",
        "\n",
        "    # Group transactions by customer\n",
        "    customer_txn_count = defaultdict(int)\n",
        "    customer_products = defaultdict(set)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id:\n",
        "            customer_txn_count[customer_id] += 1\n",
        "            if product_id:\n",
        "                customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Age group mapping\n",
        "    age_groups = [\"18-24\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    location_tiers = [\"Tier 1 (High)\", \"Tier 2 (Medium)\", \"Tier 3 (Low)\"]\n",
        "    acquisition_channels = [\"Email\", \"Referral\", \"Search\", \"Social\"]\n",
        "\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if not customer_id:\n",
        "            continue\n",
        "\n",
        "        customer_ids.append(customer_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Age group (one-hot)\n",
        "        age_group = customer.get(\"Age_Group\", \"\")\n",
        "        for age in age_groups:\n",
        "            feature_vector.append(1.0 if age_group == age else 0.0)\n",
        "\n",
        "        # Location tier (one-hot)\n",
        "        location = customer.get(\"Location_Tier\", \"\")\n",
        "        for tier in location_tiers:\n",
        "            feature_vector.append(1.0 if location == tier else 0.0)\n",
        "\n",
        "        # Acquisition channel (one-hot)\n",
        "        channel = customer.get(\"Acquisition_Channel\", \"\")\n",
        "        for acq in acquisition_channels:\n",
        "            feature_vector.append(1.0 if channel == acq else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        txn_count = customer_txn_count.get(customer_id, 0)\n",
        "        product_diversity = len(customer_products.get(customer_id, set()))\n",
        "\n",
        "        # Engagement score\n",
        "        eng_data = engagement.get(customer_id, {})\n",
        "        engagement_score = eng_data.get(\"engagement_score\", 0.0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            txn_count / 100.0,  # Normalize\n",
        "            product_diversity / 20.0,  # Normalize\n",
        "            engagement_score\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), customer_ids\n",
        "```\n",
        "\n",
        "This function is the **critical data engineering step** for your **Customer Segmentation Agent (Step 5)**. It serves as the bridge between raw, descriptive data (like age groups and transaction logs) and the numerical format required by the K-means clustering algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "## What the Function Does: Feature Engineering for Clustering\n",
        "\n",
        "The function combines demographic information with behavioral metrics into a single, normalized, numerical matrix ready for machine learning. This process is essential because clustering algorithms rely on **mathematical distance** between data points; all features must be numerical and scaled similarly.\n",
        "\n",
        "### 1. Feature Types and Transformation\n",
        "\n",
        "| Feature Category | Features Included | Transformation Method | Purpose in Clustering |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Demographic** | Age Group, Location Tier, Acquisition Channel | **One-Hot Encoding** | Allows the model to group customers based on *who they are* and *where they come from* (e.g., separates \"Tier 1\" customers from \"Tier 3\" customers). |\n",
        "| **Behavioral** | Transaction Count, Product Diversity | **Calculation & Normalization** | Captures *what they do*. **Normalization** (dividing by a maximum value) prevents high transaction counts from skewing the distance metric away from important features like product diversity. |\n",
        "| **Derived** | Engagement Score | **Retrieval** (from preprocessed data) | Incorporates high-level metrics generated earlier in the pipeline, allowing the clustering to be based on value, not just volume. |\n",
        "\n",
        "### 2. Output\n",
        "\n",
        "The function returns a **NumPy array** (the feature matrix) and a corresponding list of **Customer IDs**. This ensures that after the clustering algorithm runs (which only returns cluster indices), you can correctly map those indices back to the specific customers (e.g., $\\text{C001}$ belongs to **Customer Segment 1**).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ºï¸ Role in the Final Analysis\n",
        "\n",
        "This function directly enables the creation of your Customer Segments, which are foundational to the entire discovery process:\n",
        "\n",
        "1.  **Segmentation Basis (Step 5):** The output matrix is fed into the K-means algorithm, which creates distinct groups like **Customer Segment 1** and **Customer Segment 2** (as seen in your final report).\n",
        "2.  **Product Gap Identification:** Once segments are defined, you can determine their **Top Products** and, more importantly, their **Underserved Products** (products they *don't* buy).\n",
        "3.  **Synthesis Evidence (Step 8):** The resulting segments are used by the Synthesis Agent to generate:\n",
        "    * **Product Gap Opportunities:** \"Cross-sell $\\text{P20}$ to **Customer Segment 1**.\"\n",
        "    * **High-Value Segment Opportunities:** \"Target **Customer Segment 1** with specific marketing,\" based on their size, value, and characteristics derived from this feature set."
      ],
      "metadata": {
        "id": "1Iar79qwLd3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perpare Product Features\n",
        "\n",
        "```\n",
        "def prepare_product_features(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare feature matrix for product clustering.\n",
        "\n",
        "    Features include:\n",
        "    - Product attributes (type, monetization model) - one-hot encoded\n",
        "    - Feature set (which features A, B, C, D) - one-hot encoded\n",
        "    - Behavioral (popularity score, customer count, transaction count)\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (feature_matrix, product_ids)\n",
        "    \"\"\"\n",
        "    product_ids = []\n",
        "    features = []\n",
        "\n",
        "    # Get popularity metrics\n",
        "    popularity = derived_features.get(\"product_popularity\", {})\n",
        "\n",
        "    # Product type and monetization model mappings\n",
        "    product_types = [\"Hardware\", \"Software\", \"Service\"]\n",
        "    monetization_models = [\"One-Time Purchase\", \"Freemium\", \"Subscription\"]\n",
        "    feature_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if not product_id:\n",
        "            continue\n",
        "\n",
        "        product_ids.append(product_id)\n",
        "        feature_vector = []\n",
        "\n",
        "        # Product type (one-hot)\n",
        "        ptype = product.get(\"Product_Type\", \"\")\n",
        "        for pt in product_types:\n",
        "            feature_vector.append(1.0 if ptype == pt else 0.0)\n",
        "\n",
        "        # Monetization model (one-hot)\n",
        "        monet = product.get(\"Monetization_Model\", \"\")\n",
        "        for mm in monetization_models:\n",
        "            feature_vector.append(1.0 if monet == mm else 0.0)\n",
        "\n",
        "        # Feature set (one-hot for A, B, C, D)\n",
        "        feature_list = product.get(\"feature_list\", [])\n",
        "        for feat in feature_letters:\n",
        "            feature_vector.append(1.0 if feat in feature_list else 0.0)\n",
        "\n",
        "        # Behavioral features\n",
        "        pop_data = popularity.get(product_id, {})\n",
        "        popularity_score = pop_data.get(\"popularity_score\", 0.0)\n",
        "        customer_count = pop_data.get(\"customer_count\", 0)\n",
        "        txn_count = pop_data.get(\"transaction_count\", 0)\n",
        "\n",
        "        feature_vector.extend([\n",
        "            popularity_score,\n",
        "            customer_count / 200.0,  # Normalize\n",
        "            txn_count / 1000.0  # Normalize\n",
        "        ])\n",
        "\n",
        "        features.append(feature_vector)\n",
        "\n",
        "    return np.array(features), product_ids\n",
        "```\n",
        "\n",
        "This function, `prepare_product_features`, performs the **critical data preparation** for your **Product Bundling Analysis** (which is also part of the Clustering Agent, Step 5).\n",
        "\n",
        "It serves the same engineering role as the customer feature preparation, but for products: it transforms raw product attributes and behavioral metrics into a **normalized, numerical feature matrix** suitable for clustering algorithms (like K-means).\n",
        "\n",
        "---\n",
        "\n",
        "## What the Function Does: Feature Engineering for Product Clustering\n",
        "\n",
        "The function combines three different types of dataâ€”**inherent attributes, feature composition, and behavioral popularity**â€”into a single numerical vector for each product.\n",
        "\n",
        "### 1. Feature Types and Transformation\n",
        "\n",
        "| Feature Category | Features Included | Transformation Method | Purpose in Clustering |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Product Attributes** | Product Type, Monetization Model | **One-Hot Encoding** | Groups products based on their core definition (e.g., separating 'Software' from 'Hardware' or 'Subscription' models from 'Freemium'). |\n",
        "| **Feature Set** | Individual Features ($\\text{A, B, C, D}$) | **One-Hot Encoding** | Identifies bundles that share common characteristics (e.g., all products with Feature $\\text{C}$) to find logical and complementary combinations. |\n",
        "| **Behavioral/Derived** | Popularity Score, Customer Count, Transaction Count | **Retrieval & Normalization** | Captures *how the market perceives* the product. Normalization (dividing counts by constants like $\\text{200.0}$ or $\\text{1000.0}$) ensures highly popular products don't mathematically overshadow crucial attribute features during clustering. |\n",
        "\n",
        "### 2. Output\n",
        "\n",
        "The function returns a **NumPy array** (the feature matrix) and a corresponding list of **Product IDs**. This matrix is the direct input for the clustering algorithm that identifies your **Product Bundles** (e.g., **Product Bundle 4**: $\\text{P01, P05}$ in your final report).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ºï¸ Role in the Final Analysis (Product Bundling)\n",
        "\n",
        "This preparation step is foundational for the strategic goal of **Product Bundling**:\n",
        "\n",
        "1.  **Defining Natural Bundles (Step 5):** The clustering algorithm uses this feature matrix to group products that are structurally and functionally similar, leading to the identification of **natural product clusters**.\n",
        "2.  **Hypothesis Generation:** These bundles (like $\\text{P07, P08, P09}$) serve as strong **hypotheses** for new product offerings.\n",
        "3.  **Synthesis Evidence (Step 8):** The identified bundles are sent to the Synthesis Agent. If one of these bundles is also strongly supported by the **Association Rules** (e.g., $\\text{P01} \\to \\text{P05}$), it becomes a **\"Validated\" Bundle Opportunity** (as seen in your report) because it is proven by both product attributes and customer behavior."
      ],
      "metadata": {
        "id": "6DrmoAw1MhQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Steps\n",
        "```\n",
        "def find_optimal_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    max_clusters: int = 10,\n",
        "    min_clusters: int = 2\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Find optimal number of clusters using elbow method and silhouette score.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix for clustering\n",
        "        max_clusters: Maximum clusters to consider\n",
        "        min_clusters: Minimum clusters to consider\n",
        "\n",
        "    Returns:\n",
        "        Optimal number of clusters\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) < min_clusters:\n",
        "        return len(feature_matrix)\n",
        "\n",
        "    max_clusters = min(max_clusters, len(feature_matrix) - 1)\n",
        "\n",
        "    if max_clusters < min_clusters:\n",
        "        return min_clusters\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    best_k = min_clusters\n",
        "    best_silhouette = -1\n",
        "\n",
        "    for k in range(min_clusters, max_clusters + 1):\n",
        "        try:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "            if len(set(labels)) > 1:  # Need at least 2 clusters\n",
        "                silhouette = silhouette_score(scaled_features, labels)\n",
        "                if silhouette > best_silhouette:\n",
        "                    best_silhouette = silhouette\n",
        "                    best_k = k\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return best_k\n",
        "\n",
        "\n",
        "def cluster_customers(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster customers using K-means.\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, customer_ids = prepare_customer_features(\n",
        "        customers, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"customer_ids\": customer_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "\n",
        "def cluster_products(\n",
        "    products: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    derived_features: Dict[str, Any],\n",
        "    num_clusters: Optional[int] = None,\n",
        "    max_clusters: int = 10\n",
        ") -> Tuple[List[int], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cluster products using K-means.\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        derived_features: Derived features from preprocessing\n",
        "        num_clusters: Number of clusters (None = auto-determine)\n",
        "        max_clusters: Maximum clusters to consider if auto-determining\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cluster_labels, cluster_metadata)\n",
        "    \"\"\"\n",
        "    # Prepare features\n",
        "    feature_matrix, product_ids = prepare_product_features(\n",
        "        products, transactions, derived_features\n",
        "    )\n",
        "\n",
        "    if len(feature_matrix) == 0:\n",
        "        return [], {}\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(feature_matrix)\n",
        "\n",
        "    # Determine number of clusters\n",
        "    if num_clusters is None:\n",
        "        num_clusters = find_optimal_clusters(feature_matrix, max_clusters)\n",
        "\n",
        "    num_clusters = min(num_clusters, len(feature_matrix))\n",
        "\n",
        "    # Perform clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(scaled_features, labels) if len(set(labels)) > 1 else 0.0\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        \"num_clusters\": num_clusters,\n",
        "        \"silhouette_score\": float(silhouette),\n",
        "        \"inertia\": float(kmeans.inertia_),\n",
        "        \"product_ids\": product_ids\n",
        "    }\n",
        "\n",
        "    return labels.tolist(), metadata\n",
        "\n",
        "```\n",
        "\n",
        "These functions represent the core of your **Clustering Agent (Step 5)**. They take the prepared feature matrices and execute the K-means algorithm to create the final customer segments and product bundles.\n",
        "\n",
        "The process is robust, as it includes a crucial step to **automatically find the optimal number of clusters** ($k$).\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Finding the Optimal Number of Clusters (`find_optimal_clusters`)\n",
        "\n",
        "This helper function ensures that your clustering is statistically sound by preventing you from manually guessing the right number of groups. It uses standard metrics for unsupervised learning:\n",
        "\n",
        "* **Standard Scaling:** First, the features are **scaled** using `StandardScaler`. This is essential to prevent features with large ranges (like transaction count) from unfairly dominating the calculation of distance in the clustering process.\n",
        "* **The Silhouette Score:** The function iterates through possible cluster counts ($k$) and calculates the **Silhouette Score** for each.\n",
        "    * **What it is:** The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters.\n",
        "    * **Goal:** A score closer to $+1$ indicates that the clusters are dense and well-separated. The function selects the $k$ that yields the highest score, ensuring the most mathematically distinct grouping.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Executing the Clustering (`cluster_customers` & `cluster_products`)\n",
        "\n",
        "Both `cluster_customers` and `cluster_products` follow the exact same five-step machine learning pipeline, applying the K-means algorithm to their respective feature matrices:\n",
        "\n",
        "| Step | Action | Utility/Metric Used | Purpose |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **1. Preparation** | Calls `prepare_..._features` | (External function) | Converts raw data into a numerical `feature_matrix`. |\n",
        "| **2. Scaling** | `StandardScaler().fit_transform()` | `StandardScaler` | Normalizes all feature values (e.g., age, count, score) to have zero mean and unit variance. |\n",
        "| **3. Optimization** | Calls `find_optimal_clusters` | Silhouette Score | Automatically determines the statistically best number of groups ($k$). |\n",
        "| **4. Clustering** | `KMeans(n_clusters=k).fit_predict()` | `KMeans` | Executes the K-means algorithm to assign every customer or product to a final cluster label (0, 1, 2, etc.). |\n",
        "| **5. Validation** | Calculates final Silhouette Score & Inertia | `silhouette_score`, `inertia_` | Provides metadata on the quality of the final clusters, serving as an internal quality check for the Synthesis Agent. |\n",
        "\n",
        "### Output\n",
        "\n",
        "Both functions return two key elements:\n",
        "1.  **`labels`:** A list of integers (0, 1, 2, ...) assigning every entity to a group.\n",
        "2.  **`metadata`:** A dictionary containing quality metrics (silhouette score, inertia) and the IDs of the entities clustered.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ºï¸ Role in the Final Analysis\n",
        "\n",
        "These functions are where the **Computation** of the discovery process actually happens\n",
        "\n",
        "\n",
        "* **Customer Segmentation:** The output feeds the characteristics that you see in the final report, such as **Customer Segment 1** (12 customers, $\\text{35-44}$ age group), allowing the Synthesis Agent to recognize it as a high-value group.\n",
        "* **Product Bundling:** The output defines the **Product Bundles** (e.g., $\\text{P01, P05}$ as Product Bundle 4), which become the primary **Clustering Evidence** for the **Bundle Opportunities** in the final strategic report."
      ],
      "metadata": {
        "id": "Zs6OdlGJNNmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Clustering Node** (Step 5 of the orchestrator) is the first major analytical step that transforms raw data into **structured business entities** (segments and bundles).\n",
        "\n",
        "## ðŸ“Š Summary of the Clustering Node's Work\n",
        "\n",
        "The Clustering Node's utilities have completed three critical phases:\n",
        "\n",
        "1.  **Preparation (Feature Engineering):** We defined **two independent feature matrices**: one for customers and one for products. This involved transforming categorical data (like 'Age Group' or 'Product Type') into numerical **One-Hot Encoded** features and scaling all numerical features (like 'Transaction Count' or 'Popularity Score') to ensure fairness in distance calculations.\n",
        "2.  **Computation (K-Means Execution):** We executed the K-means algorithm on both matrices. Crucially, we implemented the `find_optimal_clusters` function using the **Silhouette Score** to automatically determine the statistically best number of clusters ($k$).\n",
        "3.  **Result:** The output is two sets of labels and metadata: **Customer Segments** (e.g., *Segment 1*) and **Product Bundles** (e.g., *Bundle 4*).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”‘ Critical Steps for Downstream Analysis\n",
        "\n",
        "Two steps in this node are absolutely critical because they prepare the data for the later **Synthesis Agent** (Step 8):\n",
        "\n",
        "### 1. The Creation of the Product Gap\n",
        "\n",
        "The Clustering Node is the **only place** where **Product Gaps** are defined.\n",
        "\n",
        "* By identifying the characteristics of a high-value customer segment (e.g., **Customer Segment 1**), the Synthesis Agent can later compare that segment's **Top Products** against the full product catalog.\n",
        "* The products the segment **does not use** become the **underserved products**, which are the basis for the **Product Gap Opportunity** (a core part of finding \"ghost demand\").\n",
        "\n",
        "### 2. The Definition of Natural Bundles\n",
        "\n",
        "The products grouped by the K-means algorithm (e.g., P01 and P05 are structurally similar) form the **Clustering Evidence** for a **Bundle Opportunity**.\n",
        "\n",
        "* This evidence is then passed to the Synthesis Agent, which must cross-validate it. If the **Association Rules** (from the next step) also show that P01 and P05 are frequently bought together, the bundle is marked as **Validated ($\\checkmark$ Yes)**, making it a high-confidence recommendation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŒŸ What Makes this Node Distinct and Powerful?\n",
        "\n",
        "The Clustering Node is powerful because it establishes the **\"WHAT\"** and **\"WHO\"** of the analysis.\n",
        "\n",
        "* **Distinctness:** It is the only agent that provides a **feature-based** grouping. It creates an explicit partitioning of the data based on *characteristics* (demographics, attributes), rather than *relationships* (patterns, graphs).\n",
        "* **Power:** It creates the **first structured business entities** in the entire pipeline:\n",
        "    * **Customer Segments** for **Targeting**.\n",
        "    * **Product Bundles** for **Cross-Sell Strategy**.\n",
        "\n",
        "It takes the massive, undifferentiated data set and breaks it down into small, manageable, and descriptive groups that the subsequent analytical agents can easily study.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EeCnNLwOrnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Customer Characteristics\n",
        "```\n",
        "def analyze_cluster_characteristics(\n",
        "    cluster_labels: List[int],\n",
        "    entity_ids: List[str],\n",
        "    entities: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    entity_type: str = \"customer\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Analyze characteristics of each cluster.\n",
        "\n",
        "    Args:\n",
        "        cluster_labels: Cluster assignment for each entity\n",
        "        entity_ids: List of entity IDs\n",
        "        entities: List of entity dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        entity_type: \"customer\" or \"product\"\n",
        "\n",
        "    Returns:\n",
        "        List of cluster analysis dictionaries\n",
        "    \"\"\"\n",
        "    # Group entities by cluster\n",
        "    clusters = defaultdict(list)\n",
        "    for idx, (entity_id, label) in enumerate(zip(entity_ids, cluster_labels)):\n",
        "        clusters[label].append((entity_id, idx))\n",
        "\n",
        "    cluster_analyses = []\n",
        "\n",
        "    # Create entity lookup\n",
        "    entity_lookup = {e.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\"): e\n",
        "                     for e in entities}\n",
        "\n",
        "    # Group transactions by entity\n",
        "    entity_transactions = defaultdict(list)\n",
        "    for txn in transactions:\n",
        "        entity_id = txn.get(\"Customer_ID\" if entity_type == \"customer\" else \"Product_ID\")\n",
        "        if entity_id:\n",
        "            entity_transactions[entity_id].append(txn)\n",
        "\n",
        "    for cluster_id, members in sorted(clusters.items()):\n",
        "        member_ids = [m[0] for m in members]\n",
        "        member_entities = [entity_lookup.get(eid) for eid in member_ids if eid in entity_lookup]\n",
        "\n",
        "        if not member_entities:\n",
        "            continue\n",
        "\n",
        "        # Analyze characteristics\n",
        "        if entity_type == \"customer\":\n",
        "            # Customer cluster analysis\n",
        "            age_groups = [e.get(\"Age_Group\", \"\") for e in member_entities if e]\n",
        "            location_tiers = [e.get(\"Location_Tier\", \"\") for e in member_entities if e]\n",
        "            channels = [e.get(\"Acquisition_Channel\", \"\") for e in member_entities if e]\n",
        "\n",
        "            # Product usage\n",
        "            products_used = set()\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    products_used.add(txn.get(\"Product_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Customer Segment {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"avg_age_group\": Counter(age_groups).most_common(1)[0][0] if age_groups else \"\",\n",
        "                    \"common_location_tiers\": [tier for tier, count in Counter(location_tiers).most_common(2)],\n",
        "                    \"common_acquisition_channels\": [ch for ch, count in Counter(channels).most_common(2)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"top_products\": list(products_used)[:5],\n",
        "                    \"product_diversity\": float(len(products_used))\n",
        "                },\n",
        "                \"underserved_products\": [],  # Will be filled by synthesis\n",
        "                \"business_value\": float(len(member_ids) * avg_usage)  # Simple estimate\n",
        "            }\n",
        "        else:\n",
        "            # Product cluster analysis\n",
        "            product_types = [e.get(\"Product_Type\", \"\") for e in member_entities if e]\n",
        "            monetization_models = [e.get(\"Monetization_Model\", \"\") for e in member_entities if e]\n",
        "            feature_sets = [e.get(\"feature_list\", []) for e in member_entities if e]\n",
        "\n",
        "            # Flatten feature sets\n",
        "            all_features = []\n",
        "            for fs in feature_sets:\n",
        "                all_features.extend(fs)\n",
        "\n",
        "            # Usage metrics\n",
        "            total_usage = 0.0\n",
        "            usage_count = 0\n",
        "            customers_using = set()\n",
        "\n",
        "            for member_id in member_ids:\n",
        "                for txn in entity_transactions.get(member_id, []):\n",
        "                    customers_using.add(txn.get(\"Customer_ID\"))\n",
        "                    usage = txn.get(\"Usage_Metric\", 0)\n",
        "                    if usage:\n",
        "                        total_usage += usage\n",
        "                        usage_count += 1\n",
        "\n",
        "            avg_usage = total_usage / usage_count if usage_count > 0 else 0.0\n",
        "\n",
        "            cluster_analysis = {\n",
        "                \"cluster_id\": int(cluster_id),\n",
        "                \"cluster_label\": f\"Product Bundle {cluster_id + 1}\",\n",
        "                \"entity_ids\": member_ids,\n",
        "                \"size\": len(member_ids),\n",
        "                \"characteristics\": {\n",
        "                    \"common_features\": list(set(all_features)),\n",
        "                    \"monetization_models\": [mm for mm, count in Counter(monetization_models).most_common(3)],\n",
        "                    \"product_types\": [pt for pt, count in Counter(product_types).most_common(3)],\n",
        "                    \"avg_usage_metric\": float(avg_usage),\n",
        "                    \"customer_count\": len(customers_using)\n",
        "                },\n",
        "                \"bundle_potential\": float(len(customers_using) / max(len(member_ids), 1))  # Simple estimate\n",
        "            }\n",
        "\n",
        "        cluster_analyses.append(cluster_analysis)\n",
        "\n",
        "    return cluster_analyses\n",
        "\n",
        "```\n",
        "\n",
        "That final utility is the function that makes the clustering results **actionable and readable** for the final report and the Synthesis Agent.\n",
        "\n",
        "It takes the raw mathematical groupings (the cluster labels) and turns them into a **descriptive profile** that answers the question: \"What does this group look like?\"\n",
        "\n",
        "---\n",
        "\n",
        "## What the Function Does: Cluster Profiling\n",
        "\n",
        "The function iterates through every customer segment and product bundle created by the K-means algorithm and extracts key, human-readable characteristics.\n",
        "\n",
        "It serves as the final, crucial step of the Clustering Agent before the data is passed to the next stage.\n",
        "\n",
        "### 1. Customer Segment Analysis (The WHO)\n",
        "\n",
        "For each `Customer Segment` (e.g., Customer Segment 1):\n",
        "\n",
        "* **Identifies the Core Persona:** It finds the **most common** demographic traits, such as the `avg_age_group` ($\\text{35-44}$) and `common_location_tiers` ($\\text{Tier 1}$).\n",
        "* **Quantifies Engagement:** It calculates the average `avg_usage_metric` and the total `product_diversity` (the number of different products they use).\n",
        "* **Defines Value:** It calculates a simple `business_value` metric (size $\\times$ average usage) to allow the Synthesis Agent to prioritize segments.\n",
        "* **Creates the Product Gap Hook:** It explicitly creates a field called `underserved_products`, which the **Synthesis Agent (Step 8)** will fill later by comparing the segment's `top_products` to the overall product catalog.\n",
        "\n",
        "### 2. Product Bundle Analysis (The WHAT)\n",
        "\n",
        "For each `Product Bundle` (e.g., Product Bundle 4):\n",
        "\n",
        "* **Defines Commonality:** It identifies the shared attributes, such as `monetization_models` and `common_features`. This confirms *why* the products were clustered together in the first place.\n",
        "* **Quantifies Bundle Potential:** It calculates the `bundle_potential` (estimated as the number of unique customers using the bundle divided by the number of products in the bundle). This gives the Synthesis Agent a preliminary score for the bundle's appeal.\n",
        "* **Customer Overlap:** It identifies the total `customer_count` using any product in the bundle.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—ºï¸ Role in the Final Analysis: Making Data Meaningful\n",
        "\n",
        "This function is the **Translator** within the Clustering Node, ensuring the analysis can be used by both machines and humans.\n",
        "\n",
        "### Critical Step: Creating the Segment Profile\n",
        "\n",
        "The most critical output is the detailed, readable profile that allows the Synthesis Agent to turn a cluster label ($\\text{Cluster ID 1}$) into an **actionable business insight** (e.g., \"High-Value Segment: Customer Segment 1\").\n",
        "\n",
        "1.  **Readiness for Synthesis:** The structured output, which includes the `business_value` and the empty `underserved_products` field, is perfectly formatted for the Synthesis Agent to begin cross-validation and opportunity ranking.\n",
        "2.  **Report Generation:** This output directly populates the \"Customer Segmentation Analysis\" and \"Product Bundling Analysis\" sections of your final report, giving the business stakeholder the necessary context for the top opportunities.\n",
        "\n",
        "Without this function, the Clustering Agent would only provide a list of numbers (cluster labels), which would be useless to the Synthesis Agent and incomprehensible in the final report."
      ],
      "metadata": {
        "id": "FP-Y1OBoQMkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization utilities for Product-Customer Fit Discovery Orchestrator"
      ],
      "metadata": {
        "id": "SX15KYNNgsFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Visualization utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def plot_customer_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    cluster_labels: List[int],\n",
        "    customer_ids: List[str],\n",
        "    output_path: str,\n",
        "    title: str = \"Customer Clusters\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Plot customer clusters using PCA for 2D visualization.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix used for clustering\n",
        "        cluster_labels: Cluster assignment for each customer\n",
        "        customer_ids: List of customer IDs\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Reduce to 2D using PCA\n",
        "    if feature_matrix.shape[1] < 2:\n",
        "        # Not enough features for PCA, create simple plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            ax.scatter([i] * mask.sum(), range(mask.sum()),\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50)\n",
        "\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Customer Index')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "    else:\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        features_2d = pca.fit_transform(feature_matrix)\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            cluster_points = features_2d[mask]\n",
        "\n",
        "            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def plot_product_clusters(\n",
        "    feature_matrix: np.ndarray,\n",
        "    cluster_labels: List[int],\n",
        "    product_ids: List[str],\n",
        "    output_path: str,\n",
        "    title: str = \"Product Clusters\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Plot product clusters using PCA for 2D visualization.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix: Feature matrix used for clustering\n",
        "        cluster_labels: Cluster assignment for each product\n",
        "        product_ids: List of product IDs\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    if len(feature_matrix) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Reduce to 2D using PCA\n",
        "    if feature_matrix.shape[1] < 2:\n",
        "        # Not enough features for PCA, create simple plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            ax.scatter([i] * mask.sum(), range(mask.sum()),\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=50)\n",
        "\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Product Index')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "    else:\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        features_2d = pca.fit_transform(feature_matrix)\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
        "\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            mask = np.array(cluster_labels) == cluster_id\n",
        "            cluster_points = features_2d[mask]\n",
        "\n",
        "            ax.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                      c=[colors[i]], label=f'Cluster {cluster_id + 1}',\n",
        "                      alpha=0.6, s=100, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "            # Annotate product IDs\n",
        "            for j, (x, y) in enumerate(cluster_points):\n",
        "                product_id = product_ids[np.where(mask)[0][j]]\n",
        "                ax.annotate(product_id, (x, y), fontsize=8, alpha=0.7)\n",
        "\n",
        "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def plot_cluster_summary(\n",
        "    customer_clusters: List[Dict[str, Any]],\n",
        "    product_clusters: List[Dict[str, Any]],\n",
        "    output_path: str,\n",
        "    title: str = \"Cluster Summary\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create a summary visualization showing cluster sizes and characteristics.\n",
        "\n",
        "    Args:\n",
        "        customer_clusters: List of customer cluster analyses\n",
        "        product_clusters: List of product cluster analyses\n",
        "        output_path: Path to save the plot\n",
        "        title: Plot title\n",
        "\n",
        "    Returns:\n",
        "        Path to saved plot file\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Customer clusters\n",
        "    if customer_clusters:\n",
        "        cluster_ids = [c[\"cluster_id\"] for c in customer_clusters]\n",
        "        cluster_sizes = [c[\"size\"] for c in customer_clusters]\n",
        "        cluster_labels = [c[\"cluster_label\"] for c in customer_clusters]\n",
        "\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_ids)))\n",
        "        ax1.bar(range(len(cluster_ids)), cluster_sizes, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax1.set_xticks(range(len(cluster_ids)))\n",
        "        ax1.set_xticklabels([f'C{cid+1}' for cid in cluster_ids], rotation=45, ha='right')\n",
        "        ax1.set_ylabel('Number of Customers')\n",
        "        ax1.set_title('Customer Cluster Sizes')\n",
        "        ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Product clusters\n",
        "    if product_clusters:\n",
        "        cluster_ids = [c[\"cluster_id\"] for c in product_clusters]\n",
        "        cluster_sizes = [c[\"size\"] for c in product_clusters]\n",
        "\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_ids)))\n",
        "        ax2.bar(range(len(cluster_ids)), cluster_sizes, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax2.set_xticks(range(len(cluster_ids)))\n",
        "        ax2.set_xticklabels([f'P{cid+1}' for cid in cluster_ids], rotation=45, ha='right')\n",
        "        ax2.set_ylabel('Number of Products')\n",
        "        ax2.set_title('Product Cluster Sizes')\n",
        "        ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return output_path\n",
        "\n"
      ],
      "metadata": {
        "id": "mSa7DiUlggWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for clustering utilities"
      ],
      "metadata": {
        "id": "_hsiG-jggosQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for clustering utilities\"\"\"\n",
        "\n",
        "import pytest\n",
        "import numpy as np\n",
        "from tools.clustering import (\n",
        "    prepare_customer_features,\n",
        "    prepare_product_features,\n",
        "    find_optimal_clusters,\n",
        "    cluster_customers,\n",
        "    cluster_products,\n",
        "    analyze_cluster_characteristics\n",
        ")\n",
        "\n",
        "\n",
        "def test_prepare_customer_features():\n",
        "    \"\"\"Test customer feature preparation\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"customer_engagement\": {\n",
        "            \"C001\": {\"engagement_score\": 0.7},\n",
        "            \"C002\": {\"engagement_score\": 0.5}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    features, customer_ids = prepare_customer_features(customers, transactions, derived_features)\n",
        "\n",
        "    assert len(features) == 2\n",
        "    assert len(customer_ids) == 2\n",
        "    assert customer_ids[0] == \"C001\"\n",
        "    assert features.shape[1] > 0  # Should have features\n",
        "\n",
        "\n",
        "def test_prepare_product_features():\n",
        "    \"\"\"Test product feature preparation\"\"\"\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\"}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"product_popularity\": {\n",
        "            \"P01\": {\"popularity_score\": 0.8, \"customer_count\": 10, \"transaction_count\": 50},\n",
        "            \"P02\": {\"popularity_score\": 0.6, \"customer_count\": 5, \"transaction_count\": 20}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    features, product_ids = prepare_product_features(products, transactions, derived_features)\n",
        "\n",
        "    assert len(features) == 2\n",
        "    assert len(product_ids) == 2\n",
        "    assert product_ids[0] == \"P01\"\n",
        "    assert features.shape[1] > 0  # Should have features\n",
        "\n",
        "\n",
        "def test_find_optimal_clusters():\n",
        "    \"\"\"Test optimal cluster finding\"\"\"\n",
        "    # Create simple 2D data with clear clusters\n",
        "    feature_matrix = np.array([\n",
        "        [1, 1], [1, 2], [2, 1], [2, 2],  # Cluster 1\n",
        "        [10, 10], [10, 11], [11, 10], [11, 11]  # Cluster 2\n",
        "    ])\n",
        "\n",
        "    optimal_k = find_optimal_clusters(feature_matrix, max_clusters=5, min_clusters=2)\n",
        "\n",
        "    assert 2 <= optimal_k <= 5\n",
        "\n",
        "\n",
        "def test_cluster_customers():\n",
        "    \"\"\"Test customer clustering\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P02\", \"Usage_Metric\": 60.0},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P01\", \"Usage_Metric\": 55.0}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"customer_engagement\": {\n",
        "            \"C001\": {\"engagement_score\": 0.7},\n",
        "            \"C002\": {\"engagement_score\": 0.5},\n",
        "            \"C003\": {\"engagement_score\": 0.6}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    labels, metadata = cluster_customers(customers, transactions, derived_features, num_clusters=2)\n",
        "\n",
        "    assert len(labels) == 3\n",
        "    assert \"num_clusters\" in metadata\n",
        "    assert \"silhouette_score\" in metadata\n",
        "    assert metadata[\"num_clusters\"] == 2\n",
        "\n",
        "\n",
        "def test_cluster_products():\n",
        "    \"\"\"Test product clustering\"\"\"\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]},\n",
        "        {\"Product_ID\": \"P03\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\"},\n",
        "        {\"Product_ID\": \"P02\", \"Customer_ID\": \"C002\"},\n",
        "        {\"Product_ID\": \"P03\", \"Customer_ID\": \"C001\"}\n",
        "    ]\n",
        "    derived_features = {\n",
        "        \"product_popularity\": {\n",
        "            \"P01\": {\"popularity_score\": 0.8, \"customer_count\": 10, \"transaction_count\": 50},\n",
        "            \"P02\": {\"popularity_score\": 0.6, \"customer_count\": 5, \"transaction_count\": 20},\n",
        "            \"P03\": {\"popularity_score\": 0.7, \"customer_count\": 8, \"transaction_count\": 40}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    labels, metadata = cluster_products(products, transactions, derived_features, num_clusters=2)\n",
        "\n",
        "    assert len(labels) == 3\n",
        "    assert \"num_clusters\" in metadata\n",
        "    assert \"silhouette_score\" in metadata\n",
        "    assert metadata[\"num_clusters\"] == 2\n",
        "\n",
        "\n",
        "def test_analyze_cluster_characteristics_customers():\n",
        "    \"\"\"Test customer cluster analysis\"\"\"\n",
        "    cluster_labels = [0, 0, 1]\n",
        "    customer_ids = [\"C001\", \"C002\", \"C003\"]\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"35-44\", \"Location_Tier\": \"Tier 2 (Medium)\", \"Acquisition_Channel\": \"Search\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Age_Group\": \"45-54\", \"Location_Tier\": \"Tier 1 (High)\", \"Acquisition_Channel\": \"Social\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\", \"Usage_Metric\": 55.0},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P02\", \"Usage_Metric\": 60.0}\n",
        "    ]\n",
        "\n",
        "    analyses = analyze_cluster_characteristics(\n",
        "        cluster_labels, customer_ids, customers, transactions, entity_type=\"customer\"\n",
        "    )\n",
        "\n",
        "    assert len(analyses) == 2  # Two clusters\n",
        "    assert analyses[0][\"cluster_id\"] == 0\n",
        "    assert \"characteristics\" in analyses[0]\n",
        "    assert \"size\" in analyses[0]\n",
        "    assert \"avg_age_group\" in analyses[0][\"characteristics\"]\n",
        "\n",
        "\n",
        "def test_analyze_cluster_characteristics_products():\n",
        "    \"\"\"Test product cluster analysis\"\"\"\n",
        "    cluster_labels = [0, 1, 0]\n",
        "    product_ids = [\"P01\", \"P02\", \"P03\"]\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\", \"Monetization_Model\": \"Subscription\", \"feature_list\": [\"C\"]},\n",
        "        {\"Product_ID\": \"P03\", \"Product_Type\": \"Hardware\", \"Monetization_Model\": \"One-Time Purchase\", \"feature_list\": [\"A\", \"B\"]}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Product_ID\": \"P01\", \"Customer_ID\": \"C001\", \"Usage_Metric\": 50.0},\n",
        "        {\"Product_ID\": \"P02\", \"Customer_ID\": \"C002\", \"Usage_Metric\": 60.0},\n",
        "        {\"Product_ID\": \"P03\", \"Customer_ID\": \"C001\", \"Usage_Metric\": 55.0}\n",
        "    ]\n",
        "\n",
        "    analyses = analyze_cluster_characteristics(\n",
        "        cluster_labels, product_ids, products, transactions, entity_type=\"product\"\n",
        "    )\n",
        "\n",
        "    assert len(analyses) == 2  # Two clusters\n",
        "    assert analyses[0][\"cluster_id\"] == 0\n",
        "    assert \"characteristics\" in analyses[0]\n",
        "    assert \"size\" in analyses[0]\n",
        "    assert \"common_features\" in analyses[0][\"characteristics\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "sMlG4JSrgmjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "8c0gfEZMhq-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_clustering.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 7 items\n",
        "\n",
        "tests/test_clustering.py::test_prepare_customer_features PASSED                                                                       [ 14%]\n",
        "tests/test_clustering.py::test_prepare_product_features PASSED                                                                        [ 28%]\n",
        "tests/test_clustering.py::test_find_optimal_clusters PASSED                                                                           [ 42%]\n",
        "tests/test_clustering.py::test_cluster_customers PASSED                                                                               [ 57%]\n",
        "tests/test_clustering.py::test_cluster_products PASSED                                                                                [ 71%]\n",
        "tests/test_clustering.py::test_analyze_cluster_characteristics_customers PASSED                                                       [ 85%]\n",
        "tests/test_clustering.py::test_analyze_cluster_characteristics_products PASSED                                                        [100%]\n",
        "\n",
        "============================================================ 7 passed in 16.70s =============================================================\n"
      ],
      "metadata": {
        "id": "1oj7iuzChovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_nodes_phase1.py tests/test_nodes_phase2.py tests/test_nodes_phase3.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 26 items\n",
        "\n",
        "tests/test_nodes_phase1.py::test_goal_node_basic PASSED                                                                               [  3%]\n",
        "tests/test_nodes_phase1.py::test_goal_node_preserves_errors PASSED                                                                    [  7%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_with_goal PASSED                                                                       [ 11%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_requires_goal PASSED                                                                   [ 15%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_plan_structure PASSED                                                                  [ 19%]\n",
        "tests/test_nodes_phase1.py::test_planning_node_dependencies PASSED                                                                    [ 23%]\n",
        "tests/test_nodes_phase1.py::test_goal_and_planning_together PASSED                                                                    [ 26%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_loads_all_data PASSED                                                            [ 30%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_customers_structure PASSED                                                       [ 34%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_transactions_structure PASSED                                                    [ 38%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_products_structure PASSED                                                        [ 42%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_uses_custom_paths PASSED                                                         [ 46%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_handles_missing_file PASSED                                                      [ 50%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_preserves_errors PASSED                                                          [ 53%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_with_goal_and_planning PASSED                                                    [ 57%]\n",
        "tests/test_nodes_phase2.py::test_data_ingestion_node_data_counts PASSED                                                               [ 61%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_basic PASSED                                                                 [ 65%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_structure PASSED                                                             [ 69%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_parses_feature_sets PASSED                                                   [ 73%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_normalizes_usage PASSED                                                      [ 76%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_builds_graphs PASSED                                                         [ 80%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_creates_derived_features PASSED                                              [ 84%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_data_quality_report PASSED                                                   [ 88%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_requires_raw_data PASSED                                                     [ 92%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_preserves_errors PASSED                                                      [ 96%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_full_workflow PASSED                                                         [100%]\n",
        "\n",
        "============================================================ 26 passed in 3.15s =============================================================\n"
      ],
      "metadata": {
        "id": "j9NaD7COiCuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aE6DaBcmj7_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}