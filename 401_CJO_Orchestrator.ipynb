{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxBV8+2ibNTcnUp9TJnEMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/401_CJO_Orchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. What This Orchestrator Actually Is\n",
        "\n",
        "At a surface level, yes — it’s a LangGraph `StateGraph`.\n",
        "\n",
        "At a **systems level**, it’s something much more important:\n",
        "\n",
        "> A deterministic, inspectable decision pipeline that converts signals → actions → outcomes → value.\n",
        "\n",
        "This line is the tell:\n",
        "\n",
        "```python\n",
        "workflow = StateGraph(CustomerJourneyOrchestratorState)\n",
        "```\n",
        "\n",
        "You are not orchestrating messages.\n",
        "You are orchestrating **state evolution**.\n",
        "\n",
        "That is the defining difference between:\n",
        "\n",
        "* AI demos\n",
        "* decision infrastructure\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why the Node Composition Is Executive-Grade\n",
        "\n",
        "Let’s map this to executive concerns.\n",
        "\n",
        "### Entry Point: Goal → Planning\n",
        "\n",
        "```python\n",
        "workflow.set_entry_point(\"goal\")\n",
        "```\n",
        "\n",
        "This matters more than it looks.\n",
        "\n",
        "You are saying:\n",
        "\n",
        "> “Nothing runs without an explicit objective.”\n",
        "\n",
        "Then immediately:\n",
        "\n",
        "```python\n",
        "goal → planning\n",
        "```\n",
        "\n",
        "Which means:\n",
        "\n",
        "* scope is defined\n",
        "* execution steps are enumerated\n",
        "* outputs are declared in advance\n",
        "\n",
        "This is **anti-autonomy by design** — and that’s a compliment.\n",
        "\n",
        "Executives do not want:\n",
        "\n",
        "> “The agent decided to…”\n",
        "\n",
        "They want:\n",
        "\n",
        "> “The system followed the agreed plan.”\n",
        "\n",
        "---\n",
        "\n",
        "### Linear Flow (And Why That’s a Feature)\n",
        "\n",
        "```python\n",
        "# Linear flow (MVP: sequential execution)\n",
        "```\n",
        "\n",
        "This comment is *perfectly placed*.\n",
        "\n",
        "You are explicitly stating:\n",
        "\n",
        "* no hidden branching\n",
        "* no emergent loops\n",
        "* no agentic surprise\n",
        "\n",
        "This buys you:\n",
        "\n",
        "* debuggability\n",
        "* reproducibility\n",
        "* auditability\n",
        "\n",
        "You can answer:\n",
        "\n",
        "> “What happened before this decision?”\n",
        "\n",
        "With absolute clarity.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Each Edge Represents a Business Question\n",
        "\n",
        "Let’s translate the flow into executive language:\n",
        "\n",
        "| Node                       | Business Question It Answers      |\n",
        "| -------------------------- | --------------------------------- |\n",
        "| `goal`                     | What are we trying to accomplish? |\n",
        "| `planning`                 | How will we do it?                |\n",
        "| `data_loading`             | What evidence are we using?       |\n",
        "| `journey_state_evaluation` | Where are customers stuck?        |\n",
        "| `signal_aggregation`       | What patterns are emerging?       |\n",
        "| `risk_scoring`             | Who should we worry about first?  |\n",
        "| `intervention_planning`    | What should we do?                |\n",
        "| `human_escalation`         | Where do we want oversight?       |\n",
        "| `outcome_tracking`         | Did it work?                      |\n",
        "| `kpi_calculation`          | Is the system healthy?            |\n",
        "| `roi_calculation`          | Was this worth the cost?          |\n",
        "| `summary_generation`       | What’s the headline?              |\n",
        "| `report_generation`        | How do we communicate this?       |\n",
        "\n",
        "That’s not an agent.\n",
        "That’s **a management operating model**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why the Lambda Pattern Is a Smart Choice\n",
        "\n",
        "```python\n",
        "workflow.add_node(\"risk_scoring\", lambda s: risk_scoring_node(s, config))\n",
        "```\n",
        "\n",
        "This looks small, but it’s architecturally clean:\n",
        "\n",
        "* Nodes remain **pure logic**\n",
        "* Config is **injected**, not global\n",
        "* Behavior is **policy-driven**, not code-driven\n",
        "\n",
        "This enables:\n",
        "\n",
        "* A/B testing config changes\n",
        "* per-client tuning\n",
        "* executive overrides without refactors\n",
        "\n",
        "In other words:\n",
        "\n",
        "> Strategy lives in config, not code.\n",
        "\n",
        "That’s exactly where executives expect it.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Why This Orchestrator Is Safer Than “Autonomous Agents”\n",
        "\n",
        "Most agent graphs optimize for:\n",
        "\n",
        "* loops\n",
        "* tool calling\n",
        "* dynamic routing\n",
        "* “agent decides next step”\n",
        "\n",
        "Your graph optimizes for:\n",
        "\n",
        "* predictability\n",
        "* accountability\n",
        "* explainability\n",
        "\n",
        "The END node seals this:\n",
        "\n",
        "```python\n",
        "workflow.add_edge(\"report_generation\", END)\n",
        "```\n",
        "\n",
        "No lingering state.\n",
        "No hidden continuation.\n",
        "No runaway process.\n",
        "\n",
        "The system **finishes**.\n",
        "\n",
        "That matters in real organizations.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. The Single Most Important Executive Property\n",
        "\n",
        "If I had to summarize what this orchestrator gives leadership:\n",
        "\n",
        "> A system whose behavior can be explained **before** it runs — not just after.\n",
        "\n",
        "That’s the line between:\n",
        "\n",
        "* experimentation\n",
        "* production governance\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Why This Completes the MVP (Cleanly)\n",
        "\n",
        "You now have:\n",
        "\n",
        "✅ Explicit state schema\n",
        "✅ Deterministic execution plan\n",
        "✅ Transparent scoring logic\n",
        "✅ Human-in-the-loop control\n",
        "✅ KPI validation\n",
        "✅ ROI math\n",
        "✅ Executive report artifact\n",
        "✅ Finite, auditable workflow\n",
        "\n",
        "There is **nothing missing** for an MVP.\n",
        "\n",
        "Everything else from here on out is *enhancement*, not foundation.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Executive Framing (You Can Reuse This)\n",
        "\n",
        "If a CEO asked *“What did you actually build?”* the honest answer is:\n",
        "\n",
        "> “We built a decision orchestration engine that turns customer signals into prioritized actions, tracks outcomes, and proves whether those actions were worth the cost — with built-in governance and human oversight.”\n",
        "\n",
        "And if they asked *“Why should I trust it?”*:\n",
        "\n",
        "> “Because every step is explicit, measurable, and reviewable — and nothing happens without visibility.”\n",
        "\n",
        "That’s not agent hype.\n",
        "That’s operational credibility.\n",
        "\n",
        "But as an MVP?\n",
        "\n",
        "This is complete.\n",
        "And it’s rare.\n"
      ],
      "metadata": {
        "id": "a_7BIPExZJlk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQpV5LMYW1hX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Customer Journey Orchestrator\n",
        "\n",
        "LangGraph workflow for monitoring, evaluating, and improving customer journeys.\n",
        "\"\"\"\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from config import CustomerJourneyOrchestratorState, CustomerJourneyOrchestratorConfig\n",
        "from agents.customer_journey_orchestrator.nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_loading_node,\n",
        "    journey_state_evaluation_node,\n",
        "    signal_aggregation_node,\n",
        "    risk_scoring_node,\n",
        "    intervention_planning_node,\n",
        "    human_escalation_node,\n",
        "    outcome_tracking_node,\n",
        "    kpi_calculation_node,\n",
        "    roi_calculation_node,\n",
        "    summary_generation_node,\n",
        "    report_generation_node\n",
        ")\n",
        "\n",
        "\n",
        "def create_orchestrator(config: CustomerJourneyOrchestratorConfig = None) -> StateGraph:\n",
        "    \"\"\"\n",
        "    Create and return the Customer Journey Orchestrator workflow.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object (defaults to CustomerJourneyOrchestratorConfig())\n",
        "\n",
        "    Returns:\n",
        "        Compiled LangGraph workflow\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    workflow = StateGraph(CustomerJourneyOrchestratorState)\n",
        "\n",
        "    # Add all nodes\n",
        "    workflow.add_node(\"goal\", goal_node)\n",
        "    workflow.add_node(\"planning\", planning_node)\n",
        "    workflow.add_node(\"data_loading\", lambda s: data_loading_node(s, config))\n",
        "    workflow.add_node(\"journey_state_evaluation\", lambda s: journey_state_evaluation_node(s, config))\n",
        "    workflow.add_node(\"signal_aggregation\", lambda s: signal_aggregation_node(s, config))\n",
        "    workflow.add_node(\"risk_scoring\", lambda s: risk_scoring_node(s, config))\n",
        "    workflow.add_node(\"intervention_planning\", lambda s: intervention_planning_node(s, config))\n",
        "    workflow.add_node(\"human_escalation\", lambda s: human_escalation_node(s, config))\n",
        "    workflow.add_node(\"outcome_tracking\", lambda s: outcome_tracking_node(s, config))\n",
        "    workflow.add_node(\"kpi_calculation\", lambda s: kpi_calculation_node(s, config))\n",
        "    workflow.add_node(\"roi_calculation\", lambda s: roi_calculation_node(s, config))\n",
        "    workflow.add_node(\"summary_generation\", lambda s: summary_generation_node(s, config))\n",
        "    workflow.add_node(\"report_generation\", lambda s: report_generation_node(s, config))\n",
        "\n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"goal\")\n",
        "\n",
        "    # Linear flow (MVP: sequential execution)\n",
        "    workflow.add_edge(\"goal\", \"planning\")\n",
        "    workflow.add_edge(\"planning\", \"data_loading\")\n",
        "    workflow.add_edge(\"data_loading\", \"journey_state_evaluation\")\n",
        "    workflow.add_edge(\"journey_state_evaluation\", \"signal_aggregation\")\n",
        "    workflow.add_edge(\"signal_aggregation\", \"risk_scoring\")\n",
        "    workflow.add_edge(\"risk_scoring\", \"intervention_planning\")\n",
        "    workflow.add_edge(\"intervention_planning\", \"human_escalation\")\n",
        "    workflow.add_edge(\"human_escalation\", \"outcome_tracking\")\n",
        "    workflow.add_edge(\"outcome_tracking\", \"kpi_calculation\")\n",
        "    workflow.add_edge(\"kpi_calculation\", \"roi_calculation\")\n",
        "    workflow.add_edge(\"roi_calculation\", \"summary_generation\")\n",
        "    workflow.add_edge(\"summary_generation\", \"report_generation\")\n",
        "    workflow.add_edge(\"report_generation\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Code"
      ],
      "metadata": {
        "id": "sb-HuPRbYMLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test file for Customer Journey Orchestrator\n",
        "\n",
        "Simple tests to verify nodes work correctly.\n",
        "Following the build guide: test each component before proceeding.\n",
        "\"\"\"\n",
        "\n",
        "from agents.customer_journey_orchestrator.nodes import goal_node, planning_node\n",
        "from config import CustomerJourneyOrchestratorState\n",
        "\n",
        "\n",
        "def test_goal_node_single_customer():\n",
        "    \"\"\"Test goal node with specific customer\"\"\"\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = goal_node(state)\n",
        "\n",
        "    assert \"goal\" in result\n",
        "    assert result[\"goal\"][\"customer_id\"] == \"C001\"\n",
        "    assert result[\"goal\"][\"scope\"] == \"single_customer\"\n",
        "    assert \"focus_areas\" in result[\"goal\"]\n",
        "    assert len(result[\"goal\"][\"focus_areas\"]) > 0\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    print(\"✅ test_goal_node_single_customer passed\")\n",
        "\n",
        "\n",
        "def test_goal_node_all_customers():\n",
        "    \"\"\"Test goal node for all customers\"\"\"\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = goal_node(state)\n",
        "\n",
        "    assert \"goal\" in result\n",
        "    assert result[\"goal\"][\"customer_id\"] is None\n",
        "    assert result[\"goal\"][\"scope\"] == \"all_customers\"\n",
        "    assert \"portfolio_analysis\" in result[\"goal\"][\"focus_areas\"]\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    print(\"✅ test_goal_node_all_customers passed\")\n",
        "\n",
        "\n",
        "def test_planning_node():\n",
        "    \"\"\"Test planning node\"\"\"\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"goal\": {\n",
        "            \"objective\": \"Monitor and improve customer journeys\",\n",
        "            \"customer_id\": None,\n",
        "            \"scope\": \"all_customers\",\n",
        "            \"focus_areas\": []\n",
        "        },\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = planning_node(state)\n",
        "\n",
        "    assert \"plan\" in result\n",
        "    assert len(result[\"plan\"]) == 11  # 11 steps in the plan\n",
        "    assert result[\"plan\"][0][\"name\"] == \"data_loading\"\n",
        "    assert result[\"plan\"][-1][\"name\"] == \"report_generation\"\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    print(\"✅ test_planning_node passed\")\n",
        "\n",
        "\n",
        "def test_planning_node_missing_goal():\n",
        "    \"\"\"Test planning node error handling when goal is missing\"\"\"\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = planning_node(state)\n",
        "\n",
        "    assert \"plan\" not in result\n",
        "    assert len(result.get(\"errors\", [])) > 0\n",
        "    assert \"planning_node: goal is required\" in result[\"errors\"]\n",
        "\n",
        "    print(\"✅ test_planning_node_missing_goal passed\")\n",
        "\n",
        "\n",
        "def test_data_loading_utilities():\n",
        "    \"\"\"Test data loading utilities with real data\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.data_loading import (\n",
        "        load_customers,\n",
        "        load_journey_state_log,\n",
        "        load_signals,\n",
        "        load_interventions,\n",
        "        load_outcomes,\n",
        "        build_customers_lookup,\n",
        "        build_journey_states_lookup,\n",
        "        build_signals_lookup,\n",
        "        build_interventions_lookup,\n",
        "        build_outcomes_lookup\n",
        "    )\n",
        "\n",
        "    data_dir = \"agents/data\"\n",
        "\n",
        "    # Test loading customers\n",
        "    customers = load_customers(data_dir)\n",
        "    assert len(customers) > 0\n",
        "    assert \"customer_id\" in customers[0]\n",
        "    print(\"✅ load_customers passed\")\n",
        "\n",
        "    # Test loading journey states\n",
        "    journey_states = load_journey_state_log(data_dir)\n",
        "    assert len(journey_states) > 0\n",
        "    assert \"customer_id\" in journey_states[0]\n",
        "    print(\"✅ load_journey_state_log passed\")\n",
        "\n",
        "    # Test loading signals\n",
        "    signals = load_signals(data_dir)\n",
        "    assert len(signals) > 0\n",
        "    assert \"signal_id\" in signals[0]\n",
        "    print(\"✅ load_signals passed\")\n",
        "\n",
        "    # Test loading interventions\n",
        "    interventions = load_interventions(data_dir)\n",
        "    assert len(interventions) > 0\n",
        "    assert \"intervention_id\" in interventions[0]\n",
        "    print(\"✅ load_interventions passed\")\n",
        "\n",
        "    # Test loading outcomes\n",
        "    outcomes = load_outcomes(data_dir)\n",
        "    assert len(outcomes) > 0\n",
        "    assert \"outcome_id\" in outcomes[0]\n",
        "    print(\"✅ load_outcomes passed\")\n",
        "\n",
        "    # Test lookup building\n",
        "    customers_lookup = build_customers_lookup(customers)\n",
        "    assert \"C001\" in customers_lookup\n",
        "    assert customers_lookup[\"C001\"][\"customer_id\"] == \"C001\"\n",
        "    print(\"✅ build_customers_lookup passed\")\n",
        "\n",
        "    journey_states_lookup = build_journey_states_lookup(journey_states)\n",
        "    assert \"C001\" in journey_states_lookup\n",
        "    print(\"✅ build_journey_states_lookup passed\")\n",
        "\n",
        "    signals_lookup = build_signals_lookup(signals)\n",
        "    assert \"C001\" in signals_lookup\n",
        "    assert isinstance(signals_lookup[\"C001\"], list)\n",
        "    print(\"✅ build_signals_lookup passed\")\n",
        "\n",
        "    interventions_lookup = build_interventions_lookup(interventions)\n",
        "    assert \"C001\" in interventions_lookup\n",
        "    assert isinstance(interventions_lookup[\"C001\"], list)\n",
        "    print(\"✅ build_interventions_lookup passed\")\n",
        "\n",
        "    outcomes_lookup = build_outcomes_lookup(outcomes)\n",
        "    assert \"I001\" in outcomes_lookup\n",
        "    print(\"✅ build_outcomes_lookup passed\")\n",
        "\n",
        "\n",
        "def test_journey_evaluation_utilities():\n",
        "    \"\"\"Test journey evaluation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.journey_evaluation import (\n",
        "        evaluate_journey_state,\n",
        "        evaluate_all_journey_states,\n",
        "        get_customers_with_friction,\n",
        "        get_customers_by_health_status\n",
        "    )\n",
        "\n",
        "    typical_durations = {\n",
        "        \"onboarding\": 14,\n",
        "        \"engagement\": 30,\n",
        "        \"support\": 7,\n",
        "        \"retention\": 90\n",
        "    }\n",
        "\n",
        "    friction_thresholds = {\n",
        "        \"onboarding_exceeded_days\": 14,\n",
        "        \"engagement_inactivity_days\": 30,\n",
        "        \"support_escalation_days\": 5\n",
        "    }\n",
        "\n",
        "    # Test evaluating a journey state with friction\n",
        "    journey_state = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"journey_stage\": \"onboarding\",\n",
        "        \"days_in_state\": 20,  # Exceeds threshold\n",
        "        \"state_entered_at\": \"2025-01-01\",\n",
        "        \"previous_stage\": None\n",
        "    }\n",
        "\n",
        "    evaluation = evaluate_journey_state(\n",
        "        \"C001\",\n",
        "        journey_state,\n",
        "        typical_durations,\n",
        "        friction_thresholds\n",
        "    )\n",
        "\n",
        "    assert evaluation[\"customer_id\"] == \"C001\"\n",
        "    assert evaluation[\"current_stage\"] == \"onboarding\"\n",
        "    assert evaluation[\"friction_detected\"] == True\n",
        "    assert len(evaluation[\"friction_reasons\"]) > 0\n",
        "    assert evaluation[\"stage_health\"] in [\"at_risk\", \"critical\"]\n",
        "    print(\"✅ evaluate_journey_state (with friction) passed\")\n",
        "\n",
        "    # Test evaluating a healthy journey state\n",
        "    healthy_state = {\n",
        "        \"customer_id\": \"C002\",\n",
        "        \"journey_stage\": \"onboarding\",\n",
        "        \"days_in_state\": 5,  # Within threshold\n",
        "        \"state_entered_at\": \"2025-01-05\",\n",
        "        \"previous_stage\": None\n",
        "    }\n",
        "\n",
        "    healthy_evaluation = evaluate_journey_state(\n",
        "        \"C002\",\n",
        "        healthy_state,\n",
        "        typical_durations,\n",
        "        friction_thresholds\n",
        "    )\n",
        "\n",
        "    assert healthy_evaluation[\"friction_detected\"] == False\n",
        "    assert healthy_evaluation[\"stage_health\"] == \"healthy\"\n",
        "    print(\"✅ evaluate_journey_state (healthy) passed\")\n",
        "\n",
        "    # Test evaluating all journey states\n",
        "    journey_states = [journey_state, healthy_state]\n",
        "    customers_lookup = {\n",
        "        \"C001\": {\"customer_id\": \"C001\", \"segment\": \"SMB\"},\n",
        "        \"C002\": {\"customer_id\": \"C002\", \"segment\": \"SMB\"}\n",
        "    }\n",
        "\n",
        "    evaluations = evaluate_all_journey_states(\n",
        "        journey_states,\n",
        "        customers_lookup,\n",
        "        typical_durations,\n",
        "        friction_thresholds\n",
        "    )\n",
        "\n",
        "    assert len(evaluations) == 2\n",
        "    print(\"✅ evaluate_all_journey_states passed\")\n",
        "\n",
        "    # Test getting customers with friction\n",
        "    friction_customers = get_customers_with_friction(evaluations)\n",
        "    assert \"C001\" in friction_customers\n",
        "    assert \"C002\" not in friction_customers\n",
        "    print(\"✅ get_customers_with_friction passed\")\n",
        "\n",
        "    # Test getting customers by health status\n",
        "    at_risk_customers = get_customers_by_health_status(evaluations, \"at_risk\")\n",
        "    healthy_customers = get_customers_by_health_status(evaluations, \"healthy\")\n",
        "    assert \"C001\" in at_risk_customers or \"C001\" in get_customers_by_health_status(evaluations, \"critical\")\n",
        "    assert \"C002\" in healthy_customers\n",
        "    print(\"✅ get_customers_by_health_status passed\")\n",
        "\n",
        "\n",
        "def test_journey_state_evaluation_node():\n",
        "    \"\"\"Test journey state evaluation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import journey_state_evaluation_node\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data first\n",
        "    from agents.customer_journey_orchestrator.nodes import data_loading_node\n",
        "\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Load data\n",
        "    state.update(data_loading_node(state, config))\n",
        "    assert len(state.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Evaluate journey states\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "\n",
        "    assert \"journey_evaluations\" in result\n",
        "    assert len(result[\"journey_evaluations\"]) > 0\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check evaluation structure\n",
        "    evaluation = result[\"journey_evaluations\"][0]\n",
        "    assert \"customer_id\" in evaluation\n",
        "    assert \"current_stage\" in evaluation\n",
        "    assert \"friction_detected\" in evaluation\n",
        "    assert \"stage_health\" in evaluation\n",
        "    print(\"✅ test_journey_state_evaluation_node passed\")\n",
        "\n",
        "\n",
        "def test_signal_aggregation_utilities():\n",
        "    \"\"\"Test signal aggregation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.signal_aggregation import (\n",
        "        aggregate_signals_for_customer,\n",
        "        aggregate_all_signals\n",
        "    )\n",
        "\n",
        "    aggregation_weights = {\n",
        "        \"negative_sentiment\": 0.30,\n",
        "        \"support_ticket_spike\": 0.25,\n",
        "        \"usage_drop\": 0.20\n",
        "    }\n",
        "\n",
        "    # Test aggregating signals for a customer with negative signals\n",
        "    signals = [\n",
        "        {\"signal_id\": \"S001\", \"customer_id\": \"C001\", \"signal_type\": \"negative_sentiment\", \"signal_strength\": 0.67},\n",
        "        {\"signal_id\": \"S002\", \"customer_id\": \"C001\", \"signal_type\": \"support_ticket_spike\", \"signal_strength\": 0.82}\n",
        "    ]\n",
        "\n",
        "    aggregation = aggregate_signals_for_customer(\"C001\", signals, aggregation_weights)\n",
        "\n",
        "    assert aggregation[\"customer_id\"] == \"C001\"\n",
        "    assert aggregation[\"total_signals\"] == 2\n",
        "    assert aggregation[\"negative_signals\"] == 2\n",
        "    assert aggregation[\"positive_signals\"] == 0\n",
        "    assert aggregation[\"average_signal_strength\"] > 0\n",
        "    assert aggregation[\"max_signal_strength\"] > 0\n",
        "    assert aggregation[\"aggregated_risk_score\"] > 0\n",
        "    print(\"✅ aggregate_signals_for_customer passed\")\n",
        "\n",
        "    # Test aggregating all signals\n",
        "    signals_lookup = {\n",
        "        \"C001\": signals,\n",
        "        \"C002\": []\n",
        "    }\n",
        "    customers_lookup = {\n",
        "        \"C001\": {\"customer_id\": \"C001\"},\n",
        "        \"C002\": {\"customer_id\": \"C002\"}\n",
        "    }\n",
        "\n",
        "    aggregated = aggregate_all_signals(signals_lookup, customers_lookup, aggregation_weights)\n",
        "    assert len(aggregated) == 2\n",
        "    assert any(agg[\"customer_id\"] == \"C001\" for agg in aggregated)\n",
        "    assert any(agg[\"customer_id\"] == \"C002\" for agg in aggregated)\n",
        "    print(\"✅ aggregate_all_signals passed\")\n",
        "\n",
        "\n",
        "def test_risk_scoring_utilities():\n",
        "    \"\"\"Test risk scoring utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.risk_scoring import (\n",
        "        calculate_risk_score,\n",
        "        calculate_all_risk_scores\n",
        "    )\n",
        "\n",
        "    risk_weights = {\n",
        "        \"signal_strength\": 0.35,\n",
        "        \"time_in_state\": 0.25,\n",
        "        \"customer_value\": 0.20,\n",
        "        \"signal_count\": 0.20\n",
        "    }\n",
        "\n",
        "    risk_tier_thresholds = {\n",
        "        \"high\": 0.70,\n",
        "        \"medium\": 0.40,\n",
        "        \"low\": 0.0\n",
        "    }\n",
        "\n",
        "    # Test calculating risk score\n",
        "    aggregated_signals = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"aggregated_risk_score\": 0.75,\n",
        "        \"max_signal_strength\": 0.82,\n",
        "        \"total_signals\": 2\n",
        "    }\n",
        "\n",
        "    journey_evaluation = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"days_in_state\": 20,\n",
        "        \"typical_duration\": 14,\n",
        "        \"friction_detected\": True\n",
        "    }\n",
        "\n",
        "    customer_data = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"account_value\": 12000\n",
        "    }\n",
        "\n",
        "    risk_score = calculate_risk_score(\n",
        "        \"C001\",\n",
        "        aggregated_signals,\n",
        "        journey_evaluation,\n",
        "        customer_data,\n",
        "        risk_weights,\n",
        "        risk_tier_thresholds\n",
        "    )\n",
        "\n",
        "    assert risk_score[\"customer_id\"] == \"C001\"\n",
        "    assert 0.0 <= risk_score[\"overall_risk_score\"] <= 1.0\n",
        "    assert 0.0 <= risk_score[\"churn_risk_score\"] <= 1.0\n",
        "    assert risk_score[\"risk_tier\"] in [\"low\", \"medium\", \"high\"]\n",
        "    assert risk_score[\"urgency\"] in [\"low\", \"medium\", \"high\"]\n",
        "    assert \"risk_factors\" in risk_score\n",
        "    print(\"✅ calculate_risk_score passed\")\n",
        "\n",
        "    # Test calculating all risk scores\n",
        "    customers_lookup = {\n",
        "        \"C001\": customer_data,\n",
        "        \"C002\": {\"customer_id\": \"C002\", \"account_value\": 8000}\n",
        "    }\n",
        "\n",
        "    aggregated_signals_list = [aggregated_signals]\n",
        "    journey_evaluations = [journey_evaluation]\n",
        "\n",
        "    risk_scores = calculate_all_risk_scores(\n",
        "        customers_lookup,\n",
        "        aggregated_signals_list,\n",
        "        journey_evaluations,\n",
        "        risk_weights,\n",
        "        risk_tier_thresholds\n",
        "    )\n",
        "\n",
        "    assert len(risk_scores) == 2\n",
        "    assert any(rs[\"customer_id\"] == \"C001\" for rs in risk_scores)\n",
        "    print(\"✅ calculate_all_risk_scores passed\")\n",
        "\n",
        "\n",
        "def test_signal_aggregation_node():\n",
        "    \"\"\"Test signal aggregation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import signal_aggregation_node, data_loading_node\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data first\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    assert len(state.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Aggregate signals\n",
        "    result = signal_aggregation_node(state, config)\n",
        "\n",
        "    assert \"aggregated_signals\" in result\n",
        "    assert len(result[\"aggregated_signals\"]) > 0\n",
        "    # Merge errors from result\n",
        "    state_errors = state.get(\"errors\", [])\n",
        "    result_errors = result.get(\"errors\", [])\n",
        "    all_errors = state_errors + result_errors\n",
        "    assert len(all_errors) == 0\n",
        "\n",
        "    # Check aggregation structure\n",
        "    aggregation = result[\"aggregated_signals\"][0]\n",
        "    assert \"customer_id\" in aggregation\n",
        "    assert \"total_signals\" in aggregation\n",
        "    assert \"aggregated_risk_score\" in aggregation\n",
        "    print(\"✅ test_signal_aggregation_node passed\")\n",
        "\n",
        "\n",
        "def test_risk_scoring_node():\n",
        "    \"\"\"Test risk scoring node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        risk_scoring_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    assert len(state.get(\"errors\", [])) == 0\n",
        "\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    state.update(result)\n",
        "\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    state.update(result)\n",
        "\n",
        "    # Calculate risk scores\n",
        "    result = risk_scoring_node(state, config)\n",
        "\n",
        "    assert \"risk_scores\" in result\n",
        "    assert len(result[\"risk_scores\"]) > 0\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check risk score structure\n",
        "    risk_score = result[\"risk_scores\"][0]\n",
        "    assert \"customer_id\" in risk_score\n",
        "    assert \"overall_risk_score\" in risk_score\n",
        "    assert \"churn_risk_score\" in risk_score\n",
        "    assert \"risk_tier\" in risk_score\n",
        "    assert \"urgency\" in risk_score\n",
        "    print(\"✅ test_risk_scoring_node passed\")\n",
        "\n",
        "\n",
        "def test_intervention_planning_utilities():\n",
        "    \"\"\"Test intervention planning utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.intervention_planning import (\n",
        "        generate_intervention_recommendation,\n",
        "        determine_intervention_action,\n",
        "        calculate_priority_score,\n",
        "        generate_all_interventions\n",
        "    )\n",
        "\n",
        "    customer_data = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"account_value\": 12000,\n",
        "        \"segment\": \"SMB\"\n",
        "    }\n",
        "\n",
        "    risk_score = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"overall_risk_score\": 0.78,\n",
        "        \"risk_tier\": \"high\",\n",
        "        \"urgency\": \"high\"\n",
        "    }\n",
        "\n",
        "    aggregated_signals = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"total_signals\": 2,\n",
        "        \"aggregated_risk_score\": 0.75\n",
        "    }\n",
        "\n",
        "    journey_evaluation = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"current_stage\": \"onboarding\",\n",
        "        \"days_in_state\": 20\n",
        "    }\n",
        "\n",
        "    signals = [\n",
        "        {\"signal_id\": \"S001\", \"signal_type\": \"negative_sentiment\"},\n",
        "        {\"signal_id\": \"S002\", \"signal_type\": \"support_ticket_spike\"}\n",
        "    ]\n",
        "\n",
        "    # Test generating intervention recommendation\n",
        "    intervention = generate_intervention_recommendation(\n",
        "        \"C001\",\n",
        "        customer_data,\n",
        "        risk_score,\n",
        "        aggregated_signals,\n",
        "        journey_evaluation,\n",
        "        signals,\n",
        "        confidence_threshold=0.50,\n",
        "        high_value_threshold=30000.0\n",
        "    )\n",
        "\n",
        "    assert intervention is not None\n",
        "    assert intervention[\"customer_id\"] == \"C001\"\n",
        "    assert intervention[\"confidence\"] >= 0.50\n",
        "    assert \"recommended_action\" in intervention\n",
        "    assert \"requires_human_approval\" in intervention\n",
        "    assert \"priority_score\" in intervention\n",
        "    print(\"✅ generate_intervention_recommendation passed\")\n",
        "\n",
        "    # Test determining intervention action\n",
        "    action = determine_intervention_action(\n",
        "        \"onboarding\",\n",
        "        [\"failed_onboarding_step\"],\n",
        "        \"high\",\n",
        "        \"high\"\n",
        "    )\n",
        "    assert action == \"onboarding_specialist_call\"\n",
        "    print(\"✅ determine_intervention_action passed\")\n",
        "\n",
        "    # Test calculating priority score\n",
        "    priority = calculate_priority_score(risk_score, 12000)\n",
        "    assert 0.0 <= priority <= 100.0\n",
        "    print(\"✅ calculate_priority_score passed\")\n",
        "\n",
        "    # Test generating all interventions\n",
        "    customers_lookup = {\"C001\": customer_data}\n",
        "    risk_scores = [risk_score]\n",
        "    aggregated_signals_list = [aggregated_signals]\n",
        "    journey_evaluations = [journey_evaluation]\n",
        "    signals_lookup = {\"C001\": signals}\n",
        "\n",
        "    interventions = generate_all_interventions(\n",
        "        customers_lookup,\n",
        "        risk_scores,\n",
        "        aggregated_signals_list,\n",
        "        journey_evaluations,\n",
        "        signals_lookup,\n",
        "        confidence_threshold=0.50,\n",
        "        high_value_threshold=30000.0\n",
        "    )\n",
        "\n",
        "    assert len(interventions) > 0\n",
        "    assert any(i[\"customer_id\"] == \"C001\" for i in interventions)\n",
        "    print(\"✅ generate_all_interventions passed\")\n",
        "\n",
        "\n",
        "def test_human_escalation_utilities():\n",
        "    \"\"\"Test human escalation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.human_escalation import (\n",
        "        create_intervention_approval_request,\n",
        "        get_pending_intervention_approvals,\n",
        "        is_intervention_approved\n",
        "    )\n",
        "\n",
        "    intervention = {\n",
        "        \"intervention_id\": \"I001\",\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"recommended_action\": \"proactive_outreach\",\n",
        "        \"confidence\": 0.78,\n",
        "        \"requires_human_approval\": True\n",
        "    }\n",
        "\n",
        "    # Test creating approval request\n",
        "    approval_request = create_intervention_approval_request(intervention)\n",
        "\n",
        "    assert \"intervention_id\" in approval_request\n",
        "    assert approval_request[\"customer_id\"] == \"C001\"\n",
        "    assert \"requested_at\" in approval_request\n",
        "    assert approval_request[\"status\"] == \"pending\"\n",
        "    print(\"✅ create_intervention_approval_request passed\")\n",
        "\n",
        "    # Test getting pending approvals\n",
        "    interventions = [intervention]\n",
        "    approval_history = []\n",
        "\n",
        "    pending = get_pending_intervention_approvals(interventions, approval_history)\n",
        "    assert len(pending) > 0\n",
        "    assert any(p[\"intervention_id\"] == \"I001\" for p in pending)\n",
        "    print(\"✅ get_pending_intervention_approvals passed\")\n",
        "\n",
        "    # Test checking if approved\n",
        "    approval_history = [\n",
        "        {\"task_id\": \"I001\", \"decision\": \"approved\", \"decided_at\": \"2025-01-15T10:00:00\"}\n",
        "    ]\n",
        "\n",
        "    is_approved = is_intervention_approved(\"I001\", approval_history)\n",
        "    assert is_approved == True\n",
        "    print(\"✅ is_intervention_approved passed\")\n",
        "\n",
        "\n",
        "def test_intervention_planning_node():\n",
        "    \"\"\"Test intervention planning node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    assert len(state.get(\"errors\", [])) == 0\n",
        "\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    state.update(result)\n",
        "\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    state.update(result)\n",
        "\n",
        "    result = risk_scoring_node(state, config)\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    state.update(result)\n",
        "\n",
        "    # Generate interventions\n",
        "    result = intervention_planning_node(state, config)\n",
        "\n",
        "    assert \"recommended_interventions\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check intervention structure\n",
        "    if result[\"recommended_interventions\"]:\n",
        "        intervention = result[\"recommended_interventions\"][0]\n",
        "        assert \"intervention_id\" in intervention\n",
        "        assert \"customer_id\" in intervention\n",
        "        assert \"recommended_action\" in intervention\n",
        "        assert \"confidence\" in intervention\n",
        "        assert \"priority_score\" in intervention\n",
        "    print(\"✅ test_intervention_planning_node passed\")\n",
        "\n",
        "\n",
        "def test_human_escalation_node():\n",
        "    \"\"\"Test human escalation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        human_escalation_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Test human escalation\n",
        "    result = human_escalation_node(state, config)\n",
        "\n",
        "    assert \"pending_approvals\" in result\n",
        "    assert \"approval_history\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # With auto_approve_for_testing=True, pending_approvals should be empty\n",
        "    # and approval_history should have entries\n",
        "    if config.auto_approve_for_testing:\n",
        "        assert len(result[\"pending_approvals\"]) == 0\n",
        "        # Approval history might have entries if interventions required approval\n",
        "    print(\"✅ test_human_escalation_node passed\")\n",
        "\n",
        "\n",
        "def test_outcome_tracking_utilities():\n",
        "    \"\"\"Test outcome tracking utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.outcome_tracking import (\n",
        "        analyze_intervention_outcome,\n",
        "        analyze_all_outcomes,\n",
        "        calculate_outcome_summary\n",
        "    )\n",
        "\n",
        "    intervention = {\n",
        "        \"intervention_id\": \"I001\",\n",
        "        \"customer_id\": \"C001\"\n",
        "    }\n",
        "\n",
        "    outcome = {\n",
        "        \"outcome_id\": \"O001\",\n",
        "        \"intervention_id\": \"I001\",\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"outcome\": \"resolved\",\n",
        "        \"resolution_time_days\": 3,\n",
        "        \"csat_delta\": 1,\n",
        "        \"churn_risk_delta\": -0.25,\n",
        "        \"estimated_revenue_saved\": 2000,\n",
        "        \"human_override\": False\n",
        "    }\n",
        "\n",
        "    # Test analyzing outcome\n",
        "    analysis = analyze_intervention_outcome(intervention, outcome)\n",
        "    assert analysis[\"intervention_id\"] == \"I001\"\n",
        "    assert analysis[\"outcome\"] == \"resolved\"\n",
        "    assert analysis[\"csat_delta\"] == 1\n",
        "    print(\"✅ analyze_intervention_outcome (with outcome) passed\")\n",
        "\n",
        "    # Test analyzing pending intervention\n",
        "    pending_analysis = analyze_intervention_outcome(intervention, None)\n",
        "    assert pending_analysis[\"outcome\"] == \"pending\"\n",
        "    assert pending_analysis[\"resolution_time_days\"] is None\n",
        "    print(\"✅ analyze_intervention_outcome (pending) passed\")\n",
        "\n",
        "    # Test analyzing all outcomes\n",
        "    interventions = [intervention]\n",
        "    outcomes_lookup = {\"I001\": outcome}\n",
        "\n",
        "    analyses = analyze_all_outcomes(interventions, outcomes_lookup)\n",
        "    assert len(analyses) == 1\n",
        "    assert analyses[0][\"outcome\"] == \"resolved\"\n",
        "    print(\"✅ analyze_all_outcomes passed\")\n",
        "\n",
        "    # Test calculating outcome summary\n",
        "    summary = calculate_outcome_summary(analyses)\n",
        "    assert \"total_interventions\" in summary\n",
        "    assert \"resolved_count\" in summary\n",
        "    assert \"average_resolution_time_days\" in summary\n",
        "    assert \"total_revenue_saved\" in summary\n",
        "    print(\"✅ calculate_outcome_summary passed\")\n",
        "\n",
        "\n",
        "def test_kpi_calculation_utilities():\n",
        "    \"\"\"Test KPI calculation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.kpi_calculation import (\n",
        "        calculate_operational_kpis,\n",
        "        calculate_effectiveness_kpis,\n",
        "        calculate_business_kpis,\n",
        "        assess_all_kpi_status\n",
        "    )\n",
        "\n",
        "    journey_evaluations = [{\"customer_id\": \"C001\", \"current_stage\": \"onboarding\"}]\n",
        "    signals = [{\"signal_id\": \"S001\", \"signal_strength\": 0.82}]\n",
        "    interventions = [{\"intervention_id\": \"I001\", \"requires_human_approval\": True, \"evaluation_latency_ms\": 240}]\n",
        "    outcomes = [{\"outcome_id\": \"O001\", \"human_override\": False}]\n",
        "    approval_history = []\n",
        "\n",
        "    # Test calculating operational KPIs\n",
        "    operational_kpis = calculate_operational_kpis(\n",
        "        journey_evaluations,\n",
        "        signals,\n",
        "        interventions,\n",
        "        outcomes,\n",
        "        approval_history\n",
        "    )\n",
        "\n",
        "    assert \"journey_state_classification_accuracy\" in operational_kpis\n",
        "    assert \"signal_detection_precision\" in operational_kpis\n",
        "    assert \"average_latency_ms\" in operational_kpis\n",
        "    assert \"human_escalation_frequency\" in operational_kpis\n",
        "    print(\"✅ calculate_operational_kpis passed\")\n",
        "\n",
        "    # Test calculating effectiveness KPIs\n",
        "    outcome_analyses = [\n",
        "        {\n",
        "            \"intervention_id\": \"I001\",\n",
        "            \"outcome\": \"resolved\",\n",
        "            \"resolution_time_days\": 3,\n",
        "            \"csat_delta\": 1,\n",
        "            \"churn_risk_delta\": -0.25\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    effectiveness_kpis = calculate_effectiveness_kpis(\n",
        "        outcome_analyses,\n",
        "        interventions,\n",
        "        approval_history\n",
        "    )\n",
        "\n",
        "    assert \"average_resolution_time_days\" in effectiveness_kpis\n",
        "    assert \"unresolved_issues_reduction\" in effectiveness_kpis\n",
        "    assert \"proactive_interventions_ratio\" in effectiveness_kpis\n",
        "    print(\"✅ calculate_effectiveness_kpis passed\")\n",
        "\n",
        "    # Test calculating business KPIs\n",
        "    customers = [{\"customer_id\": \"C001\", \"account_value\": 12000}]\n",
        "\n",
        "    business_kpis = calculate_business_kpis(\n",
        "        outcome_analyses,\n",
        "        customers\n",
        "    )\n",
        "\n",
        "    assert \"churn_rate_reduction\" in business_kpis\n",
        "    assert \"csat_delta_average\" in business_kpis\n",
        "    assert \"retention_revenue_preserved\" in business_kpis\n",
        "    print(\"✅ calculate_business_kpis passed\")\n",
        "\n",
        "    # Test assessing KPI status\n",
        "    operational_targets = {\"journey_state_classification_accuracy\": 0.90}\n",
        "    effectiveness_targets = {\"average_resolution_time_days\": 5.0}\n",
        "    business_targets = {\"churn_rate_reduction\": 0.10}\n",
        "\n",
        "    kpi_status = assess_all_kpi_status(\n",
        "        operational_kpis,\n",
        "        effectiveness_kpis,\n",
        "        business_kpis,\n",
        "        operational_targets,\n",
        "        effectiveness_targets,\n",
        "        business_targets\n",
        "    )\n",
        "\n",
        "    assert \"operational_health\" in kpi_status\n",
        "    assert \"journey_impact\" in kpi_status\n",
        "    assert \"business_value\" in kpi_status\n",
        "    print(\"✅ assess_all_kpi_status passed\")\n",
        "\n",
        "\n",
        "def test_roi_calculation_utilities():\n",
        "    \"\"\"Test ROI calculation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.roi_calculation import (\n",
        "        calculate_roi_breakdown,\n",
        "        calculate_roi_estimate\n",
        "    )\n",
        "\n",
        "    interventions = [\n",
        "        {\"intervention_id\": \"I001\", \"requires_human_approval\": True}\n",
        "    ]\n",
        "    outcomes = [\n",
        "        {\"outcome_id\": \"O001\", \"outcome\": \"resolved\", \"estimated_revenue_saved\": 2000}\n",
        "    ]\n",
        "    approval_history = []\n",
        "\n",
        "    roi_breakdown = calculate_roi_breakdown(\n",
        "        interventions,\n",
        "        outcomes,\n",
        "        approval_history,\n",
        "        cost_per_llm_call=0.01,\n",
        "        cost_per_api_call=0.001,\n",
        "        cost_per_human_review_hour=50.0,\n",
        "        infrastructure_cost_per_month=500.0\n",
        "    )\n",
        "\n",
        "    assert \"total_value\" in roi_breakdown\n",
        "    assert \"total_cost\" in roi_breakdown\n",
        "    assert \"net_benefit\" in roi_breakdown\n",
        "    assert \"roi_percent\" in roi_breakdown\n",
        "    assert \"cost_components\" in roi_breakdown\n",
        "    assert \"value_components\" in roi_breakdown\n",
        "    print(\"✅ calculate_roi_breakdown passed\")\n",
        "\n",
        "    roi_estimate = calculate_roi_estimate(roi_breakdown)\n",
        "    assert isinstance(roi_estimate, (int, float))\n",
        "    print(\"✅ calculate_roi_estimate passed\")\n",
        "\n",
        "\n",
        "def test_outcome_tracking_node():\n",
        "    \"\"\"Test outcome tracking node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        outcome_tracking_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Track outcomes\n",
        "    result = outcome_tracking_node(state, config)\n",
        "\n",
        "    assert \"outcome_analyses\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check outcome analysis structure\n",
        "    if result[\"outcome_analyses\"]:\n",
        "        analysis = result[\"outcome_analyses\"][0]\n",
        "        assert \"intervention_id\" in analysis\n",
        "        assert \"customer_id\" in analysis\n",
        "        assert \"outcome\" in analysis\n",
        "    print(\"✅ test_outcome_tracking_node passed\")\n",
        "\n",
        "\n",
        "def test_kpi_calculation_node():\n",
        "    \"\"\"Test KPI calculation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        kpi_calculation_node,\n",
        "        outcome_tracking_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node,\n",
        "        human_escalation_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "    result = human_escalation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = outcome_tracking_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Calculate KPIs\n",
        "    result = kpi_calculation_node(state, config)\n",
        "\n",
        "    assert \"operational_kpis\" in result\n",
        "    assert \"effectiveness_kpis\" in result\n",
        "    assert \"business_kpis\" in result\n",
        "    assert \"kpi_status\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check KPI structure\n",
        "    assert \"journey_state_classification_accuracy\" in result[\"operational_kpis\"]\n",
        "    assert \"average_resolution_time_days\" in result[\"effectiveness_kpis\"]\n",
        "    assert \"churn_rate_reduction\" in result[\"business_kpis\"]\n",
        "    assert \"operational_health\" in result[\"kpi_status\"]\n",
        "    print(\"✅ test_kpi_calculation_node passed\")\n",
        "\n",
        "\n",
        "def test_roi_calculation_node():\n",
        "    \"\"\"Test ROI calculation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        roi_calculation_node,\n",
        "        outcome_tracking_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node,\n",
        "        human_escalation_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "    result = human_escalation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = outcome_tracking_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Calculate ROI\n",
        "    result = roi_calculation_node(state, config)\n",
        "\n",
        "    assert \"roi_estimate\" in result\n",
        "    assert \"roi_breakdown\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check ROI structure\n",
        "    assert isinstance(result[\"roi_estimate\"], (int, float))\n",
        "    assert \"total_value\" in result[\"roi_breakdown\"]\n",
        "    assert \"total_cost\" in result[\"roi_breakdown\"]\n",
        "    assert \"cost_components\" in result[\"roi_breakdown\"]\n",
        "    assert \"value_components\" in result[\"roi_breakdown\"]\n",
        "    print(\"✅ test_roi_calculation_node passed\")\n",
        "\n",
        "\n",
        "def test_summary_generation_node():\n",
        "    \"\"\"Test summary generation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        summary_generation_node,\n",
        "        roi_calculation_node,\n",
        "        outcome_tracking_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node,\n",
        "        human_escalation_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "    result = human_escalation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = outcome_tracking_node(state, config)\n",
        "    state.update(result)\n",
        "    result = roi_calculation_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Generate summary\n",
        "    result = summary_generation_node(state, config)\n",
        "\n",
        "    assert \"journey_summary\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check summary structure\n",
        "    summary = result[\"journey_summary\"]\n",
        "    assert \"total_customers_analyzed\" in summary\n",
        "    assert \"customers_with_signals\" in summary\n",
        "    assert \"total_interventions\" in summary\n",
        "    assert \"total_revenue_preserved\" in summary\n",
        "    print(\"✅ test_summary_generation_node passed\")\n",
        "\n",
        "\n",
        "def test_report_generation_utilities():\n",
        "    \"\"\"Test report generation utilities\"\"\"\n",
        "    from agents.customer_journey_orchestrator.utilities.report_generation import (\n",
        "        generate_journey_report\n",
        "    )\n",
        "\n",
        "    # Create mock state\n",
        "    state = {\n",
        "        \"goal\": {\"scope\": \"all_customers\"},\n",
        "        \"journey_summary\": {\n",
        "            \"total_customers_analyzed\": 10,\n",
        "            \"customers_with_signals\": 8,\n",
        "            \"total_interventions\": 8,\n",
        "            \"total_revenue_preserved\": 15000.0\n",
        "        },\n",
        "        \"operational_kpis\": {\n",
        "            \"journey_state_classification_accuracy\": 0.95,\n",
        "            \"average_latency_ms\": 220.0\n",
        "        },\n",
        "        \"effectiveness_kpis\": {\n",
        "            \"average_resolution_time_days\": 4.2\n",
        "        },\n",
        "        \"business_kpis\": {\n",
        "            \"churn_rate_reduction\": 0.12,\n",
        "            \"retention_revenue_preserved\": 15000.0\n",
        "        },\n",
        "        \"kpi_status\": {\n",
        "            \"operational_health\": \"on_track\",\n",
        "            \"journey_impact\": \"on_track\",\n",
        "            \"business_value\": \"on_track\"\n",
        "        },\n",
        "        \"roi_breakdown\": {\n",
        "            \"total_value\": 15000.0,\n",
        "            \"total_cost\": 2500.0,\n",
        "            \"net_benefit\": 12500.0,\n",
        "            \"roi_percent\": 500.0\n",
        "        },\n",
        "        \"risk_scores\": [\n",
        "            {\"customer_id\": \"C001\", \"overall_risk_score\": 0.78, \"risk_tier\": \"high\", \"urgency\": \"high\"}\n",
        "        ],\n",
        "        \"recommended_interventions\": [\n",
        "            {\"customer_id\": \"C001\", \"recommended_action\": \"proactive_outreach\", \"confidence\": 0.78, \"priority_score\": 85.5, \"requires_human_approval\": True}\n",
        "        ],\n",
        "        \"outcome_analyses\": [\n",
        "            {\"intervention_id\": \"I001\", \"outcome\": \"resolved\", \"resolution_time_days\": 3, \"estimated_revenue_saved\": 2000}\n",
        "        ],\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    report = generate_journey_report(state)\n",
        "\n",
        "    assert \"# Customer Journey Orchestrator Report\" in report\n",
        "    assert \"Executive Summary\" in report\n",
        "    assert \"Operational KPIs\" in report\n",
        "    assert \"Effectiveness KPIs\" in report\n",
        "    assert \"Business KPIs\" in report\n",
        "    assert \"ROI\" in report\n",
        "    print(\"✅ generate_journey_report passed\")\n",
        "\n",
        "\n",
        "def test_report_generation_node():\n",
        "    \"\"\"Test report generation node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import (\n",
        "        report_generation_node,\n",
        "        summary_generation_node,\n",
        "        roi_calculation_node,\n",
        "        outcome_tracking_node,\n",
        "        intervention_planning_node,\n",
        "        data_loading_node,\n",
        "        journey_state_evaluation_node,\n",
        "        signal_aggregation_node,\n",
        "        risk_scoring_node,\n",
        "        human_escalation_node,\n",
        "        kpi_calculation_node\n",
        "    )\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Load data and run all dependencies\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    state.update(data_loading_node(state, config))\n",
        "    result = journey_state_evaluation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = signal_aggregation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = risk_scoring_node(state, config)\n",
        "    state.update(result)\n",
        "    result = intervention_planning_node(state, config)\n",
        "    state.update(result)\n",
        "    result = human_escalation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = outcome_tracking_node(state, config)\n",
        "    state.update(result)\n",
        "    result = kpi_calculation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = roi_calculation_node(state, config)\n",
        "    state.update(result)\n",
        "    result = summary_generation_node(state, config)\n",
        "    state.update(result)\n",
        "\n",
        "    # Generate report\n",
        "    result = report_generation_node(state, config)\n",
        "\n",
        "    assert \"journey_report\" in result\n",
        "    assert \"report_file_path\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check report content\n",
        "    assert \"# Customer Journey Orchestrator Report\" in result[\"journey_report\"]\n",
        "    assert \"Executive Summary\" in result[\"journey_report\"]\n",
        "    print(\"✅ test_report_generation_node passed\")\n",
        "\n",
        "\n",
        "def test_complete_workflow():\n",
        "    \"\"\"Test complete end-to-end workflow\"\"\"\n",
        "    from agents.customer_journey_orchestrator.orchestrator import create_orchestrator\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Create orchestrator\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Test with all customers\n",
        "    initial_state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    # Verify final state has all expected fields\n",
        "    assert \"journey_report\" in result\n",
        "    assert \"report_file_path\" in result\n",
        "    assert \"journey_summary\" in result\n",
        "    assert \"operational_kpis\" in result\n",
        "    assert \"effectiveness_kpis\" in result\n",
        "    assert \"business_kpis\" in result\n",
        "    assert \"roi_estimate\" in result\n",
        "    assert \"roi_breakdown\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Verify report was generated\n",
        "    assert len(result[\"journey_report\"]) > 0\n",
        "    assert result[\"report_file_path\"] is not None\n",
        "\n",
        "    print(\"✅ test_complete_workflow (all customers) passed\")\n",
        "\n",
        "    # Test with single customer\n",
        "    initial_state = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    assert \"journey_report\" in result\n",
        "    assert \"report_file_path\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    print(\"✅ test_complete_workflow (single customer) passed\")\n",
        "\n",
        "\n",
        "def test_data_loading_node():\n",
        "    \"\"\"Test data loading node\"\"\"\n",
        "    from agents.customer_journey_orchestrator.nodes import data_loading_node\n",
        "    from config import CustomerJourneyOrchestratorConfig\n",
        "\n",
        "    config = CustomerJourneyOrchestratorConfig()\n",
        "\n",
        "    # Test loading all customers\n",
        "    state: CustomerJourneyOrchestratorState = {\n",
        "        \"customer_id\": None,\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_loading_node(state, config)\n",
        "\n",
        "    assert \"customers\" in result\n",
        "    assert \"journey_state_log\" in result\n",
        "    assert \"signals\" in result\n",
        "    assert \"interventions\" in result\n",
        "    assert \"outcomes\" in result\n",
        "    assert \"customers_lookup\" in result\n",
        "    assert \"journey_states_lookup\" in result\n",
        "    assert \"signals_lookup\" in result\n",
        "    assert \"interventions_lookup\" in result\n",
        "    assert \"outcomes_lookup\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    assert len(result[\"customers\"]) > 0\n",
        "    print(\"✅ test_data_loading_node (all customers) passed\")\n",
        "\n",
        "    # Test loading single customer\n",
        "    state = {\n",
        "        \"customer_id\": \"C001\",\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = data_loading_node(state, config)\n",
        "\n",
        "    assert len(result[\"customers\"]) == 1\n",
        "    assert result[\"customers\"][0][\"customer_id\"] == \"C001\"\n",
        "    assert \"C001\" in result[\"customers_lookup\"]\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "    print(\"✅ test_data_loading_node (single customer) passed\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running Customer Journey Orchestrator tests...\\n\")\n",
        "\n",
        "    print(\"=== Phase 1: Foundation ===\")\n",
        "    test_goal_node_single_customer()\n",
        "    test_goal_node_all_customers()\n",
        "    test_planning_node()\n",
        "    test_planning_node_missing_goal()\n",
        "    print(\"✅ All Phase 1 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 2: Data Loading ===\")\n",
        "    test_data_loading_utilities()\n",
        "    test_data_loading_node()\n",
        "    print(\"✅ All Phase 2 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 3: Journey State Evaluation ===\")\n",
        "    test_journey_evaluation_utilities()\n",
        "    test_journey_state_evaluation_node()\n",
        "    print(\"✅ All Phase 3 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 4: Signal Aggregation & Risk Scoring ===\")\n",
        "    test_signal_aggregation_utilities()\n",
        "    test_risk_scoring_utilities()\n",
        "    test_signal_aggregation_node()\n",
        "    test_risk_scoring_node()\n",
        "    print(\"✅ All Phase 4 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 5: Intervention Planning & Human Escalation ===\")\n",
        "    test_intervention_planning_utilities()\n",
        "    test_human_escalation_utilities()\n",
        "    test_intervention_planning_node()\n",
        "    test_human_escalation_node()\n",
        "    print(\"✅ All Phase 5 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 6: Outcome Tracking & KPI Calculation ===\")\n",
        "    test_outcome_tracking_utilities()\n",
        "    test_kpi_calculation_utilities()\n",
        "    test_roi_calculation_utilities()\n",
        "    test_outcome_tracking_node()\n",
        "    test_kpi_calculation_node()\n",
        "    test_roi_calculation_node()\n",
        "    test_summary_generation_node()\n",
        "    print(\"✅ All Phase 6 tests passed!\\n\")\n",
        "\n",
        "    print(\"=== Phase 7: Report Generation & Orchestrator Wiring ===\")\n",
        "    test_report_generation_utilities()\n",
        "    test_report_generation_node()\n",
        "    test_complete_workflow()\n",
        "    print(\"✅ All Phase 7 tests passed!\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_RViRyxqYOng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Results"
      ],
      "metadata": {
        "id": "RyJVJbgnZlmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_011_Customer_Journey_Orchestrator %    python test_customer_journey_orchestrator.py\n",
        "Running Customer Journey Orchestrator tests...\n",
        "\n",
        "=== Phase 1: Foundation ===\n",
        "✅ test_goal_node_single_customer passed\n",
        "✅ test_goal_node_all_customers passed\n",
        "✅ test_planning_node passed\n",
        "✅ test_planning_node_missing_goal passed\n",
        "✅ All Phase 1 tests passed!\n",
        "\n",
        "=== Phase 2: Data Loading ===\n",
        "✅ load_customers passed\n",
        "✅ load_journey_state_log passed\n",
        "✅ load_signals passed\n",
        "✅ load_interventions passed\n",
        "✅ load_outcomes passed\n",
        "✅ build_customers_lookup passed\n",
        "✅ build_journey_states_lookup passed\n",
        "✅ build_signals_lookup passed\n",
        "✅ build_interventions_lookup passed\n",
        "✅ build_outcomes_lookup passed\n",
        "✅ test_data_loading_node (all customers) passed\n",
        "✅ test_data_loading_node (single customer) passed\n",
        "✅ All Phase 2 tests passed!\n",
        "\n",
        "=== Phase 3: Journey State Evaluation ===\n",
        "✅ evaluate_journey_state (with friction) passed\n",
        "✅ evaluate_journey_state (healthy) passed\n",
        "✅ evaluate_all_journey_states passed\n",
        "✅ get_customers_with_friction passed\n",
        "✅ get_customers_by_health_status passed\n",
        "✅ test_journey_state_evaluation_node passed\n",
        "✅ All Phase 3 tests passed!\n",
        "\n",
        "=== Phase 4: Signal Aggregation & Risk Scoring ===\n",
        "✅ aggregate_signals_for_customer passed\n",
        "✅ aggregate_all_signals passed\n",
        "✅ calculate_risk_score passed\n",
        "✅ calculate_all_risk_scores passed\n",
        "✅ test_signal_aggregation_node passed\n",
        "✅ test_risk_scoring_node passed\n",
        "✅ All Phase 4 tests passed!\n",
        "\n",
        "=== Phase 5: Intervention Planning & Human Escalation ===\n",
        "✅ generate_intervention_recommendation passed\n",
        "✅ determine_intervention_action passed\n",
        "✅ calculate_priority_score passed\n",
        "✅ generate_all_interventions passed\n",
        "✅ create_intervention_approval_request passed\n",
        "✅ get_pending_intervention_approvals passed\n",
        "✅ is_intervention_approved passed\n",
        "✅ test_intervention_planning_node passed\n",
        "✅ test_human_escalation_node passed\n",
        "✅ All Phase 5 tests passed!\n",
        "\n",
        "=== Phase 6: Outcome Tracking & KPI Calculation ===\n",
        "✅ analyze_intervention_outcome (with outcome) passed\n",
        "✅ analyze_intervention_outcome (pending) passed\n",
        "✅ analyze_all_outcomes passed\n",
        "✅ calculate_outcome_summary passed\n",
        "✅ calculate_operational_kpis passed\n",
        "✅ calculate_effectiveness_kpis passed\n",
        "✅ calculate_business_kpis passed\n",
        "✅ assess_all_kpi_status passed\n",
        "✅ calculate_roi_breakdown passed\n",
        "✅ calculate_roi_estimate passed\n",
        "✅ test_outcome_tracking_node passed\n",
        "✅ test_kpi_calculation_node passed\n",
        "✅ test_roi_calculation_node passed\n",
        "✅ test_summary_generation_node passed\n",
        "✅ All Phase 6 tests passed!\n",
        "\n",
        "=== Phase 7: Report Generation & Orchestrator Wiring ===\n",
        "✅ generate_journey_report passed\n",
        "✅ test_report_generation_node passed\n",
        "✅ test_complete_workflow (all customers) passed\n",
        "✅ test_complete_workflow (single customer) passed\n",
        "✅ All Phase 7 tests passed!\n",
        "\n",
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_011_Customer_Journey_Orchestrator %"
      ],
      "metadata": {
        "id": "9yII5GHDZhRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}