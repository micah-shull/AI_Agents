{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBqf9vUg3d6WmdOOzJiHIe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/165_Agent_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Job Application Agent\n",
        "\n",
        "This is a classic **conditional routing** assignment in LangGraph. You already have the skeleton. Let‚Äôs fill it in step by step:\n",
        "\n",
        "---\n",
        "\n",
        "## üìù What you need to do\n",
        "\n",
        "1. **Implement `categorize_candidate`** so it checks years of experience.\n",
        "2. **Add graph nodes** for each step (`categorize_candidate`, `schedule_interview`, `assign_skills_test`).\n",
        "3. **Add conditional edges** so the workflow routes to the right node depending on the candidate.\n",
        "\n",
        "---\n",
        "\n",
        "## üèÉ What Happens\n",
        "\n",
        "* Alice has 6 years ‚Üí categorized as `\"senior\"` ‚Üí routed to `schedule_interview`.\n",
        "* Bob has 3 years ‚Üí categorized as `\"junior\"` ‚Üí routed to `assign_skills_test`.\n"
      ],
      "metadata": {
        "id": "EA9siF7sX-Uw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vZ_qarHX5RR"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "\n",
        "# Define the structure of the input state (job application)\n",
        "class JobApplication(TypedDict):\n",
        "    applicant_name: str\n",
        "    years_experience: int\n",
        "    status: str\n",
        "\n",
        "# Function to categorize candidates\n",
        "def categorize_candidate(application: JobApplication):\n",
        "    if application[\"years_experience\"] >= 5:\n",
        "        return {\"status\": \"senior\"}\n",
        "    else:\n",
        "        return {\"status\": \"junior\"}\n",
        "\n",
        "# Function for interview scheduling\n",
        "def schedule_interview(application: JobApplication):\n",
        "    print(f\"Candidate {application['applicant_name']} is shortlisted for an interview.\")\n",
        "    return {\"status\": \"Interview Scheduled\"}\n",
        "\n",
        "# Function for skills test\n",
        "def assign_skills_test(application: JobApplication):\n",
        "    print(f\"Candidate {application['applicant_name']} is assigned a skills test.\")\n",
        "    return {\"status\": \"Skills Test Assigned\"}\n",
        "\n",
        "# Create the state graph\n",
        "graph = StateGraph(JobApplication)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"categorize\", categorize_candidate)\n",
        "graph.add_node(\"interview\", schedule_interview)\n",
        "graph.add_node(\"skills_test\", assign_skills_test)\n",
        "\n",
        "# Entry point\n",
        "graph.set_entry_point(\"categorize\")\n",
        "\n",
        "# Conditional edges\n",
        "graph.add_conditional_edges(\n",
        "    \"categorize\",\n",
        "    lambda state: state[\"status\"],  # decide based on status\n",
        "    {\n",
        "        \"senior\": \"interview\",\n",
        "        \"junior\": \"skills_test\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# End edges\n",
        "graph.add_edge(\"interview\", END)\n",
        "graph.add_edge(\"skills_test\", END)\n",
        "\n",
        "# Compile the workflow\n",
        "runnable = graph.compile()\n",
        "\n",
        "# Simulate job applications\n",
        "print(runnable.invoke({\"applicant_name\": \"Alice\", \"years_experience\": 6, \"status\": \"\"}))\n",
        "print(runnable.invoke({\"applicant_name\": \"Bob\", \"years_experience\": 3, \"status\": \"\"}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. What does `add_conditional_edges` do?\n",
        "\n",
        "Normally in a graph you connect Node A ‚Üí Node B. But sometimes, you want a **decision point**:\n",
        "\n",
        "* If condition = X ‚Üí go to Node B\n",
        "* If condition = Y ‚Üí go to Node C\n",
        "\n",
        "That‚Äôs what `add_conditional_edges` does: it lets you route the workflow *dynamically* based on the state.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The role of `lambda state: state[\"status\"]`\n",
        "\n",
        "* Every node in LangGraph takes the current **state** (a dictionary that matches your `TypedDict`) as input.\n",
        "* A **lambda** is just a small inline function.\n",
        "* `lambda state: state[\"status\"]` means:\n",
        "\n",
        "  > ‚ÄúLook at the current state and return the value of the `status` field.‚Äù\n",
        "\n",
        "So:\n",
        "\n",
        "* If the state has `{\"status\": \"senior\"}` ‚Üí it returns `\"senior\"`.\n",
        "* If the state has `{\"status\": \"junior\"}` ‚Üí it returns `\"junior\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How the mapping works\n",
        "\n",
        "You then tell LangGraph how to map that return value to nodes:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"senior\": \"interview\",\n",
        "    \"junior\": \"skills_test\"\n",
        "}\n",
        "```\n",
        "\n",
        "So:\n",
        "\n",
        "* If `lambda state` returns `\"senior\"`, LangGraph routes to node `\"interview\"`.\n",
        "* If it returns `\"junior\"`, LangGraph routes to `\"skills_test\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Why a lambda?\n",
        "\n",
        "Because it gives you **flexibility**. You could have written something fancier, e.g.:\n",
        "\n",
        "```python\n",
        "lambda state: \"senior\" if state[\"years_experience\"] >= 5 else \"junior\"\n",
        "```\n",
        "\n",
        "This way you don‚Äôt even need to store `\"status\"` in state ‚Äî you compute it on the fly.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary**:\n",
        "The `lambda state: state[\"status\"]` is just a tiny function that looks at the workflow‚Äôs current state and tells the graph which edge to follow. Think of it as the ‚Äúdecision rule‚Äù at a fork in the workflow.\n",
        "\n"
      ],
      "metadata": {
        "id": "CvkoKa9FYOTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Apply Reducer"
      ],
      "metadata": {
        "id": "ueAzaO3uaAIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "from operator import add\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "# Define chatbot state with accumulated orders\n",
        "class OrderState(TypedDict):\n",
        "    messages: list[AnyMessage]\n",
        "    order_id: int\n",
        "\n",
        "\n",
        "# Step 1: Take the food order\n",
        "def take_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Processing your order?\")]}\n",
        "\n",
        "\n",
        "# Step 2: Confirm the order\n",
        "def confirm_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Your order has been placed!\")], \"order_id\": 1}\n",
        "\n",
        "\n",
        "# Build chatbot conversation flow\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"take_order\", take_order)\n",
        "graph_builder.add_node(\"confirm_order\", confirm_order)\n",
        "\n",
        "# Define conversation flow\n",
        "graph_builder.add_edge(START, \"take_order\")\n",
        "graph_builder.add_edge(\"take_order\", \"confirm_order\")\n",
        "graph_builder.add_edge(\"confirm_order\", END)\n",
        "\n",
        "# Compile chatbot\n",
        "chatbot = graph_builder.compile()\n",
        "\n",
        "# Simulate a conversation\n",
        "test_input = \"I want a burger.\"\n",
        "\n",
        "messages = chatbot.invoke({\"messages\": [HumanMessage(content=test_input)]})\n",
        "\n",
        "for message in messages[\"messages\"]:\n",
        "    print(f\"Message: {message.content}\")\n",
        "\n",
        "print(\"Total Orders: \", messages[\"order_id\"])\n"
      ],
      "metadata": {
        "id": "hwYSwjjKYSJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now your `OrderState` looks like this:\n",
        "\n",
        "```python\n",
        "class OrderState(TypedDict):\n",
        "    messages: list[AnyMessage]\n",
        "    order_id: int\n",
        "```\n",
        "\n",
        "This means **by default** LangGraph will use *last-write-wins*: if multiple nodes update `messages` or `order_id`, the later one just overwrites the previous value.\n",
        "\n",
        "But your assignment is to **accumulate** both `messages` and `order_id` using reducers. That‚Äôs where `Annotated` + `operator.add` come in.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How to Apply Reducers\n",
        "\n",
        "Here‚Äôs the change:\n",
        "\n",
        "```python\n",
        "from typing import Annotated, TypedDict\n",
        "from operator import add\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "# Define chatbot state with reducers\n",
        "class OrderState(TypedDict):\n",
        "    # Accumulate all messages instead of overwriting\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "    # Add order IDs together instead of overwriting\n",
        "    order_id: Annotated[int, add]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîé What this does:\n",
        "\n",
        "* `messages: Annotated[list[AnyMessage], add]`\n",
        "  ‚Üí If one node returns `{\"messages\": [AIMessage(\"hi\")]}` and another node later returns `{\"messages\": [AIMessage(\"bye\")]}`, LangGraph will combine them into `[AIMessage(\"hi\"), AIMessage(\"bye\")]` instead of overwriting.\n",
        "\n",
        "* `order_id: Annotated[int, add]`\n",
        "  ‚Üí If one node sets `{\"order_id\": 1}` and another later sets `{\"order_id\": 1}`, the reducer will add them up ‚Üí final value `2`.\n",
        "\n",
        "This way, each new order increments the total count, instead of resetting.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Updated Section in Your Code\n",
        "\n",
        "```python\n",
        "# Define chatbot state with accumulated orders\n",
        "class OrderState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "    order_id: Annotated[int, add]\n",
        "```\n",
        "\n",
        "The rest of your workflow (`take_order`, `confirm_order`, etc.) stays the same.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lg1GgczjZ2Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "from operator import add\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "# Define chatbot state with accumulated orders\n",
        "# class OrderState(TypedDict):\n",
        "#     messages: list[AnyMessage]\n",
        "#     order_id: int\n",
        "\n",
        "# Define chatbot state with reducers\n",
        "class OrderState(TypedDict):\n",
        "    # Accumulate all messages instead of overwriting\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "    # Add order IDs together instead of overwriting\n",
        "    order_id: Annotated[int, add]\n",
        "\n",
        "# Step 1: Take the food order\n",
        "def take_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Processing your order?\")]}\n",
        "\n",
        "\n",
        "# Step 2: Confirm the order\n",
        "def confirm_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Your order has been placed!\")], \"order_id\": 1}\n",
        "\n",
        "\n",
        "# Build chatbot conversation flow\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"take_order\", take_order)\n",
        "graph_builder.add_node(\"confirm_order\", confirm_order)\n",
        "\n",
        "# Define conversation flow\n",
        "graph_builder.add_edge(START, \"take_order\")\n",
        "graph_builder.add_edge(\"take_order\", \"confirm_order\")\n",
        "graph_builder.add_edge(\"confirm_order\", END)\n",
        "\n",
        "# Compile chatbot\n",
        "chatbot = graph_builder.compile()\n",
        "\n",
        "# Simulate a conversation\n",
        "test_input = \"I want a burger.\"\n",
        "\n",
        "messages = chatbot.invoke({\"messages\": [HumanMessage(content=test_input)]})\n",
        "\n",
        "for message in messages[\"messages\"]:\n",
        "    print(f\"Message: {message.content}\")\n",
        "\n",
        "print(\"Total Orders: \", messages[\"order_id\"])\n"
      ],
      "metadata": {
        "id": "aCIzPa2pZ7Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëç Next ‚Äî you want to move away from writing a custom `TypedDict` (like `OrderState`) and instead just use LangGraph‚Äôs built-in **`MessagesState`**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîé What `MessagesState` is\n",
        "\n",
        "LangGraph provides `MessagesState` as a convenience type when your main state is a list of messages (`HumanMessage`, `AIMessage`, etc.).\n",
        "It already has the reducer `Annotated[List[AnyMessage], add]` baked in, so messages automatically accumulate across nodes.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ How to refactor your workflow\n",
        "\n",
        "Right now you have:\n",
        "\n",
        "```python\n",
        "from typing import Annotated, TypedDict\n",
        "from operator import add\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class OrderState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "    order_id: Annotated[int, add]\n",
        "```\n",
        "\n",
        "You can replace that with:\n",
        "\n",
        "```python\n",
        "from langgraph.graph import MessagesState\n",
        "from typing import Annotated\n",
        "from operator import add\n",
        "\n",
        "# Extend MessagesState to add order_id with reducer\n",
        "class OrderState(MessagesState):\n",
        "    order_id: Annotated[int, add]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Benefits\n",
        "\n",
        "* You don‚Äôt need to redefine `messages` ‚Äî `MessagesState` already handles it.\n",
        "* You still keep `order_id` as an accumulator by extending it.\n",
        "* Cleaner and more idiomatic LangGraph code.\n",
        "\n",
        "---\n",
        "\n",
        "So effectively:\n",
        "\n",
        "* ‚úÖ Use `MessagesState` for conversation memory.\n",
        "* ‚úÖ Add extra fields (like `order_id`) on top.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3MT4eRLbAIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "from operator import add\n",
        "from langgraph.graph import END, START, StateGraph\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# Extend MessagesState to add order_id with reducer\n",
        "class OrderState(MessagesState):\n",
        "    order_id: Annotated[int, add]\n",
        "\n",
        "# Step 1: Take the food order\n",
        "def take_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Processing your order?\")]}\n",
        "\n",
        "\n",
        "# Step 2: Confirm the order\n",
        "def confirm_order(state: OrderState):\n",
        "    return {\"messages\": [AIMessage(content=\"Your order has been placed!\")], \"order_id\": 1}\n",
        "\n",
        "\n",
        "# Build chatbot conversation flow\n",
        "graph_builder = StateGraph(OrderState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"take_order\", take_order)\n",
        "graph_builder.add_node(\"confirm_order\", confirm_order)\n",
        "\n",
        "# Define conversation flow\n",
        "graph_builder.add_edge(START, \"take_order\")\n",
        "graph_builder.add_edge(\"take_order\", \"confirm_order\")\n",
        "graph_builder.add_edge(\"confirm_order\", END)\n",
        "\n",
        "# Compile chatbot\n",
        "chatbot = graph_builder.compile()\n",
        "\n",
        "# Simulate a conversation\n",
        "test_input = \"I want a burger.\"\n",
        "\n",
        "messages = chatbot.invoke({\"messages\": [HumanMessage(content=test_input)]})\n",
        "\n",
        "for message in messages[\"messages\"]:\n",
        "    print(f\"Message: {message.content}\")\n",
        "\n",
        "print(\"Total Orders: \", messages[\"order_id\"])\n"
      ],
      "metadata": {
        "id": "8DrWVOqRa952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Because `MessagesState` is just:\n",
        "\n",
        "```python\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "```\n",
        "\n",
        "with the **`add` reducer baked in**, every node that appends to `state[\"messages\"]` causes the list to grow.\n",
        "\n",
        "---\n",
        "\n",
        "### üîé What this means in practice\n",
        "\n",
        "* **Yes**: at any point in execution you can inspect `state[\"messages\"]` and see **the full conversation so far** (all `HumanMessage`, `AIMessage`, `SystemMessage`, etc.).\n",
        "* It‚Äôs not overwriting ‚Äî it‚Äôs **accumulating**.\n",
        "* Think of it as a conversation log that keeps expanding as the graph progresses.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "state = MessagesState(messages=[])\n",
        "\n",
        "# node 1 appends a human message\n",
        "state[\"messages\"].append(HumanMessage(content=\"Hello\"))\n",
        "\n",
        "# node 2 appends an AI response\n",
        "state[\"messages\"].append(AIMessage(content=\"Hi there!\"))\n",
        "\n",
        "print([m.content for m in state[\"messages\"]])\n",
        "# [\"Hello\", \"Hi there!\"]\n",
        "```\n",
        "\n",
        "At this point, **any downstream node sees the *entire* conversation**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è Why this matters\n",
        "\n",
        "* Agents that need **conversation history** (like a chatbot) always get the context.\n",
        "* You don‚Äôt need to manage reducers or manually merge lists ‚Äî it‚Äôs automatic.\n",
        "* You can always branch, filter, or truncate `messages` if you want to manage context length.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lNEXPHjlfu4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üîπ When `MessagesState` *is* the best option\n",
        "\n",
        "Use `MessagesState` if:\n",
        "\n",
        "* The **main unit of state is the conversation** (chat turns).\n",
        "* You want to **persist and accumulate messages** between nodes without redefining reducers every time.\n",
        "* Your workflow is essentially ‚Äúagents talking‚Äù (customer support bot, orchestrator chat loop, multi-turn assistant).\n",
        "\n",
        "It‚Äôs a convenience wrapper:\n",
        "\n",
        "```python\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add]\n",
        "```\n",
        "\n",
        "So if your workflow is **mostly messages + a few extras**, you can just subclass it and add extra fields:\n",
        "\n",
        "```python\n",
        "class OrderState(MessagesState):\n",
        "    order_id: Annotated[int, add]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ When a custom `TypedDict` is better\n",
        "\n",
        "Stick to your own `TypedDict` if:\n",
        "\n",
        "* You don‚Äôt really care about conversation logs.\n",
        "* The workflow state is **structured data**, like `order_id`, `cart_items`, `payment_status`, etc.\n",
        "* Messages are incidental, not the core.\n",
        "\n",
        "Example: an e-commerce order processor doesn‚Äôt necessarily need `MessagesState`. It might make more sense to define:\n",
        "\n",
        "```python\n",
        "class OrderState(TypedDict):\n",
        "    order_id: Annotated[int, add]\n",
        "    items: Annotated[list[str], add]\n",
        "    status: str\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è In your case (order processing workflow):\n",
        "\n",
        "* If the focus is **simulating an agent talking with a customer about their order**, then yes ‚úÖ `MessagesState` is a neat fit.\n",
        "* If the focus is more like a **backend pipeline (orders flowing through steps)**, then a plain `TypedDict` with reducers is the better, cleaner choice.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e9UOZqWFbjsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Doctor Visit Agent"
      ],
      "metadata": {
        "id": "6N1uqxLKiqi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# -------------------------\n",
        "# Tools\n",
        "# -------------------------\n",
        "@tool\n",
        "def check_symptoms(symptom: str):\n",
        "    \"\"\"Provides possible conditions based on the symptom described.\"\"\"\n",
        "    conditions = {\n",
        "        \"fever\": [\"Flu\", \"COVID-19\", \"Common Cold\"],\n",
        "        \"cough\": [\"Bronchitis\", \"Pneumonia\", \"Common Cold\"],\n",
        "        \"headache\": [\"Migraine\", \"Tension Headache\", \"Sinus Infection\"],\n",
        "    }\n",
        "    return conditions.get(symptom.lower(), [\"No specific conditions found. Please consult a doctor.\"])\n",
        "\n",
        "@tool\n",
        "def book_doctor_appointment(specialty: str, date: str, time: str):\n",
        "    \"\"\"Books an appointment with a doctor based on the required specialty.\"\"\"\n",
        "    available_specialties = [\"General Physician\", \"Cardiologist\", \"Neurologist\", \"Pediatrician\"]\n",
        "    if specialty in available_specialties:\n",
        "        return f\"Appointment booked with {specialty} on {date} at {time}.\"\n",
        "    else:\n",
        "        return f\"Sorry, no available {specialty} at this time.\"\n",
        "\n",
        "tools = [check_symptoms, book_doctor_appointment]\n",
        "\n",
        "# -------------------------\n",
        "# LLM\n",
        "# -------------------------\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # you can change to gpt-3.5 or gpt-4\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# -------------------------\n",
        "# Nodes\n",
        "# -------------------------\n",
        "# ToolNode: executes actual tool calls\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Model call node\n",
        "def call_model(state: MessagesState):\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Routing: if AIMessage contains tool calls, go to ToolNode\n",
        "def should_continue(state: MessagesState):\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "    if isinstance(last_msg, AIMessage) and last_msg.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# -------------------------\n",
        "# Workflow\n",
        "# -------------------------\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_conditional_edges(\"model\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"model\")\n",
        "\n",
        "# -------------------------\n",
        "# Compile & Run\n",
        "# -------------------------\n",
        "checkpointer = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=checkpointer)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Step 1: Check Symptoms\n",
        "response = graph.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"I have a fever. Can you tell me what this condition might be?\")]},\n",
        "    config\n",
        ")\n",
        "\n",
        "print(response[\"messages\"][-1])\n",
        "conditions = response[\"messages\"][-1].content\n",
        "print(\"\\nüîç **Possible Conditions Based on Symptoms:**\")\n",
        "print(conditions)\n",
        "\n",
        "# Step 2: Book Doctor Appointment\n",
        "response = graph.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Book an appointment for these conditions with a General Physician for tomorrow at 10 AM.\")]},\n",
        "    config\n",
        ")\n",
        "\n",
        "final_response = response[\"messages\"][-1].content\n",
        "print(\"\\nüìÖ **Doctor Appointment Confirmation:**\")\n",
        "print(final_response)\n"
      ],
      "metadata": {
        "id": "H2x1yxYrbmYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëå ‚Äî this is one of those design subtleties in LangGraph that‚Äôs important to get clear.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ ToolNode vs add_node(tool)\n",
        "\n",
        "```python\n",
        "tools = [check_symptoms, book_doctor_appointment]\n",
        "tool_node = ToolNode(tools)\n",
        "```\n",
        "\n",
        "instead of directly:\n",
        "\n",
        "```python\n",
        "tool_node = ToolNode([check_symptoms, book_doctor_appointment])\n",
        "```\n",
        "\n",
        "That‚Äôs honestly just **style/clarity**. Both are valid.\n",
        "I made a `tools` list so it could be reused in two places:\n",
        "\n",
        "* ‚úÖ `llm_with_tools = llm.bind_tools(tools)`\n",
        "* ‚úÖ `ToolNode(tools)`\n",
        "\n",
        "This way you don‚Äôt duplicate the list of tools and risk mismatches. But if you don‚Äôt need reuse, you can inline them directly.\n",
        "\n",
        "So **it‚Äôs not because of binding** ‚Äî binding and ToolNode are two separate things:\n",
        "\n",
        "* `llm.bind_tools(tools)` ‚Üí tells the LLM how to *call* tools (generates structured tool calls).\n",
        "* `ToolNode(tools)` ‚Üí actually *executes* the tool calls when they appear in the messages.\n",
        "\n",
        "You need both if you want a ReAct-style loop (model decides to call ‚Üí ToolNode executes ‚Üí model continues).\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Difference: ToolNode vs adding tools as normal nodes\n",
        "\n",
        "You *could* just do:\n",
        "\n",
        "```python\n",
        "workflow.add_node(\"check\", check_symptoms)\n",
        "workflow.add_node(\"book\", book_doctor_appointment)\n",
        "```\n",
        "\n",
        "But that would mean:\n",
        "\n",
        "* You must manually decide when to call which tool.\n",
        "* The graph edges would hard-code ‚Äúuser ‚Üí check_symptoms‚Äù or ‚Äúuser ‚Üí book_doctor_appointment.‚Äù\n",
        "* No dynamic ‚ÄúAI decides tool usage.‚Äù\n",
        "\n",
        "Whereas with `ToolNode`:\n",
        "\n",
        "* It‚Äôs a **generic execution node**: it looks at the last `AIMessage` for tool calls, then routes them to the correct function automatically.\n",
        "* You don‚Äôt have to manually wire edges for every tool. Add 10 tools? Still one `ToolNode`.\n",
        "* Works naturally with `bind_tools`, since the LLM emits structured tool calls and ToolNode just executes them.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è So in short\n",
        "\n",
        "* **ToolNode** = dynamic, reusable executor for many tools. It plugs into the ‚Äúmodel ‚Üí decide ‚Üí tool ‚Üí model‚Äù loop.\n",
        "* **Plain add_node(tool_fn)** = static function call. Use if you want fixed steps in the workflow (like `summarizer ‚Üí writer ‚Üí editor`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jh3MjS6xjcRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let‚Äôs make the contrast super clear:\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ With `ToolNode(tools)`\n",
        "\n",
        "```python\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "```\n",
        "\n",
        "* `call_model` = LLM generates output, which *might* include a tool call.\n",
        "* If the output contains a tool call ‚Üí we route into `tool_node`.\n",
        "* `ToolNode` looks at the tool call metadata (`AIMessage.tool_calls`) and dispatches to the correct tool (`check_symptoms` or `book_doctor_appointment`).\n",
        "* Then the result goes back into the model, which can decide to call another tool, or just finish.\n",
        "\n",
        "üëâ This is **dynamic** ‚Äî the LLM decides which tool(s) to call, in what order, and how many times.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ With plain `add_node(tool1)` + `add_node(tool2)`\n",
        "\n",
        "```python\n",
        "workflow.add_node(\"check\", check_symptoms)\n",
        "workflow.add_node(\"book\", book_doctor_appointment)\n",
        "```\n",
        "\n",
        "* Each node is just a Python function.\n",
        "* You must wire the edges explicitly:\n",
        "\n",
        "  ```python\n",
        "  workflow.add_edge(\"check\", \"book\")\n",
        "  ```\n",
        "* That means `check ‚Üí book` is always executed in sequence, regardless of whether the user wanted it.\n",
        "* No ‚Äúchoice‚Äù ‚Äî it‚Äôs a fixed pipeline.\n",
        "\n",
        "üëâ This is **static** ‚Äî you‚Äôre building a deterministic workflow where tools always run in a pre-set order.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è When to use which\n",
        "\n",
        "* **Use `ToolNode(tools)`** when you want the LLM to behave like an *agent*, dynamically choosing tools and chaining them.\n",
        "* **Use plain `add_node`** when you want a *workflow pipeline* (e.g., summarizer ‚Üí writer ‚Üí editor) where each step always happens.\n",
        "\n",
        "---\n",
        "\n",
        "So your example:\n",
        "\n",
        "```python\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "```\n",
        "\n",
        "is the canonical **agent pattern** in LangGraph:\n",
        "`START ‚Üí model ‚Üí (maybe tools) ‚Üí model ‚Üí END`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d7dEFIoSksr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "19DCZp0Ar4SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, TypedDict\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Medical Knowledge Sources (Example URLs)\n",
        "medical_info_urls = [\n",
        "    \"https://www.who.int/health-topics\",\n",
        "    \"https://www.mayoclinic.org/diseases-conditions\",\n",
        "    \"https://medlineplus.gov/symptoms.html\",\n",
        "    \"https://www.webmd.com/a-to-z-guides/diseases-conditions\",\n",
        "]\n",
        "\n",
        "# Load Medical Data\n",
        "docs = [WebBaseLoader(url).load() for url in medical_info_urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split Medical Information\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Store and Retrieve Medical Data with ChromaDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"medical-diagnosis\",\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Prompt for Medical Diagnosis\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are an AI doctor providing preliminary medical diagnoses based on symptoms.\n",
        "    Use ONLY the retrieved medical information in {context} to ground your analysis.\n",
        "    If information is insufficient, say so and suggest seeing a clinician.\n",
        "\n",
        "    Patient Symptoms: {question}\n",
        "\n",
        "    Relevant Medical Information:\n",
        "    {context}\n",
        "\n",
        "    AI-Powered Diagnosis:\n",
        "    \"\"\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "diagnosis_chain = prompt | model | StrOutputParser()\n",
        "\n",
        "# Define Graph State\n",
        "class MedicalDiagnosisGraphState(TypedDict):\n",
        "    question: str\n",
        "    retrieved_data: List[str]\n",
        "    diagnosis: str\n",
        "\n",
        "# --- Implementations ---\n",
        "\n",
        "def retrieve_medical_data(state: MedicalDiagnosisGraphState):\n",
        "    \"\"\"Use the vector retriever to gather relevant medical passages for the question.\"\"\"\n",
        "    query = state[\"question\"]\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    # keep just the text; cap to a handful to control prompt size\n",
        "    passages = [d.page_content for d in docs][:8]\n",
        "    return {\"retrieved_data\": passages}\n",
        "\n",
        "def analyze_medical_diagnosis(state: MedicalDiagnosisGraphState):\n",
        "    \"\"\"Run the RAG prompt: combine retrieved context + question -> diagnosis.\"\"\"\n",
        "    context = \"\\n\\n---\\n\\n\".join(state[\"retrieved_data\"]) if state[\"retrieved_data\"] else \"No context retrieved.\"\n",
        "    diagnosis_text = diagnosis_chain.invoke({\"question\": state[\"question\"], \"context\": context})\n",
        "    return {\"diagnosis\": diagnosis_text}\n",
        "\n",
        "def create_medical_diagnosis_workflow():\n",
        "    \"\"\"Build a simple two-node LangGraph: retrieve -> analyze -> END.\"\"\"\n",
        "    builder = StateGraph(MedicalDiagnosisGraphState)\n",
        "    builder.add_node(\"retrieve\", retrieve_medical_data)\n",
        "    builder.add_node(\"analyze\", analyze_medical_diagnosis)\n",
        "\n",
        "    builder.set_entry_point(\"retrieve\")\n",
        "    builder.add_edge(\"retrieve\", \"analyze\")\n",
        "    builder.add_edge(\"analyze\", END)\n",
        "\n",
        "    checkpointer = MemorySaver()\n",
        "    return builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "# Execute the Workflow\n",
        "medical_diagnosis_graph = create_medical_diagnosis_workflow()\n",
        "\n",
        "inputs = {\"question\": \"I have a persistent cough, fever, and shortness of breath. What could it be?\"}\n",
        "\n",
        "response = medical_diagnosis_graph.invoke(inputs)\n",
        "\n",
        "print(\"\\n--- AI MEDICAL DIAGNOSIS ---\")\n",
        "print(response[\"diagnosis\"])\n"
      ],
      "metadata": {
        "id": "cw4cVK6Qjxec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human in the Loop"
      ],
      "metadata": {
        "id": "UvF2nni9tBcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.types import interrupt\n",
        "from langgraph.types import Command\n",
        "\n",
        "\n",
        "class CodingAssistantState(TypedDict):\n",
        "    task: str\n",
        "    code: str\n",
        "    tests: str\n",
        "\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "code_prompt = ChatPromptTemplate.from_template(\"Generate Python code for: {task}\")\n",
        "test_prompt = ChatPromptTemplate.from_template(\"Write unit tests for this code:\\n{code}\")\n",
        "\n",
        "code_chain = code_prompt | model | StrOutputParser()\n",
        "test_chain = test_prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "def generate_code(state):\n",
        "    print(\"Generate Code\")\n",
        "    code = code_chain.invoke({\"task\": state[\"task\"]})\n",
        "    return Command(goto=\"human_review\", update={\"code\": code})\n",
        "\n",
        "\n",
        "# TODO\n",
        "def human_review(state):\n",
        "    value = interrupt({\n",
        "        \"question\": \"Are you ok with the code.Type yes or no\"\n",
        "    })\n",
        "    if value == \"yes\":\n",
        "        return Command(goto=\"create_tests\")\n",
        "    else:\n",
        "        return Command(goto=END)\n",
        "\n",
        "\n",
        "def create_tests(state):\n",
        "    tests = test_chain.invoke({\"code\": state[\"code\"]})\n",
        "    return Command(goto=END, update={\"tests\": tests})\n",
        "\n",
        "\n",
        "def create_coding_assistant_workflow():\n",
        "    workflow = StateGraph(CodingAssistantState)\n",
        "    workflow.add_node(\"generate_code\", generate_code)\n",
        "    workflow.add_node(\"human_review\", human_review)\n",
        "    workflow.add_node(\"create_tests\", create_tests)\n",
        "    workflow.set_entry_point(\"generate_code\")\n",
        "    return workflow.compile(checkpointer=MemorySaver())\n",
        "\n",
        "\n",
        "# Run the Workflow\n",
        "coding_assistant = create_coding_assistant_workflow()\n",
        "inputs = {\"task\": \"Create a function to reverse a string in Python.\"}\n",
        "thread = {\"configurable\": {\"thread_id\": 1}}\n",
        "result = coding_assistant.invoke(inputs, config=thread)\n",
        "# TODO: Handle Interrupt\n",
        "print(\"\\n-----Generated Code--------\")\n",
        "print(result[\"code\"])\n",
        "tasks = coding_assistant.get_state(config=thread).tasks\n",
        "print(tasks)\n",
        "task = tasks[0]\n",
        "question = task.interrupts[0].value.get(\"question\")\n",
        "user_input = input(question)\n",
        "result = coding_assistant.invoke(Command(resume=user_input),config=thread)\n",
        "\n",
        "print(\"\\n--- Generated Tests ---\")\n",
        "print(result.get(\"tests\", \"No code or tests generated\"))\n"
      ],
      "metadata": {
        "id": "HDF7gfZptAkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You‚Äôve hit on one of the **biggest conceptual differences** between two styles in LangChain/LangGraph:\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. Chains (`prompt | model | parser`)\n",
        "\n",
        "This is what you see in your **human-in-the-loop** script:\n",
        "\n",
        "```python\n",
        "code_prompt = ChatPromptTemplate.from_template(\"Generate Python code for: {task}\")\n",
        "code_chain = code_prompt | model | StrOutputParser()\n",
        "```\n",
        "\n",
        "That `|` operator is just syntactic sugar for **piping one component‚Äôs output into the next**:\n",
        "\n",
        "* Prompt template fills in text (`{task}`).\n",
        "* Model runs on that text.\n",
        "* Output parser converts the raw model response into a Python type (here, `str`).\n",
        "\n",
        "üëâ Think of a `chain` as a **mini data pipeline** that executes *within a single node*. It‚Äôs sequential but self-contained. Great for small, linear transformations.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. Graph Nodes (sequential state machine)\n",
        "\n",
        "In your earlier assignments, we did this:\n",
        "\n",
        "```python\n",
        "builder.add_node(\"summarizer\", summarize)\n",
        "builder.add_node(\"writer\", write_report)\n",
        "builder.add_edge(\"summarizer\", \"writer\")\n",
        "```\n",
        "\n",
        "Here, the **graph engine** controls flow. Each node is a function that takes state and returns state (or a `Command` with routing). This lets you branch, loop, add parallel paths, or insert human review.\n",
        "\n",
        "üëâ Think of a `graph` as the **workflow orchestrator**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Key Difference\n",
        "\n",
        "* **Chains** are *intra-node pipelines*: \"prompt ‚Üí LLM ‚Üí parser\".\n",
        "\n",
        "  * Lightweight, sequential, reusable.\n",
        "  * Ideal inside a node that should just ‚Äúdo one thing.‚Äù\n",
        "* **Graphs** are *inter-node workflows*: \"node A ‚Üí node B ‚Üí ‚Ä¶\".\n",
        "\n",
        "  * Heavy-duty orchestration.\n",
        "  * Ideal when you want branching, retries, human feedback, or multi-step agent reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Why your human-in-the-loop script uses both\n",
        "\n",
        "* **Chains** (`code_chain`, `test_chain`) encapsulate *atomic reasoning tasks*: generate code, generate tests.\n",
        "* **Graph nodes** wire them together and insert interruptions for **human review** before continuing.\n",
        "\n",
        "So:\n",
        "\n",
        "* Chain = **how a node thinks**.\n",
        "* Graph = **how nodes talk**.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ The Human-in-the-Loop design is possible because `interrupt()` pauses the graph and waits for user input, something you *can‚Äôt* do with just a linear chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "4f0Kr_D4t4_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üîπ 1. Where LangChain stops and LangGraph begins\n",
        "\n",
        "* **LangChain** gives you building blocks: `PromptTemplate`, `LLM`, `OutputParser`, and the `|` operator to compose them into **chains**.\n",
        "* **LangGraph** is the orchestrator: it runs chains as nodes, controls flow, and adds features like `interrupt()` for human review, retries, error edges, and parallelism.\n",
        "\n",
        "So your code generator agent is really:\n",
        "\n",
        "* Node logic = LangChain chain (`prompt | model | parser`)\n",
        "* Workflow wiring = LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 2. Human in the loop is special\n",
        "\n",
        "Most graphs run end-to-end automatically. But with `interrupt(\"review_code\")` you‚Äôre saying:\n",
        "\n",
        "* ‚ÄúPause here, wait for human input.‚Äù\n",
        "* The graph saves state, and when you provide feedback (like ‚úÖ accept or ‚úèÔ∏è edit), it resumes right where it left off.\n",
        "\n",
        "That‚Äôs super powerful for safety-critical domains (coding, medicine, law) where you don‚Äôt want unchecked LLM outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 3. State is still the backbone\n",
        "\n",
        "Even in this setup, the `state` carries:\n",
        "\n",
        "* Conversation messages (if you subclass `MessagesState`)\n",
        "* Intermediate artifacts like `generated_code`, `tests`, `feedback`\n",
        "* Control variables (like a counter if you want multiple iterations)\n",
        "\n",
        "Because the graph persists state, you can resume after a crash or even hand off the workflow between people.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 4. You can mix patterns\n",
        "\n",
        "This agent is already combining **Chains** (intra-node pipelines) with **Graph orchestration** (inter-node workflow). But you could layer on:\n",
        "\n",
        "* **Reflection** ‚Üí after human approval, add a loop for the model to self-critique and improve code.\n",
        "* **Parallelism** ‚Üí generate multiple code candidates in parallel and vote.\n",
        "* **Routing** ‚Üí detect if a request is about Python, JS, or SQL and send to the right code-gen node.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 5. Costs & performance\n",
        "\n",
        "Every pause + human review = a potential delay in throughput. But the trade-off is higher quality and safer outputs. Usually worth it when mistakes are expensive.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ So the big picture: this agent is a **safe code-gen pipeline** where LangChain handles the micro-steps (prompts ‚Üí LLM ‚Üí parser) and LangGraph handles the macro orchestration (generate ‚Üí pause for review ‚Üí test ‚Üí final output).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Kb5sHE9uhup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great catch üôå ‚Äî you noticed that in your **code generator agent** the workflow is wrapped inside a function:\n",
        "\n",
        "```python\n",
        "def create_coding_assistant_workflow():\n",
        "    workflow = StateGraph(CodingAssistantState)\n",
        "    ...\n",
        "    return workflow.compile(checkpointer=MemorySaver())\n",
        "```\n",
        "\n",
        "Whereas before we were just building the graph at the top level.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Why wrap it in a definition?\n",
        "\n",
        "1. **Reusability**\n",
        "\n",
        "   * By putting it in a function, you can call `create_coding_assistant_workflow()` as many times as you like.\n",
        "   * Each call gives you a *fresh compiled workflow* with its own state and memory.\n",
        "   * Super useful for tests, multiple agents, or multiple sessions.\n",
        "\n",
        "2. **Encapsulation**\n",
        "\n",
        "   * Keeps all the graph-building logic in one place.\n",
        "   * Makes your main script cleaner (e.g., `workflow = create_coding_assistant_workflow()` instead of 15 lines of builder code).\n",
        "\n",
        "3. **Parameterization**\n",
        "\n",
        "   * You can add arguments to the function later (`use_persistent_memory=True`, `model=\"gpt-4o-mini\"`, etc.) to generate slightly different workflows with minimal code duplication.\n",
        "\n",
        "4. **Checkpointer lifecycle**\n",
        "\n",
        "   * Notice how it returns `workflow.compile(checkpointer=MemorySaver())`.\n",
        "   * This ensures that *every* workflow instance you spin up has its own checkpointer configured.\n",
        "   * Without wrapping, you‚Äôd risk reusing global memory unintentionally.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Difference from your earlier sequential-node graphs\n",
        "\n",
        "Earlier, we just did something like:\n",
        "\n",
        "```python\n",
        "workflow = StateGraph(MyState)\n",
        "workflow.add_node(\"summarizer\", summarize)\n",
        "workflow.set_entry_point(\"summarizer\")\n",
        "app = workflow.compile()\n",
        "```\n",
        "\n",
        "That‚Äôs fine for a single static graph in a notebook.\n",
        "But when you want to package it into a **reusable component** (like an agent or service), the function-based builder pattern is cleaner.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ So: **same graph mechanics, different packaging style**.\n",
        "\n",
        "* Top-level: good for quick demos.\n",
        "* Function factory: best for reusable, parameterized agents.\n",
        "\n"
      ],
      "metadata": {
        "id": "LerhzkUIvuIm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N3vEN1G9t85e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}