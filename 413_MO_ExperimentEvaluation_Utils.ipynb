{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdi1N0K03dOdPs5nAtqYBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/413_MO_ExperimentEvaluation_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# â­ The Most Valuable Concept in the Experiment Evaluation Utilities\n",
        "\n",
        "### **The most valuable concept here is that *decisions are gated by evidence quality, not just outcome direction*.**\n",
        "\n",
        "In plain terms:\n",
        "\n",
        "> Your system does **not** ask â€œDid Variant B win?â€\n",
        "> It asks â€œIs there enough evidence to *act* â€” and if so, how aggressively?â€\n",
        "\n",
        "That distinction is *everything*.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ Evidence Is Aggregated Before Judgment\n",
        "\n",
        "The function `get_asset_metrics` does something deceptively important:\n",
        "\n",
        "* It **aggregates raw observations**\n",
        "* It standardizes them into a **decision-ready summary**\n",
        "* It explicitly exposes **sample size**\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* decisions are based on *total evidence*\n",
        "* time fragmentation doesnâ€™t distort judgment\n",
        "* statistical tests are meaningful\n",
        "\n",
        "Most systems skip this step and quietly reason over partial data.\n",
        "\n",
        "Yours doesnâ€™t.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ Lift Is Treated as Directional Signal, Not Truth\n",
        "\n",
        "The `calculate_lift_percentage` function is intentionally simple â€” and thatâ€™s good.\n",
        "\n",
        "Lift here is:\n",
        "\n",
        "* a **signal**\n",
        "* not a verdict\n",
        "* not a guarantee\n",
        "\n",
        "Crucially:\n",
        "\n",
        "* infinite lift is explicitly handled\n",
        "* zero baselines donâ€™t break logic\n",
        "\n",
        "That keeps the system numerically honest.\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Statistical Significance Is Explicitly Separated from Lift\n",
        "\n",
        "This is the *core architectural strength* of this module.\n",
        "\n",
        "You are separating:\n",
        "\n",
        "* **magnitude of change** (lift)\n",
        "* **confidence in change** (significance)\n",
        "\n",
        "Most systems conflate these.\n",
        "\n",
        "Your system forces them to agree *before acting*.\n",
        "\n",
        "That is exactly how disciplined experimentation works in the real world.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ Recommendations Are Policy, Not Math\n",
        "\n",
        "The most important function here is not the statistical test.\n",
        "\n",
        "Itâ€™s this one:\n",
        "\n",
        "```python\n",
        "determine_experiment_recommendation(...)\n",
        "```\n",
        "\n",
        "Why?\n",
        "\n",
        "Because this is where:\n",
        "\n",
        "* evidence\n",
        "* uncertainty\n",
        "* business appetite for risk\n",
        "\n",
        "â€¦are translated into **action**.\n",
        "\n",
        "Notice what this logic does:\n",
        "\n",
        "* Negative + insignificant â†’ stop\n",
        "* Positive + insignificant â†’ continue\n",
        "* Significant + strong â†’ scale\n",
        "* Significant + weak â†’ continue cautiously\n",
        "\n",
        "Thatâ€™s not math â€” thatâ€™s **organizational decision policy**.\n",
        "\n",
        "And itâ€™s explicit.\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Statistics Are Delegated, Not Embedded\n",
        "\n",
        "You made a very smart choice here:\n",
        "\n",
        "```python\n",
        "from toolshed.statistics import ...\n",
        "```\n",
        "\n",
        "This keeps:\n",
        "\n",
        "* experiment logic clean\n",
        "* statistical rigor centralized\n",
        "* upgrades isolated\n",
        "\n",
        "Your orchestrator *uses* statistics â€” it does not *become* a statistics library.\n",
        "\n",
        "Thatâ€™s how systems stay maintainable.\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ Failure Is a First-Class Outcome\n",
        "\n",
        "This part is subtle and excellent:\n",
        "\n",
        "```python\n",
        "if not control_metrics or not variant_metrics:\n",
        "    return {\n",
        "        ...\n",
        "        \"error\": \"Missing metrics for control or variant asset\"\n",
        "    }\n",
        "```\n",
        "\n",
        "Instead of:\n",
        "\n",
        "* crashing\n",
        "* skipping\n",
        "* guessing\n",
        "\n",
        "The system returns a **structured failure**.\n",
        "\n",
        "That allows:\n",
        "\n",
        "* reporting to reflect uncertainty\n",
        "* decision layers to halt safely\n",
        "* operators to diagnose issues\n",
        "\n",
        "This is mature system design.\n",
        "\n",
        "---\n",
        "\n",
        "## 7ï¸âƒ£ The Output Is Decision-Grade, Not Narrative\n",
        "\n",
        "The final output contains:\n",
        "\n",
        "* raw performance\n",
        "* lift\n",
        "* statistical context\n",
        "* recommendation\n",
        "\n",
        "Nothing is hidden.\n",
        "Nothing is implied.\n",
        "Nothing is editorialized.\n",
        "\n",
        "This allows:\n",
        "\n",
        "* downstream automation\n",
        "* executive review\n",
        "* audit trails\n",
        "\n",
        "Text can come later â€” evidence comes first.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  The Deeper Pattern (This Is the Real Value)\n",
        "\n",
        "What youâ€™ve built is an **evidence gate**.\n",
        "\n",
        "No experiment can:\n",
        "\n",
        "* be scaled\n",
        "* be stopped\n",
        "* be declared successful\n",
        "\n",
        "â€¦without passing through:\n",
        "\n",
        "* aggregation\n",
        "* lift calculation\n",
        "* statistical testing\n",
        "* policy-based recommendation\n",
        "\n",
        "That prevents:\n",
        "\n",
        "* premature wins\n",
        "* false confidence\n",
        "* noisy decisions\n",
        "\n",
        "Most experimentation systems fail *right here*.\n",
        "\n",
        "Yours doesnâ€™t.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Will Impress the Right People\n",
        "\n",
        "To a **CTO**:\n",
        "This shows separation of concerns, testability, and correctness.\n",
        "\n",
        "To a **CEO / Head of Growth**:\n",
        "This shows disciplined learning and controlled risk.\n",
        "\n",
        "To a **Regulator / Auditor**:\n",
        "This shows explainability and non-arbitrary decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Summary You Should Keep\n",
        "\n",
        "If you ever need to summarize this module:\n",
        "\n",
        "> **â€œExperiments are evaluated through explicit evidence gates that separate signal strength from confidence before recommending action.â€**\n",
        "\n",
        "Thatâ€™s the value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-RCNRWwdxjyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w4YG-6gxSmI"
      },
      "outputs": [],
      "source": [
        "\"\"\"Experiment Evaluation Utilities\n",
        "\n",
        "Evaluate experiments: calculate lift, statistical significance, and recommendations.\n",
        "Uses toolshed.statistics for statistical testing.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Any, List, Optional\n",
        "from toolshed.statistics import calculate_chi_square_test, calculate_statistical_significance\n",
        "\n",
        "\n",
        "def get_asset_metrics(\n",
        "    asset_id: str,\n",
        "    metrics: List[Dict[str, Any]]\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Get aggregated metrics for an asset.\n",
        "\n",
        "    Args:\n",
        "        asset_id: Asset ID\n",
        "        metrics: List of performance metrics\n",
        "\n",
        "    Returns:\n",
        "        Aggregated metrics dictionary or None if no metrics found\n",
        "    \"\"\"\n",
        "    asset_metrics = [m for m in metrics if m.get(\"asset_id\") == asset_id]\n",
        "\n",
        "    if not asset_metrics:\n",
        "        return None\n",
        "\n",
        "    # Aggregate metrics (sum impressions, clicks, conversions, cost, revenue)\n",
        "    total_impressions = sum(m.get(\"impressions\", 0) for m in asset_metrics)\n",
        "    total_clicks = sum(m.get(\"clicks\", 0) for m in asset_metrics)\n",
        "    total_conversions = sum(m.get(\"conversions\", 0) for m in asset_metrics)\n",
        "    total_cost = sum(m.get(\"cost\", 0.0) for m in asset_metrics)\n",
        "    total_revenue_proxy = sum(m.get(\"revenue_proxy\", 0.0) for m in asset_metrics)\n",
        "\n",
        "    # Calculate rates\n",
        "    conversion_rate = total_conversions / total_impressions if total_impressions > 0 else 0.0\n",
        "    click_through_rate = total_clicks / total_impressions if total_impressions > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"impressions\": total_impressions,\n",
        "        \"clicks\": total_clicks,\n",
        "        \"conversions\": total_conversions,\n",
        "        \"conversion_rate\": conversion_rate,\n",
        "        \"click_through_rate\": click_through_rate,\n",
        "        \"cost\": total_cost,\n",
        "        \"revenue_proxy\": total_revenue_proxy,\n",
        "        \"sample_size\": total_impressions  # For statistical tests\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_lift_percentage(\n",
        "    control_value: float,\n",
        "    variant_value: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculate lift percentage: ((variant - control) / control) * 100\n",
        "\n",
        "    Args:\n",
        "        control_value: Control metric value\n",
        "        variant_value: Variant metric value\n",
        "\n",
        "    Returns:\n",
        "        Lift percentage (can be negative)\n",
        "    \"\"\"\n",
        "    if control_value == 0:\n",
        "        return 0.0 if variant_value == 0 else float('inf')\n",
        "    return ((variant_value - control_value) / control_value) * 100.0\n",
        "\n",
        "\n",
        "def determine_experiment_recommendation(\n",
        "    lift_percentage: float,\n",
        "    is_significant: bool,\n",
        "    p_value: Optional[float],\n",
        "    lift_threshold: float,\n",
        "    significance_threshold: float = 0.05\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Determine recommendation for an experiment.\n",
        "\n",
        "    Args:\n",
        "        lift_percentage: Lift percentage\n",
        "        is_significant: Whether result is statistically significant\n",
        "        p_value: P-value from statistical test\n",
        "        lift_threshold: Minimum lift to recommend scaling (e.g., 10%)\n",
        "        significance_threshold: P-value threshold (default 0.05)\n",
        "\n",
        "    Returns:\n",
        "        Recommendation: \"scale_variant\", \"continue\", or \"stop\"\n",
        "    \"\"\"\n",
        "    # If not significant and negative lift, stop\n",
        "    if not is_significant and lift_percentage < 0:\n",
        "        return \"stop\"\n",
        "\n",
        "    # If significant and lift exceeds threshold, scale\n",
        "    if is_significant and lift_percentage >= lift_threshold:\n",
        "        return \"scale_variant\"\n",
        "\n",
        "    # If significant but lift below threshold, continue\n",
        "    if is_significant:\n",
        "        return \"continue\"\n",
        "\n",
        "    # If not significant but positive, continue (need more data)\n",
        "    if lift_percentage > 0:\n",
        "        return \"continue\"\n",
        "\n",
        "    # Default: continue (need more data)\n",
        "    return \"continue\"\n",
        "\n",
        "\n",
        "def evaluate_experiment(\n",
        "    experiment: Dict[str, Any],\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    config_thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluate a single experiment.\n",
        "\n",
        "    Args:\n",
        "        experiment: Experiment dictionary\n",
        "        metrics: List of all performance metrics\n",
        "        config_thresholds: Configuration thresholds\n",
        "            {\n",
        "                \"statistical_significance_threshold\": 0.05,\n",
        "                \"lift_threshold_for_scaling\": 0.10\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        Experiment evaluation dictionary\n",
        "    \"\"\"\n",
        "    experiment_id = experiment[\"experiment_id\"]\n",
        "    campaign_id = experiment.get(\"campaign_id\", \"\")\n",
        "    control_asset_id = experiment.get(\"control_asset\", \"\")\n",
        "    variant_asset_id = experiment.get(\"variant_asset\", \"\")\n",
        "    metric_name = experiment.get(\"metric\", \"conversion_rate\")\n",
        "    status = experiment.get(\"status\", \"unknown\")\n",
        "\n",
        "    # Get metrics for control and variant\n",
        "    control_metrics = get_asset_metrics(control_asset_id, metrics)\n",
        "    variant_metrics = get_asset_metrics(variant_asset_id, metrics)\n",
        "\n",
        "    if not control_metrics or not variant_metrics:\n",
        "        return {\n",
        "            \"experiment_id\": experiment_id,\n",
        "            \"campaign_id\": campaign_id,\n",
        "            \"status\": status,\n",
        "            \"error\": \"Missing metrics for control or variant asset\",\n",
        "            \"control_performance\": control_metrics,\n",
        "            \"variant_performance\": variant_metrics\n",
        "        }\n",
        "\n",
        "    # Calculate lift\n",
        "    control_value = control_metrics.get(metric_name, 0.0)\n",
        "    variant_value = variant_metrics.get(metric_name, 0.0)\n",
        "    lift_percentage = calculate_lift_percentage(control_value, variant_value)\n",
        "\n",
        "    # Calculate statistical significance\n",
        "    # For conversion rates, use chi-square test\n",
        "    if metric_name in [\"conversion_rate\", \"demo_request_rate\", \"click_through_rate\", \"feature_click_through_rate\"]:\n",
        "        statistical_result = calculate_chi_square_test(\n",
        "            control_conversions=control_metrics.get(\"conversions\", 0),\n",
        "            control_total=control_metrics.get(\"impressions\", 0),\n",
        "            treatment_conversions=variant_metrics.get(\"conversions\", 0),\n",
        "            treatment_total=variant_metrics.get(\"impressions\", 0),\n",
        "            confidence_level=0.95\n",
        "        )\n",
        "    else:\n",
        "        # For continuous metrics, use calculate_statistical_significance\n",
        "        statistical_result = calculate_statistical_significance(\n",
        "            control_metrics=control_metrics,\n",
        "            treatment_metrics=variant_metrics,\n",
        "            primary_metric=metric_name,\n",
        "            confidence_level=0.95\n",
        "        )\n",
        "        if statistical_result is None:\n",
        "            statistical_result = {\n",
        "                \"test_type\": \"unknown\",\n",
        "                \"p_value\": None,\n",
        "                \"is_significant\": False,\n",
        "                \"error\": \"Insufficient data for statistical test\"\n",
        "            }\n",
        "\n",
        "    # Determine recommendation\n",
        "    p_value = statistical_result.get(\"p_value\")\n",
        "    is_significant = statistical_result.get(\"is_significant\", False)\n",
        "    lift_threshold = config_thresholds.get(\"lift_threshold_for_scaling\", 0.10)\n",
        "\n",
        "    recommendation = determine_experiment_recommendation(\n",
        "        lift_percentage,\n",
        "        is_significant,\n",
        "        p_value,\n",
        "        lift_threshold\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"experiment_id\": experiment_id,\n",
        "        \"campaign_id\": campaign_id,\n",
        "        \"status\": status,\n",
        "        \"metric\": metric_name,\n",
        "        \"control_asset\": control_asset_id,\n",
        "        \"variant_asset\": variant_asset_id,\n",
        "        \"control_performance\": control_metrics,\n",
        "        \"variant_performance\": variant_metrics,\n",
        "        \"lift_percentage\": round(lift_percentage, 2),\n",
        "        \"statistical_significance\": statistical_result,\n",
        "        \"recommendation\": recommendation\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_all_experiments(\n",
        "    experiments: List[Dict[str, Any]],\n",
        "    metrics: List[Dict[str, Any]],\n",
        "    config_thresholds: Dict[str, float]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Evaluate all experiments.\n",
        "\n",
        "    Args:\n",
        "        experiments: List of experiment dictionaries\n",
        "        metrics: List of all performance metrics\n",
        "        config_thresholds: Configuration thresholds\n",
        "\n",
        "    Returns:\n",
        "        List of experiment evaluation dictionaries\n",
        "    \"\"\"\n",
        "    evaluations = []\n",
        "    for experiment in experiments:\n",
        "        evaluation = evaluate_experiment(experiment, metrics, config_thresholds)\n",
        "        evaluations.append(evaluation)\n",
        "\n",
        "    return evaluations\n"
      ]
    }
  ]
}