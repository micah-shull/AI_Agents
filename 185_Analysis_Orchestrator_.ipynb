{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvDdKqWim7jND81pJKLxDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/185_Analysis_Orchestrator_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflecting on what stood out, then creating a SWOT for the agent.\n",
        "\n",
        "## What stands out\n",
        "\n",
        "### What stands out\n",
        "\n",
        "1. Modular design worked well\n",
        "   - Each analyzer is self-contained, easy to add frameworks, and follows a clear pattern that speeds iteration.\n",
        "\n",
        "2. LLM prompts matter (a lot)\n",
        "   - Initial SWOT produced only 2 of 4 categories.\n",
        "   - Fixes: persona, JSON formatting requirements, explicit “all 4 required,” and examples of each category.\n",
        "   - Lesson: be explicit and provide structure.\n",
        "\n",
        "3. Real-world debugging vs. theory\n",
        "   - JSON parsing had to handle code blocks and plain text.\n",
        "   - Category name mismatches (e.g., pluralization) broke formatting.\n",
        "   - Fallbacks were essential when analysis failed.\n",
        "\n",
        "4. Incremental testing validated each step\n",
        "   - SWOT → Porter → PESTEL → Ansoff → BCG → GE McKinsey.\n",
        "   - All six ran end-to-end.\n",
        "\n",
        "5. State management is straightforward with TypedDict\n",
        "   - Central schema keeps data flow simple and type-safe.\n",
        "\n",
        "Running a SWOT on the Business Analysis Agent.\n",
        "\n",
        "\n",
        "## Meta-SWOT takeaway\n",
        "\n",
        "Metacognitive exercise: analyzing the analysis tool. It highlights patterns that tend to emerge in system design.\n",
        "\n",
        "### What stands out most\n",
        "\n",
        "1. Modular beats monolithic\n",
        "   - Adding six frameworks felt like plug-and-play. Solid architecture reduces complexity.\n",
        "\n",
        "2. Explicit instructions improve LLM behavior\n",
        "   - Prompts need to say “all 4 required,” “min 2 per category,” and include examples.\n",
        "\n",
        "3. MVP constraints encouraged focus\n",
        "   - Limited data sources (Tavily only) forced improvements to prompts and retry logic.\n",
        "\n",
        "4. Build-incrementally testing exposed issues early\n",
        "   - Iterating across six frameworks surfaced most bugs quickly.\n",
        "\n",
        "5. Fallbacks are necessary\n",
        "   - LLMs can fail quietly; graceful degradation matters.\n",
        "\n",
        "### Most surprising insight\n",
        "\n",
        "The agent analyzes its own capabilities. The SWOT identifies:\n",
        "- Its own limitations\n",
        "- Where it may mislead\n",
        "- What it needs to improve\n",
        "\n",
        "This kind of self-assessment is a useful capability for AI systems.\n",
        "\n",
        "Takeaway: Start simple, then iterate. Six frameworks in one session is a product of that design."
      ],
      "metadata": {
        "id": "mvyevATa-NOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If starting over, I’d keep these core choices and change a few things:\n",
        "\n",
        "Keep:\n",
        "- Modular analyzers and step-by-step addition of frameworks\n",
        "- LangGraph for state and execution\n",
        "- Explicit JSON schemas and typed state\n",
        "\n",
        "Change:\n",
        "- Simplify from 8+ nodes to 5: goal → templates → data → analyze → report\n",
        "  - Goal and template selection can be combined; quality checks removed\n",
        "\n",
        "Reduce node count from 8+ to 4 core nodes.\n",
        "\n",
        "- `goal_and_template` (combine goal + template selection)\n",
        "- `collect_data`\n",
        "- `analyze` (with built-in quality check)\n",
        "- `generate_report`\n",
        "\n",
        "- Drop retry/escalation logic for MVP\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "Top two changes if starting over:\n",
        "\n",
        "### 1. Simplify the graph\n",
        "\n",
        "We built 8+ nodes with retry/escalation/routing logic we didn’t use. Reduce to 4 nodes:\n",
        "- setup → collect → analyze → report\n",
        "\n",
        "The “review” node mostly checked category counts—better as inline checks.\n",
        "\n",
        "### 2. Centralize prompts\n",
        "\n",
        "Each analyzer duplicated prompts. Use a base class with framework-specific overrides so shared instructions (e.g., “return only valid JSON”) live in one place.\n",
        "\n",
        "Keep as-is:\n",
        "- Modular analyzers\n",
        "- Incremental addition of frameworks\n",
        "- Type-safe state\n",
        "- LangGraph\n",
        "\n",
        "## Principle\n",
        "\n",
        "> Build the simplest thing that works, then add complexity only when needed.\n",
        "\n",
        "The retry logic and escalation paths were speculative complexity we didn’t need. The analyzers did the real work; we should have optimized them sooner."
      ],
      "metadata": {
        "id": "sFOFTFb9-8go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SWOT Analysis: Business Analysis Agent\n",
        "\n",
        "**Date:** October 28, 2025  \n",
        "**Context:** Evaluation of the AI Business Analysis Agent we built\n",
        "\n",
        "---\n",
        "\n",
        "## Strengths\n",
        "\n",
        "### 1. **Comprehensive Framework Coverage**\n",
        "- All 6 major strategic analysis frameworks implemented\n",
        "- Each framework properly templated with specific requirements\n",
        "- Different frameworks serve different analytical purposes\n",
        "- **Evidence:** Successfully tested SWOT, Porter's, PESTEL, Ansoff, BCG, and GE McKinsey\n",
        "\n",
        "### 2. **Modular, Extensible Architecture**\n",
        "- Clean separation of concerns: collectors, analyzers, nodes\n",
        "- Each analyzer is self-contained and follows consistent patterns\n",
        "- Easy to add new frameworks without touching existing code\n",
        "- **Evidence:** Added 5 new frameworks without breaking previous ones\n",
        "\n",
        "### 3. **Robust Prompt Engineering**\n",
        "- Detailed LLM personas for each framework\n",
        "- Explicit JSON output requirements\n",
        "- Fallback logic when analysis fails\n",
        "- **Evidence:** After fixing prompt issues, all 4 SWOT categories appeared consistently\n",
        "\n",
        "### 4. **Quality Assurance Built-In**\n",
        "- Review node validates completeness for each framework\n",
        "- Confidence scoring system (0.0-1.0)\n",
        "- Retry logic with iteration limits\n",
        "- **Evidence:** Systems caught missing categories and flagged low-confidence insights\n",
        "\n",
        "### 5. **Human-Readable Output**\n",
        "- Executive summaries tailored to each framework\n",
        "- Framework-specific recommendations\n",
        "- Professional markdown formatting\n",
        "- **Evidence:** Reports are immediately usable by business professionals\n",
        "\n",
        "### 6. **Data Collection Integration**\n",
        "- Tavily search for external data\n",
        "- Multiple targeted searches for comprehensive coverage\n",
        "- Data summarization for LLM context\n",
        "- **Evidence:** 15-21 data points collected per analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Weaknesses\n",
        "\n",
        "### 1. **Limited Data Sources (MVP Stage)**\n",
        "- Currently only using Tavily for data collection\n",
        "- No integration with financial APIs (Alpha Vantage planned but not implemented)\n",
        "- No access to proprietary databases or industry reports\n",
        "- **Impact:** Confidence scores may be lower due to limited data depth\n",
        "\n",
        "### 2. **No Caching System**\n",
        "- Every analysis triggers new API calls\n",
        "- No learning from previous analyses\n",
        "- Repeated analyses of same company incur same costs\n",
        "- **Impact:** Higher API costs and slower performance for repeated queries\n",
        "\n",
        "### 3. **Single-Template Execution**\n",
        "- Agent runs one framework at a time\n",
        "- No multi-framework analysis or framework chaining\n",
        "- Can't combine insights across frameworks automatically\n",
        "- **Impact:** Users must manually correlate findings from different frameworks\n",
        "\n",
        "### 4. **Basic Quality Scoring**\n",
        "- Confidence scores come from LLM self-assessment\n",
        "- No objective data quality metrics\n",
        "- No source credibility scoring beyond simple checks\n",
        "- **Impact:** May not catch subtle data quality issues\n",
        "\n",
        "### 5. **Limited Goal-Framework Variety**\n",
        "- Some goals use the same framework (3 use SWOT, 3 use PESTEL)\n",
        "- Not all possible goal-framework combinations exposed\n",
        "- **Impact:** Less flexibility for users who want specific analysis types\n",
        "\n",
        "### 6. **No Industry-Specific Specialization**\n",
        "- Generic templates work across industries but lack deep vertical expertise\n",
        "- No industry-specific KPIs or benchmarks\n",
        "- No regulatory or compliance checking\n",
        "- **Impact:** Analyses may miss industry-specific nuances\n",
        "\n",
        "---\n",
        "\n",
        "## Opportunities\n",
        "\n",
        "### 1. **Additional Data Source Integration**\n",
        "- Integrate Alpha Vantage for financial data\n",
        "- Add Wikipedia for company background\n",
        "- Add Bing News for recent developments\n",
        "- Add industry-specific data providers\n",
        "- **Impact:** Higher confidence scores, more actionable insights\n",
        "\n",
        "### 2. **Advanced Caching and Memory**\n",
        "- Cache API responses to reduce costs\n",
        "- Learn from previous analyses to improve prompts\n",
        "- Build a knowledge base of company facts\n",
        "- **Impact:** Faster, cheaper, smarter analyses\n",
        "\n",
        "### 3. **Multi-Framework Analysis**\n",
        "- Allow chaining multiple frameworks in sequence\n",
        "- Enable comparative analysis across frameworks\n",
        "- Auto-select complementary frameworks\n",
        "- **Impact:** More comprehensive strategic insights\n",
        "\n",
        "### 4. **Visual Output Generation**\n",
        "- Generate actual BCG Matrix charts\n",
        "- Create Porter's Five Forces diagrams\n",
        "- Build interactive dashboards\n",
        "- **Impact:** Enhanced visual communication of insights\n",
        "\n",
        "### 5. **Iterative Refinement Loop**\n",
        "- Incorporate human feedback to improve analysis\n",
        "- Ask clarifying questions when data is ambiguous\n",
        "- Refine insights based on additional information\n",
        "- **Impact:** More personalized and accurate analyses\n",
        "\n",
        "### 6. **Industry-Specific Templates**\n",
        "- Healthcare compliance checks\n",
        "- Tech industry innovation metrics\n",
        "- Retail market positioning analysis\n",
        "- **Impact:** Domain-specific expertise and higher value\n",
        "\n",
        "### 7. **Export and Integration**\n",
        "- Export to PowerPoint presentations\n",
        "- Integrate with BI tools (Tableau, Power BI)\n",
        "- API for programmatic access\n",
        "- **Impact:** Easier integration into existing workflows\n",
        "\n",
        "### 8. **Explainability Features**\n",
        "- Show source citations for each insight\n",
        "- Trace evidence chains\n",
        "- Explain reasoning behind recommendations\n",
        "- **Impact:** Increased trust and verification capability\n",
        "\n",
        "---\n",
        "\n",
        "## Threats\n",
        "\n",
        "### 1. **LLM Hallucination and Accuracy**\n",
        "- LLMs may generate plausible but incorrect information\n",
        "- Confidence scores may not reflect actual accuracy\n",
        "- No human verification step built in\n",
        "- **Impact:** Risk of incorrect strategic recommendations\n",
        "\n",
        "### 2. **API Costs and Rate Limits**\n",
        "- Tavily and OpenAI costs scale with usage\n",
        "- Rate limits may affect performance under load\n",
        "- No cost optimization beyond basic requests\n",
        "- **Impact:** Operational cost concerns and potential service interruptions\n",
        "\n",
        "### 3. **Data Recency Issues**\n",
        "- Tavily search may not return most recent information\n",
        "- No timestamp validation on collected data\n",
        "- Stale information could lead to outdated insights\n",
        "- **Impact:** Recommendations based on outdated market conditions\n",
        "\n",
        "### 4. **Limited Validation of LLM Outputs**\n",
        "- No fact-checking against known databases\n",
        "- No cross-referencing multiple LLM responses\n",
        "- No verification of statistical claims\n",
        "- **Impact:** Potential for false or misleading insights\n",
        "\n",
        "### 5. **Competition from Established Tools**\n",
        "- Consulting firms have proprietary frameworks\n",
        "- Existing tools (e.g., Pitchbook, CB Insights) have deep data\n",
        "- Must differentiate on speed, cost, or ease of use\n",
        "- **Impact:** Market positioning challenges\n",
        "\n",
        "### 6. **Regulatory and Compliance Risks**\n",
        "- Financial analysis may require certifications\n",
        "- Data privacy concerns with external API usage\n",
        "- Liability for incorrect business recommendations\n",
        "- **Impact:** Legal and regulatory constraints\n",
        "\n",
        "### 7. **User Expectations vs. Reality**\n",
        "- Users may expect consulting-level insights\n",
        "- May not understand limitations of automated analysis\n",
        "- Over-reliance on AI without human oversight\n",
        "- **Impact:** Potential misuse or disappointed users\n",
        "\n",
        "### 8. **Technology Obsolescence**\n",
        "- LLM capabilities evolving rapidly\n",
        "- Need to stay current with latest models\n",
        "- Framework may become outdated\n",
        "- **Impact:** Continuous maintenance required\n",
        "\n",
        "---\n",
        "\n",
        "## Key Recommendations\n",
        "\n",
        "### Immediate Actions (Next Sprint)\n",
        "1. **Add Wikipedia integration** for basic company facts\n",
        "2. **Implement basic caching** for repeated company analyses\n",
        "3. **Add source citations** to improve explainability\n",
        "4. **Expand goal-framework mapping** for more variety\n",
        "\n",
        "### Medium-Term (Next Quarter)\n",
        "1. **Integrate Alpha Vantage** for financial data\n",
        "2. **Build multi-framework analysis** capability\n",
        "3. **Create visual output generators** (charts, diagrams)\n",
        "4. **Add iterative refinement** with human feedback\n",
        "\n",
        "### Long-Term (Roadmap)\n",
        "1. **Industry-specific specializations** for key verticals\n",
        "2. **Advanced validation systems** for fact-checking\n",
        "3. **Cost optimization** through smart caching and model selection\n",
        "4. **Enterprise features** (API, integrations, compliance)\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The Business Analysis Agent is a **strong MVP** with excellent foundational architecture. Its modular design enables rapid expansion, while built-in quality checks ensure reliability. The main gaps are in data richness and advanced features. With the recommended improvements, this could become a competitive business intelligence tool.\n",
        "\n",
        "**Overall Assessment:** Ready for beta testing with clear path to enhanced capabilities.\n",
        "\n",
        "**Confidence Score:** 0.85 (High confidence in current implementation, moderate confidence in ability to scale to enterprise needs without additional resources)\n",
        "\n"
      ],
      "metadata": {
        "id": "m64YGkAsAYOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If I Had to Start Over: Lessons Learned\n",
        "\n",
        "## Approach Assessment\n",
        "\n",
        "### ✅ What I Would Keep\n",
        "\n",
        "1. **Modular Analyzer Architecture** - This was perfect. Each analyzer as a self-contained class made adding frameworks trivial.\n",
        "\n",
        "2. **Explicit Template Selection** - Goal-to-framework mapping worked great and is easily configurable.\n",
        "\n",
        "3. **Type-Safe State Management** - TypedDict was the right choice for clarity and type checking.\n",
        "\n",
        "4. **Incremental Development** - Building one framework at a time and testing each was smart.\n",
        "\n",
        "5. **LangGraph** - Overall good choice for structured workflows, though could be simpler.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 What I Would Do Differently\n",
        "\n",
        "#### 1. **Simplify the Graph Structure** ⭐ MOST IMPORTANT\n",
        "\n",
        "**Current:** 8 nodes with conditional routing, retry logic, escalation paths\n",
        "```python\n",
        "# Too complex for MVP\n",
        "set_goal → select_templates → collect_data → analyze → review →\n",
        "  (conditional routing) → retry_increment/escalate/generate_report\n",
        "```\n",
        "\n",
        "**Better:** 4 simple nodes, linear flow\n",
        "```python\n",
        "# Simpler and cleaner\n",
        "combine_goal_and_template → collect_data → analyze → generate_report\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- We never actually used the retry logic in practice\n",
        "- Escalation to human isn't needed for MVP\n",
        "- More nodes = more places for bugs\n",
        "- The \"review\" node just checked if we had X categories - could be inline\n",
        "- KISS principle - we added complexity we didn't use\n",
        "\n",
        "**Evidence:** The successful runs didn't use any of the \"advanced\" routing. Analysis either worked on first try or we fixed the code.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Centralize Prompt Templates** ⭐ SECOND MOST IMPORTANT\n",
        "\n",
        "**Current:** Each analyzer has its own prompt with 90% duplicate code\n",
        "```python\n",
        "# Lot of repetition across 6 analyzers\n",
        "self.swot_prompt = ChatPromptTemplate.from_messages([...])\n",
        "self.porter_prompt = ChatPromptTemplate.from_messages([...])\n",
        "# etc. - lots of duplicated structure\n",
        "```\n",
        "\n",
        "**Better:** Base prompt class with framework-specific overrides\n",
        "```python\n",
        "class BaseAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(...)\n",
        "        self.base_prompt = self._create_base_prompt()\n",
        "        \n",
        "    def _create_base_prompt(self):\n",
        "        # Common structure for all frameworks\n",
        "        return ChatPromptTemplate.from_messages([\n",
        "            (\"system\", self._get_persona()),\n",
        "            (\"user\", self._get_prompt_template())\n",
        "        ])\n",
        "        \n",
        "    def _get_persona(self):\n",
        "        return \"You are an expert business analyst...\"\n",
        "        \n",
        "    def _get_prompt_template(self):\n",
        "        return \"\"\"Company: {company}...\"\"\"\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Less code duplication\n",
        "- Easier to update common elements (e.g., JSON format requirements)\n",
        "- Change persona once, not in 6 places\n",
        "- Framework-specific analyzers just override what's different\n",
        "\n",
        "**Evidence:** I had to add \"CRITICAL: Return ONLY valid JSON\" to every single prompt. Should be in one place.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Use Environment Variables from the Start**\n",
        "\n",
        "**Current:** Had to update `API_KEYS.env` and load it manually\n",
        "\n",
        "**Better:** Use `python-dotenv` from the start\n",
        "```python\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Then just use os.getenv() everywhere\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Industry standard\n",
        "- Easier for deployment\n",
        "- Less code in config loading\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Start with More Data Sources** (Controversial)\n",
        "\n",
        "**Current:** Tavily only, others \"for later\"\n",
        "\n",
        "**Better:** Include at least Wikipedia from day 1\n",
        "\n",
        "**Why:**\n",
        "- Wikipedia gives free, reliable company basics\n",
        "- Helps when Tavily returns irrelevant results\n",
        "- Very simple integration\n",
        "- Provides fallback data\n",
        "\n",
        "**Evidence:** Some analyses had lower confidence scores due to data limitations.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Pre-define Data Collection Queries**\n",
        "\n",
        "**Current:** Generic searches like `\"{company} {region} {goal}\"`\n",
        "\n",
        "**Better:** Framework-specific search strategies\n",
        "```python\n",
        "SEARCH_STRATEGIES = {\n",
        "    \"swot\": [\n",
        "        f\"{company} strengths competitive advantages\",\n",
        "        f\"{company} weaknesses challenges problems\",\n",
        "        f\"{company} opportunities market growth\",\n",
        "        f\"{company} threats risks competition\"\n",
        "    ],\n",
        "    \"porter_five_forces\": [\n",
        "        f\"{company} industry competition analysis\",\n",
        "        f\"{company} barriers to entry market\",\n",
        "        # etc.\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- More targeted data collection\n",
        "- Higher quality sources\n",
        "- Less noise to filter through\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Validation as Utilities, Not a Node**\n",
        "\n",
        "**Current:** Separate \"review\" node that checks completeness\n",
        "\n",
        "**Better:** Inline validation in the analyze node\n",
        "```python\n",
        "def analyze_and_synthesize(state):\n",
        "    insights = analyzer.analyze(state)\n",
        "    \n",
        "    # Inline validation\n",
        "    if current_template == \"swot\":\n",
        "        required_categories = {\"Strength\", \"Weakness\", \"Opportunity\", \"Threat\"}\n",
        "        found = set([i.get(\"category\") for i in insights])\n",
        "        if not required_categories.issubset(found):\n",
        "            print(f\"⚠️ Missing categories: {required_categories - found}\")\n",
        "            # Fix it here, don't route to another node\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Linear flow is easier to debug\n",
        "- One less node to maintain\n",
        "- Faster execution (no extra function call overhead)\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Template-Based Report Generation**\n",
        "\n",
        "**Current:** Massive if/elif chains in `report_node.py` handling all 6 frameworks\n",
        "\n",
        "**Better:** Use Jinja2 templates\n",
        "```python\n",
        "# templates/report.md.j2\n",
        "{{framework_name}} Analysis Report\n",
        "\n",
        "{% for category in categories %}\n",
        "### {{category}}\n",
        "{% for insight in insights_by_category[category] %}\n",
        "{{insight|format_insight(framework=current_template)}}\n",
        "{% endfor %}\n",
        "{% endfor %}\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Cleaner code\n",
        "- Easier to modify output format\n",
        "- Reusable across frameworks\n",
        "- Separates logic from presentation\n",
        "\n",
        "---\n",
        "\n",
        "### 🤔 What I'm Unsure About\n",
        "\n",
        "#### Single-Analyzer vs. Multi-Analyzer Classes\n",
        "\n",
        "**Option A (Current):** One class per framework\n",
        "- Pros: Clear separation, easy to reason about\n",
        "- Cons: 6 files with similar code\n",
        "\n",
        "**Option B (Alternative):** Single analyzer with framework parameter\n",
        "```python\n",
        "class MultiFrameworkAnalyzer:\n",
        "    def analyze(self, framework: str, state: BusinessAnalysisState):\n",
        "        prompt = self._get_prompt(framework)\n",
        "        schema = self._get_schema(framework)\n",
        "        # etc.\n",
        "```\n",
        "- Pros: Less code duplication\n",
        "- Cons: More complex class, harder to customize per framework\n",
        "\n",
        "**Verdict:** I think keeping them separate is probably better for long-term maintainability.\n",
        "\n",
        "---\n",
        "\n",
        "## The Ideal Simplified Architecture\n",
        "\n",
        "```python\n",
        "# agents/simple_agent.py\n",
        "def create_simple_agent():\n",
        "    workflow = StateGraph(BusinessAnalysisState)\n",
        "    \n",
        "    # Just 4 nodes\n",
        "    workflow.add_node(\"setup\", combine_goal_and_template)\n",
        "    workflow.add_node(\"collect\", collect_analysis_data)\n",
        "    workflow.add_node(\"analyze\", analyze_with_inline_validation)\n",
        "    workflow.add_node(\"report\", generate_final_report)\n",
        "    \n",
        "    # Linear flow\n",
        "    workflow.add_edge(\"setup\", \"collect\")\n",
        "    workflow.add_edge(\"collect\", \"analyze\")\n",
        "    workflow.add_edge(\"analyze\", \"report\")\n",
        "    workflow.add_edge(\"report\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "```\n",
        "\n",
        "## Key Insight\n",
        "\n",
        "**The MVP didn't need retry logic, escalation paths, or complex routing.** We built for problems we didn't have yet. The successful implementation was much simpler than the initial design suggested we needed.\n",
        "\n",
        "The principle: **Build the simplest thing that works, then add complexity only when you need it.**\n",
        "\n",
        "---\n",
        "\n",
        "## Retrospective Rating\n",
        "\n",
        "| Aspect | Current Approach | Ideal Approach | Priority |\n",
        "|--------|-----------------|----------------|----------|\n",
        "| Modular analyzers | ✅ Excellent | Same | ⭐ Keep |\n",
        "| Graph complexity | ⚠️ Over-engineered | ✅ Simpler | 🔴 High |\n",
        "| Prompt management | ⚠️ Duplicated | ✅ Centralized | 🟡 Medium |\n",
        "| Data sources | ⚠️ Too few | ✅ More from start | 🟡 Medium |\n",
        "| Report generation | ⚠️ Hard-coded | ✅ Templated | 🟢 Low |\n",
        "| Validation approach | ⚠️ Separate node | ✅ Inline | 🟡 Medium |\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The biggest change I would make: **Simplify the graph from 8+ nodes to 4 core nodes** and **remove all the retry/escalation logic** that never got used. The modular analyzer architecture was spot-on and enabled rapid iteration.\n",
        "\n",
        "The second biggest: **Centralize prompt templates** to reduce duplication and make it easier to update common requirements (like JSON formatting).\n",
        "\n",
        "Everything else was pretty solid for a first iteration!\n",
        "\n"
      ],
      "metadata": {
        "id": "FKs7YDCaAcsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centralized Prompt Pattern - Concrete Example\n",
        "\n",
        "## Problem We Have Now\n",
        "\n",
        "**Every analyzer repeats the same structure:**\n",
        "\n",
        "```python\n",
        "# swot_analyzer.py\n",
        "self.swot_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert strategic business analyst with 15+ years of experience...\n",
        "     [300 lines of persona and instructions]\n",
        "     \n",
        "     CRITICAL: Return ONLY valid JSON. No markdown...\n",
        "     Format your response as JSON with this EXACT structure...\n",
        "     \"\"\"),\n",
        "    (\"user\", \"\"\"Company: {company}\n",
        "Goal: {goal}\n",
        "...\n",
        "\"\"\")\n",
        "])\n",
        "\n",
        "# porter_analyzer.py  \n",
        "self.porter_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are an expert competitive intelligence analyst with 15+ years of experience...\n",
        "     [300 lines of almost identical persona and instructions]\n",
        "     \n",
        "     CRITICAL: Return ONLY valid JSON. No markdown...\n",
        "     Format your response as JSON with this EXACT structure...\n",
        "     \"\"\"),\n",
        "    (\"user\", \"\"\"Company: {company}\n",
        "Goal: {goal}\n",
        "...\n",
        "\"\"\")\n",
        "])\n",
        "\n",
        "# Repeat 6 times with minor variations...\n",
        "```\n",
        "\n",
        "**Problems:**\n",
        "- Want to change \"Return ONLY valid JSON\"? Update 6 files\n",
        "- Want to update output structure? Update 6 places\n",
        "- Waste of tokens (repeating same instructions)\n",
        "- Easy to miss an update in one analyzer\n",
        "\n",
        "---\n",
        "\n",
        "## Solution: Centralized Prompt Base\n",
        "\n",
        "### 1. Create a Base Prompt Class\n",
        "\n",
        "```python\n",
        "# agents/prompts/base_prompt.py\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from typing import Dict, Any\n",
        "\n",
        "class BaseAnalyzerPrompt:\n",
        "    \"\"\"\n",
        "    Base class for all analyzer prompts.\n",
        "    Provides common structure and instructions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Shared persona components\n",
        "    BASE_PERSONA = \"\"\"You are an expert business analyst with 15+ years of experience in strategic analysis, market research, and corporate strategy. You specialize in conducting thorough business analyses for investors, executives, and strategic decision-makers.\n",
        "\n",
        "Your expertise includes:\n",
        "- Analyzing company performance and market positioning\n",
        "- Identifying competitive advantages and vulnerabilities  \n",
        "- Assessing market opportunities and industry threats\n",
        "- Evaluating strategic options with evidence-based insights\"\"\"\n",
        "\n",
        "    # Common output requirements (used by ALL frameworks)\n",
        "    OUTPUT_REQUIREMENTS = \"\"\"\n",
        "    \n",
        "CRITICAL: Return ONLY valid JSON. No markdown, no code blocks, no explanations. Just raw JSON.\n",
        "\n",
        "Guidelines:\n",
        "- Be specific and actionable in your insights\n",
        "- Use concrete examples from the data\n",
        "- Prioritize strategic importance over quantity\n",
        "- If data is limited, acknowledge it with lower confidence scores\n",
        "- Focus on insights that matter for decision-making\"\"\"\n",
        "\n",
        "    # Common user template\n",
        "    USER_TEMPLATE = \"\"\"Company: {company}\n",
        "Goal: {goal}\n",
        "Region: {region}\n",
        "Competitors: {competitors}\n",
        "\n",
        "Company Data:\n",
        "{data}\n",
        "\n",
        "Generate the {framework_name} analysis:\"\"\"\n",
        "\n",
        "    def __init__(self, framework_name: str, framework_specific_instructions: str):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            framework_name: e.g., \"SWOT\", \"Porter's Five Forces\"\n",
        "            framework_specific_instructions: Instructions unique to this framework\n",
        "        \"\"\"\n",
        "        self.framework_name = framework_name\n",
        "        self.framework_specific = framework_specific_instructions\n",
        "        \n",
        "        # Build the prompt\n",
        "        self.prompt = self._build_prompt()\n",
        "    \n",
        "    def _build_prompt(self) -> ChatPromptTemplate:\n",
        "        \"\"\"Construct the complete prompt with all components\"\"\"\n",
        "        \n",
        "        # Combine all system message components\n",
        "        system_message = (\n",
        "            self.BASE_PERSONA + \"\\n\\n\" +\n",
        "            f\"You are conducting a {self.framework_name} analysis.\\n\\n\" +\n",
        "            self.framework_specific + \"\\n\\n\" +\n",
        "            self.OUTPUT_REQUIREMENTS\n",
        "        )\n",
        "        \n",
        "        return ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_message),\n",
        "            (\"user\", self.USER_TEMPLATE)\n",
        "        ])\n",
        "    \n",
        "    def format(self, **kwargs) -> Any:\n",
        "        \"\"\"Format the prompt with data\"\"\"\n",
        "        return self.prompt.format(\n",
        "            company=kwargs.get(\"company\"),\n",
        "            goal=kwargs.get(\"goal\"),\n",
        "            region=kwargs.get(\"region\", \"Global\"),\n",
        "            competitors=kwargs.get(\"competitors\", \"Not specified\"),\n",
        "            data=kwargs.get(\"data\"),\n",
        "            framework_name=self.framework_name\n",
        "        )\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Framework-Specific Prompts (Much Shorter!)\n",
        "\n",
        "```python\n",
        "# agents/prompts/swot_prompt.py\n",
        "from agents.prompts.base_prompt import BaseAnalyzerPrompt\n",
        "\n",
        "class SWOTPrompt(BaseAnalyzerPrompt):\n",
        "    \"\"\"\n",
        "    SWOT-specific prompt. Only defines what's different.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Only the SWOT-specific instructions!\n",
        "        swot_instructions = \"\"\"Analyze the provided company information and generate a comprehensive SWOT analysis.\n",
        "\n",
        "REQUIRED: You MUST provide insights for ALL FOUR categories (minimum 2 items per category):\n",
        "- Strengths: 2-4 key internal advantages and capabilities\n",
        "- Weaknesses: 2-4 key internal limitations and vulnerabilities\n",
        "- Opportunities: 2-4 key external trends, markets, or situations that could benefit the company\n",
        "- Threats: 2-4 key external risks, competitors, or challenges that could harm the company\n",
        "\n",
        "For each item, provide:\n",
        "1. A clear, concise insight\n",
        "2. Supporting evidence from the data\n",
        "3. A confidence score (0.0-1.0)\n",
        "4. An impact level (Very Low to Very High)\n",
        "\n",
        "Format your response as JSON with this structure:\n",
        "{{\n",
        "    \"strengths\": [\n",
        "        {{\"insight\": \"...\", \"evidence\": \"...\", \"confidence\": 0.8, \"impact\": \"High\"}}\n",
        "    ],\n",
        "    \"weaknesses\": [\n",
        "        {{\"insight\": \"...\", \"evidence\": \"...\", \"confidence\": 0.6, \"impact\": \"Moderate\"}}\n",
        "    ],\n",
        "    \"opportunities\": [\n",
        "        {{\"insight\": \"...\", \"evidence\": \"...\", \"confidence\": 0.7, \"impact\": \"High\"}}\n",
        "    ],\n",
        "    \"threats\": [\n",
        "        {{\"insight\": \"...\", \"evidence\": \"...\", \"confidence\": 0.7, \"impact\": \"High\"}}\n",
        "    ]\n",
        "}}\"\"\"\n",
        "        \n",
        "        super().__init__(\n",
        "            framework_name=\"SWOT\",\n",
        "            framework_specific_instructions=swot_instructions\n",
        "        )\n",
        "\n",
        "\n",
        "# agents/prompts/porter_prompt.py\n",
        "from agents.prompts.base_prompt import BaseAnalyzerPrompt\n",
        "\n",
        "class PorterPrompt(BaseAnalyzerPrompt):\n",
        "    \"\"\"Porter's Five Forces prompt. Only defines what's different.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        porter_instructions = \"\"\"Analyze the provided company and industry information and generate a comprehensive Porter's Five Forces analysis.\n",
        "\n",
        "REQUIRED: You MUST provide insights for ALL FIVE forces:\n",
        "- Competitive Rivalry\n",
        "- Threat of New Entrants\n",
        "- Threat of Substitutes\n",
        "- Bargaining Power of Suppliers\n",
        "- Bargaining Power of Buyers\n",
        "\n",
        "For each force, provide:\n",
        "1. A clear, concise insight\n",
        "2. A rating (Low, Moderate, High)\n",
        "3. Supporting evidence\n",
        "4. Confidence score (0.0-1.0)\n",
        "5. Impact level\n",
        "\n",
        "Format your response as JSON with this structure:\n",
        "{{\n",
        "    \"competitive_rivalry\": [\n",
        "        {{\"insight\": \"...\", \"rating\": \"High\", \"evidence\": \"...\", \"confidence\": 0.8, \"impact\": \"High\"}}\n",
        "    ],\n",
        "    ... [other forces]\n",
        "}}\"\"\"\n",
        "        \n",
        "        super().__init__(\n",
        "            framework_name=\"Porter's Five Forces\",\n",
        "            framework_specific_instructions=porter_instructions\n",
        "        )\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Updated Analyzers (Much Cleaner!)\n",
        "\n",
        "```python\n",
        "# agents/analyzers/swot_analyzer.py\n",
        "from agents.prompts.swot_prompt import SWOTPrompt\n",
        "\n",
        "class SWOTAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(...)\n",
        "        \n",
        "        # Just instantiate the prompt - that's it!\n",
        "        self.prompt = SWOTPrompt()\n",
        "    \n",
        "    def analyze(self, state: BusinessAnalysisState) -> List[Dict]:\n",
        "        # The prompt class handles all the formatting\n",
        "        messages = self.prompt.format(\n",
        "            company=state[\"company_name\"],\n",
        "            goal=state[\"goal_type\"].value,\n",
        "            region=state.get(\"geographic_region\", \"Global\"),\n",
        "            competitors=\", \".join(state.get(\"competitors\", []) or []),\n",
        "            data=self._summarize_data(state[\"raw_data\"])\n",
        "        )\n",
        "        \n",
        "        response = self.llm.invoke(messages)\n",
        "        # ... parse and return\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Benefits\n",
        "\n",
        "### 1. **Single Source of Truth for Common Elements**\n",
        "\n",
        "```python\n",
        "# Want to change JSON requirement? One place:\n",
        "class BaseAnalyzerPrompt:\n",
        "    OUTPUT_REQUIREMENTS = \"\"\"\n",
        "    \n",
        "CRITICAL: Return ONLY valid JSON. No markdown, no code blocks, no explanations.\n",
        "Enhancement: Format numbers as decimals (0.75, not 0.8) for consistency.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Update once, all 6 frameworks benefit instantly!\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Consistent Persona Across All Frameworks**\n",
        "\n",
        "```python\n",
        "# Want to change expertise level? One place:\n",
        "class BaseAnalyzerPrompt:\n",
        "    BASE_PERSONA = \"\"\"You are an expert business analyst with 20+ years of experience...\"\"\"\n",
        "    # All analyzers get the update\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Easy to Add New Frameworks**\n",
        "\n",
        "```python\n",
        "# Adding a new framework is now just 20 lines instead of 100+\n",
        "class NewFrameworkPrompt(BaseAnalyzerPrompt):\n",
        "    def __init__(self):\n",
        "        instructions = \"\"\"[Just the framework-specific requirements]\"\"\"\n",
        "        super().__init__(\"New Framework\", instructions)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Reusable Components for Common Patterns**\n",
        "\n",
        "```python\n",
        "# agents/prompts/base_prompt.py - Add reusable helpers\n",
        "\n",
        "class BaseAnalyzerPrompt:\n",
        "    @staticmethod\n",
        "    def confidence_guidance() -> str:\n",
        "        \"\"\"Reusable confidence score guidance\"\"\"\n",
        "        return \"\"\"\n",
        "Confidence scores (0.0-1.0):\n",
        "- 0.7-1.0: High confidence (strong evidence, multiple sources)\n",
        "- 0.4-0.7: Moderate confidence (adequate evidence)\n",
        "- 0.0-0.4: Low confidence (limited evidence, needs more data)\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def impact_guidance() -> str:\n",
        "        \"\"\"Reusable impact level guidance\"\"\"\n",
        "        return \"\"\"\n",
        "Impact levels:\n",
        "- Very High: Critical to success/failure\n",
        "- High: Significantly affects competitive position\n",
        "- Moderate: Noticeable impact\n",
        "- Low: Limited strategic relevance\n",
        "- Very Low: Minimal impact\"\"\"\n",
        "```\n",
        "\n",
        "Now any framework can use these:\n",
        "\n",
        "```python\n",
        "def __init__(self):\n",
        "    instructions = f\"\"\"\n",
        "    Analyze and provide:\n",
        "    1. Insight with evidence\n",
        "    2. Confidence score: {self.confidence_guidance()}\n",
        "    3. Impact level: {self.impact_guidance()}\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced Pattern: Prompt Registry\n",
        "\n",
        "For even more flexibility, create a registry:\n",
        "\n",
        "```python\n",
        "# agents/prompts/__init__.py\n",
        "from agents.prompts.swot_prompt import SWOTPrompt\n",
        "from agents.prompts.porter_prompt import PorterPrompt\n",
        "\n",
        "PROMPT_REGISTRY = {\n",
        "    \"swot\": SWOTPrompt,\n",
        "    \"porter_five_forces\": PorterPrompt,\n",
        "    # ... etc\n",
        "}\n",
        "\n",
        "def get_prompt(framework_id: str) -> BaseAnalyzerPrompt:\n",
        "    \"\"\"Factory function to get prompt by ID\"\"\"\n",
        "    prompt_class = PROMPT_REGISTRY.get(framework_id)\n",
        "    if prompt_class:\n",
        "        return prompt_class()\n",
        "    raise ValueError(f\"No prompt found for framework: {framework_id}\")\n",
        "\n",
        "# Now analyzers don't even need to know about prompts:\n",
        "class SWOTAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(...)\n",
        "        self.prompt = get_prompt(\"swot\")  # Clean!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Migration Path\n",
        "\n",
        "### Phase 1: Create Base Class (No Changes to Existing Analyzers)\n",
        "```python\n",
        "# Add base_prompt.py\n",
        "# Existing analyzers still work independently\n",
        "```\n",
        "\n",
        "### Phase 2: Migrate One Framework\n",
        "```python\n",
        "# Migrate SWOT first\n",
        "# Test thoroughly\n",
        "# All other frameworks unchanged\n",
        "```\n",
        "\n",
        "### Phase 3: Migrate Remaining Frameworks\n",
        "```python\n",
        "# One by one\n",
        "# Easy to rollback if issues\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Answer to Your Question\n",
        "\n",
        "> \"Should we be reusing the persona pattern - save it as the pattern then reference it in the prompt?\"\n",
        "\n",
        "**Exactly!** The persona becomes:\n",
        "1. A **class constant** in the base prompt class\n",
        "2. Automatically included in all framework-specific prompts\n",
        "3. Referenced via inheritance, not copied\n",
        "\n",
        "The pattern is:\n",
        "```\n",
        "BaseAnalyzerPrompt (has persona)\n",
        "    ↓ extends\n",
        "SWOTPrompt (has framework-specific instructions)\n",
        "    ↓ used by\n",
        "SWOTAnalyzer (just calls it)\n",
        "```\n",
        "\n",
        "This is the **DRY (Don't Repeat Yourself)** principle applied to LLM prompts.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "| Current Approach | Centralized Approach | Benefit |\n",
        "|-----------------|---------------------|---------|\n",
        "| 300 lines per analyzer | 20 lines per analyzer | 15x less code |\n",
        "| Update 6 files | Update 1 file | Maintainability |\n",
        "| Inconsistent formatting | Consistent across all | Quality |\n",
        "| Hard to add frameworks | Easy to add | Scalability |\n",
        "\n",
        "**Bottom line:** Yes, save the persona (and all common elements) as a reusable pattern that gets referenced, not copied. This is the difference between maintainable and technical debt.\n",
        "\n"
      ],
      "metadata": {
        "id": "0sSn5_TiAMJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Five insights for the next build\n",
        "\n",
        "### 1. **Structure testing early**\n",
        "Add `pytest` from day one. Manually running the agent slows iteration and misses edge cases. Unit tests for each analyzer save hours later.\n",
        "\n",
        "### 2. **Logging and observability**\n",
        "Make logs searchable and structured; add context and IDs. A short deployment log helps surface issues quickly.\n",
        "\n",
        "### 3. **Mock data**\n",
        "Use in-memory fixtures during development. Avoid API costs while coding, test offline, and simulate failures.\n",
        "\n",
        "### 4. **Output validation with Pydantic**\n",
        "Enforce schemas. Fail fast on malformed responses with clear errors.\n",
        "\n",
        "### 5. **Progressive data sources**\n",
        "Enable fallbacks and caching. If one source is down, others continue.\n",
        "\n",
        "### Bonus\n",
        "Cost tracking prevents surprises and config management avoids hardcoded values.\n",
        "\n",
        "## Principle\n",
        "**Build for observability from the start.** You can’t fix what you can’t see. Logging, tests, and validation make debugging faster.\n",
        "\n",
        "These tools compound over time—early effort pays off as the agent grows."
      ],
      "metadata": {
        "id": "IMDcQNLGBHE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Insights for Agent Development\n",
        "\n",
        "## Beyond Simplified Architecture and Centralized Prompts\n",
        "\n",
        "These are the \"lessons learned\" that would make the next agent development cycle even better:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Structured Testing from Day 1** ⭐ CRITICAL\n",
        "\n",
        "### Problem We Had\n",
        "- Tested manually by running the full agent each time\n",
        "- No way to test analyzers in isolation\n",
        "- No validation that reports format correctly\n",
        "- Hard to reproduce failures\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "# tests/test_swot_analyzer.py\n",
        "import pytest\n",
        "from agents.analyzers.swot_analyzer import SWOTAnalyzer\n",
        "\n",
        "def test_swot_returns_all_4_categories():\n",
        "    analyzer = SWOTAnalyzer()\n",
        "    mock_data = [\n",
        "        {\"title\": \"Test\", \"content\": \"Company has strengths and weaknesses\"}\n",
        "    ]\n",
        "    \n",
        "    mock_state = {\n",
        "        \"company_name\": \"Tesla\",\n",
        "        \"raw_data\": mock_data,\n",
        "        \"goal_type\": GoalType.INVESTMENT_DUE_DILIGENCE\n",
        "    }\n",
        "    \n",
        "    result = analyzer.analyze(mock_state)\n",
        "    \n",
        "    categories = set([insight.get(\"category\") for insight in result])\n",
        "    assert \"Strength\" in categories\n",
        "    assert \"Weakness\" in categories\n",
        "    assert \"Opportunity\" in categories\n",
        "    assert \"Threat\" in categories\n",
        "\n",
        "def test_swot_handles_malformed_json():\n",
        "    # Mock LLM returning bad JSON\n",
        "    # Test fallback logic\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Why:** Catches issues before you run the full agent. Provides regression tests when you refactor.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Better Logging and Observability**\n",
        "\n",
        "### Problem We Had\n",
        "```python\n",
        "# Only print statements\n",
        "print(f\"📊 Analysis categories found: {list(analysis_result.keys())}\")\n",
        "```\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "import logging\n",
        "from contextlib import contextmanager\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AnalysisLogger:\n",
        "    \"\"\"Structured logging for analysis pipeline\"\"\"\n",
        "    \n",
        "    @contextmanager\n",
        "    def log_step(self, step_name: str, context: dict):\n",
        "        \"\"\"Log start and end of a step\"\"\"\n",
        "        logger.info(f\"Starting {step_name}\", extra=context)\n",
        "        try:\n",
        "            yield\n",
        "            logger.info(f\"Completed {step_name}\", extra=context)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed {step_name}: {e}\", extra=context, exc_info=True)\n",
        "            raise\n",
        "    \n",
        "    def log_insight_count(self, framework: str, count: int):\n",
        "        logger.info(\"insight_count\", extra={\n",
        "            \"framework\": framework,\n",
        "            \"count\": count,\n",
        "            \"metric_type\": \"counter\"\n",
        "        })\n",
        "\n",
        "# Usage\n",
        "logger = AnalysisLogger()\n",
        "\n",
        "with logger.log_step(\"swot_analysis\", {\"company\": company}):\n",
        "    insights = analyzer.analyze(state)\n",
        "    \n",
        "logger.log_insight_count(\"swot\", len(insights))\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Can track performance over time\n",
        "- Easier debugging with context\n",
        "- Can build dashboards/monitoring\n",
        "- Find bottlenecks in the pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Mock Data for Development**\n",
        "\n",
        "### Problem We Had\n",
        "- Had to make real API calls to Tavily every time\n",
        "- Cost money and time\n",
        "- Couldn't test offline\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "# agents/data_collectors/base_collector.py\n",
        "class MockDataCollector:\n",
        "    \"\"\"Returns canned responses for development\"\"\"\n",
        "    \n",
        "    def collect_company_data(self, company: str, region: str, goal: str):\n",
        "        return [\n",
        "            {\n",
        "                \"title\": f\"{company} Overview\",\n",
        "                \"content\": f\"{company} is a leading company in the industry with strong market position...\",\n",
        "                \"url\": \"https://example.com\",\n",
        "                \"source\": \"mock\"\n",
        "            },\n",
        "            # ... more realistic mock data\n",
        "        ]\n",
        "\n",
        "# config.py\n",
        "if os.getenv(\"USE_MOCK_DATA\") == \"true\":\n",
        "    COLLECTOR = MockDataCollector()\n",
        "else:\n",
        "    COLLECTOR = TavilyDataCollector()\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Fast iteration during development\n",
        "- No API costs while coding\n",
        "- Reproducible test cases\n",
        "- Can simulate edge cases (empty results, malformed data)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Cost Tracking**\n",
        "\n",
        "### Problem We Had\n",
        "- No idea how much each analysis costs\n",
        "- Can't optimize expensive calls\n",
        "- Surprise bills\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "# agents/utils/cost_tracker.py\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.costs = {\n",
        "            \"tavily\": {\"count\": 0, \"estimated_cost\": 0},\n",
        "            \"openai\": {\"tokens\": 0, \"estimated_cost\": 0}\n",
        "        }\n",
        "    \n",
        "    def track_tavily_call(self, results: int):\n",
        "        self.costs[\"tavily\"][\"count\"] += 1\n",
        "        # Tavily costs ~$0.10 per 100 results\n",
        "        self.costs[\"tavily\"][\"estimated_cost\"] += (results / 100) * 0.10\n",
        "    \n",
        "    def track_openai_tokens(self, prompt_tokens: int, completion_tokens: int):\n",
        "        # gpt-4o-mini costs\n",
        "        prompt_cost = (prompt_tokens / 1_000_000) * 0.15\n",
        "        completion_cost = (completion_tokens / 1_000_000) * 0.60\n",
        "        self.costs[\"openai\"][\"tokens\"] += prompt_tokens + completion_tokens\n",
        "        self.costs[\"openai\"][\"estimated_cost\"] += prompt_cost + completion_cost\n",
        "    \n",
        "    def get_summary(self) -> str:\n",
        "        return f\"\"\"\n",
        "Cost Summary:\n",
        "- Tavily: {self.costs['tavily']['count']} calls, ${self.costs['tavily']['estimated_cost']:.2Dao}f}\n",
        "- OpenAI: {self.costs['openai']['tokens']:,} tokens, ${self.costs['openai']['estimated_cost']:.2f}\n",
        "- Total: ${self.costs['tavily']['estimated_cost'] + self.costs['openai']['estimated_cost']:.2f}\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**Why:** Know your costs before they surprise you. Can optimize expensive operations 👀\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Configuration Management**\n",
        "\n",
        "### Problem We Had\n",
        "```python\n",
        "# agents/config.py - hard-coded values\n",
        "LLM_MODEL = \"gpt-4o-mini\"\n",
        "LLM_TEMPERATURE = 0.7\n",
        "MAX_ITERATIONS = 2\n",
        "```\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "# config.py\n",
        "import os\n",
        "from pydantic import BaseSettings\n",
        "\n",
        "class AgentConfig(BaseSettings):\n",
        "    # LLM settings\n",
        "    llm_model: str = \"gpt-4o-mini\"\n",
        "    llm_temperature: float = 0.7\n",
        "    \n",
        "    # Data collection\n",
        "    tavily_max_results: int = 10\n",
        "    data_points_to_summarize: int = 15\n",
        "    \n",
        "    # Quality thresholds\n",
        "    min_confidence: float = 0.5\n",
        "    min_data_coverage: float = 0.8\n",
        "    \n",
        "    # Framework-specific settings\n",
        "    swot_min_items_per_category: int = 2\n",
        "    porter_min_forces: int = 5\n",
        "    pestel_min_dimensions: int = 6\n",
        "    \n",
        "    class Config:\n",
        "        env_prefix = \"AGENT_\"\n",
        "        env_file = \".env\"\n",
        "\n",
        "config = AgentConfig()\n",
        "\n",
        "# Now easily configurable:\n",
        "# .env file or environment variables:\n",
        "# AGENT_LLM_MODEL=gpt-4o\n",
        "# AGENT_LLM_TEMPERATURE=0.5\n",
        "# AGENT_MIN_CONFIDENCE=0.6\n",
        "```\n",
        "\n",
        "entry Understanding Configuration Complexity Trade-offs\n",
        "\n",
        "**Why:**\n",
        "- Environment-specific configs (dev vs prod)\n",
        "- No code changes to adjust behavior\n",
        "- Can experiment with settings easily\n",
        "\n",
        "---\n",
        "\n",
        "## 6. **Output Validation and Schema Enforcement**\n",
        "\n",
        "### Problem We Had\n",
        "- LLM might return wrong JSON structure\n",
        "- Had to handle parsing errors manually\n",
        "- Silent failures possible\n",
        "\n",
        "### Better Approach\n",
        "```python\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "class SWOTInsight(BaseModel):\n",
        "    \"\"\"Validated SWOT insight schema\"\"\"\n",
        "    category: str = Field(regex=\"^(Strength|Weakness|Opportunity|Threat)$\")\n",
        "    insight: str = Field(min_length=10)\n",
        "    evidence: str = Field(min_length=5)\n",
        "    confidence: float = Field(ge=0.0, le=1.0)\n",
        "    impact: str = Field(regex=\"^(Very Low|Low|Moderate|High|Very High)$\")\n",
        "    \n",
        "    @validator('insight')\n",
        "    def insight_not_generic(cls, v):\n",
        "        # Reject generic statements\n",
        "        generic = ['good', 'bad', 'ok', 'fine']\n",
        "        if any(word in v.lower() for word in generic):\n",
        "            raise ValueError(f\"Insight too generic: {v}\")\n",
        "        return v\n",
        "\n",
        "class SWOTResult(BaseModel):\n",
        "    \"\"\"Validated SWOT result schema\"\"\"\n",
        "    strengths: List[SWOTInsight] = Field(min_items=2)\n",
        "    weaknesses: List[SWOTInsight] = Field(min_items=2)\n",
        "    opportunities: List[SWOTInsight] = Field(min_items=2)\n",
        "    threats: List[SWOTInsight] = Field(min_items=2)\n",
        "\n",
        "# In analyzer:\n",
        "def analyze(self, state):\n",
        "    response = self.llm.invoke(messages)\n",
        "    raw_result = json.loads(response.content)\n",
        "    \n",
        "    # Validate against schema\n",
        "    try:\n",
        "        validated = SWOTResult(**raw_result)\n",
        "        # Now guaranteed to have correct structure!\n",
        "        return self._format_insights(validated)\n",
        "    except ValidationError as e:\n",
        "        logger.error(f\"Schema validation failed: {e}\")\n",
        "        return self._get_fallback_insights(state)\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Fail fast on malformed responses\n",
        "- Clear error messages\n",
        "- Type safety\n",
        "- Self-documenting data structures\n",
        "\n",
        "---\n",
        "\n",
        "## 7. **Progressive Enhancement of Data Sources**\n",
        "\n",
        "### Better Pattern\n",
        "```python\n",
        "# agents/data_collectors/data_orchestrator.py\n",
        "class DataOrchestrator:\n",
        "    \"\"\"Coordinates multiple data sources with fallbacks\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.sources = [\n",
        "            TavilyCollector(),\n",
        "            WikipediaCollector(),\n",
        "            AlphaVantageCollector()  # Future\n",
        "        ]\n",
        "        self.cache = {}\n",
        "    \n",
        "    def collect_data(self, company: str, **kwargs):\n",
        "        # Check cache first\n",
        "        cache_key = self._make_cache_key(company, **kwargs)\n",
        "        if cache_key in self.cache:\n",
        "            logger.info(\"Cache hit\")\n",
        "            return self.cache[cache_key]\n",
        "        \n",
        "        all_data = []\n",
        "        for source in self.sources:\n",
        "            try:\n",
        "                data = source.collect(company, **kwargs)\n",
        "                all_data.extend(data)\n",
        "                \n",
        "                # If we have enough, return early\n",
        "                if len(all_data) >= 20:\n",
        "                    logger.info(f\"Sufficient data from {source}\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"{source} failed: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Cache for next time\n",
        "        self.cache[cache_key] = all_data\n",
        "        return all_data\n",
        "```\n",
        "\n",
        "**Why:**\n",
        "- Resilience (if one source fails, others work)\n",
        "- Performance (cache repeated queries)\n",
        "- Scalability (easy to add new sources)\n",
        "- Cost optimization (stop when you have enough)\n",
        "\n",
        "---\n",
        "\n",
        "## 8. **Git Hooks for Quality Checks**\n",
        "\n",
        "### Pre-commit Hook\n",
        "```bash\n",
        "#!/bin/sh\n",
        "# .git/hooks/pre-commit\n",
        "\n",
        "# Run linters\n",
        "black agents/\n",
        "isort agents/\n",
        "flake8 agents/\n",
        "\n",
        "# Run tests\n",
        "pytest tests/ -v\n",
        "\n",
        "# Type checking\n",
        "mypy agents/\n",
        "```\n",
        "\n",
        "**Why:** Prevents committing broken code. Forces best practices.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. **Documentation as Code**\n",
        "\n",
        "### Automated API Documentation\n",
        "```python\n",
        "# agents/analyzer.py\n",
        "def analyze(\n",
        "    state: BusinessAnalysisState\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Perform SWOT analysis on company data.\n",
        "    \n",
        "    Args:\n",
        "        state: Current agent state with company data and goal\n",
        "        \n",
        "    Returns:\n",
        "        List of insights, each containing:\n",
        "        - category: SWOT category (Strength, Weakness, etc.)\n",
        "        - insight: Description of the finding\n",
        "        - evidence: Supporting evidence from data\n",
        "        - confidence: 0.0-1.0 confidence score\n",
        "        - impact: Impact level (Very Low to Very High)\n",
        "        \n",
        "    Raises:\n",
        "        AnalysisError: If data is insufficient\n",
        "        \n",
        "    Example:\n",
        "        >>> insights = analyzer.analyze(state)\n",
        "        >>> assert len(insights) >= 8  # Min 2 per category\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Why:** Self-documenting code. Can auto-generate API docs.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. **User Experience Polish**\n",
        "\n",
        "### Current CLI\n",
        "```python\n",
        "company_name = input(\"Enter company name: \")\n",
        "# Not very helpful\n",
        "```\n",
        "\n",
        "### Better CLI\n",
        "```python\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.prompt import Prompt\n",
        "from rich.table import Table\n",
        "\n",
        "console = Console()\n",
        "\n",
        "def interactive_setup():\n",
        "    console.print(\"[bold blue]Business Analysis Agent[/bold blue]\")\n",
        "    console.print(\"\")\n",
        "    \n",
        "    # Better prompts with help text\n",
        "    company = Prompt.ask(\n",
        "        \"Company name\",\n",
        "        default=\"Tesla\",\n",
        "        console=console\n",
        "    )\n",
        "    \n",
        "    # Visual goal selection\n",
        "    table = Table(title=\"Select Analysis Goal\")\n",
        "    table.add_column(\"ID\", style=\"cyan\")\n",
        "    table.add_column(\"Goal\", style=\"magenta\")\n",
        "    table.add_column(\"Framework\", style=\"green\")\n",
        "    \n",
        "    for goal_id, goal_info in GOAL_OPTIONS.items():\n",
        "        table.add_row(str(goal_id), goal_info[\"name\"], goal_info[\"framework\"])\n",
        "    \n",
        "    console.print(table)\n",
        "    \n",
        "    # Better selection\n",
        "    goal_id = Prompt.ask(\n",
        "        \"Select goal\",\n",
        "        choices=list(GOAL_OPTIONS.keys()),\n",
        "        default=\"1\"\n",
        "    )\n",
        "    \n",
        "    return company, GOAL_OPTIONS[goal_id]\n",
        "```\n",
        "\n",
        "**Why:** More professional. Easier for end users. Can still run non-interactively.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. **Metrics and Analytics**\n",
        "\n",
        "```python\n",
        "# Track what users actually want\n",
        "class AgentMetrics:\n",
        "    def track_run(self, goal: str, framework: str, success: bool):\n",
        "        # Log to analytics service\n",
        "        # Track most used frameworks\n",
        "        # Track success rate\n",
        "        pass\n",
        "    \n",
        "    def track_performance(self, step: str, duration: float):\n",
        "        # Track which steps are slowest\n",
        "        # Identify bottlenecks\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Why:** Data-driven improvements. Know what to optimize.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. **Incremental Feature Flags**\n",
        "\n",
        "```python\n",
        "# config.py\n",
        "FEATURES = {\n",
        "    \"enable_retry_logic\": False,  # Start disabled for MVP\n",
        "    \"enable_data_caching\": True,\n",
        "    \"enable_cost_tracking\": True,\n",
        "    \"enable_visual_output\": False,  # Future feature\n",
        "}\n",
        "\n",
        "# Usage\n",
        "if FEATURES[\"enable_retry_logic\"]:\n",
        "    insights = analyze_with_retry(state)\n",
        "else:\n",
        "    insights = analyze(state)\n",
        "```\n",
        "\n",
        "**Why:** Safe deployment. Easy rollback. Can test features with subset of users.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Top 5 for Next Time\n",
        "\n",
        "1. **Testing framework** - Catch bugs before production\n",
        "2. **Logging system** - Visibility into what's happening\n",
        "3. **Mock data** - Fast development without API costs\n",
        "4. **Schema validation** - Guarantee correct outputs\n",
        "5. **Configuration management** - Flexible, environment-aware\n",
        "\n",
        "These would have saved us time during development and made the agent production-ready faster.\n",
        "\n",
        "## The Meta-Insight\n",
        "\n",
        "**Build for observability from day one.** The biggest issue is not knowing what's going wrong when things fail. Good logging, testing, and validation make debugging 10x easier.\n",
        "\n"
      ],
      "metadata": {
        "id": "VYLhbXILBicb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## What is pytest?\n",
        "\n",
        "**pytest** is a Python testing framework. It runs automated checks to verify your code.\n",
        "\n",
        "---\n",
        "\n",
        "## Simple analogy\n",
        "\n",
        "**Without pytest:**\n",
        "- Manually run the agent\n",
        "- Check output\n",
        "- Guess if anything broke\n",
        "\n",
        "**With pytest:**\n",
        "- Run `pytest` to run all tests\n",
        "- Pass/fail feedback\n",
        "- Automated checks\n",
        "\n",
        "---\n",
        "\n",
        "## Why it matters for the agent\n",
        "\n",
        "Without pytest:\n",
        "```bash\n",
        "# Every time you change code:\n",
        "python run_agent.py\n",
        "# Wait 30 seconds...\n",
        "# Manually check output\n",
        "# Hope nothing broke\n",
        "```\n",
        "\n",
        "With pytest:\n",
        "```bash\n",
        "# Instant feedback:\n",
        "pytest\n",
        "# Done in 2 seconds, shows you exactly what broke\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Simple example\n",
        "\n",
        "```python\n",
        "# tests/test_swot.py\n",
        "\n",
        "def test_swot_returns_all_4_categories():\n",
        "    analyzer = SWOTAnalyzer()\n",
        "    insights = analyzer.analyze(state)\n",
        "    \n",
        "    categories = [i.get(\"category\") for i in insights]\n",
        "    \n",
        "    # This test FAILED when we only got 2 categories!\n",
        "    assert \"Strength\" in categories\n",
        "    assert \"Weakness\" in categories\n",
        "    assert \"Opportunity\" in categories\n",
        "    assert \"Threat\" in categories\n",
        "```\n",
        "\n",
        "This would have caught the “only 2 categories” bug immediately.\n",
        "\n",
        "---\n",
        "\n",
        "## Bottom line\n",
        "\n",
        "pytest helps you:\n",
        "- Catch bugs before users do\n",
        "- Ship with more confidence\n",
        "- Refactor safely\n",
        "- Document expected behavior\n",
        "- Integrate with CI/CD (automated runs)"
      ],
      "metadata": {
        "id": "bK6FOUA2BoQc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ro_WBe9AJok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}