{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVdii7ZSUdEnH0MwIcpFGK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/253_Product_CustomerFitDiscoveryOrchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing utilities for Product-Customer Fit Discovery Orchestrator\n",
        "\n",
        "\n",
        "This is arguably the most important stage because **the quality of the analysis depends entirely on the quality of the data preparation.**\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Agent Architecture: Analytical Specialization\n",
        "\n",
        "The focus of these utilities is to transform raw, tabular data into **multi-modal, structured data** that is optimized for the three specialized agents downstream (Clustering, Pattern Mining, and Graph Motifs).\n",
        "\n",
        "### ðŸŽ¯ What to Focus On\n",
        "\n",
        "1.  **High-Value Feature Engineering (`create_derived_features`):**\n",
        "    * This is the difference between simple analysis and **actionable business intelligence**. Functions like calculating `engagement_score` and `product_diversity` turn raw counts into meaningful, scaled metrics.\n",
        "    * **Focus:** Understand how these scores are calculated (e.g., based on transaction count, product diversity, and usage). These engineered features are the **inputs** the `clustering_agent` will use to define customer segments. If the features are weak, the segments will be meaningless.\n",
        "\n",
        "2.  **Standardization and Scaling (`normalize_usage_metrics`):**\n",
        "    * The use of **Min-Max normalization** is essential for any distance-based algorithm, such as the clustering algorithms likely used in the `clustering_agent`.\n",
        "    * **Focus:** Scaling numerical features (like `Usage_Metric`) to a common range (0 to 1) prevents features with naturally large values from unfairly dominating the similarity calculations.\n",
        "\n",
        "3.  **The Shift to Graph Theory (`networkx`):**\n",
        "    * The inclusion of the `networkx` library and three distinct graph-building functions is a key architectural decision.\n",
        "    * **Focus:** You are learning to move beyond traditional tabular data. You are using **Graph Theory** to model relationships:\n",
        "        * **Customer-Product Graph:** Represents individual purchases.\n",
        "        * **Product Co-occurrence Graph:** Represents product bundles/synergy.\n",
        "        * **Customer Similarity Graph:** Represents shared behavioral groups.\n",
        "    * This directly enables the **`graph_motif_agent`** to look for recurring structures and patterns that would be invisible in a flat spreadsheet.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Multi-Modal and Deep Structural Analysis\n",
        "\n",
        "This level of preprocessing confirms that your orchestrator is not just a simple data processing pipeline; it is a **Deep Structural Analysis Agent**.\n",
        "\n",
        "* **Simple Agent Limitation:** A simple LLM-based agent would likely attempt analysis directly on the raw CSVs or might only perform basic filtering/aggregations.\n",
        "* **Orchestrator/Hybrid Power:** Your agent is capable of generating **multiple, specialized data structures** from a single source, ready for parallel execution:\n",
        "    1.  **Tabular Data:** Normalized usage metrics for statistical analysis.\n",
        "    2.  **Engineered Data:** High-value scores and tiers for customer profiling.\n",
        "    3.  **Network Data (The Power):** Multiple graph representations to detect complex social or purchasing **structures** that reveal \"ghost demand.\"\n",
        "\n",
        "This **Hybrid Architecture**, which combines traditional, high-performance data science code (`pandas`, `numpy`, `networkx`) with an LLM-driven orchestration layer, is what makes this agent **vastly more robust and analytically powerful** than a general-purpose agent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhWx-ieKNvEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy8d2fHnI9eL"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data preprocessing utilities for Product-Customer Fit Discovery Orchestrator\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any, Set\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def parse_feature_set(feature_string: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Parse comma-separated feature set string into sorted list.\n",
        "\n",
        "    Args:\n",
        "        feature_string: Comma-separated feature string (e.g., \"A, B, C\")\n",
        "\n",
        "    Returns:\n",
        "        Sorted list of features (e.g., [\"A\", \"B\", \"C\"])\n",
        "    \"\"\"\n",
        "    if not feature_string or not isinstance(feature_string, str):\n",
        "        return []\n",
        "\n",
        "    # Split by comma, strip whitespace, filter empty strings\n",
        "    features = [f.strip() for f in feature_string.split(\",\") if f.strip()]\n",
        "\n",
        "    # Return sorted for consistency\n",
        "    return sorted(features)\n",
        "\n",
        "\n",
        "def normalize_usage_metrics(transactions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Normalize usage metrics in transactions for clustering.\n",
        "\n",
        "    Adds normalized_usage field to each transaction.\n",
        "\n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Transactions with added normalized_usage field\n",
        "    \"\"\"\n",
        "    if not transactions:\n",
        "        return transactions\n",
        "\n",
        "    # Extract usage metrics\n",
        "    usage_values = [t.get(\"Usage_Metric\", 0) for t in transactions if \"Usage_Metric\" in t]\n",
        "\n",
        "    if not usage_values:\n",
        "        return transactions\n",
        "\n",
        "    # Calculate min-max normalization\n",
        "    min_usage = min(usage_values)\n",
        "    max_usage = max(usage_values)\n",
        "    range_usage = max_usage - min_usage if max_usage != min_usage else 1.0\n",
        "\n",
        "    # Add normalized field\n",
        "    normalized_transactions = []\n",
        "    for txn in transactions:\n",
        "        txn_copy = txn.copy()\n",
        "        if \"Usage_Metric\" in txn:\n",
        "            txn_copy[\"normalized_usage\"] = (txn[\"Usage_Metric\"] - min_usage) / range_usage\n",
        "        else:\n",
        "            txn_copy[\"normalized_usage\"] = 0.0\n",
        "        normalized_transactions.append(txn_copy)\n",
        "\n",
        "    return normalized_transactions\n",
        "\n",
        "\n",
        "def build_customer_product_graph(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    products: List[Dict[str, Any]]\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build bipartite customer-product graph.\n",
        "\n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are customers and products\n",
        "    - Edges connect customers to products they've purchased\n",
        "    - Edge weights represent number of transactions\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        products: List of product dictionaries\n",
        "\n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add customer nodes\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if customer_id:\n",
        "            G.add_node(customer_id, node_type=\"customer\", **customer)\n",
        "\n",
        "    # Add product nodes\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if product_id:\n",
        "            G.add_node(product_id, node_type=\"product\", **product)\n",
        "\n",
        "    # Add edges (customer-product relationships)\n",
        "    # Count transactions per customer-product pair\n",
        "    edge_weights = defaultdict(int)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            edge_key = (customer_id, product_id)\n",
        "            edge_weights[edge_key] += 1\n",
        "\n",
        "    # Add weighted edges\n",
        "    for (customer_id, product_id), weight in edge_weights.items():\n",
        "        G.add_edge(customer_id, product_id, weight=weight, transactions=weight)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def build_product_cooccurrence_graph(\n",
        "    transactions: List[Dict[str, Any]]\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build product co-occurrence graph.\n",
        "\n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are products\n",
        "    - Edges connect products purchased by the same customer\n",
        "    - Edge weights represent number of customers who bought both\n",
        "\n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "\n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Group transactions by customer\n",
        "    customer_products = defaultdict(set)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Add product nodes\n",
        "    all_products = set()\n",
        "    for products in customer_products.values():\n",
        "        all_products.update(products)\n",
        "\n",
        "    for product_id in all_products:\n",
        "        G.add_node(product_id, node_type=\"product\")\n",
        "\n",
        "    # Count co-occurrences\n",
        "    cooccurrence = defaultdict(int)\n",
        "    for customer_id, products in customer_products.items():\n",
        "        products_list = list(products)\n",
        "        # Create edges for all pairs of products this customer bought\n",
        "        for i in range(len(products_list)):\n",
        "            for j in range(i + 1, len(products_list)):\n",
        "                p1, p2 = sorted([products_list[i], products_list[j]])\n",
        "                cooccurrence[(p1, p2)] += 1\n",
        "\n",
        "    # Add weighted edges\n",
        "    for (p1, p2), weight in cooccurrence.items():\n",
        "        G.add_edge(p1, p2, weight=weight, customers=weight)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def build_customer_similarity_graph(\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    min_shared_products: int = 2\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build customer similarity graph based on shared products.\n",
        "\n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are customers\n",
        "    - Edges connect customers who share products\n",
        "    - Edge weights represent number of shared products\n",
        "\n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "        min_shared_products: Minimum shared products to create edge\n",
        "\n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Group products by customer\n",
        "    customer_products = defaultdict(set)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Add customer nodes\n",
        "    for customer_id in customer_products.keys():\n",
        "        G.add_node(customer_id, node_type=\"customer\")\n",
        "\n",
        "    # Calculate similarity (shared products)\n",
        "    customer_list = list(customer_products.keys())\n",
        "    for i in range(len(customer_list)):\n",
        "        for j in range(i + 1, len(customer_list)):\n",
        "            c1, c2 = customer_list[i], customer_list[j]\n",
        "            shared = customer_products[c1] & customer_products[c2]\n",
        "            if len(shared) >= min_shared_products:\n",
        "                G.add_edge(c1, c2, weight=len(shared), shared_products=list(shared))\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def create_derived_features(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    products: List[Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create derived features for analysis.\n",
        "\n",
        "    Calculates:\n",
        "    - Customer engagement scores (transaction frequency, product diversity)\n",
        "    - Product popularity scores (usage frequency, customer count)\n",
        "    - Usage intensity tiers (high/medium/low)\n",
        "\n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        products: List of product dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with derived features\n",
        "    \"\"\"\n",
        "    # Customer engagement metrics\n",
        "    customer_txn_count = defaultdict(int)\n",
        "    customer_products = defaultdict(set)\n",
        "    customer_total_usage = defaultdict(float)\n",
        "\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        usage = txn.get(\"Usage_Metric\", 0)\n",
        "\n",
        "        if customer_id:\n",
        "            customer_txn_count[customer_id] += 1\n",
        "            customer_total_usage[customer_id] += usage\n",
        "            if product_id:\n",
        "                customer_products[customer_id].add(product_id)\n",
        "\n",
        "    # Calculate customer engagement scores\n",
        "    customer_engagement = {}\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if customer_id:\n",
        "            txn_count = customer_txn_count.get(customer_id, 0)\n",
        "            product_diversity = len(customer_products.get(customer_id, set()))\n",
        "            avg_usage = customer_total_usage.get(customer_id, 0) / max(txn_count, 1)\n",
        "\n",
        "            # Simple engagement score (0-1 scale)\n",
        "            engagement_score = min(1.0, (txn_count * 0.3 + product_diversity * 0.4 + (avg_usage / 100) * 0.3))\n",
        "\n",
        "            customer_engagement[customer_id] = {\n",
        "                \"transaction_count\": txn_count,\n",
        "                \"product_diversity\": product_diversity,\n",
        "                \"average_usage\": avg_usage,\n",
        "                \"engagement_score\": engagement_score,\n",
        "                \"engagement_tier\": \"high\" if engagement_score > 0.7 else \"medium\" if engagement_score > 0.4 else \"low\"\n",
        "            }\n",
        "\n",
        "    # Product popularity metrics\n",
        "    product_txn_count = defaultdict(int)\n",
        "    product_customers = defaultdict(set)\n",
        "    product_total_usage = defaultdict(float)\n",
        "\n",
        "    for txn in transactions:\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        usage = txn.get(\"Usage_Metric\", 0)\n",
        "\n",
        "        if product_id:\n",
        "            product_txn_count[product_id] += 1\n",
        "            product_total_usage[product_id] += usage\n",
        "            if customer_id:\n",
        "                product_customers[product_id].add(customer_id)\n",
        "\n",
        "    # Calculate product popularity scores\n",
        "    product_popularity = {}\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if product_id:\n",
        "            txn_count = product_txn_count.get(product_id, 0)\n",
        "            customer_count = len(product_customers.get(product_id, set()))\n",
        "            avg_usage = product_total_usage.get(product_id, 0) / max(txn_count, 1)\n",
        "\n",
        "            # Simple popularity score (0-1 scale)\n",
        "            popularity_score = min(1.0, (txn_count * 0.4 + customer_count * 0.4 + (avg_usage / 100) * 0.2))\n",
        "\n",
        "            product_popularity[product_id] = {\n",
        "                \"transaction_count\": txn_count,\n",
        "                \"customer_count\": customer_count,\n",
        "                \"average_usage\": avg_usage,\n",
        "                \"popularity_score\": popularity_score,\n",
        "                \"popularity_tier\": \"high\" if popularity_score > 0.7 else \"medium\" if popularity_score > 0.4 else \"low\"\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        \"customer_engagement\": customer_engagement,\n",
        "        \"product_popularity\": product_popularity\n",
        "    }\n",
        "\n",
        "\n",
        "def preprocess_products(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Preprocess products: parse Feature_Set strings into lists.\n",
        "\n",
        "    Args:\n",
        "        products: List of product dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Products with parsed feature_sets (list instead of string)\n",
        "    \"\"\"\n",
        "    preprocessed = []\n",
        "    for product in products:\n",
        "        product_copy = product.copy()\n",
        "        feature_string = product.get(\"Feature_Set\", \"\")\n",
        "        product_copy[\"feature_list\"] = parse_feature_set(feature_string)\n",
        "        preprocessed.append(product_copy)\n",
        "\n",
        "    return preprocessed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, `build_customer_product_graph`, is a highly specialized piece of code that translates your raw transactional data into a **Bipartite Network Graph**. This is a massive step up in analytical capability for your agent.\n",
        "\n",
        "Here is a detailed breakdown of the function's architecture and why it's so important for your \"ghost demand\" discovery goal.\n",
        "\n",
        "```\n",
        "import networkx as nx\n",
        "\n",
        "def build_customer_product_graph(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    products: List[Dict[str, Any]]\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build bipartite customer-product graph.\n",
        "    \n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are customers and products\n",
        "    - Edges connect customers to products they've purchased\n",
        "    - Edge weights represent number of transactions\n",
        "    \n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        products: List of product dictionaries\n",
        "        \n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add customer nodes\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if customer_id:\n",
        "            G.add_node(customer_id, node_type=\"customer\", **customer)\n",
        "    \n",
        "    # Add product nodes\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if product_id:\n",
        "            G.add_node(product_id, node_type=\"product\", **product)\n",
        "    \n",
        "    # Add edges (customer-product relationships)\n",
        "    # Count transactions per customer-product pair\n",
        "    edge_weights = defaultdict(int)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            edge_key = (customer_id, product_id)\n",
        "            edge_weights[edge_key] += 1\n",
        "    \n",
        "    # Add weighted edges\n",
        "    for (customer_id, product_id), weight in edge_weights.items():\n",
        "        G.add_edge(customer_id, product_id, weight=weight, transactions=weight)\n",
        "    \n",
        "    return G\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  What is a Bipartite Customer-Product Graph?\n",
        "\n",
        "The graph created here is specifically **bipartite**, meaning it has two distinct types of nodes, and edges only exist **between** the two types, never within a type.\n",
        "\n",
        "| Element | Type | Role |\n",
        "| :--- | :--- | :--- |\n",
        "| **Nodes** | **Customers** (e.g., C1, C2...) | One set of nodes, added with the `node_type=\"customer\"` attribute. |\n",
        "| **Nodes** | **Products** (e.g., P01, P02...) | The second set of nodes, added with the `node_type=\"product\"` attribute. |\n",
        "| **Edges** | Connections | An edge exists only **between a Customer node and a Product node** (it represents a transaction/purchase). |\n",
        "\n",
        "### Why Bipartite?\n",
        "\n",
        "This structure is the most **faithful representation** of your purchasing data. It explicitly separates the buyers from the things they buy, which is crucial for subsequent analysis (like finding market gaps or making recommendations).\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ› ï¸ Step-by-Step Code Analysis\n",
        "\n",
        "The function performs three main architectural actions:\n",
        "\n",
        "### 1. Adding Nodes (`G.add_node(...)`)\n",
        "* **Action:** It iterates through the `customers` and `products` lists, adding each unique ID as a node to the graph (`G`).\n",
        "* **The Power:** When adding nodes, it uses `**customer` (and `**product`) to attach **all the raw metadata** (like customer age, loyalty status, product type, etc.) directly to the graph node. This means the graph is not just a structure of connections; it's a **Rich, Attributed Graph**. Later, the `graph_motif_agent` can use this attached metadata (attributes) in its analysis.\n",
        "\n",
        "### 2. Counting Edge Weights (`defaultdict(int)`)\n",
        "* **Action:** It iterates through the `transactions` list to count how many times each unique (Customer ID, Product ID) pair appears.\n",
        "* **Focus:** This uses the highly efficient `defaultdict(int)` to tally the number of transactions. If Customer C1 bought Product P01 three times, the count for the edge (C1, P01) will be 3.\n",
        "* **The Power:** This transforms a simple purchase record (an *unweighted* edge) into a **measure of intensity** (a *weighted* edge). The weight represents the **Frequency of Interaction**.\n",
        "\n",
        "### 3. Adding Weighted Edges (`G.add_edge(...)`)\n",
        "* **Action:** It iterates through the tallied `edge_weights` and adds the corresponding connections to the graph.\n",
        "* **The Power:** It attaches the weight to the edge using `weight=weight, transactions=weight`. This makes the graph immediately ready for centrality analysis, where a node (Customer or Product) connected by many high-weight edges is considered **more central** or important.\n",
        "\n",
        "## âœ¨ Differentiation: Enabling Advanced Network Analysis\n",
        "\n",
        "This function is critical because it prepares the data for **Network Analysis**, a technique far beyond the capabilities of a simple data crunching agent.\n",
        "\n",
        "* **Centrality Metrics:** Once the graph is built, the `graph_motif_agent` can calculate metrics like **Betweenness Centrality** (identifying customers or products that act as critical connectors in the network) or **Closeness Centrality** (identifying nodes that are most efficiently connected to the rest of the network).\n",
        "* **Community Detection:** This bipartite graph can be projected into a **Product-to-Product** graph, where communities of products that are frequently co-purchased (potential bundles) can be found.\n",
        "* **Recommendation Systems:** The graph structure is the bedrock of many **Collaborative Filtering** recommendation algorithms.\n",
        "\n",
        "By using **`networkx`** to create this structured representation, your agent has gained the power to perform sophisticated **Topological Data Analysis** on its input data."
      ],
      "metadata": {
        "id": "o71riT8cQIGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Cooccurrence Graph\n",
        "```\n",
        "def build_product_cooccurrence_graph(\n",
        "    transactions: List[Dict[str, Any]]\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build product co-occurrence graph.\n",
        "    \n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are products\n",
        "    - Edges connect products purchased by the same customer\n",
        "    - Edge weights represent number of customers who bought both\n",
        "    \n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "        \n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Group transactions by customer\n",
        "    customer_products = defaultdict(set)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            customer_products[customer_id].add(product_id)\n",
        "    \n",
        "    # Add product nodes\n",
        "    all_products = set()\n",
        "    for products in customer_products.values():\n",
        "        all_products.update(products)\n",
        "    \n",
        "    for product_id in all_products:\n",
        "        G.add_node(product_id, node_type=\"product\")\n",
        "    \n",
        "    # Count co-occurrences\n",
        "    cooccurrence = defaultdict(int)\n",
        "    for customer_id, products in customer_products.items():\n",
        "        products_list = list(products)\n",
        "        # Create edges for all pairs of products this customer bought\n",
        "        for i in range(len(products_list)):\n",
        "            for j in range(i + 1, len(products_list)):\n",
        "                p1, p2 = sorted([products_list[i], products_list[j]])\n",
        "                cooccurrence[(p1, p2)] += 1\n",
        "    \n",
        "    # Add weighted edges\n",
        "    for (p1, p2), weight in cooccurrence.items():\n",
        "        G.add_edge(p1, p2, weight=weight, customers=weight)\n",
        "    \n",
        "    return G\n",
        "```\n",
        "\n",
        "This function, `build_product_cooccurrence_graph`, is analytically crucial. It moves the focus from individual customer actions to **structural relationships between products**, which is the prerequisite for discovering product bundles and cross-selling opportunities.\n",
        "\n",
        "This graph is a **Product-only Projection** of your data, transforming the *bipartite* relationship into a *monopartite* (single-set) relationship.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Analytical Concept: Co-occurrence and Projection\n",
        "\n",
        "### 1. Grouping Transactions (`customer_products = defaultdict(set)`)\n",
        "* **Action:** This is the most important preparatory step. The code first aggregates all products bought by a single customer into a unique set.\n",
        "* **Focus:** This ensures that if Customer C1 bought Product P01 yesterday and P02 today, the agent treats them as a single group of products purchased by that customer (`{P01, P02}`). The analysis is focused on **who bought what**, not *when* they bought it (though the `Transaction_Date` could be used for sequential analysis later).\n",
        "\n",
        "### 2. Identifying and Counting Pairs (`cooccurrence = defaultdict(int)`)\n",
        "* **Action:** The nested loops iterate over the unique products purchased by *each* customer and generate every possible unique pair of products (`(p1, p2)`).\n",
        "* **Focus:** The use of `sorted([products_list[i], products_list[j]])` is a simple but critical programming pattern. It guarantees that the pair is always counted the same way (e.g., always `(A, B)` and never `(B, A)`), ensuring accurate weight summation.\n",
        "* **The Power:** The final count, stored in `cooccurrence`, is the **Support Metric** (the number of times two items appeared together) for Association Rule Mining.\n",
        "\n",
        "### 3. Graph Structure and Edge Weight\n",
        "* **Nodes:** Every **unique product ID** becomes a node.\n",
        "* **Edges:** An edge exists between two products if **at least one customer bought both**.\n",
        "* **Weight:** The edge weight is the **number of customers** who purchased that specific pair. This weight is the direct measure of their co-dependence.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Discovering Product Bundles\n",
        "\n",
        "This function is directly responsible for powering the **\"product\\_bundling\"** and **\"association\\_patterns\"** goals in your orchestrator, making it vastly more powerful than a general agent.\n",
        "\n",
        "* **Quantitative Bundling:** The resulting graph allows the `pattern_mining_agent` to immediately see which products are most strongly linked. Products with **high edge weights** are prime candidates for mandatory or recommended bundles.\n",
        "* **Hidden Synergies:** This graph reveals subtle relationships. For example, if Products A and B are never advertised together, but 500 customers purchased both, that co-occurrence count (the edge weight) reveals a **hidden product synergy** that the business can exploit.\n",
        "* **Visual Analysis:** The graph can be visualized , where the thickness of the line connecting two products immediately tells the story of their relationship strength.\n",
        "\n",
        "By creating this structured graph, your agent moves from reporting past sales figures to **predicting future sales potential** based on structural relationships."
      ],
      "metadata": {
        "id": "Ceb4cXyRR7Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Similarity Graph\n",
        "\n",
        "```\n",
        "def build_customer_similarity_graph(\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    min_shared_products: int = 2\n",
        ") -> nx.Graph:\n",
        "    \"\"\"\n",
        "    Build customer similarity graph based on shared products.\n",
        "    \n",
        "    Creates an undirected graph where:\n",
        "    - Nodes are customers\n",
        "    - Edges connect customers who share products\n",
        "    - Edge weights represent number of shared products\n",
        "    \n",
        "    Args:\n",
        "        transactions: List of transaction dictionaries\n",
        "        min_shared_products: Minimum shared products to create edge\n",
        "        \n",
        "    Returns:\n",
        "        NetworkX Graph object\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Group products by customer\n",
        "    customer_products = defaultdict(set)\n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        if customer_id and product_id:\n",
        "            customer_products[customer_id].add(product_id)\n",
        "    \n",
        "    # Add customer nodes\n",
        "    for customer_id in customer_products.keys():\n",
        "        G.add_node(customer_id, node_type=\"customer\")\n",
        "    \n",
        "    # Calculate similarity (shared products)\n",
        "    customer_list = list(customer_products.keys())\n",
        "    for i in range(len(customer_list)):\n",
        "        for j in range(i + 1, len(customer_list)):\n",
        "            c1, c2 = customer_list[i], customer_list[j]\n",
        "            shared = customer_products[c1] & customer_products[c2]\n",
        "            if len(shared) >= min_shared_products:\n",
        "                G.add_edge(c1, c2, weight=len(shared), shared_products=list(shared))\n",
        "    \n",
        "    return G\n",
        "```\n",
        "\n",
        "This function is the third key graph utility, and it is specifically engineered to find **structurally similar customers**, which is the direct input needed for the **Customer Segmentation** focus area.\n",
        "\n",
        "It is conceptually the **Customer-only Projection** of your data.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Analytical Concept: Behavioral Similarity\n",
        "\n",
        "### 1. The Foundation: Shared Products\n",
        "The primary goal is to redefine the customer base based on **shared purchasing behavior**. A simple customer list provides demographics; this function provides **predictive behavioral data**.\n",
        "\n",
        "* **Step 1: Grouping (`customer_products = defaultdict(set)`):** Just as in the previous graph, the first step is to distill the transactions down to the **unique set of products** each customer owns. This product set becomes the customer's \"behavioral fingerprint.\"\n",
        "* **Step 2: Calculating Similarity via Set Intersection:**\n",
        "    * The line `shared = customer_products[c1] & customer_products[c2]` is the analytical core. The **set intersection operator ($\\&$)** is used to instantly find all products that both Customer 1 ($\\text{c1}$) and Customer 2 ($\\text{c2}$) have bought.\n",
        "    * The **Edge Weight** is then set to the size of this shared set (`len(shared)`), making it a direct, quantifiable measure of similarity.\n",
        "\n",
        "### 2. Filtering for Meaningful Connections\n",
        "* **The Threshold:** The argument `min_shared_products: int = 2` and the condition `if len(shared) >= min_shared_products:` are vital.\n",
        "* **Focus:** This acts as a **noise filter**. If two customers only share one obscure, cheap product, they aren't truly similar. By setting a minimum threshold, the resulting graph is sparse but **analytically rich**, containing only the strong, meaningful connections necessary for reliable segmentation.\n",
        "\n",
        "### 3. Graph Structure and Output\n",
        "* **Nodes:** Every **unique Customer ID** becomes a node, added with the `node_type=\"customer\"` attribute.\n",
        "* **Edges:** An edge connects any two customers who meet the `min_shared_products` threshold.\n",
        "* **Weight:** The weight is the **number of shared products**, making the graph ready for clustering algorithms.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Enabling Community Detection\n",
        "\n",
        "This function makes your agent vastly more powerful by providing the necessary input for **Community Detection** algorithms, a key technique in modern data science.\n",
        "\n",
        "* **Behavioral Segmentation:** The graph is the direct input for the **`clustering_agent`** (Step 3 in the plan). By applying graph-based clustering algorithms to this network, the agent can automatically identify **true behavioral communities** (segments) that are strongly inter-connected by their shared product choices.\n",
        "* **Strategic Opportunity:** Analyzing this graph allows the agent to find customers on the *periphery* of a strong community. These are customers who are **very similar** to the core group (high edge weight) but who are currently *missing* one or two of the core products. This is a clear, actionable opportunity for **targeted marketing** and aligns perfectly with the goal of discovering **untapped market demand (ghost demand)**."
      ],
      "metadata": {
        "id": "297jBOB9StgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Derived Features\n",
        "\n",
        "```\n",
        "def create_derived_features(\n",
        "    customers: List[Dict[str, Any]],\n",
        "    transactions: List[Dict[str, Any]],\n",
        "    products: List[Dict[str, Any]]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create derived features for analysis.\n",
        "    \n",
        "    Calculates:\n",
        "    - Customer engagement scores (transaction frequency, product diversity)\n",
        "    - Product popularity scores (usage frequency, customer count)\n",
        "    - Usage intensity tiers (high/medium/low)\n",
        "    \n",
        "    Args:\n",
        "        customers: List of customer dictionaries\n",
        "        transactions: List of transaction dictionaries\n",
        "        products: List of product dictionaries\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with derived features\n",
        "    \"\"\"\n",
        "    # Customer engagement metrics\n",
        "    customer_txn_count = defaultdict(int)\n",
        "    customer_products = defaultdict(set)\n",
        "    customer_total_usage = defaultdict(float)\n",
        "    \n",
        "    for txn in transactions:\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        usage = txn.get(\"Usage_Metric\", 0)\n",
        "        \n",
        "        if customer_id:\n",
        "            customer_txn_count[customer_id] += 1\n",
        "            customer_total_usage[customer_id] += usage\n",
        "            if product_id:\n",
        "                customer_products[customer_id].add(product_id)\n",
        "    \n",
        "    # Calculate customer engagement scores\n",
        "    customer_engagement = {}\n",
        "    for customer in customers:\n",
        "        customer_id = customer.get(\"Customer_ID\")\n",
        "        if customer_id:\n",
        "            txn_count = customer_txn_count.get(customer_id, 0)\n",
        "            product_diversity = len(customer_products.get(customer_id, set()))\n",
        "            avg_usage = customer_total_usage.get(customer_id, 0) / max(txn_count, 1)\n",
        "            \n",
        "            # Simple engagement score (0-1 scale)\n",
        "            engagement_score = min(1.0, (txn_count * 0.3 + product_diversity * 0.4 + (avg_usage / 100) * 0.3))\n",
        "            \n",
        "            customer_engagement[customer_id] = {\n",
        "                \"transaction_count\": txn_count,\n",
        "                \"product_diversity\": product_diversity,\n",
        "                \"average_usage\": avg_usage,\n",
        "                \"engagement_score\": engagement_score,\n",
        "                \"engagement_tier\": \"high\" if engagement_score > 0.7 else \"medium\" if engagement_score > 0.4 else \"low\"\n",
        "            }\n",
        "    \n",
        "    # Product popularity metrics\n",
        "    product_txn_count = defaultdict(int)\n",
        "    product_customers = defaultdict(set)\n",
        "    product_total_usage = defaultdict(float)\n",
        "    \n",
        "    for txn in transactions:\n",
        "        product_id = txn.get(\"Product_ID\")\n",
        "        customer_id = txn.get(\"Customer_ID\")\n",
        "        usage = txn.get(\"Usage_Metric\", 0)\n",
        "        \n",
        "        if product_id:\n",
        "            product_txn_count[product_id] += 1\n",
        "            product_total_usage[product_id] += usage\n",
        "            if customer_id:\n",
        "                product_customers[product_id].add(customer_id)\n",
        "    \n",
        "    # Calculate product popularity scores\n",
        "    product_popularity = {}\n",
        "    for product in products:\n",
        "        product_id = product.get(\"Product_ID\")\n",
        "        if product_id:\n",
        "            txn_count = product_txn_count.get(product_id, 0)\n",
        "            customer_count = len(product_customers.get(product_id, set()))\n",
        "            avg_usage = product_total_usage.get(product_id, 0) / max(txn_count, 1)\n",
        "            \n",
        "            # Simple popularity score (0-1 scale)\n",
        "            popularity_score = min(1.0, (txn_count * 0.4 + customer_count * 0.4 + (avg_usage / 100) * 0.2))\n",
        "            \n",
        "            product_popularity[product_id] = {\n",
        "                \"transaction_count\": txn_count,\n",
        "                \"customer_count\": customer_count,\n",
        "                \"average_usage\": avg_usage,\n",
        "                \"popularity_score\": popularity_score,\n",
        "                \"popularity_tier\": \"high\" if popularity_score > 0.7 else \"medium\" if popularity_score > 0.4 else \"low\"\n",
        "            }\n",
        "    \n",
        "    return {\n",
        "        \"customer_engagement\": customer_engagement,\n",
        "        \"product_popularity\": product_popularity\n",
        "    }\n",
        "\n",
        "```\n",
        "\n",
        "This function, `create_derived_features`, is the **Feature Engineering** powerhouse of your agent, representing a crucial step that transforms raw numbers into **actionable, business-meaningful metrics**.\n",
        "\n",
        "This is the code that **embeds business intelligence** directly into the data, making the downstream analysis infinitely more powerful.\n",
        "\n",
        "***\n",
        "\n",
        "## ðŸ§  Core Analytical Concept: Composite Scoring and Tiering\n",
        "\n",
        "The function's architecture is a systematic application of Feature Engineering to create **composite scores** for two distinct entities: customers and products.\n",
        "\n",
        "### 1. Customer Engagement Metrics\n",
        "\n",
        "The agent defines **Customer Engagement** not by one metric, but by three weighted components:\n",
        "\n",
        "| Component | Calculation | Business Weight (Score $\\times$ Factor) | Strategic Purpose |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Transaction Count** | Total purchases. | $\\times 0.3$ | Measures **Frequency** of interaction. |\n",
        "| **Product Diversity** | Number of unique products purchased. | $\\times 0.4$ | Measures **Breadth** and **Stickiness** (highest weight). |\n",
        "| **Average Usage** | Total usage divided by transaction count. | $\\times 0.3$ | Measures **Depth** of engagement per transaction. |\n",
        "\n",
        "* **Composite Score:** The final `engagement_score` is a weighted average of these three factors, scaled to a 0-1 range. This score acts as a single, powerful input for the `clustering_agent`.\n",
        "* **Tiering:** The `engagement_tier` (e.g., \"high\" if score $\\text{>} 0.7$) converts this continuous number into a **categorical feature**. This is an excellent technique for simplifying the data for visualization and for the final **Synthesis Agent** (the LLM) to reason about.\n",
        "\n",
        "### 2. Product Popularity Metrics\n",
        "\n",
        "The agent applies the same logic to the product catalog:\n",
        "\n",
        "* **Composite Score:** The `popularity_score` is calculated based on **Transaction Count** ($\\times 0.4$), **Customer Count** ($\\times 0.4$), and **Average Usage** ($\\times 0.2$).\n",
        "* **Strategic Encoding:** By giving high weight to both frequency ($\\text{Txn Count}$) and reach ($\\text{Customer Count}$), the score reflects a balanced view of market adoption. Products with high popularity scores are often core products, while low-scoring products might represent potential market gaps or failures.\n",
        "\n",
        "***\n",
        "\n",
        "## âœ¨ Differentiation: Bridging Data Science and Strategy\n",
        "\n",
        "This function is the **strategic core** that elevates your orchestrator beyond simple data analysis\n",
        "\n",
        "[Image of Feature Engineering Process]\n",
        ".\n",
        "\n",
        "* **Turning Data into Strategy:** Instead of passing the Synthesis Agent 10 raw columns, you pass it two highly-compressed, high-value metrics: `engagement_score` and `popularity_score`, along with clear `tiers`. This makes the final decision-making step more accurate and efficient.\n",
        "* **Enabling Segmentation:** The `clustering_agent` will use these scores to create meaningful customer segments (e.g., \"High Engagement/Low Diversity Customers\" or \"Low Engagement/High Diversity Customers\"). These segments directly enable the discovery of **untapped market opportunities** (e.g., cross-selling a second product to a high-diversity customer who only bought one product).\n",
        "* **Directly Addressing the Goal:** By explicitly calculating metrics for **Customer Segmentation** and features for **Strategic Opportunities**, this function is perfectly aligned with the overall goal defined in the `goal_node`."
      ],
      "metadata": {
        "id": "HpzRMhyLbAhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for data preprocessing utilities"
      ],
      "metadata": {
        "id": "XMIup_h7b01v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tests for data preprocessing utilities\"\"\"\n",
        "\n",
        "import pytest\n",
        "import networkx as nx\n",
        "from tools.data_preprocessing import (\n",
        "    parse_feature_set,\n",
        "    normalize_usage_metrics,\n",
        "    build_customer_product_graph,\n",
        "    build_product_cooccurrence_graph,\n",
        "    build_customer_similarity_graph,\n",
        "    create_derived_features,\n",
        "    preprocess_products\n",
        ")\n",
        "\n",
        "\n",
        "def test_parse_feature_set_single():\n",
        "    \"\"\"Test parsing single feature\"\"\"\n",
        "    result = parse_feature_set(\"A\")\n",
        "    assert result == [\"A\"]\n",
        "\n",
        "\n",
        "def test_parse_feature_set_multiple():\n",
        "    \"\"\"Test parsing multiple features\"\"\"\n",
        "    result = parse_feature_set(\"A, B, C\")\n",
        "    assert result == [\"A\", \"B\", \"C\"]\n",
        "\n",
        "\n",
        "def test_parse_feature_set_sorted():\n",
        "    \"\"\"Test features are sorted\"\"\"\n",
        "    result = parse_feature_set(\"C, A, B\")\n",
        "    assert result == [\"A\", \"B\", \"C\"]\n",
        "\n",
        "\n",
        "def test_parse_feature_set_with_spaces():\n",
        "    \"\"\"Test parsing with extra spaces\"\"\"\n",
        "    result = parse_feature_set(\" A , B , C \")\n",
        "    assert result == [\"A\", \"B\", \"C\"]\n",
        "\n",
        "\n",
        "def test_parse_feature_set_empty():\n",
        "    \"\"\"Test parsing empty string\"\"\"\n",
        "    result = parse_feature_set(\"\")\n",
        "    assert result == []\n",
        "\n",
        "\n",
        "def test_parse_feature_set_single_letter():\n",
        "    \"\"\"Test parsing single letter feature\"\"\"\n",
        "    result = parse_feature_set(\"D\")\n",
        "    assert result == [\"D\"]\n",
        "\n",
        "\n",
        "def test_normalize_usage_metrics():\n",
        "    \"\"\"Test usage metric normalization\"\"\"\n",
        "    transactions = [\n",
        "        {\"Transaction_ID\": \"T1\", \"Usage_Metric\": 10.0},\n",
        "        {\"Transaction_ID\": \"T2\", \"Usage_Metric\": 50.0},\n",
        "        {\"Transaction_ID\": \"T3\", \"Usage_Metric\": 90.0}\n",
        "    ]\n",
        "\n",
        "    result = normalize_usage_metrics(transactions)\n",
        "\n",
        "    assert len(result) == 3\n",
        "    assert \"normalized_usage\" in result[0]\n",
        "    assert result[0][\"normalized_usage\"] == 0.0  # Min value\n",
        "    assert result[2][\"normalized_usage\"] == 1.0  # Max value\n",
        "    assert 0.0 <= result[1][\"normalized_usage\"] <= 1.0\n",
        "\n",
        "\n",
        "def test_normalize_usage_metrics_empty():\n",
        "    \"\"\"Test normalization with empty list\"\"\"\n",
        "    result = normalize_usage_metrics([])\n",
        "    assert result == []\n",
        "\n",
        "\n",
        "def test_build_customer_product_graph():\n",
        "    \"\"\"Test building customer-product bipartite graph\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\", \"Age_Group\": \"35-44\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Age_Group\": \"45-54\"}\n",
        "    ]\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Product_Type\": \"Hardware\"},\n",
        "        {\"Product_ID\": \"P02\", \"Product_Type\": \"Software\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\"}\n",
        "    ]\n",
        "\n",
        "    G = build_customer_product_graph(customers, transactions, products)\n",
        "\n",
        "    assert isinstance(G, nx.Graph)\n",
        "    assert \"C001\" in G.nodes\n",
        "    assert \"C002\" in G.nodes\n",
        "    assert \"P01\" in G.nodes\n",
        "    assert \"P02\" in G.nodes\n",
        "    assert G.has_edge(\"C001\", \"P01\")\n",
        "    assert G.has_edge(\"C001\", \"P02\")\n",
        "    assert G.has_edge(\"C002\", \"P01\")\n",
        "    assert G[\"C001\"][\"P01\"][\"weight\"] == 1\n",
        "\n",
        "\n",
        "def test_build_product_cooccurrence_graph():\n",
        "    \"\"\"Test building product co-occurrence graph\"\"\"\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P03\"}\n",
        "    ]\n",
        "\n",
        "    G = build_product_cooccurrence_graph(transactions)\n",
        "\n",
        "    assert isinstance(G, nx.Graph)\n",
        "    assert \"P01\" in G.nodes\n",
        "    assert \"P02\" in G.nodes\n",
        "    assert \"P03\" in G.nodes\n",
        "    assert G.has_edge(\"P01\", \"P02\")\n",
        "    assert G[\"P01\"][\"P02\"][\"weight\"] == 2  # C001 and C002 both have P01 and P02\n",
        "    assert G.has_edge(\"P02\", \"P03\")\n",
        "    assert G[\"P02\"][\"P03\"][\"weight\"] == 1  # Only C003 has both\n",
        "\n",
        "\n",
        "def test_build_customer_similarity_graph():\n",
        "    \"\"\"Test building customer similarity graph\"\"\"\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\"},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P02\"},\n",
        "        {\"Customer_ID\": \"C003\", \"Product_ID\": \"P03\"}\n",
        "    ]\n",
        "\n",
        "    G = build_customer_similarity_graph(transactions, min_shared_products=2)\n",
        "\n",
        "    assert isinstance(G, nx.Graph)\n",
        "    assert \"C001\" in G.nodes\n",
        "    assert \"C002\" in G.nodes\n",
        "    assert \"C003\" in G.nodes\n",
        "    assert G.has_edge(\"C001\", \"C002\")  # Share P01 and P02\n",
        "    assert G[\"C001\"][\"C002\"][\"weight\"] == 2\n",
        "    assert not G.has_edge(\"C001\", \"C003\")  # No shared products\n",
        "\n",
        "\n",
        "def test_create_derived_features():\n",
        "    \"\"\"Test creating derived features\"\"\"\n",
        "    customers = [\n",
        "        {\"Customer_ID\": \"C001\"},\n",
        "        {\"Customer_ID\": \"C002\"}\n",
        "    ]\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\"},\n",
        "        {\"Product_ID\": \"P02\"}\n",
        "    ]\n",
        "    transactions = [\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P01\", \"Usage_Metric\": 50.0},\n",
        "        {\"Customer_ID\": \"C001\", \"Product_ID\": \"P02\", \"Usage_Metric\": 60.0},\n",
        "        {\"Customer_ID\": \"C002\", \"Product_ID\": \"P01\", \"Usage_Metric\": 70.0}\n",
        "    ]\n",
        "\n",
        "    result = create_derived_features(customers, transactions, products)\n",
        "\n",
        "    assert \"customer_engagement\" in result\n",
        "    assert \"product_popularity\" in result\n",
        "    assert \"C001\" in result[\"customer_engagement\"]\n",
        "    assert \"P01\" in result[\"product_popularity\"]\n",
        "    assert \"engagement_score\" in result[\"customer_engagement\"][\"C001\"]\n",
        "    assert \"popularity_score\" in result[\"product_popularity\"][\"P01\"]\n",
        "\n",
        "\n",
        "def test_preprocess_products():\n",
        "    \"\"\"Test preprocessing products with Feature_Set parsing\"\"\"\n",
        "    products = [\n",
        "        {\"Product_ID\": \"P01\", \"Feature_Set\": \"A, B\"},\n",
        "        {\"Product_ID\": \"P02\", \"Feature_Set\": \"C\"}\n",
        "    ]\n",
        "\n",
        "    result = preprocess_products(products)\n",
        "\n",
        "    assert len(result) == 2\n",
        "    assert \"feature_list\" in result[0]\n",
        "    assert result[0][\"feature_list\"] == [\"A\", \"B\"]\n",
        "    assert result[1][\"feature_list\"] == [\"C\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "fsOPu6uqRucP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_data_preprocessing.py -v\n",
        "\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 13 items\n",
        "\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_single PASSED                                                                [  7%]\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_multiple PASSED                                                              [ 15%]\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_sorted PASSED                                                                [ 23%]\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_with_spaces PASSED                                                           [ 30%]\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_empty PASSED                                                                 [ 38%]\n",
        "tests/test_data_preprocessing.py::test_parse_feature_set_single_letter PASSED                                                         [ 46%]\n",
        "tests/test_data_preprocessing.py::test_normalize_usage_metrics PASSED                                                                 [ 53%]\n",
        "tests/test_data_preprocessing.py::test_normalize_usage_metrics_empty PASSED                                                           [ 61%]\n",
        "tests/test_data_preprocessing.py::test_build_customer_product_graph PASSED                                                            [ 69%]\n",
        "tests/test_data_preprocessing.py::test_build_product_cooccurrence_graph PASSED                                                        [ 76%]\n",
        "tests/test_data_preprocessing.py::test_build_customer_similarity_graph PASSED                                                         [ 84%]\n",
        "tests/test_data_preprocessing.py::test_create_derived_features PASSED                                                                 [ 92%]\n",
        "tests/test_data_preprocessing.py::test_preprocess_products PASSED                                                                     [100%]\n",
        "\n",
        "============================================================ 13 passed in 0.13s =============================================================\n"
      ],
      "metadata": {
        "id": "FJ4uWsKscuD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator % python3 -m pytest tests/test_nodes_phase3.py -v\n",
        "============================================================ test session starts ============================================================\n",
        "platform darwin -- Python 3.13.7, pytest-9.0.1, pluggy-1.6.0 -- /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator/.venv/bin/python3\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /Users/micahshull/Documents/AI_LangGraph/LG_Cursor_035_Product-CustomerFitDiscoveryOrchestrator\n",
        "plugins: langsmith-0.4.53, anyio-4.12.0, asyncio-1.3.0, cov-7.0.0\n",
        "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
        "collected 10 items\n",
        "\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_basic PASSED                                                                 [ 10%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_structure PASSED                                                             [ 20%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_parses_feature_sets PASSED                                                   [ 30%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_normalizes_usage PASSED                                                      [ 40%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_builds_graphs PASSED                                                         [ 50%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_creates_derived_features PASSED                                              [ 60%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_data_quality_report PASSED                                                   [ 70%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_requires_raw_data PASSED                                                     [ 80%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_preserves_errors PASSED                                                      [ 90%]\n",
        "tests/test_nodes_phase3.py::test_data_preprocessing_node_full_workflow PASSED                                                         [100%]\n",
        "\n",
        "============================================================ 10 passed in 0.47s =============================================================\n"
      ],
      "metadata": {
        "id": "PQZG3k9uc00D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Phase 3 complete â€” all tests passing\n",
        "\n",
        "### Summary\n",
        "\n",
        "**Phase 1: Foundation**\n",
        "- Goal Node âœ“\n",
        "- Planning Node âœ“\n",
        "- 7/7 tests passing\n",
        "\n",
        "**Phase 2: Data Loading**\n",
        "- Data Loading Utilities (3 functions) âœ“\n",
        "- Data Ingestion Node âœ“\n",
        "- 12 utility + 9 node = 21/21 tests passing\n",
        "\n",
        "**Phase 3: Data Preprocessing**\n",
        "- Preprocessing Utilities (7 functions) âœ“\n",
        "- Data Preprocessing Node âœ“\n",
        "- 13 utility + 10 node = 23/23 tests passing\n",
        "\n",
        "**Total progress:** 51/51 tests passing across all phases\n",
        "\n",
        "---\n",
        "\n",
        "### What we've built\n",
        "\n",
        "1. Complete data pipeline:\n",
        "   - Goal â†’ Planning â†’ Data Ingestion â†’ Data Preprocessing\n",
        "   - All nodes tested independently and in workflow\n",
        "\n",
        "2. Graph structures:\n",
        "   - Customer-Product bipartite graph\n",
        "   - Product co-occurrence graph\n",
        "   - Customer similarity graph\n",
        "\n",
        "3. Derived features:\n",
        "   - Customer engagement scores\n",
        "   - Product popularity scores\n",
        "   - Normalized usage metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Phase 4 â€” Clustering Agent\n",
        "\n",
        "This phase will:\n",
        "1. Build clustering utilities (K-means for customers and products)\n",
        "2. Analyze cluster characteristics\n",
        "3. Identify underserved segments\n",
        "4. Build clustering agent node\n",
        "\n"
      ],
      "metadata": {
        "id": "YO2AADasfB5l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcgU10AgfFN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}