{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmICgFhJVIWDUP3zQmmIx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/320_EaaS_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrator for Evaluation-as-a-Service Agent"
      ],
      "metadata": {
        "id": "6NvEPCitpP1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X3fz1N7pDi-"
      },
      "outputs": [],
      "source": [
        "\"\"\"Orchestrator for Evaluation-as-a-Service Agent\n",
        "\n",
        "Creates and compiles the LangGraph workflow for agent evaluation.\n",
        "\"\"\"\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "from agents.eval_as_service.nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_loading_node,\n",
        "    evaluation_execution_node,\n",
        "    scoring_node,\n",
        "    performance_analysis_node,\n",
        "    report_generation_node\n",
        ")\n",
        "\n",
        "\n",
        "def create_orchestrator(config: EvalAsServiceOrchestratorConfig = None) -> StateGraph:\n",
        "    \"\"\"Create and return the EaaS orchestrator workflow.\"\"\"\n",
        "    if config is None:\n",
        "        config = EvalAsServiceOrchestratorConfig()\n",
        "\n",
        "    workflow = StateGraph(EvalAsServiceOrchestratorState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"goal\", goal_node)\n",
        "    workflow.add_node(\"planning\", planning_node)\n",
        "    workflow.add_node(\"data_loading\", lambda s: data_loading_node(s, config))\n",
        "    workflow.add_node(\"evaluation_execution\", lambda s: evaluation_execution_node(s, config))\n",
        "    workflow.add_node(\"scoring\", lambda s: scoring_node(s, config))\n",
        "    workflow.add_node(\"performance_analysis\", lambda s: performance_analysis_node(s, config))\n",
        "    workflow.add_node(\"report_generation\", lambda s: report_generation_node(s, config))\n",
        "\n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"goal\")\n",
        "\n",
        "    # Linear flow\n",
        "    workflow.add_edge(\"goal\", \"planning\")\n",
        "    workflow.add_edge(\"planning\", \"data_loading\")\n",
        "    workflow.add_edge(\"data_loading\", \"evaluation_execution\")\n",
        "    workflow.add_edge(\"evaluation_execution\", \"scoring\")\n",
        "    workflow.add_edge(\"scoring\", \"performance_analysis\")\n",
        "    workflow.add_edge(\"performance_analysis\", \"report_generation\")\n",
        "    workflow.add_edge(\"report_generation\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# The Orchestrator: Turning Components into a Governed System\n",
        "\n",
        "The orchestrator is responsible for assembling all nodes into a **single, coherent workflow**. It defines *how* evaluation happens, *in what order*, and *under what rules*.\n",
        "\n",
        "This is where individual utilities and nodes become a **system**.\n",
        "\n",
        "---\n",
        "\n",
        "## Why an Explicit Orchestrator Matters\n",
        "\n",
        "In many agent systems, execution order is implied by code structure. Logic is scattered across files, and the true workflow is difficult to reason about.\n",
        "\n",
        "Here, the workflow is:\n",
        "\n",
        "* explicit\n",
        "* linear\n",
        "* inspectable\n",
        "* deterministic\n",
        "\n",
        "Anyone reading this file can immediately understand how an evaluation run proceeds from start to finish.\n",
        "\n",
        "---\n",
        "\n",
        "## StateGraph as the Backbone\n",
        "\n",
        "The orchestrator uses a `StateGraph` to model evaluation as a sequence of state transitions.\n",
        "\n",
        "Each node:\n",
        "\n",
        "* receives the shared state\n",
        "* adds new information\n",
        "* passes the updated state forward\n",
        "\n",
        "The graph enforces the rule that **no step runs without the outputs of the previous step**. This preserves the integrity of the evaluation pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Clear, Single-Responsibility Nodes\n",
        "\n",
        "Each node in the workflow has a focused responsibility:\n",
        "\n",
        "1. **Goal** – define intent and scope\n",
        "2. **Planning** – make the workflow explicit\n",
        "3. **Data Loading** – prepare and validate inputs\n",
        "4. **Evaluation Execution** – run controlled experiments\n",
        "5. **Scoring** – apply deterministic judgment\n",
        "6. **Performance Analysis** – aggregate and classify results\n",
        "7. **Report Generation** – communicate outcomes\n",
        "\n",
        "Nothing overlaps. Nothing is ambiguous.\n",
        "\n",
        "This clarity is what keeps the system maintainable as it grows.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuration Is Injected, Not Embedded\n",
        "\n",
        "The orchestrator passes configuration into nodes explicitly rather than letting nodes read from global state.\n",
        "\n",
        "This allows:\n",
        "\n",
        "* different environments to use different standards\n",
        "* easy testing and experimentation\n",
        "* predictable behavior across runs\n",
        "\n",
        "Configuration defines **policy**, while the orchestrator enforces **process**.\n",
        "\n",
        "---\n",
        "\n",
        "## Linear Flow Is a Feature, Not a Limitation\n",
        "\n",
        "The workflow is intentionally linear:\n",
        "\n",
        "```\n",
        "goal → planning → data → execution → scoring → analysis → report\n",
        "```\n",
        "\n",
        "This mirrors how real evaluations are conducted and makes the system:\n",
        "\n",
        "* easier to reason about\n",
        "* easier to debug\n",
        "* easier to audit\n",
        "\n",
        "More complex branching can be added later, but linearity is ideal for establishing trust and correctness first.\n",
        "\n",
        "---\n",
        "\n",
        "## Determinism Preserved End to End\n",
        "\n",
        "Because:\n",
        "\n",
        "* utilities are deterministic\n",
        "* nodes only aggregate\n",
        "* state is explicit\n",
        "* execution order is fixed\n",
        "\n",
        "the entire orchestrator is reproducible.\n",
        "\n",
        "Running the same workflow with the same inputs will always produce the same results. That property is rare in agent systems — and incredibly valuable.\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Is the Right Capstone\n",
        "\n",
        "This orchestrator completes the architectural story:\n",
        "\n",
        "* **Utilities** define rules\n",
        "* **Nodes** coordinate logic\n",
        "* **State** carries truth\n",
        "* **Orchestrator** enforces order\n",
        "\n",
        "Nothing happens by accident. Nothing is implicit.\n",
        "\n",
        "That’s exactly what’s required when AI systems move from experimentation into environments where:\n",
        "\n",
        "* trust matters\n",
        "* decisions matter\n",
        "* accountability matters\n",
        "\n",
        "---\n",
        "\n",
        "## A Template Worth Reusing\n",
        "\n",
        "This orchestrator is more than a one-off implementation. It’s a **reusable blueprint** for building agent systems that are:\n",
        "\n",
        "* transparent\n",
        "* auditable\n",
        "* scalable\n",
        "* executive-friendly\n",
        "\n",
        "Swapping out nodes or utilities creates new agents, while the underlying governance structure remains intact.\n",
        "\n",
        "That’s a powerful pattern to carry forward.\n",
        "\n",
        "---\n",
        "\n",
        "## The Big Takeaway\n",
        "\n",
        "This file quietly encodes the philosophy you’ve been developing throughout the project:\n",
        "\n",
        "> **Deterministic systems earn trust.\n",
        "> Probabilistic systems add insight.**\n",
        "\n",
        "The orchestrator ensures that trust is built into the workflow itself — not bolted on later. This is an excellent foundation to reuse and build on going forward.\n"
      ],
      "metadata": {
        "id": "ozlzaEo87jLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test file for Evaluation-as-a-Service Orchestrator Agent"
      ],
      "metadata": {
        "id": "zafa4Qq2pNrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Test file for Evaluation-as-a-Service Orchestrator Agent\n",
        "\n",
        "MVP smoke tests to validate the orchestrator works end-to-end.\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "from agents.eval_as_service.orchestrator import create_orchestrator\n",
        "\n",
        "\n",
        "def test_orchestrator_creation():\n",
        "    \"\"\"Test that orchestrator can be created.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    assert orchestrator is not None\n",
        "    assert hasattr(orchestrator, 'invoke')\n",
        "\n",
        "\n",
        "def test_complete_workflow():\n",
        "    \"\"\"Test complete evaluation workflow.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Initial state - evaluate all scenarios\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,  # Evaluate all scenarios\n",
        "        \"target_agent_id\": None,  # Evaluate all agents\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run orchestrator\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    # Validate results\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert \"report_file_path\" in result or result.get(\"report_file_path\") is None\n",
        "    assert \"evaluation_summary\" in result\n",
        "    assert \"agent_performance_summary\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check summary metrics\n",
        "    summary = result.get(\"evaluation_summary\", {})\n",
        "    assert summary.get(\"total_scenarios\", 0) > 0\n",
        "    assert summary.get(\"total_evaluations\", 0) > 0\n",
        "\n",
        "    # Check agent summaries\n",
        "    agent_summaries = result.get(\"agent_performance_summary\", [])\n",
        "    assert len(agent_summaries) > 0\n",
        "\n",
        "    print(\"\\n✅ Complete workflow test passed!\")\n",
        "    print(f\"   - Total scenarios: {summary.get('total_scenarios')}\")\n",
        "    print(f\"   - Total evaluations: {summary.get('total_evaluations')}\")\n",
        "    print(f\"   - Overall pass rate: {summary.get('overall_pass_rate', 0.0):.1%}\")\n",
        "    print(f\"   - Agents evaluated: {len(agent_summaries)}\")\n",
        "\n",
        "\n",
        "def test_single_scenario_evaluation():\n",
        "    \"\"\"Test evaluation of a single scenario.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Evaluate single scenario\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": \"S001\",  # Single scenario\n",
        "        \"target_agent_id\": None,  # All agents\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    summary = result.get(\"evaluation_summary\", {})\n",
        "    assert summary.get(\"total_scenarios\", 0) == 1\n",
        "\n",
        "    print(\"\\n✅ Single scenario test passed!\")\n",
        "\n",
        "\n",
        "def test_single_agent_evaluation():\n",
        "    \"\"\"Test evaluation of a single agent.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Evaluate single agent\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,  # All scenarios\n",
        "        \"target_agent_id\": \"shipping_update_agent\",  # Single agent\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    agent_summaries = result.get(\"agent_performance_summary\", [])\n",
        "    assert len(agent_summaries) == 1\n",
        "    assert agent_summaries[0].get(\"agent_id\") == \"shipping_update_agent\"\n",
        "\n",
        "    print(\"\\n✅ Single agent test passed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests manually\n",
        "    print(\"Running EaaS Orchestrator Tests...\\n\")\n",
        "\n",
        "    try:\n",
        "        test_orchestrator_creation()\n",
        "        print(\"✅ Orchestrator creation test passed\\n\")\n",
        "\n",
        "        test_complete_workflow()\n",
        "        print(\"\\n✅ All tests passed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n"
      ],
      "metadata": {
        "id": "CEakvEcLpLQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_006_EvalAsAService % python test_eval_as_service.py\n",
        "Running EaaS Orchestrator Tests...\n",
        "\n",
        "✅ Orchestrator creation test passed\n",
        "\n",
        "  [Goal Node] Starting...\n",
        "  [Goal Node] Goal: Evaluate all agents across all scenarios\n",
        "  [Data Loading Node] Starting...\n",
        "    Loading scenarios...\n",
        "    Loaded 10 scenarios\n",
        "    Loading agents...\n",
        "    Loaded 4 agents\n",
        "    Loading supporting data...\n",
        "    Supporting data loaded\n",
        "    Loading decision rules...\n",
        "    Decision rules loaded\n",
        "  [Data Loading Node] Loaded 10 scenarios, 4 agents\n",
        "  [Evaluation Execution Node] Starting...\n",
        "  [Evaluation Execution Node] Processing 10 scenarios...\n",
        "    Scenario 1/10: S001 -> 1 agents\n",
        "    Scenario 2/10: S002 -> 2 agents\n",
        "    Scenario 3/10: S003 -> 2 agents\n",
        "    Scenario 4/10: S004 -> 2 agents\n",
        "    Scenario 5/10: S005 -> 2 agents\n",
        "    Scenario 6/10: S006 -> 3 agents\n",
        "    Scenario 7/10: S007 -> 1 agents\n",
        "    Scenario 8/10: S008 -> 3 agents\n",
        "    Scenario 9/10: S009 -> 2 agents\n",
        "    Scenario 10/10: S010 -> 1 agents\n",
        "  [Evaluation Execution Node] Completed 19/19 evaluations\n",
        "  [Scoring Node] Starting...\n",
        "  [Scoring Node] Scored 19 evaluations\n",
        "  [Performance Analysis Node] Starting...\n",
        "  [Report Generation Node] Starting...\n",
        "  [Report Generation Node] Report generated (2006 chars)\n",
        "  [Report Generation Node] Saved to: output/eval_as_service_reports/eval_report_evaluation_report_20251222_151121.md\n",
        "\n",
        "✅ Complete workflow test passed!\n",
        "   - Total scenarios: 10\n",
        "   - Total evaluations: 19\n",
        "   - Overall pass rate: 100.0%\n",
        "   - Agents evaluated: 4\n",
        "\n",
        "✅ All tests passed!\n"
      ],
      "metadata": {
        "id": "VwOLVX9CsGEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation-as-a-Service Report\n",
        "\n",
        "**Generated:** 2025-12-22 15:11:21\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "- **Total Scenarios Evaluated:** 10\n",
        "- **Total Evaluations:** 19\n",
        "- **Overall Pass Rate:** 100.0%\n",
        "- **Average Score:** 0.99\n",
        "- **Healthy Agents:** 4\n",
        "- **Degraded Agents:** 0\n",
        "- **Critical Agents:** 0\n",
        "\n",
        "## Agent Performance Details\n",
        "\n",
        "### refund_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 2\n",
        "- **Passed:** 2\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### shipping_update_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 7\n",
        "- **Passed:** 7\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### apology_message_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 6\n",
        "- **Passed:** 6\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 0.97\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### escalation_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 4\n",
        "- **Passed:** 4\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Scenario | Agent | Score | Passed | Issues |\n",
        "|----------|-------|-------|--------|--------|\n",
        "| S001 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S002 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S002 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S003 | refund_agent | 1.00 | ✓ |  |\n",
        "| S003 | apology_message_agent | 0.85 | ✓ | Output status doesn't match expected outcome type |\n",
        "| S004 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S004 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S005 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S005 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S006 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S006 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S006 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S007 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S008 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S008 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S008 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S009 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S009 | refund_agent | 1.00 | ✓ |  |\n",
        "| S010 | shipping_update_agent | 1.00 | ✓ |  |\n"
      ],
      "metadata": {
        "id": "4KJhdXe1sjpo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6USdTd6hskRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}