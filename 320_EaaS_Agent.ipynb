{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv3pOFjlZ4rcDRQAqVNImS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/AI_Agents/blob/main/320_EaaS_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrator for Evaluation-as-a-Service Agent"
      ],
      "metadata": {
        "id": "6NvEPCitpP1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X3fz1N7pDi-"
      },
      "outputs": [],
      "source": [
        "\"\"\"Orchestrator for Evaluation-as-a-Service Agent\n",
        "\n",
        "Creates and compiles the LangGraph workflow for agent evaluation.\n",
        "\"\"\"\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "from agents.eval_as_service.nodes import (\n",
        "    goal_node,\n",
        "    planning_node,\n",
        "    data_loading_node,\n",
        "    evaluation_execution_node,\n",
        "    scoring_node,\n",
        "    performance_analysis_node,\n",
        "    report_generation_node\n",
        ")\n",
        "\n",
        "\n",
        "def create_orchestrator(config: EvalAsServiceOrchestratorConfig = None) -> StateGraph:\n",
        "    \"\"\"Create and return the EaaS orchestrator workflow.\"\"\"\n",
        "    if config is None:\n",
        "        config = EvalAsServiceOrchestratorConfig()\n",
        "\n",
        "    workflow = StateGraph(EvalAsServiceOrchestratorState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"goal\", goal_node)\n",
        "    workflow.add_node(\"planning\", planning_node)\n",
        "    workflow.add_node(\"data_loading\", lambda s: data_loading_node(s, config))\n",
        "    workflow.add_node(\"evaluation_execution\", lambda s: evaluation_execution_node(s, config))\n",
        "    workflow.add_node(\"scoring\", lambda s: scoring_node(s, config))\n",
        "    workflow.add_node(\"performance_analysis\", lambda s: performance_analysis_node(s, config))\n",
        "    workflow.add_node(\"report_generation\", lambda s: report_generation_node(s, config))\n",
        "\n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"goal\")\n",
        "\n",
        "    # Linear flow\n",
        "    workflow.add_edge(\"goal\", \"planning\")\n",
        "    workflow.add_edge(\"planning\", \"data_loading\")\n",
        "    workflow.add_edge(\"data_loading\", \"evaluation_execution\")\n",
        "    workflow.add_edge(\"evaluation_execution\", \"scoring\")\n",
        "    workflow.add_edge(\"scoring\", \"performance_analysis\")\n",
        "    workflow.add_edge(\"performance_analysis\", \"report_generation\")\n",
        "    workflow.add_edge(\"report_generation\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test file for Evaluation-as-a-Service Orchestrator Agent"
      ],
      "metadata": {
        "id": "zafa4Qq2pNrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Test file for Evaluation-as-a-Service Orchestrator Agent\n",
        "\n",
        "MVP smoke tests to validate the orchestrator works end-to-end.\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from config import EvalAsServiceOrchestratorState, EvalAsServiceOrchestratorConfig\n",
        "from agents.eval_as_service.orchestrator import create_orchestrator\n",
        "\n",
        "\n",
        "def test_orchestrator_creation():\n",
        "    \"\"\"Test that orchestrator can be created.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    assert orchestrator is not None\n",
        "    assert hasattr(orchestrator, 'invoke')\n",
        "\n",
        "\n",
        "def test_complete_workflow():\n",
        "    \"\"\"Test complete evaluation workflow.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Initial state - evaluate all scenarios\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,  # Evaluate all scenarios\n",
        "        \"target_agent_id\": None,  # Evaluate all agents\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    # Run orchestrator\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    # Validate results\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert \"report_file_path\" in result or result.get(\"report_file_path\") is None\n",
        "    assert \"evaluation_summary\" in result\n",
        "    assert \"agent_performance_summary\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    # Check summary metrics\n",
        "    summary = result.get(\"evaluation_summary\", {})\n",
        "    assert summary.get(\"total_scenarios\", 0) > 0\n",
        "    assert summary.get(\"total_evaluations\", 0) > 0\n",
        "\n",
        "    # Check agent summaries\n",
        "    agent_summaries = result.get(\"agent_performance_summary\", [])\n",
        "    assert len(agent_summaries) > 0\n",
        "\n",
        "    print(\"\\n✅ Complete workflow test passed!\")\n",
        "    print(f\"   - Total scenarios: {summary.get('total_scenarios')}\")\n",
        "    print(f\"   - Total evaluations: {summary.get('total_evaluations')}\")\n",
        "    print(f\"   - Overall pass rate: {summary.get('overall_pass_rate', 0.0):.1%}\")\n",
        "    print(f\"   - Agents evaluated: {len(agent_summaries)}\")\n",
        "\n",
        "\n",
        "def test_single_scenario_evaluation():\n",
        "    \"\"\"Test evaluation of a single scenario.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Evaluate single scenario\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": \"S001\",  # Single scenario\n",
        "        \"target_agent_id\": None,  # All agents\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    summary = result.get(\"evaluation_summary\", {})\n",
        "    assert summary.get(\"total_scenarios\", 0) == 1\n",
        "\n",
        "    print(\"\\n✅ Single scenario test passed!\")\n",
        "\n",
        "\n",
        "def test_single_agent_evaluation():\n",
        "    \"\"\"Test evaluation of a single agent.\"\"\"\n",
        "    config = EvalAsServiceOrchestratorConfig()\n",
        "    orchestrator = create_orchestrator(config)\n",
        "\n",
        "    # Evaluate single agent\n",
        "    initial_state: EvalAsServiceOrchestratorState = {\n",
        "        \"scenario_id\": None,  # All scenarios\n",
        "        \"target_agent_id\": \"shipping_update_agent\",  # Single agent\n",
        "        \"errors\": []\n",
        "    }\n",
        "\n",
        "    result = orchestrator.invoke(initial_state)\n",
        "\n",
        "    assert \"evaluation_report\" in result\n",
        "    assert len(result.get(\"errors\", [])) == 0\n",
        "\n",
        "    agent_summaries = result.get(\"agent_performance_summary\", [])\n",
        "    assert len(agent_summaries) == 1\n",
        "    assert agent_summaries[0].get(\"agent_id\") == \"shipping_update_agent\"\n",
        "\n",
        "    print(\"\\n✅ Single agent test passed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests manually\n",
        "    print(\"Running EaaS Orchestrator Tests...\\n\")\n",
        "\n",
        "    try:\n",
        "        test_orchestrator_creation()\n",
        "        print(\"✅ Orchestrator creation test passed\\n\")\n",
        "\n",
        "        test_complete_workflow()\n",
        "        print(\"\\n✅ All tests passed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n"
      ],
      "metadata": {
        "id": "CEakvEcLpLQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(.venv) micahshull@Micahs-iMac AI_AGENTS_006_EvalAsAService % python test_eval_as_service.py\n",
        "Running EaaS Orchestrator Tests...\n",
        "\n",
        "✅ Orchestrator creation test passed\n",
        "\n",
        "  [Goal Node] Starting...\n",
        "  [Goal Node] Goal: Evaluate all agents across all scenarios\n",
        "  [Data Loading Node] Starting...\n",
        "    Loading scenarios...\n",
        "    Loaded 10 scenarios\n",
        "    Loading agents...\n",
        "    Loaded 4 agents\n",
        "    Loading supporting data...\n",
        "    Supporting data loaded\n",
        "    Loading decision rules...\n",
        "    Decision rules loaded\n",
        "  [Data Loading Node] Loaded 10 scenarios, 4 agents\n",
        "  [Evaluation Execution Node] Starting...\n",
        "  [Evaluation Execution Node] Processing 10 scenarios...\n",
        "    Scenario 1/10: S001 -> 1 agents\n",
        "    Scenario 2/10: S002 -> 2 agents\n",
        "    Scenario 3/10: S003 -> 2 agents\n",
        "    Scenario 4/10: S004 -> 2 agents\n",
        "    Scenario 5/10: S005 -> 2 agents\n",
        "    Scenario 6/10: S006 -> 3 agents\n",
        "    Scenario 7/10: S007 -> 1 agents\n",
        "    Scenario 8/10: S008 -> 3 agents\n",
        "    Scenario 9/10: S009 -> 2 agents\n",
        "    Scenario 10/10: S010 -> 1 agents\n",
        "  [Evaluation Execution Node] Completed 19/19 evaluations\n",
        "  [Scoring Node] Starting...\n",
        "  [Scoring Node] Scored 19 evaluations\n",
        "  [Performance Analysis Node] Starting...\n",
        "  [Report Generation Node] Starting...\n",
        "  [Report Generation Node] Report generated (2006 chars)\n",
        "  [Report Generation Node] Saved to: output/eval_as_service_reports/eval_report_evaluation_report_20251222_151121.md\n",
        "\n",
        "✅ Complete workflow test passed!\n",
        "   - Total scenarios: 10\n",
        "   - Total evaluations: 19\n",
        "   - Overall pass rate: 100.0%\n",
        "   - Agents evaluated: 4\n",
        "\n",
        "✅ All tests passed!\n"
      ],
      "metadata": {
        "id": "VwOLVX9CsGEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation-as-a-Service Report\n",
        "\n",
        "**Generated:** 2025-12-22 15:11:21\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "- **Total Scenarios Evaluated:** 10\n",
        "- **Total Evaluations:** 19\n",
        "- **Overall Pass Rate:** 100.0%\n",
        "- **Average Score:** 0.99\n",
        "- **Healthy Agents:** 4\n",
        "- **Degraded Agents:** 0\n",
        "- **Critical Agents:** 0\n",
        "\n",
        "## Agent Performance Details\n",
        "\n",
        "### refund_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 2\n",
        "- **Passed:** 2\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### shipping_update_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 7\n",
        "- **Passed:** 7\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### apology_message_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 6\n",
        "- **Passed:** 6\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 0.97\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "### escalation_agent\n",
        "\n",
        "- **Status:** healthy\n",
        "- **Total Evaluations:** 4\n",
        "- **Passed:** 4\n",
        "- **Failed:** 0\n",
        "- **Average Score:** 1.00\n",
        "- **Average Response Time:** 0.00s\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Scenario | Agent | Score | Passed | Issues |\n",
        "|----------|-------|-------|--------|--------|\n",
        "| S001 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S002 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S002 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S003 | refund_agent | 1.00 | ✓ |  |\n",
        "| S003 | apology_message_agent | 0.85 | ✓ | Output status doesn't match expected outcome type |\n",
        "| S004 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S004 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S005 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S005 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S006 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S006 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S006 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S007 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S008 | shipping_update_agent | 1.00 | ✓ |  |\n",
        "| S008 | apology_message_agent | 1.00 | ✓ |  |\n",
        "| S008 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S009 | escalation_agent | 1.00 | ✓ |  |\n",
        "| S009 | refund_agent | 1.00 | ✓ |  |\n",
        "| S010 | shipping_update_agent | 1.00 | ✓ |  |\n"
      ],
      "metadata": {
        "id": "4KJhdXe1sjpo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6USdTd6hskRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}